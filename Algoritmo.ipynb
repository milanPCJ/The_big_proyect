{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de red convolucional para la clasificación detección temprana de supernovas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se abre la data pre-procesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys de pkl dict_keys(['Train', 'Validation', 'Test'])\n",
      "keys de Test dict_keys(['images', 'labels', 'features'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para cargar el contenido del archivo .pkl\n",
    "def load_pickle_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "# Cargar el archivo\n",
    "filename = 'processed_data.pkl' \n",
    "data_procesada = load_pickle_data(filename)\n",
    "\n",
    "# Visualizar el contenido\n",
    "print('keys de pkl', data_procesada.keys())\n",
    "print('keys de Test',data_procesada['Validation'].keys())\n",
    "data_procesada['Validation']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipo de dato de Train: <built-in method type of Tensor object at 0x0000011F9ABC6160>\n",
      "forma de Train: torch.Size([72710, 3, 21, 21])\n",
      "forma de Validation: torch.Size([500, 3, 21, 21])\n",
      "forma de Test: torch.Size([500, 3, 21, 21])\n"
     ]
    }
   ],
   "source": [
    "def preparar_imagenes_para_modelo(data_dict, key_principal='Train', key_imagenes='images'):\n",
    "    # Extraer imágenes del diccionario\n",
    "    imagenes = data_dict[key_principal][key_imagenes]\n",
    "    \n",
    "    # Cambiar el orden de las dimensiones a [canales, altura, ancho]\n",
    "    imagenes = np.transpose(imagenes, (0, 3, 1, 2))\n",
    "    \n",
    "    # Convertir a tensor de PyTorch\n",
    "    imagenes_tensor = torch.tensor(imagenes, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    return imagenes_tensor\n",
    "\n",
    "# Preparar las imágenes para el modelo\n",
    "Train = preparar_imagenes_para_modelo(data_procesada)\n",
    "Val = preparar_imagenes_para_modelo(data_procesada, key_principal='Validation')\n",
    "Test = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "\n",
    "print('tipo de dato de Train:', Train.type)\n",
    "print('forma de Train:', Train.shape)\n",
    "print('forma de Validation:', Val.shape)\n",
    "print('forma de Test:', Test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_etiquetas(data_dict, key_principal='Train', key_etiquetas='labels'):\n",
    "    # Extraer etiquetas del diccionario\n",
    "    etiquetas = data_dict[key_principal][key_etiquetas]\n",
    "    # Convertir a tensor de PyTorch\n",
    "    etiquetas_tensor = torch.tensor(etiquetas, dtype=torch.long)  # Usamos dtype long porque son índices de clase\n",
    "    return etiquetas_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forma de metadata_train: torch.Size([72710, 26])\n",
      "forma de metadata_val: torch.Size([500, 26])\n",
      "forma de metadata_test: torch.Size([500, 26])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7788, -0.6295,  0.0887,  0.4651,  1.1523,  0.0935,  2.1225, -0.8817,\n",
       "         1.1350,  1.1945,  0.2198,  0.3951,  1.1309,  0.9652, -0.6643, -0.0798,\n",
       "        -2.0312, -0.2051, -0.1635,  0.5879,  0.2048,  1.6357, -0.8917, -0.1424,\n",
       "        -0.2300, -0.1352])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extraer_metadata(data_dict, key_principal='Train', key_metadata='features'):\n",
    "    # Extraer características de metadatos del diccionario\n",
    "    metadata = data_dict[key_principal][key_metadata]\n",
    "    \n",
    "    # Convertir explícitamente a float32\n",
    "    metadata = metadata.astype(np.float32)\n",
    "    \n",
    "    # Convertir a tensor de PyTorch\n",
    "    metadata_tensor = torch.tensor(metadata, dtype=torch.float32)\n",
    "    \n",
    "    return metadata_tensor\n",
    "\n",
    "\n",
    "# Extraer características de metadatos para los conjuntos de datos\n",
    "metadata_train = extraer_metadata(data_procesada)\n",
    "metadata_val = extraer_metadata(data_procesada, key_principal='Validation')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "print('forma de metadata_train:', metadata_train.shape)\n",
    "print('forma de metadata_val:', metadata_val.shape)\n",
    "print('forma de metadata_test:', metadata_test.shape)\n",
    "\n",
    "metadata_val[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo convolucional, incorporando invariancia rotacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, dropout_p, num_classes=5):\n",
    "        super().__init__()\n",
    "        # Zero padding para evitar perder datos de las imágenes después de las convoluciones\n",
    "        self.padding = nn.ZeroPad2d(3)\n",
    "\n",
    "        # Bloques de convolución\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def rotate_input(self, x):\n",
    "        # Rotaciones para la invariancia \n",
    "        x_90 = torch.rot90(x, 1, [2, 3])\n",
    "        x_180 = torch.rot90(x, 2, [2, 3])\n",
    "        x_270 = torch.rot90(x, 3, [2, 3])\n",
    "        return torch.cat([x, x_90, x_180, x_270], dim=0)\n",
    "\n",
    "    def cyclic_pooling(self, x):\n",
    "        B = x.size(0) // 4\n",
    "        return (x[:B] + x[B:2*B] + x[2*B:3*B] + x[3*B:]) / 4.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Zero padding\n",
    "        x = self.padding(x)\n",
    "        \n",
    "        # Rotación de la entrada\n",
    "        x_rotated = self.rotate_input(x)\n",
    "        \n",
    "        # Pasar por bloques convolucionales\n",
    "        x_rotated = self.conv_blocks(x_rotated)\n",
    "        \n",
    "        # Pooling Cíclico\n",
    "        x_pooled = self.cyclic_pooling(x_rotated)\n",
    "        \n",
    "        # Pasar por MLP\n",
    "        out = self.mlp(x_pooled)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de entrenamiento y visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_curves(curves):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    epochs = np.arange(len(curves[\"val_loss\"])) + 1\n",
    "\n",
    "    ax[0].plot(epochs, curves['val_loss'], label='validation')\n",
    "    ax[0].plot(epochs, curves['train_loss'], label='training')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Loss evolution during training')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(epochs, curves['val_acc'], label='validation')\n",
    "    ax[1].plot(epochs, curves['train_acc'], label='training')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Accuracy evolution during training')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu):\n",
    "    # Predicción\n",
    "    y_predicted = model(x_batch)\n",
    "\n",
    "    # Cálculo de loss\n",
    "    loss = criterion(y_predicted, y_batch)\n",
    "\n",
    "    # Actualización de parámetros\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return y_predicted, loss\n",
    "\n",
    "\n",
    "def evaluate(val_loader, model, criterion, use_gpu):\n",
    "    cumulative_loss = 0\n",
    "    cumulative_predictions = 0\n",
    "    data_count = 0\n",
    "\n",
    "    for x_val, y_val in val_loader:\n",
    "        if use_gpu:\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "\n",
    "        y_predicted = model(x_val)\n",
    "        \n",
    "        loss = criterion(y_predicted, y_val)\n",
    "\n",
    "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "\n",
    "        cumulative_predictions += (y_val == class_prediction).sum().item()\n",
    "        cumulative_loss += loss.item()\n",
    "        data_count += y_val.shape[0]\n",
    "\n",
    "    val_acc = cumulative_predictions / data_count\n",
    "    val_loss = cumulative_loss / len(val_loader)\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_data_images,\n",
    "    train_data_labels,\n",
    "    val_data_images,\n",
    "    val_data_labels,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu,\n",
    "    patience=5\n",
    "):\n",
    "\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # Definición de dataloader\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data_images, train_data_labels)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_data_images, val_data_labels)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_data_images), shuffle=False, pin_memory=use_gpu)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    curves = {\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
    "        cumulative_train_loss = 0\n",
    "        cumulative_train_corrects = 0\n",
    "        train_loss_count = 0\n",
    "        train_acc_count = 0\n",
    "\n",
    "        # Entrenamiento del modelo\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            (x_batch, y_batch) = data  \n",
    "            if use_gpu:\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "\n",
    "            y_predicted, loss = train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu)\n",
    "\n",
    "            cumulative_train_loss += loss.item()\n",
    "            train_loss_count += 1\n",
    "            train_acc_count += y_batch.shape[0]\n",
    "\n",
    "            # Calculamos número de aciertos\n",
    "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
    "\n",
    "            if (i + 1) % 117 == 0:\n",
    "                print(f\"Iteration {i + 1} - Batch {i + 1}/{len(train_loader)} - Train loss: {cumulative_train_loss / train_loss_count}, Train acc: {cumulative_train_corrects / train_acc_count}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc, val_loss = evaluate(val_loader, model, criterion, use_gpu)\n",
    "\n",
    "        print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
    "\n",
    "        train_loss = cumulative_train_loss / train_loss_count\n",
    "        train_acc = cumulative_train_corrects / train_acc_count\n",
    "\n",
    "        curves[\"train_acc\"].append(train_acc)\n",
    "        curves[\"val_acc\"].append(val_acc)\n",
    "        curves[\"train_loss\"].append(train_loss)\n",
    "        curves[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # Lógica de early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement == patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement after {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Tiempo total de entrenamiento: {time.perf_counter() - t0:.4f} [s]\")\n",
    "    model.cpu()\n",
    "\n",
    "    return curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de hiperparámetros con funciones de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir las imágenes y metadatos en conjuntos de entrenamiento y validación\n",
    "Train_images = preparar_imagenes_para_modelo(data_procesada)\n",
    "Val_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Validation')\n",
    "Train_labels = extraer_etiquetas(data_procesada)\n",
    "Val_labels = extraer_etiquetas(data_procesada, key_principal='Validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 1.6098175374870627, Train acc: 0.202991452991453\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.60683015269092, Train acc: 0.21955128205128205\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.592263707747826, Train acc: 0.24465811965811965\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.5663935323046823, Train acc: 0.2765758547008547\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.532999097587716, Train acc: 0.30833333333333335\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 1.5004800415446615, Train acc: 0.333244301994302\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 1.4662965285035716, Train acc: 0.35752442002442003\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 1.4375426694113984, Train acc: 0.37580128205128205\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 1.4110672029561353, Train acc: 0.3922127255460589\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 1.3863321759252467, Train acc: 0.4083333333333333\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 1.364488520822325, Train acc: 0.4216200466200466\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 1.3453681296602613, Train acc: 0.4322916666666667\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 1.32934868743277, Train acc: 0.44165023011176857\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 1.3104099910410623, Train acc: 0.4526862026862027\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 1.2973901682429843, Train acc: 0.46068376068376066\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 1.2831804447640212, Train acc: 0.46814903846153844\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 1.2712640235806423, Train acc: 0.47473604826546\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 1.2566627378364121, Train acc: 0.48228276353276356\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 1.2452306510948459, Train acc: 0.4877136752136752\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 1.2355054552865843, Train acc: 0.4929754273504274\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 1.22497981208148, Train acc: 0.4985754985754986\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 1.2186354165194844, Train acc: 0.5033022533022533\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 1.2090332163771216, Train acc: 0.5085470085470085\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 1.2012175840466282, Train acc: 0.5128205128205128\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 1.1937455281131288, Train acc: 0.5168376068376068\n",
      "Val loss: 0.86414635181427, Val acc: 0.652\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.9886860236143454, Train acc: 0.6340811965811965\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.9783424978327547, Train acc: 0.6225961538461539\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.9908744579876251, Train acc: 0.6191239316239316\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.9959039552471577, Train acc: 0.6132478632478633\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.9915023168437501, Train acc: 0.6134615384615385\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.9862521230070679, Train acc: 0.6151175213675214\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.977651405902136, Train acc: 0.6176739926739927\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.9748433239949055, Train acc: 0.6183226495726496\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.9746217602281941, Train acc: 0.6178774928774928\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.9709226159713207, Train acc: 0.6191239316239316\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.9725139506259628, Train acc: 0.6200951825951826\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.9696601870392803, Train acc: 0.6215722934472935\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.965781551395726, Train acc: 0.6240959894806049\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.9640427690285903, Train acc: 0.6251144688644689\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.9614205943383383, Train acc: 0.6258903133903134\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.9582329405646803, Train acc: 0.6277043269230769\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.9567008863267376, Train acc: 0.6276395173453997\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.954906219289287, Train acc: 0.6284425451092118\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.9529646941792895, Train acc: 0.6292735042735043\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.949961117381214, Train acc: 0.6308760683760684\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.947958763714578, Train acc: 0.6323005698005698\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.94693398650478, Train acc: 0.6322358197358198\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.9449106806474563, Train acc: 0.633221850613155\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.9434012130773135, Train acc: 0.6340589387464387\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.9410477198902358, Train acc: 0.6346581196581197\n",
      "Val loss: 0.7894039154052734, Val acc: 0.694\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.862391649148403, Train acc: 0.6730769230769231\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.8723352156643175, Train acc: 0.6669337606837606\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.8713933389920455, Train acc: 0.6691595441595442\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.8644140628922699, Train acc: 0.671073717948718\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.8603808775926247, Train acc: 0.6672008547008547\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.8637519436344462, Train acc: 0.6644408831908832\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.8654438029424321, Train acc: 0.6648351648351648\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.8675504680245351, Train acc: 0.6628605769230769\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.8700898042318375, Train acc: 0.6623338081671415\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.8673704338634116, Train acc: 0.663034188034188\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.869434055076983, Train acc: 0.663510101010101\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.8696889410331379, Train acc: 0.6642628205128205\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.8666390088300499, Train acc: 0.665844838921762\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.866580311049763, Train acc: 0.6653693528693528\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.8678905086809414, Train acc: 0.6640669515669516\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.8667058212220924, Train acc: 0.6650641025641025\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.8634855701822681, Train acc: 0.6668552036199095\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.8616908103482336, Train acc: 0.6680614909781576\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.8585051060446843, Train acc: 0.6694219523166891\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.8575758468136828, Train acc: 0.6709401709401709\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.8568203378412653, Train acc: 0.6717287342287342\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.855814340509179, Train acc: 0.6717414529914529\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.8528726670277779, Train acc: 0.6734253065774805\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.8507047886734674, Train acc: 0.6749910968660968\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.8484363301391276, Train acc: 0.6763461538461538\n",
      "Val loss: 0.7178488969802856, Val acc: 0.712\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.8404699520677583, Train acc: 0.6826923076923077\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.8359071470988102, Train acc: 0.6840277777777778\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.8160320745404289, Train acc: 0.6908831908831908\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.8149481120272579, Train acc: 0.6947115384615384\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.8110339806630061, Train acc: 0.6949786324786325\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.8160223342754223, Train acc: 0.6940883190883191\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.8101020222836799, Train acc: 0.6961996336996337\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.8101781711428084, Train acc: 0.6965811965811965\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.8062259598731542, Train acc: 0.6977089268755935\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.8052470749005293, Train acc: 0.6986645299145299\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.8019032434566872, Train acc: 0.6995920745920746\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7995273726448374, Train acc: 0.7002314814814815\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.798617824765117, Train acc: 0.7006492439184747\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.7975151341258387, Train acc: 0.7004349816849816\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.7953048317350893, Train acc: 0.7015669515669516\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.7950831732402245, Train acc: 0.7021233974358975\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.7935161580986686, Train acc: 0.7024572649572649\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.7919904854138013, Train acc: 0.7034959639126306\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.7897800513607288, Train acc: 0.7041722896986055\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.7888146730442332, Train acc: 0.7053151709401709\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.7878027331140112, Train acc: 0.7057387057387058\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.7862981953126289, Train acc: 0.706269425019425\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.7859694370768053, Train acc: 0.7062662578966927\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.7840976304350755, Train acc: 0.7071536680911681\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7831824002918015, Train acc: 0.7071794871794872\n",
      "Val loss: 0.6905331611633301, Val acc: 0.726\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7686634802410746, Train acc: 0.7045940170940171\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.7528870785847689, Train acc: 0.7155448717948718\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.7486559165339185, Train acc: 0.7209757834757835\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.7520056860441835, Train acc: 0.7198183760683761\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7613038564849104, Train acc: 0.7163461538461539\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.7584035009824992, Train acc: 0.7159009971509972\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.7550771089045556, Train acc: 0.7174908424908425\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.7510531845574195, Train acc: 0.718215811965812\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.7503696305346511, Train acc: 0.7187796771130105\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.7510066784090466, Train acc: 0.7194978632478632\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.7516704770883295, Train acc: 0.7186771561771562\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7483696265310643, Train acc: 0.7203080484330484\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.7462615009634844, Train acc: 0.7211949375410914\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.7410666810505556, Train acc: 0.7231761294261294\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.7419205701588905, Train acc: 0.7230413105413105\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.7393530602765899, Train acc: 0.7235243055555556\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.7374638832052969, Train acc: 0.724201860231272\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.7370415660362525, Train acc: 0.7241512345679012\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.7374506060183182, Train acc: 0.7243870895186685\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.7370582666980405, Train acc: 0.7239850427350427\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.7378052250794099, Train acc: 0.7235958485958486\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.7359600634644823, Train acc: 0.7248445998445998\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.7360715630361467, Train acc: 0.7254505759940543\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.7346367572097272, Train acc: 0.7258502492877493\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7334625134610723, Train acc: 0.7259401709401709\n",
      "Val loss: 0.6439718008041382, Val acc: 0.77\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6860258253211649, Train acc: 0.7467948717948718\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6864745793943732, Train acc: 0.7441239316239316\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.7038036098337581, Train acc: 0.7405626780626781\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.7013259933162959, Train acc: 0.7409188034188035\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7010510037597428, Train acc: 0.7431623931623932\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6965141906442806, Train acc: 0.7444800569800569\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.7018378660428509, Train acc: 0.7418345543345544\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6996384719943899, Train acc: 0.741653311965812\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.7002684697862591, Train acc: 0.7403846153846154\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.7012975625502758, Train acc: 0.7412393162393163\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6996873703777281, Train acc: 0.7413073038073038\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7003272038876501, Train acc: 0.7410078347578347\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6996886111222781, Train acc: 0.7404257067718606\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6985655988183359, Train acc: 0.7403083028083028\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6965979830067381, Train acc: 0.7402065527065527\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6975456765876749, Train acc: 0.7400841346153846\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6953009693256391, Train acc: 0.7413273001508296\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6965430659460433, Train acc: 0.7413936372269706\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6955583532731704, Train acc: 0.7418747188484031\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6951039083620422, Train acc: 0.7413461538461539\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6952170722723299, Train acc: 0.7410714285714286\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6959799232298585, Train acc: 0.7408702408702409\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6949726725678956, Train acc: 0.741243961352657\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6926723212491922, Train acc: 0.7421652421652422\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.691598858058962, Train acc: 0.742948717948718\n",
      "Val loss: 0.6025404930114746, Val acc: 0.766\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7030783769411918, Train acc: 0.7414529914529915\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6872572499590043, Train acc: 0.7513354700854701\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6869631735263048, Train acc: 0.7492877492877493\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6847667537756965, Train acc: 0.7479967948717948\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6897142803312367, Train acc: 0.7452991452991453\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6850696901055823, Train acc: 0.7459935897435898\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6772490439615844, Train acc: 0.7506868131868132\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6776419693651872, Train acc: 0.7495993589743589\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6757752109400341, Train acc: 0.7497625830959165\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6763405285075179, Train acc: 0.7500534188034188\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6765003765889074, Train acc: 0.7501942501942502\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6752403248791341, Train acc: 0.7512464387464387\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6760363100705219, Train acc: 0.7507807363576594\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6739672076716673, Train acc: 0.7524801587301587\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6726223876843086, Train acc: 0.7539173789173789\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6737036725235546, Train acc: 0.7530048076923077\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6732663533806261, Train acc: 0.7534879336349924\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6721497100812418, Train acc: 0.754392212725546\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6698125198892361, Train acc: 0.7551731893837157\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6694698486700017, Train acc: 0.7549145299145299\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6672864625693629, Train acc: 0.7557234432234432\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6668058439232438, Train acc: 0.7563131313131313\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6663962088245656, Train acc: 0.7562941285767373\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.664900662738331, Train acc: 0.7567218660968661\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6644424983935479, Train acc: 0.7571581196581196\n",
      "Val loss: 0.5903509259223938, Val acc: 0.774\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6401436321246319, Train acc: 0.7697649572649573\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6478839289938283, Train acc: 0.7660256410256411\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6582336435970079, Train acc: 0.7587250712250713\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6565457308330597, Train acc: 0.7586805555555556\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6479592448625809, Train acc: 0.7619658119658119\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.647691706455501, Train acc: 0.7616631054131054\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.640364018515644, Train acc: 0.7637362637362637\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.636448350122087, Train acc: 0.7640892094017094\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6365232734259038, Train acc: 0.7659069325735992\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6387630604780637, Train acc: 0.7654380341880341\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6404056425648507, Train acc: 0.7638403263403264\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6420337709457616, Train acc: 0.7631766381766382\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6409229340716305, Train acc: 0.7643819855358317\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6425105650091637, Train acc: 0.7640796703296703\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6409260176950031, Train acc: 0.7651709401709401\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6388568695491323, Train acc: 0.7659922542735043\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6403176434761378, Train acc: 0.7652086475615887\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6394378845133276, Train acc: 0.7653727445394112\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6390137634115663, Train acc: 0.766278677462888\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.638855940864509, Train acc: 0.7661324786324787\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.639197749565979, Train acc: 0.7659493284493285\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6381051495480232, Train acc: 0.7669483294483295\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6368437033826336, Train acc: 0.7670940170940171\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6367724825710356, Train acc: 0.7668491809116809\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6365525680194554, Train acc: 0.7672222222222222\n",
      "Val loss: 0.5599467754364014, Val acc: 0.788\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5958213694075234, Train acc: 0.780448717948718\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5995079033777245, Train acc: 0.7791132478632479\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.617149986135654, Train acc: 0.7727920227920227\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6149234062968156, Train acc: 0.7716346153846154\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6198273749687733, Train acc: 0.771474358974359\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6187895399934885, Train acc: 0.7756410256410257\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.617166437618025, Train acc: 0.7764041514041514\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.613311120428336, Train acc: 0.7785122863247863\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6160635645912583, Train acc: 0.7774810066476733\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6154080206257665, Train acc: 0.7761752136752137\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6125657858895811, Train acc: 0.7776806526806527\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6152757989651287, Train acc: 0.7771545584045584\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6159563603799332, Train acc: 0.7772435897435898\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6153123097014369, Train acc: 0.7777396214896215\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6159114830599212, Train acc: 0.7778133903133904\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6156179671072297, Train acc: 0.7777110042735043\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.615526579728733, Train acc: 0.7778406234288587\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6158744305485447, Train acc: 0.7775403608736942\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6168756533047126, Train acc: 0.7774685110211426\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6178091598435854, Train acc: 0.7770566239316239\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6170179050205004, Train acc: 0.7772944647944648\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6148035929892168, Train acc: 0.7779963092463092\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6150610787535813, Train acc: 0.7774061687105165\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6148260720498712, Train acc: 0.7769542378917379\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6133544400614551, Train acc: 0.7774358974358975\n",
      "Val loss: 0.5656455159187317, Val acc: 0.776\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6114154873240707, Train acc: 0.7868589743589743\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6192500861282022, Train acc: 0.7769764957264957\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6189064655208859, Train acc: 0.7788461538461539\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6224048363092618, Train acc: 0.7765758547008547\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.62454140323859, Train acc: 0.7766025641025641\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6193593857422514, Train acc: 0.7778668091168092\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6150101926154997, Train acc: 0.7801434676434676\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6135850784838455, Train acc: 0.7801148504273504\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6118121138240538, Train acc: 0.7802706552706553\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.610817079551709, Train acc: 0.7803952991452991\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6083175832088733, Train acc: 0.780448717948718\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6056191157284285, Train acc: 0.7817396723646723\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.602442716235475, Train acc: 0.7827909270216963\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6027348946702641, Train acc: 0.7826236263736264\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6009798046190854, Train acc: 0.7834757834757835\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5975444040730851, Train acc: 0.7839877136752137\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5974406628467259, Train acc: 0.7842194570135747\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5975431831304164, Train acc: 0.783980294396961\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.59677176163425, Train acc: 0.7842723796671165\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5958703339099884, Train acc: 0.784241452991453\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.594426622618055, Train acc: 0.7845950345950345\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5947037832963439, Train acc: 0.7848436285936286\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5937365442625168, Train acc: 0.7846060943887031\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5930446923851033, Train acc: 0.7847889957264957\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5926129372456135, Train acc: 0.785042735042735\n",
      "Val loss: 0.5440328121185303, Val acc: 0.784\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5591599901771953, Train acc: 0.7975427350427351\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5743176295843899, Train acc: 0.7970085470085471\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5758531264684819, Train acc: 0.7938034188034188\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.575878702613533, Train acc: 0.7940705128205128\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5780643359208718, Train acc: 0.791559829059829\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5781882920029157, Train acc: 0.7897970085470085\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5779380243048708, Train acc: 0.7886904761904762\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5770295717331588, Train acc: 0.788795405982906\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5743844374264294, Train acc: 0.7897079772079773\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5753346907277392, Train acc: 0.7908119658119658\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5759233702104408, Train acc: 0.7919094794094794\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5771804078564345, Train acc: 0.79255698005698\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5777130795399035, Train acc: 0.7924884944115713\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5788069430816013, Train acc: 0.7919337606837606\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5803794736621047, Train acc: 0.7921296296296296\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5783007655006188, Train acc: 0.7925347222222222\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5797933345092128, Train acc: 0.7909439416792358\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.57976869082632, Train acc: 0.7908653846153846\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5785876277581132, Train acc: 0.7913292847503374\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5776091837411762, Train acc: 0.7916399572649573\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5762598648770437, Train acc: 0.7922517297517297\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5765052167952014, Train acc: 0.7924193861693861\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5758560717449185, Train acc: 0.7922473058342624\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5760542860578353, Train acc: 0.7921118233618234\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5759512535170612, Train acc: 0.7923931623931624\n",
      "Val loss: 0.5310506224632263, Val acc: 0.806\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5285105793776675, Train acc: 0.8082264957264957\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5413716178482924, Train acc: 0.8060897435897436\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5565680728806051, Train acc: 0.8009259259259259\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5564418664186174, Train acc: 0.8006143162393162\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5631390045723345, Train acc: 0.7961538461538461\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5676205226265297, Train acc: 0.7950498575498576\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5676584288987339, Train acc: 0.7947191697191697\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.568870193643384, Train acc: 0.7950053418803419\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.566637752452071, Train acc: 0.7946937321937322\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5666650962052692, Train acc: 0.7947115384615384\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5663126337558332, Train acc: 0.7943376068376068\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5645188514801257, Train acc: 0.7950053418803419\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.564405875342955, Train acc: 0.7952827087442472\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5626453261353958, Train acc: 0.7968559218559218\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5610560412293146, Train acc: 0.7977920227920228\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5605641335734508, Train acc: 0.7976428952991453\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5604961588454342, Train acc: 0.7977941176470589\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5595048575326871, Train acc: 0.7983736942070275\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5580113134168849, Train acc: 0.7994826810616285\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5572839120864613, Train acc: 0.8\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5585271796991682, Train acc: 0.7988654863654864\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5576743611974733, Train acc: 0.7993152680652681\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5586537857346489, Train acc: 0.7989594946116685\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.559212638572007, Train acc: 0.7987224002849003\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5603195271354455, Train acc: 0.7988034188034188\n",
      "Val loss: 0.520517110824585, Val acc: 0.806\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5376458187133838, Train acc: 0.8082264957264957\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5529392753274013, Train acc: 0.8034188034188035\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5582071200140521, Train acc: 0.8044871794871795\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5609288967063284, Train acc: 0.8031517094017094\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5597028299529329, Train acc: 0.8026709401709402\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.555534103402385, Train acc: 0.8033297720797721\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5550767948011776, Train acc: 0.8031898656898657\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.554232235902395, Train acc: 0.8036858974358975\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.553829346761348, Train acc: 0.8038342830009497\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.552832152427007, Train acc: 0.8040064102564103\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5533350687748995, Train acc: 0.8038558663558664\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.555533762517096, Train acc: 0.8026620370370371\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5547553474018638, Train acc: 0.8026380670611439\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5527491946460199, Train acc: 0.8030372405372406\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5520898460705056, Train acc: 0.8032407407407407\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5529683179253887, Train acc: 0.8028512286324786\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5505115110043187, Train acc: 0.8038273001508296\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5494880822627802, Train acc: 0.8039529914529915\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5503338506420989, Train acc: 0.8035874943769681\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5497716904482526, Train acc: 0.8034188034188035\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5490329281211288, Train acc: 0.8036731786731787\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.548993903610974, Train acc: 0.8043414918414918\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5486739531702067, Train acc: 0.8045104050538833\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5500739716614286, Train acc: 0.8039529914529915\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5495750664008988, Train acc: 0.8042735042735043\n",
      "Val loss: 0.534028947353363, Val acc: 0.798\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5456304999752941, Train acc: 0.8028846153846154\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5475537581448882, Train acc: 0.8055555555555556\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5428052744974098, Train acc: 0.8041310541310541\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5419532192122732, Train acc: 0.8048878205128205\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5400457852671289, Train acc: 0.8053418803418804\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5362387277674131, Train acc: 0.8069800569800569\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5380118578781575, Train acc: 0.8057081807081807\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5363676701552975, Train acc: 0.8059561965811965\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5380341033437992, Train acc: 0.8059710351377019\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5374479230716188, Train acc: 0.8061965811965812\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5390702082884803, Train acc: 0.8057983682983683\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5399997125875916, Train acc: 0.8056445868945868\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5410773058046564, Train acc: 0.8057199211045365\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5409024432392493, Train acc: 0.8057844932844933\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5395513616565966, Train acc: 0.805982905982906\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5401341003230494, Train acc: 0.8055555555555556\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5381863770276952, Train acc: 0.8065610859728507\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5374321984047563, Train acc: 0.8062974833808167\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5379768837037195, Train acc: 0.805921052631579\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5379089949795833, Train acc: 0.8060096153846154\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5357531193229887, Train acc: 0.8074379324379325\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5347484005202642, Train acc: 0.8076923076923077\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5367641372419963, Train acc: 0.8068329617242661\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5352263706908734, Train acc: 0.8074474715099715\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5348205666868096, Train acc: 0.8077991452991453\n",
      "Val loss: 0.5040651559829712, Val acc: 0.796\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5140159704491624, Train acc: 0.8157051282051282\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5220130974283586, Train acc: 0.8090277777777778\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5199174480220871, Train acc: 0.8125\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5128216937056973, Train acc: 0.8166399572649573\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.51299601292763, Train acc: 0.815491452991453\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5141126687221887, Train acc: 0.8169515669515669\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5163081321542371, Train acc: 0.8159340659340659\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5182907737266177, Train acc: 0.8145032051282052\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5189440503348539, Train acc: 0.8145180436847104\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5188872494822384, Train acc: 0.8141025641025641\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5196054198351495, Train acc: 0.8143939393939394\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.51878548017106, Train acc: 0.8150819088319088\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5184130690732259, Train acc: 0.8157873109796187\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5183725350733133, Train acc: 0.8154380341880342\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5189084489758198, Train acc: 0.81502849002849\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5185504477892995, Train acc: 0.8150040064102564\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5188103841927676, Train acc: 0.815296631473102\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5188141618408652, Train acc: 0.8150522317188984\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5182784014003158, Train acc: 0.8151709401709402\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5186145488554851, Train acc: 0.8149305555555556\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5190396479834712, Train acc: 0.814585877085877\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5186268913515474, Train acc: 0.8151952214452215\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5183538906234696, Train acc: 0.8153102935711631\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5191276534686019, Train acc: 0.8146367521367521\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5181054075635396, Train acc: 0.8149786324786324\n",
      "Val loss: 0.5020120739936829, Val acc: 0.812\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5097691592497703, Train acc: 0.8157051282051282\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5158358063453283, Train acc: 0.8151709401709402\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5174663555503232, Train acc: 0.8126780626780626\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5224231825425074, Train acc: 0.8112980769230769\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5202087280205172, Train acc: 0.8141025641025641\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5215482729728576, Train acc: 0.8129451566951567\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5161170828382465, Train acc: 0.8150946275946276\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5180781086206309, Train acc: 0.8146367521367521\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5136327770771237, Train acc: 0.8157051282051282\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5104047204948898, Train acc: 0.8166666666666667\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5111589970033856, Train acc: 0.8167249417249417\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5132153581753586, Train acc: 0.8158831908831908\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5123951548929672, Train acc: 0.81603385930309\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5126259780701554, Train acc: 0.8161630036630036\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.51194351692485, Train acc: 0.816559829059829\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5126080130521431, Train acc: 0.8159722222222222\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.513660571753769, Train acc: 0.8157051282051282\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5111129384378196, Train acc: 0.8167735042735043\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5117901515783729, Train acc: 0.8169984255510572\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5129787340569191, Train acc: 0.816292735042735\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5122036907274548, Train acc: 0.817104192104192\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5115895238303361, Train acc: 0.8176961926961926\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5123952620592493, Train acc: 0.8169360832404311\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5123677818460173, Train acc: 0.8168847934472935\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5125662514949456, Train acc: 0.8167521367521368\n",
      "Val loss: 0.4972938299179077, Val acc: 0.806\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5019872726665603, Train acc: 0.8247863247863247\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5006231460879501, Train acc: 0.8245192307692307\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.49942669083504937, Train acc: 0.8247863247863247\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4956618703296806, Train acc: 0.8235844017094017\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4963493170009719, Train acc: 0.8232905982905983\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4963027178243524, Train acc: 0.8242521367521367\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.49780397214477606, Train acc: 0.8237942612942613\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.49701350627260077, Train acc: 0.8243856837606838\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4940232047342394, Train acc: 0.8249643874643875\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.49404015545534274, Train acc: 0.8255876068376068\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4944983050227165, Train acc: 0.8250777000777001\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4957579616457224, Train acc: 0.8245637464387464\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.49448499524871586, Train acc: 0.8248685075608152\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4949796865301909, Train acc: 0.8239468864468864\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4957575129435273, Train acc: 0.8235754985754986\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4965719364257131, Train acc: 0.8231169871794872\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4958087638799661, Train acc: 0.8236865258924082\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.49604327807951176, Train acc: 0.8239850427350427\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.49830762866014733, Train acc: 0.8228463787674314\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.49778935693713844, Train acc: 0.8230502136752137\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.49800461604978963, Train acc: 0.8232346357346357\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.497522414983963, Train acc: 0.8230380730380731\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4976368267677478, Train acc: 0.8230676328502415\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4975654602135688, Train acc: 0.8229834401709402\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4970875580086667, Train acc: 0.8230555555555555\n",
      "Val loss: 0.4942828118801117, Val acc: 0.83\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4949965014671668, Train acc: 0.8274572649572649\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5044564680061024, Train acc: 0.8189102564102564\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5100759793911395, Train acc: 0.8192663817663818\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5062237091314716, Train acc: 0.8219818376068376\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5033992654556393, Train acc: 0.8222222222222222\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4972209648900477, Train acc: 0.823539886039886\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4928266171761703, Train acc: 0.8251678876678876\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.48781413646637756, Train acc: 0.8262553418803419\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4904357338357798, Train acc: 0.825201804368471\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.48956198291455066, Train acc: 0.825\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.49153746932270276, Train acc: 0.8243978243978244\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.49134777330573215, Train acc: 0.8241185897435898\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.49291119431338365, Train acc: 0.8231837606837606\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.49147435128734296, Train acc: 0.8235653235653235\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.492283966740397, Train acc: 0.8232905982905983\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.49209938789956653, Train acc: 0.8234508547008547\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.491173234335093, Train acc: 0.8239693313222725\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4909438534046312, Train acc: 0.8238663342830009\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4915215585546696, Train acc: 0.8234367971210077\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4905888351635673, Train acc: 0.8237446581196581\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4887082555904032, Train acc: 0.8247863247863247\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4890513716348054, Train acc: 0.8247377622377622\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.49023298562730344, Train acc: 0.8246934225195095\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4900234237278777, Train acc: 0.8248308404558404\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4883264551649236, Train acc: 0.825448717948718\n",
      "Val loss: 0.4747050702571869, Val acc: 0.83\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4832043829891417, Train acc: 0.8311965811965812\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4652402611751842, Train acc: 0.8338675213675214\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4753163971877166, Train acc: 0.8285256410256411\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.47622045352418196, Train acc: 0.8274572649572649\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4790944265631529, Train acc: 0.8270299145299145\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4781738699565076, Train acc: 0.8285256410256411\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4766947139732276, Train acc: 0.82997557997558\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4813631020693315, Train acc: 0.828125\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4857274844733059, Train acc: 0.8271011396011396\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.48635937640936966, Train acc: 0.8277243589743589\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4826333729757203, Train acc: 0.8293997668997669\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.483414439585304, Train acc: 0.828926282051282\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4845940601190872, Train acc: 0.827827087442472\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4837216137839033, Train acc: 0.8284111721611722\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4869322292570375, Train acc: 0.8275997150997151\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4872600591630062, Train acc: 0.8278912927350427\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.48659506884050946, Train acc: 0.8277086475615887\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4853510112171782, Train acc: 0.8280508072174739\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.48521717203411496, Train acc: 0.8283007197480882\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.48535172416167893, Train acc: 0.8279380341880341\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.48431443358746107, Train acc: 0.8281695156695157\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4845497410139467, Train acc: 0.8280400155400155\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.484475185099985, Train acc: 0.8283862876254181\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.48412186581577754, Train acc: 0.828659188034188\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4837278850796895, Train acc: 0.8286111111111111\n",
      "Val loss: 0.4699447751045227, Val acc: 0.84\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4714010733569789, Train acc: 0.8247863247863247\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4923505216327488, Train acc: 0.8229166666666666\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4799226623484891, Train acc: 0.8267450142450142\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.47935490865801644, Train acc: 0.8271901709401709\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.47747766169217915, Train acc: 0.8274572649572649\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.47345377063309707, Train acc: 0.8297720797720798\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4769015430276065, Train acc: 0.8295940170940171\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.48031104153865933, Train acc: 0.828125\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.477795223423332, Train acc: 0.8282288698955366\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.47903485803777335, Train acc: 0.8286324786324787\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.476029951620352, Train acc: 0.8298368298368298\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4724060979451549, Train acc: 0.8315972222222222\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.473053495490888, Train acc: 0.8320184089414858\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.47251357702636165, Train acc: 0.8327228327228328\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.47140758568235275, Train acc: 0.8333689458689458\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4700132664452251, Train acc: 0.8340344551282052\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.47042875558272745, Train acc: 0.8339617898441428\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.47197055906510194, Train acc: 0.8333926875593543\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4717594669629902, Train acc: 0.8333614484930274\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.47229223364693484, Train acc: 0.8331463675213675\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.472371190477058, Train acc: 0.8328245828245828\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4726920906821933, Train acc: 0.8328477078477079\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4725004725631663, Train acc: 0.8327759197324415\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.47185742216412924, Train acc: 0.8331330128205128\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.47215185394908626, Train acc: 0.8326923076923077\n",
      "Val loss: 0.5233016610145569, Val acc: 0.8\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.48511527606055266, Train acc: 0.8263888888888888\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.48232508396618384, Train acc: 0.8215811965811965\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.48744897134708204, Train acc: 0.8205128205128205\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.49079243732122785, Train acc: 0.8231837606837606\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.47795829960168934, Train acc: 0.8289529914529915\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.47731291768048223, Train acc: 0.8291488603988604\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.47496288866532443, Train acc: 0.8295177045177046\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.47524190507033187, Train acc: 0.8296607905982906\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.47074560431192175, Train acc: 0.8308998100664767\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4727030541079167, Train acc: 0.8297008547008548\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4737885795307882, Train acc: 0.8292540792540792\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4748673294471879, Train acc: 0.8287927350427351\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.47567087791022467, Train acc: 0.8282790927021696\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4752445769961355, Train acc: 0.8282585470085471\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.47565314628799416, Train acc: 0.8287393162393163\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4763463927496575, Train acc: 0.8284588675213675\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.47656685439424934, Train acc: 0.8285570638511816\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4760294805677981, Train acc: 0.8289411206077872\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4743507436593534, Train acc: 0.8296783625730995\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4731490929634907, Train acc: 0.8306089743589744\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4720772000793087, Train acc: 0.831425518925519\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.47097356953218916, Train acc: 0.8317793317793318\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.47034238310429033, Train acc: 0.8315217391304348\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.46955085549758285, Train acc: 0.8322872150997151\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4694508205698087, Train acc: 0.8320299145299145\n",
      "Val loss: 0.49556753039360046, Val acc: 0.824\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.439515724650815, Train acc: 0.8530982905982906\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.46898338300550085, Train acc: 0.8373397435897436\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.46762937430472795, Train acc: 0.8367165242165242\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4655424430966377, Train acc: 0.8331997863247863\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4646633738635952, Train acc: 0.8354700854700855\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.462005951157917, Train acc: 0.8358262108262108\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.46122330718279175, Train acc: 0.836462148962149\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4597461085177512, Train acc: 0.8368055555555556\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.46020243830575563, Train acc: 0.8361229819563153\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.46120596623573545, Train acc: 0.8364850427350428\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.45880461670603045, Train acc: 0.8367327117327117\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4608015336276714, Train acc: 0.8357816951566952\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4580046818355915, Train acc: 0.8368261012491782\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.46077362122298454, Train acc: 0.8358134920634921\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.45867979073541454, Train acc: 0.8363247863247864\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45797993468208253, Train acc: 0.8367053952991453\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4584491815588443, Train acc: 0.8364756158873806\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4584002394887202, Train acc: 0.8371320037986705\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4582503740661084, Train acc: 0.8374943769680612\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.45762000133116276, Train acc: 0.8379540598290598\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.45742786692521364, Train acc: 0.8378866503866503\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4570334238776678, Train acc: 0.8379710567210568\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.45689920338361467, Train acc: 0.8379087699739873\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4582806357071122, Train acc: 0.8374732905982906\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4583252695241036, Train acc: 0.8373076923076923\n",
      "Val loss: 0.50148606300354, Val acc: 0.816\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.46905200590944696, Train acc: 0.8370726495726496\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4649351242706816, Train acc: 0.8373397435897436\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.47201378857902654, Train acc: 0.8327991452991453\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.46630190597831184, Train acc: 0.8318643162393162\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.463761135082469, Train acc: 0.832905982905983\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4614078233872893, Train acc: 0.8346688034188035\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.46273799165992363, Train acc: 0.8345543345543346\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4592375624526897, Train acc: 0.8352697649572649\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.45738250094289906, Train acc: 0.8355887939221273\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4557294962115777, Train acc: 0.8361645299145299\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4569797419712075, Train acc: 0.8363927738927739\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4582043610502597, Train acc: 0.8358707264957265\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4563123630378138, Train acc: 0.8374424720578567\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4573456140346507, Train acc: 0.8371108058608059\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.45776920190394094, Train acc: 0.8367877492877492\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45603314258603966, Train acc: 0.8373397435897436\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.45516382449233034, Train acc: 0.8375754147812972\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.45402406993103617, Train acc: 0.8379332858499525\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4543462835195308, Train acc: 0.8375506072874493\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4560486536886957, Train acc: 0.8371260683760684\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.45642969868910427, Train acc: 0.8370726495726496\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.45646117880894127, Train acc: 0.8366841491841492\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4573058957854907, Train acc: 0.8357952434039391\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4574258453862747, Train acc: 0.8355368589743589\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.456577542719678, Train acc: 0.8358974358974359\n",
      "Val loss: 0.4719838798046112, Val acc: 0.838\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4789386374446062, Train acc: 0.8274572649572649\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.46150595468715727, Train acc: 0.8333333333333334\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4533321094776151, Train acc: 0.8374287749287749\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4484128729145751, Train acc: 0.8400106837606838\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.45301395143963336, Train acc: 0.8385683760683761\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.44953075604263876, Train acc: 0.8403668091168092\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.44866319334587046, Train acc: 0.840964590964591\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4508900575292034, Train acc: 0.8413461538461539\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4515804403938233, Train acc: 0.8403371320037987\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4517297399031301, Train acc: 0.840224358974359\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4510770565783135, Train acc: 0.8406177156177156\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.45271999058326934, Train acc: 0.8402777777777778\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4526054777258874, Train acc: 0.8404421433267587\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.45301961889706543, Train acc: 0.8405448717948718\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4551762663910532, Train acc: 0.8399216524216524\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45376789535626644, Train acc: 0.8402777777777778\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.452568859360127, Train acc: 0.8404034690799397\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4531737519224711, Train acc: 0.8402777777777778\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.45124346433769624, Train acc: 0.8410087719298246\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4510308774649842, Train acc: 0.8413461538461539\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4484921744048571, Train acc: 0.8418040293040293\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4488745304960656, Train acc: 0.8419046231546231\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.44892761433259654, Train acc: 0.841996469713861\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4485818567046328, Train acc: 0.8418803418803419\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4481780845589108, Train acc: 0.8421794871794872\n",
      "Val loss: 0.48798057436943054, Val acc: 0.828\n",
      "Early stopping at epoch 24 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 580.8857 [s]\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\milan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    result_msg += f\"test={test_scores[scorer_name]:.3f})\"\n",
      "  File \"c:\\Users\\milan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    Test data that will be fed to estimator.predict.\n",
      "  File \"c:\\Users\\milan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 276, in _score\n",
      "    ``self._kwargs`` and ``kwargs`` passed as metadata.\n",
      "  File \"c:\\Users\\milan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 73, in _cached_call\n",
      "    homogeneity_score,\n",
      "  File \"C:\\Users\\milan\\AppData\\Local\\Temp\\ipykernel_7992\\74241984.py\", line 34, in predict\n",
      "    self._model.eval()\n",
      "AttributeError: 'dict' object has no attribute 'eval'\n",
      "\n",
      "  end_msg += result_msg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 117 - Batch 117/3030 - Train loss: 1.6098300147260356, Train acc: 0.2094017094017094\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.6053762272891836, Train acc: 0.22115384615384615\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.5905217567400376, Train acc: 0.25\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.5637160799442194, Train acc: 0.27737713675213677\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.540253694037087, Train acc: 0.2986111111111111\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 1.5165706254817821, Train acc: 0.317485754985755\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 1.4901832426569546, Train acc: 0.33814102564102566\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 1.465917016260135, Train acc: 0.3550347222222222\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 1.4449394119431151, Train acc: 0.3691832858499525\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 1.4216750134260228, Train acc: 0.38344017094017097\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 1.4022495889256144, Train acc: 0.3967074592074592\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 1.383848388862406, Train acc: 0.4074519230769231\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 1.3665391695805085, Train acc: 0.4176528599605523\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 1.3511160611014663, Train acc: 0.42536630036630035\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 1.3368743881880387, Train acc: 0.43365384615384617\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 1.3263933539517925, Train acc: 0.4399706196581197\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 1.3140262416285327, Train acc: 0.4464869281045752\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 1.301601818031282, Train acc: 0.45346628679962014\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 1.2905509133516964, Train acc: 0.45959851551956815\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 1.2797893985214397, Train acc: 0.46455662393162395\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 1.2707535254067408, Train acc: 0.4688390313390313\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 1.2606373261877428, Train acc: 0.4730720668220668\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 1.2513783771678977, Train acc: 0.47788926049795616\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 1.2433933754082758, Train acc: 0.48288372507122507\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 1.2354964022758679, Train acc: 0.4872435897435897\n",
      "Val loss: 0.9026815295219421, Val acc: 0.642\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.9903022601054265, Train acc: 0.594017094017094\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.0002477209792178, Train acc: 0.5924145299145299\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.0071188406726914, Train acc: 0.5909900284900285\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.006322399354898, Train acc: 0.5916132478632479\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.0053583619431552, Train acc: 0.5916666666666667\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.9998920393041056, Train acc: 0.5949964387464387\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.9949684077930684, Train acc: 0.597985347985348\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.9953286867811639, Train acc: 0.5994925213675214\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.9970977112495208, Train acc: 0.5985873694207028\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.9963993258710601, Train acc: 0.5993055555555555\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.9927705352678722, Train acc: 0.6018842268842269\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.9875256439347213, Train acc: 0.6047008547008547\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.9889955856273396, Train acc: 0.6044953977646286\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.9861847402775826, Train acc: 0.6051968864468864\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.9847057333868793, Train acc: 0.6057692307692307\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.9831910452240298, Train acc: 0.6072048611111112\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.9830079827403351, Train acc: 0.607120412267471\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.9796191033078508, Train acc: 0.6082621082621082\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.9752459350706917, Train acc: 0.6099302744039586\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.9731362226045029, Train acc: 0.6107905982905983\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.9711794178079287, Train acc: 0.6119505494505495\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.9693415291613169, Train acc: 0.612786519036519\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.9682326041292143, Train acc: 0.613271088814567\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.9671973840803163, Train acc: 0.6134704415954416\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.9654247774425735, Train acc: 0.614465811965812\n",
      "Val loss: 0.8090188503265381, Val acc: 0.65\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.9419882424876221, Train acc: 0.6201923076923077\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.9299531023726504, Train acc: 0.6180555555555556\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.9214961255377854, Train acc: 0.6299857549857549\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.9185108203663785, Train acc: 0.6315438034188035\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.9142048326312986, Train acc: 0.6329059829059829\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.9130266494730599, Train acc: 0.6348824786324786\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.9062219659487406, Train acc: 0.637591575091575\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.905414419551181, Train acc: 0.6376201923076923\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.9040197136282129, Train acc: 0.6387108262108262\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.9042740626212878, Train acc: 0.6392628205128205\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.9048064135495507, Train acc: 0.6387432012432013\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.90287924702266, Train acc: 0.6382211538461539\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.9047140073690032, Train acc: 0.6361357659434582\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.9056482734988752, Train acc: 0.6357982295482295\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.9055031110588302, Train acc: 0.6366096866096866\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.9037003346010406, Train acc: 0.6377537393162394\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.9038598515868727, Train acc: 0.6373177476118652\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.9012726859489397, Train acc: 0.6391856600189934\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.8999211425133455, Train acc: 0.6398166891587944\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.9004253330011653, Train acc: 0.6396634615384615\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.8997664991485719, Train acc: 0.6407458282458283\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.8999898414928597, Train acc: 0.6412927350427351\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.8990441760424122, Train acc: 0.6414437012263099\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.8975284909060028, Train acc: 0.6423165954415955\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.8963268603830256, Train acc: 0.6430555555555556\n",
      "Val loss: 0.7508020401000977, Val acc: 0.696\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.880684576737575, Train acc: 0.6490384615384616\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.8856442829227855, Train acc: 0.6535790598290598\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.8846447444879092, Train acc: 0.6529558404558404\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.8805460808241469, Train acc: 0.6541132478632479\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.8742536371589726, Train acc: 0.6557692307692308\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.8700525086489838, Train acc: 0.6578525641025641\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.8702262574135893, Train acc: 0.6561355311355311\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.8652312199975181, Train acc: 0.6588541666666666\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.8658490358990708, Train acc: 0.6584164292497626\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.863747143974671, Train acc: 0.6592948717948718\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.8647929185939187, Train acc: 0.6588966588966589\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.8636967789201315, Train acc: 0.6606570512820513\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.8649066591012016, Train acc: 0.658982577251808\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.86292910321146, Train acc: 0.6599511599511599\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.860503552680002, Train acc: 0.6611823361823361\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.8591358321917872, Train acc: 0.6617922008547008\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.8587474885629373, Train acc: 0.6631158873805932\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.8570156131091847, Train acc: 0.6635208926875593\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.8569487830137166, Train acc: 0.6635739991003149\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.857065745895235, Train acc: 0.6633279914529915\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.8565551592389598, Train acc: 0.6635124135124135\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.8561257352964213, Train acc: 0.6635829448329449\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.8564487203972263, Train acc: 0.6633454106280193\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.8547947427358722, Train acc: 0.6636396011396012\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.8533347725766337, Train acc: 0.6644444444444444\n",
      "Val loss: 0.7348039150238037, Val acc: 0.69\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.824753866236434, Train acc: 0.6736111111111112\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.8278376246109987, Train acc: 0.6738782051282052\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.8252550602978111, Train acc: 0.6748575498575499\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.8206614047034174, Train acc: 0.6772168803418803\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.821037401449986, Train acc: 0.6798076923076923\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.8227426495477345, Train acc: 0.676727207977208\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.8244692114464965, Train acc: 0.6768162393162394\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.8230450509959816, Train acc: 0.6779513888888888\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.8221027684404186, Train acc: 0.6780626780626781\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.824279788773284, Train acc: 0.6772435897435898\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.8233264061306091, Train acc: 0.678030303030303\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.8208834260404959, Train acc: 0.6793091168091168\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.8205793902013429, Train acc: 0.6796926364234056\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.8209991965793136, Train acc: 0.6785332722832723\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.8189083408765983, Train acc: 0.6796652421652422\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.8182163592189168, Train acc: 0.6795873397435898\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.8172276375535267, Train acc: 0.6803670186023127\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.8160369803409064, Train acc: 0.68085232668566\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.8151701906954872, Train acc: 0.6815677013045434\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.8148458231527071, Train acc: 0.6814903846153846\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.8152113702962396, Train acc: 0.681064306064306\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.8142921996549664, Train acc: 0.6815996503496503\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.8136961561835685, Train acc: 0.6819955406911928\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.8119429658058277, Train acc: 0.6826923076923077\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.811342481502101, Train acc: 0.682991452991453\n",
      "Val loss: 0.7184045910835266, Val acc: 0.716\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.8127518846438482, Train acc: 0.6949786324786325\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.8086478831166894, Train acc: 0.6936431623931624\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.806542237429877, Train acc: 0.6915954415954416\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.7976762694426072, Train acc: 0.6948450854700855\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7980284757593759, Train acc: 0.6918803418803419\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.7956571674499756, Train acc: 0.6912393162393162\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.7960728865039509, Train acc: 0.6893315018315018\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.7893484949619851, Train acc: 0.6919070512820513\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.7845579680101371, Train acc: 0.6940289648622981\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.7824335333883253, Train acc: 0.6954059829059829\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.7807580085808488, Train acc: 0.6964840714840714\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7805881097148626, Train acc: 0.6964921652421653\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.7809835996853529, Train acc: 0.6965401051939514\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.7774624934053829, Train acc: 0.6981837606837606\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.7759119085946314, Train acc: 0.6985042735042735\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.7752722230954812, Train acc: 0.6996193910256411\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.774285862303787, Train acc: 0.700069130216189\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.7720030463970171, Train acc: 0.7012701804368471\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.7702659611974383, Train acc: 0.7019793072424652\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.7705355563734332, Train acc: 0.7014423076923076\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.7695744621423769, Train acc: 0.7011090761090761\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.7684006021989809, Train acc: 0.700781857031857\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.767633676927624, Train acc: 0.701249535488666\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.7683670910186747, Train acc: 0.7007211538461539\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7683184079736726, Train acc: 0.701025641025641\n",
      "Val loss: 0.6387916803359985, Val acc: 0.756\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.742633764560406, Train acc: 0.7270299145299145\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.7390733301384836, Train acc: 0.7243589743589743\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.728619376903246, Train acc: 0.7270299145299145\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.7404688269409359, Train acc: 0.7227564102564102\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7408645829074403, Train acc: 0.7216880341880342\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.74388656567814, Train acc: 0.7218660968660968\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.7457574076486595, Train acc: 0.7210775335775336\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.7440051540222943, Train acc: 0.7206864316239316\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.7442494587794102, Train acc: 0.7210351377018044\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.7385442667919346, Train acc: 0.7231303418803419\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.7386703610813886, Train acc: 0.7216880341880342\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7398294870478984, Train acc: 0.7203970797720798\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.739152261285622, Train acc: 0.7197567389875082\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.7422548187794266, Train acc: 0.71875\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.7398872925600095, Train acc: 0.7196581196581197\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.7378704801169981, Train acc: 0.7202857905982906\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.7386345770743817, Train acc: 0.720148315736551\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.73785983620376, Train acc: 0.7210351377018044\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.7379486354899953, Train acc: 0.7214068825910931\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.7385195691870828, Train acc: 0.72053952991453\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.7391551349359069, Train acc: 0.7202380952380952\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.7379227238240498, Train acc: 0.7204496891996892\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.7374716974332491, Train acc: 0.7211306205871423\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.737961490864088, Train acc: 0.721176103988604\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7377917787254367, Train acc: 0.7214529914529915\n",
      "Val loss: 0.6242718696594238, Val acc: 0.748\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7134946630551264, Train acc: 0.7211538461538461\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.7147440139809226, Train acc: 0.7270299145299145\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.7191911041736603, Train acc: 0.7248931623931624\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.721511762493696, Train acc: 0.7236912393162394\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7240082512044499, Train acc: 0.7215811965811966\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.7228964580769552, Train acc: 0.7226673789173789\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.7198293540940617, Train acc: 0.7243589743589743\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.7185938417847849, Train acc: 0.725761217948718\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.7145529845756003, Train acc: 0.7282763532763533\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.7147693163946144, Train acc: 0.7287393162393162\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.7162688089116377, Train acc: 0.7280982905982906\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7145380884875939, Train acc: 0.7301014957264957\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.7117376799531707, Train acc: 0.730276134122288\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.7087209428477491, Train acc: 0.7311507936507936\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.7080215677576527, Train acc: 0.7309472934472935\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.7080533182901195, Train acc: 0.7311364850427351\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.7068136326477958, Train acc: 0.7310834590246355\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.7058625414698665, Train acc: 0.7321047008547008\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.7056919668997615, Train acc: 0.7327372919478182\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.7057746256645928, Train acc: 0.7323450854700855\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.7056423357870213, Train acc: 0.7326516076516076\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.7036848954433791, Train acc: 0.7336829836829837\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.7044860114842177, Train acc: 0.7333240431066518\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.7051408152358655, Train acc: 0.7331730769230769\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7048607245214984, Train acc: 0.7332692307692308\n",
      "Val loss: 0.6178386807441711, Val acc: 0.752\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6845039126709995, Train acc: 0.7478632478632479\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.671507624988882, Train acc: 0.7454594017094017\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6794043820977551, Train acc: 0.7430555555555556\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6853109492450697, Train acc: 0.7394497863247863\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6814428659840527, Train acc: 0.739957264957265\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6757205533887926, Train acc: 0.7412749287749287\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6770041695772073, Train acc: 0.7408424908424909\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6805287288008337, Train acc: 0.7408520299145299\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6820170305817895, Train acc: 0.7402065527065527\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6817910837057309, Train acc: 0.740224358974359\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6777750769834141, Train acc: 0.7413073038073038\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6780695280755348, Train acc: 0.7421652421652422\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6784804430664723, Train acc: 0.7433842866535174\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6775604400133329, Train acc: 0.7440094627594628\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.676074282761313, Train acc: 0.7447293447293447\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6768328794795606, Train acc: 0.7440571581196581\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.678497496735756, Train acc: 0.7428670186023127\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6806782467426964, Train acc: 0.7417497625830959\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6813162685285213, Train acc: 0.7418184885290149\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6816149888830817, Train acc: 0.741693376068376\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6828668375700822, Train acc: 0.7419363044363044\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6818617913458083, Train acc: 0.7425213675213675\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6812548265276412, Train acc: 0.7424981419546637\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6797974759877322, Train acc: 0.7431000712250713\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6792048810537045, Train acc: 0.7436538461538461\n",
      "Val loss: 0.6238892078399658, Val acc: 0.766\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6738279726770189, Train acc: 0.7516025641025641\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6661888864050564, Train acc: 0.7545405982905983\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.668529821257306, Train acc: 0.7501780626780626\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.667008992507417, Train acc: 0.7520032051282052\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6706765735760714, Train acc: 0.7502136752136752\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6691839961584477, Train acc: 0.7508903133903134\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.670912458601161, Train acc: 0.7493131868131868\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6766394150212535, Train acc: 0.7487980769230769\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6710302565540457, Train acc: 0.7498812915479582\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6718097232218482, Train acc: 0.7485576923076923\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6720703167445732, Train acc: 0.7480089355089355\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6733431977742588, Train acc: 0.7471064814814815\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6730751794059366, Train acc: 0.7470825115055885\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6715990002375092, Train acc: 0.7479014041514042\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.671874754619055, Train acc: 0.7469017094017094\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6709797046521407, Train acc: 0.7478966346153846\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6699452758614534, Train acc: 0.7490258924082454\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6709029741960261, Train acc: 0.7484271130104464\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6708589727431871, Train acc: 0.7484536662168241\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6702241596184735, Train acc: 0.7493322649572649\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6695311186174867, Train acc: 0.7498219373219374\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6689458458504199, Train acc: 0.749732905982906\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6667808054692681, Train acc: 0.7507664437012264\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6657156254459395, Train acc: 0.7509348290598291\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6647352982739098, Train acc: 0.7513034188034188\n",
      "Val loss: 0.5640171766281128, Val acc: 0.784\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6490011156624199, Train acc: 0.7553418803418803\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6501068602133001, Train acc: 0.7537393162393162\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6556098229779816, Train acc: 0.7556980056980057\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6469534049049402, Train acc: 0.7596153846153846\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6505057420231338, Train acc: 0.7592948717948718\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6461651194469202, Train acc: 0.7600605413105413\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6418594806767791, Train acc: 0.7617521367521367\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6403371825113765, Train acc: 0.7616853632478633\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.641255433120166, Train acc: 0.761633428300095\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6427546170659554, Train acc: 0.7602564102564102\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6395986807165723, Train acc: 0.7613636363636364\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6382043456441147, Train acc: 0.7614405270655271\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6386843262672581, Train acc: 0.7607248520710059\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6410792536842518, Train acc: 0.760340354090354\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6393989077150991, Train acc: 0.7612179487179487\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6396866435519396, Train acc: 0.7605502136752137\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6386056903500602, Train acc: 0.7605580693815988\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6388245569603962, Train acc: 0.7604463437796771\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6394424395643266, Train acc: 0.7603182636077372\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6403955915162706, Train acc: 0.7598023504273504\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.641517573539251, Train acc: 0.7591829466829467\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6416482498063166, Train acc: 0.759469696969697\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6422858422338985, Train acc: 0.7586399108138239\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6409140470413826, Train acc: 0.7588363603988604\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6414206193705909, Train acc: 0.7585042735042735\n",
      "Val loss: 0.5863211154937744, Val acc: 0.778\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6153175469137665, Train acc: 0.7751068376068376\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6273301942990377, Train acc: 0.7729700854700855\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6220867632970511, Train acc: 0.7720797720797721\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6301990985934042, Train acc: 0.7693643162393162\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6366243360134272, Train acc: 0.7673076923076924\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6373117883872782, Train acc: 0.7670940170940171\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6369803194889073, Train acc: 0.766941391941392\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6379702455149248, Train acc: 0.765357905982906\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6343238411465941, Train acc: 0.7663224121557455\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6328541344174972, Train acc: 0.767147435897436\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.632717910705376, Train acc: 0.7661227661227661\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.632028557264652, Train acc: 0.766159188034188\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6303651270742561, Train acc: 0.7663132807363576\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6301087472355846, Train acc: 0.7665598290598291\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6304280078648842, Train acc: 0.7664529914529915\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6290764974700844, Train acc: 0.7667935363247863\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6299641231096468, Train acc: 0.7661199095022625\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6283672600772306, Train acc: 0.7665895061728395\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6265824984455773, Train acc: 0.7666441745389114\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6261562567427118, Train acc: 0.7668803418803419\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6246278573086846, Train acc: 0.7677045177045178\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6254926689299114, Train acc: 0.7668512043512044\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6241462796992947, Train acc: 0.7672565960609439\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6237264011676112, Train acc: 0.7672720797720798\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6236918777711371, Train acc: 0.7666880341880342\n",
      "Val loss: 0.5753893256187439, Val acc: 0.778\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6320702670476376, Train acc: 0.7665598290598291\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6285750474303197, Train acc: 0.7657585470085471\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6293560959759601, Train acc: 0.7669159544159544\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6297194709698869, Train acc: 0.7665598290598291\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6272713808167694, Train acc: 0.7654914529914529\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6255641370544746, Train acc: 0.7654024216524217\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6210906568901006, Train acc: 0.7664835164835165\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6194037320808723, Train acc: 0.7664930555555556\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.61465403518276, Train acc: 0.769349477682811\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.612906057610471, Train acc: 0.7702457264957265\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6154349330977682, Train acc: 0.7689879564879565\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6130450397644966, Train acc: 0.7687856125356125\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6124949455653108, Train acc: 0.7689842209072978\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6109893271319741, Train acc: 0.769459706959707\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6097328753298165, Train acc: 0.7701923076923077\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6117471935561835, Train acc: 0.7689302884615384\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6122192010740469, Train acc: 0.7686651583710408\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.613227891602181, Train acc: 0.7684294871794872\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6123037790319054, Train acc: 0.7685278902384165\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6126083900594813, Train acc: 0.7685096153846154\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6112242804148065, Train acc: 0.769027269027269\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6126660033192112, Train acc: 0.7687451437451438\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6122577100970054, Train acc: 0.7688359345968042\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6101290637897885, Train acc: 0.7697426994301995\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6099928902866494, Train acc: 0.7697863247863248\n",
      "Val loss: 0.5578417778015137, Val acc: 0.802\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5689143585598367, Train acc: 0.7884615384615384\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5955130884535292, Train acc: 0.7793803418803419\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5940729006488099, Train acc: 0.7759971509971509\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6021332394045132, Train acc: 0.7736378205128205\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5990029281530624, Train acc: 0.7754273504273504\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5983722993196585, Train acc: 0.7757300569800569\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.60053531248782, Train acc: 0.7749542124542125\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6047850104255809, Train acc: 0.7735710470085471\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6023333190116901, Train acc: 0.7754629629629629\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6007925385083908, Train acc: 0.7758012820512821\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6018031605033793, Train acc: 0.7748154623154623\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6027010634244337, Train acc: 0.7748397435897436\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6042464691545523, Train acc: 0.7744904667981591\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6051534402192148, Train acc: 0.7746489621489622\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6046835606424218, Train acc: 0.7753205128205128\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.603175964891019, Train acc: 0.7759415064102564\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6041339032160931, Train acc: 0.7766151332327803\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6041131861797991, Train acc: 0.7761752136752137\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6046719553030347, Train acc: 0.7761752136752137\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6058641460015733, Train acc: 0.7759882478632478\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6040003886723062, Train acc: 0.7766330891330891\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.602680274083481, Train acc: 0.7764180264180264\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6022090623587846, Train acc: 0.7767326272761056\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6005006935235188, Train acc: 0.7774661680911681\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6016128721043595, Train acc: 0.7768162393162393\n",
      "Val loss: 0.5580088496208191, Val acc: 0.784\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.582786890431347, Train acc: 0.7751068376068376\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5833852180303671, Train acc: 0.7783119658119658\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5794170516678411, Train acc: 0.7774216524216524\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5798149952967452, Train acc: 0.7813835470085471\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5847695044472686, Train acc: 0.7823717948717949\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5857135817578375, Train acc: 0.7824074074074074\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.580074529580872, Train acc: 0.784035409035409\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5841519192306914, Train acc: 0.7847889957264957\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5829978003772462, Train acc: 0.7849002849002849\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5816544270413554, Train acc: 0.7844551282051282\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5826332077279791, Train acc: 0.7831196581196581\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5861650152487463, Train acc: 0.78125\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5855296214578342, Train acc: 0.7811472715318869\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5856290679693368, Train acc: 0.7808302808302808\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.587525637188868, Train acc: 0.7801994301994302\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.58709072550902, Train acc: 0.7807491987179487\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5873484954185495, Train acc: 0.7802601809954751\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5885927495524528, Train acc: 0.7799442070275404\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5883252318718923, Train acc: 0.7805611785874944\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5871591614097611, Train acc: 0.7806891025641025\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5867005341364496, Train acc: 0.780499592999593\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5870998579641137, Train acc: 0.780715811965812\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5865299133165137, Train acc: 0.7807274247491639\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5856472543650373, Train acc: 0.7814280626780626\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5846295829970612, Train acc: 0.7815384615384615\n",
      "Val loss: 0.547053337097168, Val acc: 0.79\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6078778337209653, Train acc: 0.7767094017094017\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5952505341325051, Train acc: 0.7777777777777778\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5814222384382177, Train acc: 0.7836538461538461\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5793426000855417, Train acc: 0.7824519230769231\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5805671230595336, Train acc: 0.782051282051282\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5819964214649975, Train acc: 0.7835648148148148\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5794030968486462, Train acc: 0.7846459096459096\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5752855823972286, Train acc: 0.7861244658119658\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5737427975292559, Train acc: 0.7860873694207028\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5729014904835286, Train acc: 0.7866987179487179\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5739321239761539, Train acc: 0.7866161616161617\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5697481502908842, Train acc: 0.7881499287749287\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5736858842258857, Train acc: 0.7871877054569362\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5729113343297999, Train acc: 0.7869734432234432\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5745003508154484, Train acc: 0.7863247863247863\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.573221739839253, Train acc: 0.7870926816239316\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.57413231759754, Train acc: 0.7866704374057315\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5723192085320058, Train acc: 0.7869183285849952\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5721525629220704, Train acc: 0.7869152046783626\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.573231407162598, Train acc: 0.7861111111111111\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.573270803281797, Train acc: 0.7860958485958486\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5723914850210856, Train acc: 0.7863005050505051\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.571744196166761, Train acc: 0.7867892976588629\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5711443211400399, Train acc: 0.7867476851851852\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5709083598521021, Train acc: 0.7871581196581197\n",
      "Val loss: 0.5501550436019897, Val acc: 0.796\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6047369389452486, Train acc: 0.7676282051282052\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5764442345398104, Train acc: 0.7796474358974359\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5721870333465755, Train acc: 0.7836538461538461\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5688286822321068, Train acc: 0.7852564102564102\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5720339554242598, Train acc: 0.7866452991452991\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5728408745635948, Train acc: 0.7863247863247863\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5732506633314312, Train acc: 0.7868589743589743\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5685972740921454, Train acc: 0.7883947649572649\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5644404774731607, Train acc: 0.7905389363722697\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5596355773253828, Train acc: 0.7920405982905983\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5600137736332815, Train acc: 0.7920551670551671\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5618059382617389, Train acc: 0.7913105413105413\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5615442697397194, Train acc: 0.7913790269559501\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5645931485781085, Train acc: 0.7899496336996337\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5617008780227427, Train acc: 0.7907051282051282\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5594673130350808, Train acc: 0.7908987713675214\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5582806855822221, Train acc: 0.7911639014580191\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5587415892917377, Train acc: 0.7910434472934473\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5571090798394347, Train acc: 0.7911887089518669\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5575392458906285, Train acc: 0.7911057692307693\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5570701459224844, Train acc: 0.7914377289377289\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5577981837915883, Train acc: 0.791253885003885\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5575136243604851, Train acc: 0.7914111854329245\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5562718057888228, Train acc: 0.7922453703703703\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5564384358943019, Train acc: 0.7925\n",
      "Val loss: 0.5348670482635498, Val acc: 0.816\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5728554017523415, Train acc: 0.780982905982906\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5497376494682752, Train acc: 0.7908653846153846\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5549768698521149, Train acc: 0.7907763532763533\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.556012120989398, Train acc: 0.7930021367521367\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.552134403713748, Train acc: 0.7957264957264957\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5456831739774967, Train acc: 0.7977207977207977\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5469772271784671, Train acc: 0.7965506715506715\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5437346612875406, Train acc: 0.7967414529914529\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.547383248883831, Train acc: 0.7965337132003799\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5491431588482144, Train acc: 0.7955662393162393\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5463647501370938, Train acc: 0.7964257964257965\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5487535907024587, Train acc: 0.7950053418803419\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5495418045500796, Train acc: 0.7949539776462854\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5481340960896277, Train acc: 0.7961309523809523\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5469582817535794, Train acc: 0.7961182336182336\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5463644275163165, Train acc: 0.7958400106837606\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5453335341157897, Train acc: 0.7964429361488184\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5457675650836112, Train acc: 0.7963259734093068\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5458946290132274, Train acc: 0.7961369770580297\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5458539610394301, Train acc: 0.7955395299145299\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5455541446453244, Train acc: 0.7956349206349206\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5455122950610581, Train acc: 0.7954545454545454\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5468198373077616, Train acc: 0.795127276105537\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5464426162321003, Train acc: 0.7953614672364673\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5475957528902934, Train acc: 0.79508547008547\n",
      "Val loss: 0.5491114258766174, Val acc: 0.798\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5188159640782919, Train acc: 0.8146367521367521\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5296824022363393, Train acc: 0.8012820512820513\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5396087863761135, Train acc: 0.7955840455840456\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5320415471990904, Train acc: 0.7988782051282052\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5340106517331213, Train acc: 0.8002136752136753\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5345525394307922, Train acc: 0.8005698005698005\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5343884163250707, Train acc: 0.8017399267399268\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5307960536442379, Train acc: 0.8043536324786325\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5326827154525777, Train acc: 0.8045465337132004\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5342978348780392, Train acc: 0.8037393162393163\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5331602930131122, Train acc: 0.8033216783216783\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.532724751488563, Train acc: 0.8028846153846154\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5334544916741947, Train acc: 0.8028846153846154\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5341369599076636, Train acc: 0.801510989010989\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5336331856344161, Train acc: 0.8014957264957265\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5350659454726956, Train acc: 0.8010149572649573\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5329412044390713, Train acc: 0.8011563599798894\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5345407607315102, Train acc: 0.8010743114909782\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5342953152187064, Train acc: 0.8007478632478633\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5343040096740692, Train acc: 0.8010683760683761\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.536027114466894, Train acc: 0.8003917378917379\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5354719340552307, Train acc: 0.8001651126651127\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5350202797088026, Train acc: 0.8001207729468599\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5361331313068414, Train acc: 0.8001246438746439\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5370902831253842, Train acc: 0.7992948717948718\n",
      "Val loss: 0.574661910533905, Val acc: 0.78\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5351684218288487, Train acc: 0.7986111111111112\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.536039377634342, Train acc: 0.8012820512820513\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5323254341455946, Train acc: 0.8012820512820513\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5367241817502639, Train acc: 0.8012820512820513\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5414123743135705, Train acc: 0.8007478632478633\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.541188004721179, Train acc: 0.801727207977208\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5407146113641533, Train acc: 0.8015873015873016\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5370940589385792, Train acc: 0.8014823717948718\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5340965943727625, Train acc: 0.8026471984805318\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5299385788858446, Train acc: 0.8045405982905983\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5255165991620121, Train acc: 0.8057012432012433\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5273179537550337, Train acc: 0.8047987891737892\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5262308529211376, Train acc: 0.8049802761341223\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5285982087718479, Train acc: 0.8042582417582418\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5284825063711218, Train acc: 0.8039529914529915\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5295425031740123, Train acc: 0.803886217948718\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5290807909128533, Train acc: 0.8040472599296129\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5285544403583688, Train acc: 0.8046949192782527\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5280252411807382, Train acc: 0.8049651372019793\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5266252396962582, Train acc: 0.8053952991452992\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5252406993561709, Train acc: 0.8061914936914937\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5250394308176861, Train acc: 0.8063325563325563\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5256879075597494, Train acc: 0.8057181345224823\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.526503369628725, Train acc: 0.8050881410256411\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5255501655100757, Train acc: 0.8053418803418804\n",
      "Val loss: 0.5473777651786804, Val acc: 0.798\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5293068622167294, Train acc: 0.8066239316239316\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5239142214513233, Train acc: 0.8079594017094017\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5070616861439159, Train acc: 0.8125\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5146487450115701, Train acc: 0.8096955128205128\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.524373124832781, Train acc: 0.805982905982906\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5240093693986237, Train acc: 0.8050213675213675\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5196853923964996, Train acc: 0.806013431013431\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5168880037485789, Train acc: 0.8060229700854701\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5112907340412239, Train acc: 0.8093542260208927\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5154549021624092, Train acc: 0.8087606837606838\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5122326678699918, Train acc: 0.8102661227661228\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5159138538622619, Train acc: 0.8084935897435898\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5162488266600346, Train acc: 0.8083086785009862\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5152345348618043, Train acc: 0.8088751526251526\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5152673646722764, Train acc: 0.809579772079772\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5149814867669255, Train acc: 0.8095953525641025\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5155462863391042, Train acc: 0.8088863750628457\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5152597451814276, Train acc: 0.8087606837606838\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5149337899608489, Train acc: 0.8090699505173189\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5141209061106301, Train acc: 0.809241452991453\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5133493377557604, Train acc: 0.8092185592185592\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5141585167546604, Train acc: 0.8090034965034965\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5134926644987619, Train acc: 0.8090626161278335\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5136263125557505, Train acc: 0.8091835826210826\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5131563782080626, Train acc: 0.8093803418803419\n",
      "Val loss: 0.565136730670929, Val acc: 0.812\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5073892670309442, Train acc: 0.8071581196581197\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5076932337166916, Train acc: 0.8087606837606838\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4999968999641234, Train acc: 0.811965811965812\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5029978525753205, Train acc: 0.8094284188034188\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5024358946543473, Train acc: 0.8099358974358974\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.49955454143958217, Train acc: 0.8109864672364673\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5013996893596883, Train acc: 0.8104395604395604\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.501013261735694, Train acc: 0.8114316239316239\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5036967537115555, Train acc: 0.8106600189933523\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5032495597234139, Train acc: 0.8103098290598291\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5055882067252429, Train acc: 0.8091491841491841\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.504014971307837, Train acc: 0.8108084045584045\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5023737553553626, Train acc: 0.8115959894806049\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5037008303425687, Train acc: 0.8107066544566545\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.503553069291631, Train acc: 0.8111823361823362\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5034560843362895, Train acc: 0.8110309829059829\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5045062683151857, Train acc: 0.8112745098039216\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5043509473558958, Train acc: 0.8116393637226971\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5044178015749786, Train acc: 0.8123594242015295\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5048665241171152, Train acc: 0.8121527777777777\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5057177318394257, Train acc: 0.8114570614570614\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5056384418973093, Train acc: 0.8116501554001554\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.504163369198618, Train acc: 0.8119425863991081\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5048897519569706, Train acc: 0.811698717948718\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.505673965806635, Train acc: 0.8109615384615385\n",
      "Val loss: 0.5130141973495483, Val acc: 0.822\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.477400840475009, Train acc: 0.8221153846153846\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4869245667106066, Train acc: 0.8194444444444444\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.48561929072919396, Train acc: 0.8238960113960114\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4945125384017443, Train acc: 0.8166399572649573\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4947303858322975, Train acc: 0.8176282051282051\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4895087564178342, Train acc: 0.8193554131054132\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.48862559260102273, Train acc: 0.8193681318681318\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4882040856899614, Train acc: 0.8194444444444444\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.48727570836417244, Train acc: 0.8193850902184235\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.487361218003381, Train acc: 0.819551282051282\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4876468396328083, Train acc: 0.818958818958819\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.48874186041454476, Train acc: 0.817619301994302\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4893853610074105, Train acc: 0.8179240631163708\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.48940657267531196, Train acc: 0.817269536019536\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.49185244023290453, Train acc: 0.8167378917378917\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4943430120220933, Train acc: 0.8160056089743589\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.49437728720492846, Train acc: 0.8155794369029663\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4929083104529272, Train acc: 0.8164767331433999\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4927850130223177, Train acc: 0.8166048133153396\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.49341794756742624, Train acc: 0.8165331196581197\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.49374257253106163, Train acc: 0.816468253968254\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.49221081617805695, Train acc: 0.8169434731934732\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.49262741606216015, Train acc: 0.8165876997398737\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.49191367102420736, Train acc: 0.8169738247863247\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4921571973704884, Train acc: 0.8169871794871795\n",
      "Val loss: 0.5292719602584839, Val acc: 0.816\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4819687391575585, Train acc: 0.8194444444444444\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4853521191602589, Train acc: 0.8167735042735043\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4793219610921338, Train acc: 0.8244301994301995\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4767886134835645, Train acc: 0.8243856837606838\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4811928610006968, Train acc: 0.822542735042735\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.48503115030265603, Train acc: 0.8212250712250713\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.48951243846626075, Train acc: 0.820054945054945\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.49222814256691527, Train acc: 0.8193108974358975\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4920021954708063, Train acc: 0.8190883190883191\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4914705090224743, Train acc: 0.8186431623931624\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.49234577102793586, Train acc: 0.8179390054390054\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.49082623549506194, Train acc: 0.8178418803418803\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.49260128480011817, Train acc: 0.8179651545036161\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4902953658985276, Train acc: 0.8189102564102564\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.48804974356862557, Train acc: 0.8192663817663818\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.48781196740018123, Train acc: 0.8189102564102564\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4878843040390492, Train acc: 0.8192873303167421\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.488060548367352, Train acc: 0.81914767331434\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4867938176916893, Train acc: 0.8198099415204678\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4864896396190947, Train acc: 0.8197115384615384\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.48659430698658035, Train acc: 0.8192663817663818\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4866933348359252, Train acc: 0.8190559440559441\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4854689830383241, Train acc: 0.8200250836120402\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4856104594933936, Train acc: 0.8199786324786325\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4863620901770062, Train acc: 0.8197222222222222\n",
      "Val loss: 0.4986439049243927, Val acc: 0.828\n",
      "Epoch 25/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4612785731880074, Train acc: 0.8247863247863247\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4596350026500021, Train acc: 0.8253205128205128\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4612674848773541, Train acc: 0.8265669515669516\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4578960087054815, Train acc: 0.8277243589743589\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.45797180007410865, Train acc: 0.8285256410256411\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4657835850327613, Train acc: 0.8255876068376068\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.47236857737013127, Train acc: 0.8231837606837606\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.47341656341002536, Train acc: 0.8231169871794872\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4752893467899854, Train acc: 0.8225308641975309\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4750590031791447, Train acc: 0.8219551282051282\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.47651448682031616, Train acc: 0.8208527583527584\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.475751016018355, Train acc: 0.8214476495726496\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.47460732333739286, Train acc: 0.8220332018408941\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4742735050600454, Train acc: 0.8232600732600732\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.47590928244539815, Train acc: 0.8223290598290598\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4750820046895717, Train acc: 0.8226829594017094\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4741867498207656, Train acc: 0.8233094519859225\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4743591845636977, Train acc: 0.8233024691358025\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4752749483018537, Train acc: 0.8227620332883491\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4748156043772514, Train acc: 0.8231036324786325\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.475166373885789, Train acc: 0.8228021978021978\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.47562682608369716, Train acc: 0.8226495726495726\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4757318568136647, Train acc: 0.8225798959494611\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.47458189770451975, Train acc: 0.8229834401709402\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4747962417663672, Train acc: 0.8230555555555555\n",
      "Val loss: 0.5002204775810242, Val acc: 0.842\n",
      "Epoch 26/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5028019820013617, Train acc: 0.8103632478632479\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.470998908018964, Train acc: 0.8250534188034188\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4849418565163925, Train acc: 0.823539886039886\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4834466576257832, Train acc: 0.8243856837606838\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.48116826384495465, Train acc: 0.823076923076923\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.48193360258031775, Train acc: 0.8214921652421653\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.47728674782392305, Train acc: 0.8236416361416361\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.47813843339522427, Train acc: 0.8232505341880342\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4744700585321257, Train acc: 0.824133428300095\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4731768768758346, Train acc: 0.8243589743589743\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.47119516009217377, Train acc: 0.8256604506604507\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4740414564150181, Train acc: 0.8248753561253561\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.47522346384426545, Train acc: 0.8246219592373438\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.47600864752745015, Train acc: 0.8249007936507936\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4750941305163919, Train acc: 0.8254273504273504\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4743850177281305, Train acc: 0.8256877670940171\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.47389418638160086, Train acc: 0.8260746606334841\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4734434582936911, Train acc: 0.8259140550807218\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4730626276509482, Train acc: 0.8252080521817364\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.470689886674667, Train acc: 0.8259615384615384\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.47063799553592855, Train acc: 0.825956450956451\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.47077227192811477, Train acc: 0.8257090132090132\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4710302177535606, Train acc: 0.8252508361204013\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4701614462560908, Train acc: 0.8255430911680912\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4710280923405264, Train acc: 0.8254273504273504\n",
      "Val loss: 0.5328510403633118, Val acc: 0.814\n",
      "Epoch 27/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.47088172955390734, Train acc: 0.8376068376068376\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4509639530960057, Train acc: 0.843215811965812\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4587025065983293, Train acc: 0.8363603988603988\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4585644026708781, Train acc: 0.8333333333333334\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4610415510705903, Train acc: 0.8321581196581197\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4643330760883894, Train acc: 0.8303952991452992\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.46246036446411093, Train acc: 0.8315018315018315\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.46394232160244614, Train acc: 0.8315304487179487\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4587554272373914, Train acc: 0.833511396011396\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4574136864967071, Train acc: 0.8332264957264958\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.459544939840706, Train acc: 0.8322649572649573\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.460504717458878, Train acc: 0.8315972222222222\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.46114576077682656, Train acc: 0.8307034845496384\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4619752759038856, Train acc: 0.8303189865689866\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.46090502071372125, Train acc: 0.8306980056980057\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.46272020142835874, Train acc: 0.8301615918803419\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.46263344924711536, Train acc: 0.8299710910005028\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.462476061791507, Train acc: 0.8296236942070275\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4617023025959851, Train acc: 0.8302687809266757\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.46162310378132465, Train acc: 0.8298076923076924\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4614951551630699, Train acc: 0.8302553927553927\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.46071178144352004, Train acc: 0.8304924242424242\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.460580843709484, Train acc: 0.830685618729097\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.46163436838497335, Train acc: 0.8301059472934473\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.46064261633234144, Train acc: 0.8307478632478632\n",
      "Val loss: 0.5207080841064453, Val acc: 0.828\n",
      "Epoch 28/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4593505265875759, Train acc: 0.8274572649572649\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4584754778024478, Train acc: 0.8277243589743589\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4499050895033399, Train acc: 0.8331552706552706\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4573805431684113, Train acc: 0.8311965811965812\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4570757984223529, Train acc: 0.8313034188034188\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4586423917692102, Train acc: 0.8308404558404558\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.45661961025261616, Train acc: 0.8302808302808303\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4532723855943634, Train acc: 0.8316639957264957\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4512665090695066, Train acc: 0.8326804368471035\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4551176055501669, Train acc: 0.8314102564102565\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.45341879920780703, Train acc: 0.8322649572649573\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.45349604476699634, Train acc: 0.8324875356125356\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.45502332822321123, Train acc: 0.8320595003287311\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4544742296566966, Train acc: 0.8323412698412699\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4584904557491979, Train acc: 0.8306267806267806\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.46058842045867926, Train acc: 0.8297275641025641\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.46025155390279865, Train acc: 0.8305052790346908\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.46004857934112786, Train acc: 0.8308701329534662\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.45932043727204114, Train acc: 0.8311122357174989\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.457524656278328, Train acc: 0.8314903846153846\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4559267494413588, Train acc: 0.8318579568579568\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.45504383602712906, Train acc: 0.8324349261849262\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.45520339125164466, Train acc: 0.832427536231884\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.45496782770067284, Train acc: 0.8326210826210826\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4538977771004041, Train acc: 0.8328846153846153\n",
      "Val loss: 0.5133852958679199, Val acc: 0.824\n",
      "Epoch 29/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.43288846460417807, Train acc: 0.8344017094017094\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.43482817104484284, Train acc: 0.8357371794871795\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4353063700248373, Train acc: 0.8395655270655271\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4361001546375262, Train acc: 0.8414797008547008\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.43700978640053006, Train acc: 0.8408119658119658\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.44367745733074315, Train acc: 0.8380519943019943\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4435440340103247, Train acc: 0.837530525030525\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.44264754858345556, Train acc: 0.8371394230769231\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.44235529765444265, Train acc: 0.8363010446343779\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.44274777765584805, Train acc: 0.8368055555555556\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.448045141769178, Train acc: 0.8346445221445221\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.44768562956986435, Train acc: 0.8346242877492878\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4472449113327684, Train acc: 0.8349358974358975\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.44640228433377577, Train acc: 0.8346688034188035\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4476653928515578, Train acc: 0.834508547008547\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.44569506163064104, Train acc: 0.8357705662393162\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.44644848547235333, Train acc: 0.8355957767722474\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4472821879307655, Train acc: 0.8354700854700855\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4492909218135633, Train acc: 0.8342611336032388\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4501804926456549, Train acc: 0.8340010683760684\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.44980353066348233, Train acc: 0.834045584045584\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4494811829176631, Train acc: 0.8344017094017094\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4496543953013172, Train acc: 0.8339836492010405\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4490322361851477, Train acc: 0.8344462250712251\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4495195831829666, Train acc: 0.8344230769230769\n",
      "Val loss: 0.49946141242980957, Val acc: 0.838\n",
      "Early stopping at epoch 29 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 707.6255 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 1.6132810156569521, Train acc: 0.20085470085470086\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.610903199411865, Train acc: 0.20512820512820512\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.605242301935484, Train acc: 0.21474358974358973\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.5883614301172078, Train acc: 0.234375\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.5747715695291502, Train acc: 0.2467948717948718\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 1.5606463437746054, Train acc: 0.26086182336182334\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 1.5470619863933988, Train acc: 0.2719017094017094\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 1.5275169568948257, Train acc: 0.2877938034188034\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 1.5087966106669288, Train acc: 0.30662393162393164\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 1.4872920031730945, Train acc: 0.32553418803418804\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 1.47021041651891, Train acc: 0.33916083916083917\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 1.4585356600688733, Train acc: 0.3505608974358974\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 1.4482488064370918, Train acc: 0.360741288625904\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 1.4359814281574244, Train acc: 0.3708409645909646\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 1.421905793151964, Train acc: 0.38073361823361823\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 1.4101258599095874, Train acc: 0.3891559829059829\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 1.4002918634898942, Train acc: 0.39809577677224733\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 1.3897010876752034, Train acc: 0.4058641975308642\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 1.3821915034447712, Train acc: 0.41222447143499774\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 1.3722598663762084, Train acc: 0.4188568376068376\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 1.3621187581975236, Train acc: 0.4249084249084249\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 1.3512925056403426, Train acc: 0.4310897435897436\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 1.3414848135865254, Train acc: 0.43687290969899667\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 1.3336645624364203, Train acc: 0.4417957621082621\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 1.3271845824901873, Train acc: 0.44735042735042735\n",
      "Val loss: 0.9938944578170776, Val acc: 0.634\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 1.0759713608994443, Train acc: 0.5950854700854701\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.0606442012338557, Train acc: 0.59375\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.063730314076796, Train acc: 0.5993589743589743\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.0692791691702654, Train acc: 0.5948183760683761\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.0725258664188222, Train acc: 0.5961538461538461\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 1.0668507135152137, Train acc: 0.5973112535612536\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 1.0614343134183732, Train acc: 0.6002747252747253\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 1.0574101004590335, Train acc: 0.6032986111111112\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 1.0548114379247029, Train acc: 0.6047602089268755\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 1.0531016637626875, Train acc: 0.6052884615384615\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 1.048037772643631, Train acc: 0.6075660450660451\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 1.04509010359093, Train acc: 0.6090633903133903\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 1.0431915442462347, Train acc: 0.6097961867192636\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 1.0385316288296556, Train acc: 0.6109966422466423\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 1.0333997027826445, Train acc: 0.6136039886039886\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 1.0299869038355656, Train acc: 0.6140491452991453\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 1.028032392068626, Train acc: 0.6154788838612368\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 1.0250222092566894, Train acc: 0.617818138651472\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 1.0213351700952662, Train acc: 0.6197143499775079\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 1.0184684147182692, Train acc: 0.6201121794871794\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 1.0167167156413168, Train acc: 0.6200142450142451\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 1.0129185882574288, Train acc: 0.6206536519036518\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 1.011002940101191, Train acc: 0.6215161649944259\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 1.0082074700087904, Train acc: 0.6226629273504274\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 1.0050662689738803, Train acc: 0.6238675213675213\n",
      "Val loss: 0.801070511341095, Val acc: 0.704\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.893849400883047, Train acc: 0.6575854700854701\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.9066831744634188, Train acc: 0.655982905982906\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.9056217590628186, Train acc: 0.657051282051282\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.9048163393496448, Train acc: 0.6577190170940171\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.9052411207785973, Train acc: 0.6547008547008547\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.9026917342276994, Train acc: 0.6549145299145299\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.9016475372189277, Train acc: 0.6576617826617827\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.900836887013199, Train acc: 0.6581864316239316\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.9001510344792408, Train acc: 0.6591286799620133\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.896218027951371, Train acc: 0.6607371794871795\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.8958270479841759, Train acc: 0.6602564102564102\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.8966475416325096, Train acc: 0.6599448005698005\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.8949515136310257, Train acc: 0.6606262327416174\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.8910240476202761, Train acc: 0.6617445054945055\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.8873569513148392, Train acc: 0.6636396011396012\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.8851748611778021, Train acc: 0.664863782051282\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.8838628997839894, Train acc: 0.6663524384112619\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.8813522771665966, Train acc: 0.6680318138651472\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.8796031062955852, Train acc: 0.6684379217273954\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.8773938990938358, Train acc: 0.6693108974358974\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.8759021828535567, Train acc: 0.6703551078551079\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.8722079788980877, Train acc: 0.6718871406371406\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.8704358408859519, Train acc: 0.6728446674098848\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.8689635633000111, Train acc: 0.6738559472934473\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.8669327681594424, Train acc: 0.6738675213675214\n",
      "Val loss: 0.7518784403800964, Val acc: 0.728\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.8538718022342421, Train acc: 0.6981837606837606\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.8213231479510282, Train acc: 0.7037927350427351\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.8245328749686565, Train acc: 0.7012108262108262\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.8355020550835845, Train acc: 0.6940438034188035\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.835976386274028, Train acc: 0.6919871794871795\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.8308920497313524, Train acc: 0.6924857549857549\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.8227315170278771, Train acc: 0.6958943833943834\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.8219841040161431, Train acc: 0.6972489316239316\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.8166228771945576, Train acc: 0.6983024691358025\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.8130017849115225, Train acc: 0.6985042735042735\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.81403704593139, Train acc: 0.697940947940948\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.8121056682788409, Train acc: 0.6987624643874644\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.8121161705016463, Train acc: 0.6985535831689678\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.810483741400006, Train acc: 0.6986034798534798\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.8115962464755078, Train acc: 0.6974358974358974\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.812029181414435, Train acc: 0.6978498931623932\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.810218308920714, Train acc: 0.6980580693815988\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.8088723721808744, Train acc: 0.6983618233618234\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.8073886573797295, Train acc: 0.6984086819613136\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.8070828322926138, Train acc: 0.6989316239316239\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.8045686195670079, Train acc: 0.7001678876678876\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.8044762059687272, Train acc: 0.6998106060606061\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.8033915078803892, Train acc: 0.7000650315867707\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.801917900195998, Train acc: 0.7009214743589743\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7998448951835306, Train acc: 0.7013461538461538\n",
      "Val loss: 0.6814116835594177, Val acc: 0.736\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7868044192974384, Train acc: 0.7067307692307693\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.766648889097393, Train acc: 0.7112713675213675\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.7759863907455379, Train acc: 0.7113603988603988\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.7772917353954071, Train acc: 0.7096688034188035\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7725548039644192, Train acc: 0.7100427350427351\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.7676570520781384, Train acc: 0.7114494301994302\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.7650948015470354, Train acc: 0.7136752136752137\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.7656428821894348, Train acc: 0.7138755341880342\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.7658888061105469, Train acc: 0.71409069325736\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.7654350175052627, Train acc: 0.7145833333333333\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.7611124211952681, Train acc: 0.716977466977467\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7609894699350721, Train acc: 0.7165687321937322\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.7589905880572841, Train acc: 0.7171268902038133\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.7605110284245785, Train acc: 0.7161935286935287\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.7599130383753708, Train acc: 0.7167022792022792\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.760905772431666, Train acc: 0.7158453525641025\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.7592857394792615, Train acc: 0.7161890397184515\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.7580744058799427, Train acc: 0.7159603513770181\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.7571970338468555, Train acc: 0.7162899235267657\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.7566073721035933, Train acc: 0.7166399572649572\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.7551222385055514, Train acc: 0.7172619047619048\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.7545264757700734, Train acc: 0.7174873737373737\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.7544120011310602, Train acc: 0.7178325901151988\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.7537266441230845, Train acc: 0.7180377492877493\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7547839346044084, Train acc: 0.7178846153846153\n",
      "Val loss: 0.6658253073692322, Val acc: 0.746\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.737519630509564, Train acc: 0.7216880341880342\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.7483807035490998, Train acc: 0.719284188034188\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.7404630823522551, Train acc: 0.7193732193732194\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.7344012301829126, Train acc: 0.7199519230769231\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7252097300484649, Train acc: 0.7251068376068376\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.727081249369855, Train acc: 0.7239138176638177\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.7277609796168895, Train acc: 0.7232142857142857\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.7307176999429352, Train acc: 0.7237580128205128\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.730939769252413, Train acc: 0.7239434947768281\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.7336040532741791, Train acc: 0.7237179487179487\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.7307036551006468, Train acc: 0.7242618492618492\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.7276135710866702, Train acc: 0.7263621794871795\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.7284345949738532, Train acc: 0.7266190006574622\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.7272757476102447, Train acc: 0.7275259462759462\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.7275719194500535, Train acc: 0.7270655270655271\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.725069439977917, Train acc: 0.7283653846153846\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.724363740542955, Train acc: 0.7286010558069381\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.7241905569633981, Train acc: 0.7285137701804368\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.7234647923152939, Train acc: 0.728969860548808\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.7230130788098034, Train acc: 0.7291399572649573\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.7238261820608201, Train acc: 0.7282254782254782\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.7220251056443098, Train acc: 0.7291666666666666\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.7221979819811307, Train acc: 0.7287718320327016\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.7218768665742161, Train acc: 0.7286324786324786\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.7203202637036642, Train acc: 0.7292094017094017\n",
      "Val loss: 0.6645063161849976, Val acc: 0.758\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7169306127943544, Train acc: 0.7259615384615384\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6929818970015925, Train acc: 0.7395833333333334\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6879551798699588, Train acc: 0.7425213675213675\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6953974679812917, Train acc: 0.7389155982905983\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6949006274979339, Train acc: 0.7387820512820513\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6969376802019923, Train acc: 0.7410968660968661\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.693448708589406, Train acc: 0.7425976800976801\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.693788022105383, Train acc: 0.7423210470085471\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6957250197134127, Train acc: 0.7410968660968661\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.694996974356154, Train acc: 0.7422008547008547\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6963418429918593, Train acc: 0.7429098679098679\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.695356967219515, Train acc: 0.7429220085470085\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6958704067934194, Train acc: 0.7422748191978962\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6936825563328316, Train acc: 0.7429029304029304\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.694089789132447, Train acc: 0.7433048433048433\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6942882042091626, Train acc: 0.7437566773504274\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6917517220797162, Train acc: 0.7444067370537959\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6911557212415358, Train acc: 0.7448658594491928\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6893584910634397, Train acc: 0.7455015744489428\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6885143369245224, Train acc: 0.7457264957264957\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6893912617285464, Train acc: 0.7452940577940578\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6887174506194849, Train acc: 0.7458964646464646\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6893464872237698, Train acc: 0.7457032701597919\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6880764812602531, Train acc: 0.7463051994301995\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6861427580444222, Train acc: 0.7470299145299145\n",
      "Val loss: 0.617406964302063, Val acc: 0.778\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6839783874332396, Train acc: 0.7526709401709402\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6732995036957611, Train acc: 0.7521367521367521\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6588304586644865, Train acc: 0.7564102564102564\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6586206391071662, Train acc: 0.7596153846153846\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6636542653426146, Train acc: 0.7569444444444444\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6634435573874036, Train acc: 0.7552528490028491\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6563655330875708, Train acc: 0.7583180708180708\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6563870537803214, Train acc: 0.757278311965812\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6578061512307903, Train acc: 0.7564696106362773\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6543209991011865, Train acc: 0.7563568376068376\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6537152006353809, Train acc: 0.7561674436674437\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6555902491246703, Train acc: 0.7544960826210826\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6529866206771366, Train acc: 0.7555473372781065\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6528194291978819, Train acc: 0.7558760683760684\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6524665818424986, Train acc: 0.7561253561253561\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6530581577243204, Train acc: 0.7558092948717948\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6534378332609864, Train acc: 0.7561902966314731\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6549614190008113, Train acc: 0.7563509021842355\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.654747707547232, Train acc: 0.7567195231668916\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6535520563777696, Train acc: 0.7569978632478632\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6534537512765068, Train acc: 0.7563085063085063\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6544499623168784, Train acc: 0.7557789432789432\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6546110474500724, Train acc: 0.7560850984764028\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6533210796126273, Train acc: 0.7567663817663818\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6534144685716711, Train acc: 0.7571367521367521\n",
      "Val loss: 0.6032238006591797, Val acc: 0.772\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6450211808212802, Train acc: 0.7735042735042735\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6533247789001873, Train acc: 0.7622863247863247\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6443269087208642, Train acc: 0.7647792022792023\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6421121164965324, Train acc: 0.7649572649572649\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6391561740483993, Train acc: 0.76741452991453\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6407393433174856, Train acc: 0.7656695156695157\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6346580400171443, Train acc: 0.768925518925519\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.635633228919827, Train acc: 0.7687633547008547\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6380199633347683, Train acc: 0.7665598290598291\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6397215090118922, Train acc: 0.7661858974358975\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6429871536195046, Train acc: 0.7658313908313908\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6418182493062781, Train acc: 0.766159188034188\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6420461873056699, Train acc: 0.7645463510848126\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6395927142620232, Train acc: 0.7650335775335775\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6391138193614123, Train acc: 0.7650641025641025\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.638140958479136, Train acc: 0.7648904914529915\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6379078558232689, Train acc: 0.764894419306184\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6358613957706679, Train acc: 0.7653430674264008\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6358398460572333, Train acc: 0.765210301394512\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6349447526483454, Train acc: 0.7657051282051283\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6347017329933685, Train acc: 0.7659493284493285\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6353706676214818, Train acc: 0.7660742035742035\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6358202015686903, Train acc: 0.7653753251579338\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6360236871138513, Train acc: 0.7652466168091168\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.635114193079818, Train acc: 0.7654273504273504\n",
      "Val loss: 0.6175529956817627, Val acc: 0.77\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6468950559695562, Train acc: 0.7548076923076923\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6283916946914461, Train acc: 0.7609508547008547\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6333772102397391, Train acc: 0.7576566951566952\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6321727150780523, Train acc: 0.7624198717948718\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6285875339283903, Train acc: 0.7639957264957264\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6176138075753155, Train acc: 0.7670940170940171\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6229139142493479, Train acc: 0.7649572649572649\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6253600601170562, Train acc: 0.7650240384615384\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6206345465476469, Train acc: 0.7664411206077872\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6222046418449818, Train acc: 0.7662393162393163\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6224015273715325, Train acc: 0.7657342657342657\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6214779038884362, Train acc: 0.7665598290598291\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6202673469422132, Train acc: 0.7669707429322814\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6179513696203592, Train acc: 0.7671321733821734\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6161203758244501, Train acc: 0.7679843304843305\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6147444625265706, Train acc: 0.7691639957264957\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6160282134455374, Train acc: 0.769482151835093\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6180267120726541, Train acc: 0.7687262583095916\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6168325852621154, Train acc: 0.7692026540710751\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6166677775482337, Train acc: 0.7689903846153846\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6171319722510665, Train acc: 0.7689000814000814\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6169698629890104, Train acc: 0.7689393939393939\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6154686055775692, Train acc: 0.769253994797473\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6142741824338409, Train acc: 0.7703436609686609\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6148626954942686, Train acc: 0.7705555555555555\n",
      "Val loss: 0.5813387632369995, Val acc: 0.782\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5962356713592497, Train acc: 0.781517094017094\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6194206760225133, Train acc: 0.7719017094017094\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6078015324312058, Train acc: 0.7747507122507122\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6037583288410281, Train acc: 0.7727029914529915\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6012804332961385, Train acc: 0.7738247863247864\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6062662601895482, Train acc: 0.7721688034188035\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6036126953669083, Train acc: 0.7728174603174603\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6054420860118082, Train acc: 0.7724358974358975\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6034524830346547, Train acc: 0.7753442545109211\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6024113208437577, Train acc: 0.7758547008547009\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.602703759859214, Train acc: 0.7753496503496503\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6032877732227501, Train acc: 0.7750623219373219\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6035353736333515, Train acc: 0.7755177514792899\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.603687343466231, Train acc: 0.7767094017094017\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6022223236129495, Train acc: 0.7777065527065528\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6020839581759567, Train acc: 0.7778445512820513\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6008082722871372, Train acc: 0.7781548516842635\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5994561245171433, Train acc: 0.7783416429249762\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5992751663405779, Train acc: 0.7788461538461539\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5973089834436393, Train acc: 0.77946047008547\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.598279579455017, Train acc: 0.778998778998779\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.598770289559557, Train acc: 0.7784090909090909\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5986611701074651, Train acc: 0.7788693794128577\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.597652972365419, Train acc: 0.7792467948717948\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5961531136535172, Train acc: 0.7799358974358974\n",
      "Val loss: 0.5522910356521606, Val acc: 0.792\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5880501514826065, Train acc: 0.7900641025641025\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5723638518625854, Train acc: 0.7905982905982906\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5670044792768283, Train acc: 0.7936253561253561\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5753369370523171, Train acc: 0.7889957264957265\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5772006753927622, Train acc: 0.7881410256410256\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.572345511377537, Train acc: 0.7896189458689459\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5731317334522985, Train acc: 0.7893772893772893\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5722202570615416, Train acc: 0.7889957264957265\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5767589511468313, Train acc: 0.7869183285849952\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5786528244487241, Train acc: 0.7865918803418803\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.578692761936573, Train acc: 0.7862276612276612\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5804430473903646, Train acc: 0.786102207977208\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5788205110423271, Train acc: 0.7867357001972387\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5805221226280134, Train acc: 0.7861340048840049\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5785990944020768, Train acc: 0.7869301994301995\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5797174039702767, Train acc: 0.7867588141025641\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5819453336054983, Train acc: 0.7857277526395173\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5816768406548052, Train acc: 0.7864434947768281\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5799822555473674, Train acc: 0.7873931623931624\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5795421467887031, Train acc: 0.7879006410256411\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5787553406211725, Train acc: 0.7884106634106635\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.577512975452177, Train acc: 0.7890685703185704\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5772588487779594, Train acc: 0.7885544407283538\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5774029274670105, Train acc: 0.7884170227920227\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5759213745211944, Train acc: 0.7886538461538461\n",
      "Val loss: 0.5671387910842896, Val acc: 0.772\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5446590800315906, Train acc: 0.7975427350427351\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5542770923457594, Train acc: 0.7927350427350427\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5510641464225927, Train acc: 0.7945156695156695\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5510847676449862, Train acc: 0.797409188034188\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5531587002623795, Train acc: 0.7963675213675213\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5594161593931013, Train acc: 0.7953169515669516\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5573488866162096, Train acc: 0.7960164835164835\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5545187780522128, Train acc: 0.7970753205128205\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.555058456044591, Train acc: 0.797602089268756\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5589446393088398, Train acc: 0.7955128205128205\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5582575867388169, Train acc: 0.7960858585858586\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5600261933825634, Train acc: 0.7958956552706553\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5613540672574052, Train acc: 0.7953648915187377\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5599756760202048, Train acc: 0.7960546398046398\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5617500277750852, Train acc: 0.7955128205128205\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5607996623183036, Train acc: 0.7957064636752137\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5614224325972265, Train acc: 0.7955316742081447\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5617255479033686, Train acc: 0.795613722697056\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5610773596130026, Train acc: 0.7957433648223122\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5609399803460409, Train acc: 0.7955128205128205\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5607718607159858, Train acc: 0.7952787952787953\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.559452058607025, Train acc: 0.7954788267288267\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.560579222760228, Train acc: 0.7944769602378298\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.559678016371091, Train acc: 0.7950053418803419\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5590090261590787, Train acc: 0.7947863247863248\n",
      "Val loss: 0.5514252185821533, Val acc: 0.8\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.548018309677768, Train acc: 0.782051282051282\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5529539655161719, Train acc: 0.7860576923076923\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5571576941047299, Train acc: 0.7868589743589743\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5558824344004831, Train acc: 0.7899305555555556\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5509909960194531, Train acc: 0.7935897435897435\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5498226239555581, Train acc: 0.7956730769230769\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5480840171213115, Train acc: 0.7953296703296703\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5453915865375445, Train acc: 0.7956063034188035\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5464007119450927, Train acc: 0.796415004748338\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.547063407403791, Train acc: 0.7957799145299145\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5489091314462237, Train acc: 0.7951631701631702\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5467767755997147, Train acc: 0.7961182336182336\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5453395547931873, Train acc: 0.7973372781065089\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.54725440022091, Train acc: 0.7967032967032966\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5476374001924129, Train acc: 0.7967948717948717\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5468510497544502, Train acc: 0.7971087072649573\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5461578750097913, Train acc: 0.7979826546003017\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5480835175978379, Train acc: 0.7973646723646723\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.547702349652002, Train acc: 0.7979644624381467\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.548017470099223, Train acc: 0.7980235042735043\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5470008568449335, Train acc: 0.7987637362637363\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.545216810771589, Train acc: 0.7993881118881119\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5439440492625044, Train acc: 0.799842066146414\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5432969423334653, Train acc: 0.8004585113960114\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5432146820451459, Train acc: 0.8002991452991453\n",
      "Val loss: 0.5497448444366455, Val acc: 0.81\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.517190885467407, Train acc: 0.8103632478632479\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5241762481184087, Train acc: 0.8079594017094017\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5293136426696071, Train acc: 0.8050213675213675\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5329311999818708, Train acc: 0.8066239316239316\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5336910139164354, Train acc: 0.8042735042735043\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5353173007419136, Train acc: 0.80252849002849\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5330973036396198, Train acc: 0.8047924297924298\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5319071549078466, Train acc: 0.8057558760683761\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5285332298431641, Train acc: 0.806386514719848\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5275973165773938, Train acc: 0.8065705128205128\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5310170633577985, Train acc: 0.8053613053613053\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5304271212415478, Train acc: 0.8053329772079773\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5291020586880941, Train acc: 0.8055966469428008\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.528784991183997, Train acc: 0.8057844932844933\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5268087924630553, Train acc: 0.8070868945868945\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5266115846287491, Train acc: 0.8076589209401709\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5286261760945773, Train acc: 0.8069695827048768\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5281962363680883, Train acc: 0.8074845679012346\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5270095677678509, Train acc: 0.8075798470535313\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5283471745908515, Train acc: 0.8069444444444445\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5277996431091572, Train acc: 0.8074124949124949\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5272216952457248, Train acc: 0.8074494949494949\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5275267230887388, Train acc: 0.8075761798587886\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5272489890132492, Train acc: 0.8077145655270656\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5272793320432687, Train acc: 0.8076923076923077\n",
      "Val loss: 0.5436460375785828, Val acc: 0.812\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5364110976712316, Train acc: 0.7986111111111112\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5260368744150187, Train acc: 0.8052884615384616\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5257149792974152, Train acc: 0.8068019943019943\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5327237918654568, Train acc: 0.8026175213675214\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5275526374578476, Train acc: 0.8064102564102564\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5270789361577428, Train acc: 0.8062678062678063\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.520787035743905, Train acc: 0.8091422466422467\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5205592813814043, Train acc: 0.8099626068376068\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5183935241821485, Train acc: 0.8105413105413105\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5176926180592969, Train acc: 0.8106303418803419\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5158144548801646, Train acc: 0.8116258741258742\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5169752465131191, Train acc: 0.8106303418803419\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.517663936040693, Train acc: 0.8104865220249836\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5197967159833591, Train acc: 0.809981684981685\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5204651357908534, Train acc: 0.8103276353276353\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5177159796858954, Train acc: 0.8117321047008547\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5178105906663437, Train acc: 0.8113373554550025\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5164764863204526, Train acc: 0.811965811965812\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5176859547359365, Train acc: 0.8112910481331534\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5180987785482764, Train acc: 0.8116185897435897\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5171550728674822, Train acc: 0.8119403744403745\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.517177424112099, Train acc: 0.8118444055944056\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5160051649972791, Train acc: 0.8121516164994426\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5168045399620322, Train acc: 0.8114316239316239\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5174645193519756, Train acc: 0.8112393162393162\n",
      "Val loss: 0.5423105955123901, Val acc: 0.824\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5361565604933307, Train acc: 0.813034188034188\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.506245451605218, Train acc: 0.8191773504273504\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5096235074039198, Train acc: 0.8169515669515669\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5008019845709841, Train acc: 0.8193108974358975\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5087161600080311, Train acc: 0.8158119658119658\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5091188994907586, Train acc: 0.8165954415954416\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5104015299979875, Train acc: 0.8157051282051282\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5094286481189167, Train acc: 0.8154380341880342\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5103369340696792, Train acc: 0.814161918328585\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.508630889456751, Train acc: 0.815491452991453\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5062214719839307, Train acc: 0.8160936285936286\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5065961484492165, Train acc: 0.8152599715099715\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5062424217710206, Train acc: 0.8158284023668639\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5044860909800972, Train acc: 0.8160485347985348\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5039842881283529, Train acc: 0.8162037037037037\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.504455777275193, Train acc: 0.8164730235042735\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5052105547451026, Train acc: 0.815893665158371\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5040031960030623, Train acc: 0.8165064102564102\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5042304590920902, Train acc: 0.8164642375168691\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5033021237446457, Train acc: 0.8173076923076923\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5020820400762908, Train acc: 0.8176638176638177\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5016899757874549, Train acc: 0.8177933177933178\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.503934414476566, Train acc: 0.8172844667409885\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5041394514506656, Train acc: 0.8174189814814815\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5031615895771573, Train acc: 0.8178846153846154\n",
      "Val loss: 0.5415366888046265, Val acc: 0.836\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.46252854525024056, Train acc: 0.8242521367521367\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.477995539131837, Train acc: 0.8191773504273504\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.48244592252365204, Train acc: 0.8189102564102564\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.48740111372600764, Train acc: 0.8179754273504274\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4859675601124763, Train acc: 0.8206196581196581\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4890035625727598, Train acc: 0.8190883190883191\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4909554831001348, Train acc: 0.8182234432234432\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4932146056388051, Train acc: 0.8177751068376068\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.49511859559605265, Train acc: 0.8170109211775879\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.49398421668726156, Train acc: 0.8175213675213675\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.49605191419105155, Train acc: 0.817016317016317\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.49863185849242403, Train acc: 0.8158386752136753\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4978306222312706, Train acc: 0.8170200525969756\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.49584769827833397, Train acc: 0.8179945054945055\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.49458300574892267, Train acc: 0.8184472934472935\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.49344729620995176, Train acc: 0.8193442841880342\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.49416745247674265, Train acc: 0.818753142282554\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.49409591386064616, Train acc: 0.8188509021842355\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4944120715906424, Train acc: 0.8191070625281152\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.49724381077302315, Train acc: 0.8186431623931624\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4967752213388572, Train acc: 0.8190883190883191\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4964401426102037, Train acc: 0.819225912975913\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.49696121201478427, Train acc: 0.8187476774433297\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.49638394906087396, Train acc: 0.8190883190883191\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.49675856383692507, Train acc: 0.8190811965811966\n",
      "Val loss: 0.540241003036499, Val acc: 0.814\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.47404393782982457, Train acc: 0.8295940170940171\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.48711209470390254, Train acc: 0.8186431623931624\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.48733510722292117, Train acc: 0.8205128205128205\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.48081626650741977, Train acc: 0.8211805555555556\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4830068568388621, Train acc: 0.8205128205128205\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.48906705874493317, Train acc: 0.8195334757834758\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.48850120627690874, Train acc: 0.820054945054945\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.48520978533814096, Train acc: 0.8219150641025641\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.48470005186421466, Train acc: 0.82247150997151\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.48322005005728486, Train acc: 0.8219017094017094\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4867221647417629, Train acc: 0.8210470085470085\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.48761202505341283, Train acc: 0.8208689458689459\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4854452161217582, Train acc: 0.822197567389875\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.48956654714340836, Train acc: 0.8211233211233211\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.48948408963418755, Train acc: 0.8212962962962963\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.48755601760891515, Train acc: 0.8221821581196581\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4846630891388897, Train acc: 0.8238436400201106\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4847696301476908, Train acc: 0.82380698005698\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4836557285304655, Train acc: 0.8240834457939721\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.48472098629826155, Train acc: 0.8231837606837606\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.48403358279955205, Train acc: 0.8235907610907611\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4843955888718992, Train acc: 0.8234994172494172\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4832700166071892, Train acc: 0.8238340765514679\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.48285210926213074, Train acc: 0.8239627849002849\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4835257062723494, Train acc: 0.8231837606837606\n",
      "Val loss: 0.543658971786499, Val acc: 0.824\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4665143554791426, Train acc: 0.8242521367521367\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.47151623093164885, Train acc: 0.8239850427350427\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4779366595919995, Train acc: 0.8219373219373219\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.480082516088827, Train acc: 0.8205128205128205\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.47584932873136976, Train acc: 0.8233974358974359\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.47388886795127155, Train acc: 0.8242521367521367\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4700623025733327, Train acc: 0.8267704517704517\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.47162847640390837, Train acc: 0.8263221153846154\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.47502984540273435, Train acc: 0.8256766381766382\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4724001623180687, Train acc: 0.8278311965811965\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.47181308613952594, Train acc: 0.8286713286713286\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4713259519494412, Train acc: 0.8285701566951567\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.47280756785809247, Train acc: 0.8289776462853385\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.47670762639797504, Train acc: 0.8265796703296703\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4787918341507939, Train acc: 0.8262820512820512\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.47882101219147444, Train acc: 0.8265892094017094\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.47838475797540764, Train acc: 0.8268602312719959\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4763696067820587, Train acc: 0.8275462962962963\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4765055111156516, Train acc: 0.8270074224021593\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4758798774299968, Train acc: 0.8274572649572649\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4745218306052088, Train acc: 0.8275335775335775\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.47388203346865904, Train acc: 0.8282099844599845\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4736933952695107, Train acc: 0.8279914529914529\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.47369144299462906, Train acc: 0.8278133903133903\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4733114268968248, Train acc: 0.8279914529914529\n",
      "Val loss: 0.5374399423599243, Val acc: 0.832\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4483609688587678, Train acc: 0.8381410256410257\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4666599923283116, Train acc: 0.8327991452991453\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.46565494215131825, Train acc: 0.8319088319088319\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4692501108775027, Train acc: 0.8298611111111112\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4729247952500979, Train acc: 0.8308760683760684\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.47311354012485923, Train acc: 0.8294159544159544\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4698853961968742, Train acc: 0.8300518925518926\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.47207350777382523, Train acc: 0.828125\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4697743817130838, Train acc: 0.8298907882241215\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.46675261387076133, Train acc: 0.830715811965812\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4646849000063763, Train acc: 0.8307595182595182\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.46323900901980464, Train acc: 0.8318198005698005\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4655389599306148, Train acc: 0.8311965811965812\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4649378824433237, Train acc: 0.8307005494505495\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.46628126744233983, Train acc: 0.8301638176638176\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4672835523951162, Train acc: 0.8298944978632479\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4672762222378292, Train acc: 0.8294683257918553\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.46644949229524346, Train acc: 0.8300985280151947\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4659542845435578, Train acc: 0.8306342780026991\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.46728009295323464, Train acc: 0.8298611111111112\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4675432040796175, Train acc: 0.8296448921448921\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4680506786057999, Train acc: 0.8293026418026418\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4691948500991045, Train acc: 0.8289669267930138\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4689785837331134, Train acc: 0.8287927350427351\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.46874321663481555, Train acc: 0.8286965811965812\n",
      "Val loss: 0.530737042427063, Val acc: 0.816\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4590091324514813, Train acc: 0.8354700854700855\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4569570772413515, Train acc: 0.8378739316239316\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4601325294231078, Train acc: 0.8347578347578347\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4610878964806469, Train acc: 0.8349358974358975\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4590676079193751, Train acc: 0.8354700854700855\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.45719730955922705, Train acc: 0.8355591168091168\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.45220672330103806, Train acc: 0.837530525030525\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4541249169976029, Train acc: 0.836738782051282\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4547770224150769, Train acc: 0.835707502374169\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4556130901870564, Train acc: 0.8356837606837607\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.45842147883094364, Train acc: 0.8350330225330226\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4594112459113795, Train acc: 0.8344017094017094\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.46035394366931004, Train acc: 0.834155161078238\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.46001261351254835, Train acc: 0.8341346153846154\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.45913847618208314, Train acc: 0.834045584045584\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45846822979645085, Train acc: 0.8340010683760684\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4579618527938398, Train acc: 0.8340874811463047\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.45797173839490524, Train acc: 0.8341049382716049\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.45858261850547316, Train acc: 0.8334176788124157\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4586956008249878, Train acc: 0.8334935897435898\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.45933055048023824, Train acc: 0.8327991452991453\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.45995546497039863, Train acc: 0.8325320512820513\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4596447081747041, Train acc: 0.832566889632107\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.461103403155408, Train acc: 0.8319756054131054\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.46186321680617126, Train acc: 0.8319230769230769\n",
      "Val loss: 0.5319766998291016, Val acc: 0.828\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4897878559736105, Train acc: 0.8151709401709402\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.46284200607711434, Train acc: 0.8271901709401709\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.451206717332374, Train acc: 0.8322649572649573\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.44762577317081964, Train acc: 0.8341346153846154\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4444964025519852, Train acc: 0.8341880341880342\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4438191293035647, Train acc: 0.8353810541310541\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4415128924525701, Train acc: 0.8372252747252747\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.44339038980089956, Train acc: 0.8376068376068376\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4492149368593609, Train acc: 0.836241690408357\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4469331517051428, Train acc: 0.8364850427350428\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.44818305195813596, Train acc: 0.8361499611499611\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4496452288006955, Train acc: 0.8362713675213675\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4499142083151414, Train acc: 0.836620644312952\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.44874313482798645, Train acc: 0.8372252747252747\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4509444638237654, Train acc: 0.8367521367521368\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45077362540775, Train acc: 0.8375066773504274\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4515479322262539, Train acc: 0.8369469582704877\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4512196304975299, Train acc: 0.8368055555555556\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4529804585889498, Train acc: 0.836144849302744\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.45248082192280353, Train acc: 0.8358440170940171\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.452560150337064, Train acc: 0.8358007733007733\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.45220287717884605, Train acc: 0.8358585858585859\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.45212794536885964, Train acc: 0.835934596804162\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4526176180934634, Train acc: 0.8356036324786325\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.45340564619781626, Train acc: 0.8354700854700855\n",
      "Val loss: 0.5183840990066528, Val acc: 0.838\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.42747455115756416, Train acc: 0.8498931623931624\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4466492812284547, Train acc: 0.844284188034188\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.44788567968073734, Train acc: 0.8431267806267806\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.43906630941817903, Train acc: 0.8436164529914529\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.44468219262412473, Train acc: 0.840491452991453\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4464269347191706, Train acc: 0.8396545584045584\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.44402838725783156, Train acc: 0.839514652014652\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4482784993851032, Train acc: 0.8386752136752137\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4489880572201406, Train acc: 0.8381410256410257\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.44653661670720474, Train acc: 0.8389957264957265\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4469261742381505, Train acc: 0.8382381507381508\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.45124264090637706, Train acc: 0.8362713675213675\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4514995749542621, Train acc: 0.8359220907297831\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4516300985109748, Train acc: 0.8355463980463981\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4521069555922791, Train acc: 0.835505698005698\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4516544997310027, Train acc: 0.8350360576923077\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4522154478870366, Train acc: 0.8344017094017094\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.45492786728730117, Train acc: 0.8340159069325735\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.45380970657381237, Train acc: 0.8345704003598741\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4519363394214047, Train acc: 0.8353632478632479\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4514553642910964, Train acc: 0.835495522995523\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.45079948758907623, Train acc: 0.8358585858585859\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4501237707431987, Train acc: 0.8363062058714232\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.45021611791730265, Train acc: 0.8363158831908832\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4498201817121261, Train acc: 0.8363461538461539\n",
      "Val loss: 0.516045093536377, Val acc: 0.848\n",
      "Epoch 25/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4444743666766036, Train acc: 0.8408119658119658\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.44318301227485013, Train acc: 0.844551282051282\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4390839614462309, Train acc: 0.8450854700854701\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4329959699390536, Train acc: 0.8472222222222222\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4384661533256881, Train acc: 0.8442307692307692\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.44144920538845567, Train acc: 0.8431267806267806\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4417805910401583, Train acc: 0.844017094017094\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4465356654782071, Train acc: 0.8415464743589743\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.44798174820988945, Train acc: 0.8397435897435898\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.44723376003213416, Train acc: 0.8401709401709402\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4493083100716736, Train acc: 0.8396464646464646\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4476317161710089, Train acc: 0.8403222934472935\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4472814279459637, Train acc: 0.8398668639053254\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4490026623256259, Train acc: 0.8390186202686203\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.44743889682737853, Train acc: 0.8396367521367522\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4481586318455127, Train acc: 0.8398103632478633\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.44797013017132153, Train acc: 0.8396178984414279\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.44756112285559785, Train acc: 0.8392687559354226\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4472141671143187, Train acc: 0.8395748987854251\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.44632623507044256, Train acc: 0.8397702991452991\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.44648758422675827, Train acc: 0.8390059015059015\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.44362121824936385, Train acc: 0.8400835275835276\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.44373861121498975, Train acc: 0.8396506874767744\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.442635840549269, Train acc: 0.8403668091168092\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4423783765554938, Train acc: 0.8408119658119658\n",
      "Val loss: 0.5222040414810181, Val acc: 0.836\n",
      "Epoch 26/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4454275553043072, Train acc: 0.8525641025641025\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4442731990900814, Train acc: 0.8453525641025641\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.442364521367088, Train acc: 0.843482905982906\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.43754676196119213, Train acc: 0.8457532051282052\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4390101649822333, Train acc: 0.8450854700854701\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4460953282155203, Train acc: 0.8425925925925926\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4377068246786411, Train acc: 0.8447039072039072\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.438637847884789, Train acc: 0.8430822649572649\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.44035123096175793, Train acc: 0.8414648622981956\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.440440791233992, Train acc: 0.8425747863247863\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.438149994735627, Train acc: 0.8426573426573427\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.43716455625099504, Train acc: 0.8435719373219374\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4381442928416097, Train acc: 0.8431130834976989\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4399929290071075, Train acc: 0.8427197802197802\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4391688858426874, Train acc: 0.842948717948718\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4404451455323933, Train acc: 0.8420138888888888\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.44103385329943257, Train acc: 0.8420374560080442\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4401819444538323, Train acc: 0.8422067901234568\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4398523291947468, Train acc: 0.8421333783175888\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.43984887330418726, Train acc: 0.8417735042735043\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4407936348301223, Train acc: 0.8413207163207164\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.43973011261093914, Train acc: 0.8415889665889665\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.43944619962442366, Train acc: 0.8416713117800074\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.43905169138302785, Train acc: 0.8417913105413105\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4385931080770798, Train acc: 0.8417735042735043\n",
      "Val loss: 0.5111679434776306, Val acc: 0.834\n",
      "Epoch 27/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.43398947695381623, Train acc: 0.8461538461538461\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4249208758975196, Train acc: 0.8477564102564102\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4297334086521399, Train acc: 0.8472222222222222\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4283426602363077, Train acc: 0.8462873931623932\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.43048168122768404, Train acc: 0.8451923076923077\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.42659788862125486, Train acc: 0.8466880341880342\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4326658400187271, Train acc: 0.8449328449328449\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4407274692725295, Train acc: 0.8418803418803419\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.43645886905979564, Train acc: 0.8421177587844254\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.43726009643103325, Train acc: 0.8425747863247863\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.43667895679753604, Train acc: 0.843482905982906\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.43804352680629816, Train acc: 0.8428596866096866\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4370031497446743, Train acc: 0.8423734385272846\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4357565437808578, Train acc: 0.8425671550671551\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4361123434711046, Train acc: 0.8426282051282051\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4353292058818998, Train acc: 0.8425480769230769\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4354330266897495, Train acc: 0.842885872297637\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.43461903148213116, Train acc: 0.84375\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4341478522396066, Train acc: 0.8436797121007648\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.43428540230434165, Train acc: 0.8435096153846153\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.43425651241547214, Train acc: 0.8439916564916565\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.43367409487866393, Train acc: 0.8438714063714063\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4334844526295235, Train acc: 0.8434596804162021\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.43271389749017874, Train acc: 0.8435274216524217\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4321692456750788, Train acc: 0.8436752136752137\n",
      "Val loss: 0.5314245223999023, Val acc: 0.838\n",
      "Epoch 28/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.42133794266443986, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.41468342973126304, Train acc: 0.844284188034188\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.41267595740507473, Train acc: 0.8472222222222222\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4160498646127745, Train acc: 0.8474893162393162\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.41627416988341215, Train acc: 0.8483974358974359\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4185580934565037, Train acc: 0.8497150997150997\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4222639808988469, Train acc: 0.8483669108669109\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4264171230606735, Train acc: 0.8465544871794872\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.42969224854935834, Train acc: 0.8455009496676164\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4345305074643121, Train acc: 0.8433760683760684\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4334222574826119, Train acc: 0.8443570318570318\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4296350745998981, Train acc: 0.8455751424501424\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4284050681240695, Train acc: 0.846112754766601\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4262496248224207, Train acc: 0.8466498778998779\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4276600913656743, Train acc: 0.846545584045584\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4261638192364421, Train acc: 0.8477230235042735\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.42606047857431023, Train acc: 0.8479449472096531\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.42731578194546, Train acc: 0.8471035137701804\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4283054331179291, Train acc: 0.8467161493477283\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4284032771976776, Train acc: 0.846340811965812\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.42852596634688855, Train acc: 0.846433658933659\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.42774490899860096, Train acc: 0.8462752525252525\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.42717142876643244, Train acc: 0.8465022296544036\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.42781177694919886, Train acc: 0.8461538461538461\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.42728616766758964, Train acc: 0.8461752136752136\n",
      "Val loss: 0.5252611041069031, Val acc: 0.838\n",
      "Epoch 29/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.42770784507449877, Train acc: 0.843482905982906\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4163637017656086, Train acc: 0.8461538461538461\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.42415300289704927, Train acc: 0.8441951566951567\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.42371485667287284, Train acc: 0.8450854700854701\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.42499521885417463, Train acc: 0.8451923076923077\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.42364291190846354, Train acc: 0.8454415954415955\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.42205436670554575, Train acc: 0.8464590964590964\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4230479573448881, Train acc: 0.8458199786324786\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.42137698296383236, Train acc: 0.8471628679962013\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4213596292667919, Train acc: 0.8474893162393162\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.42450152940197144, Train acc: 0.8464452214452215\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.423042568810645, Train acc: 0.8457532051282052\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.42106070479697727, Train acc: 0.8467291255752795\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4190821726652947, Train acc: 0.8473748473748474\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.42027784880579705, Train acc: 0.846937321937322\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4207492044123893, Train acc: 0.8461872329059829\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4193612007810999, Train acc: 0.8466566113624937\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.41875747500689736, Train acc: 0.8463319088319088\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4185365707526805, Train acc: 0.8464349977507872\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4188712655160672, Train acc: 0.8463942307692308\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4184496728146178, Train acc: 0.8466625966625967\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.41824241928762024, Train acc: 0.8470036907536908\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.41797489468015203, Train acc: 0.8471525455221107\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4173910236914783, Train acc: 0.8478899572649573\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4165888214416993, Train acc: 0.8480128205128206\n",
      "Val loss: 0.5109007358551025, Val acc: 0.856\n",
      "Epoch 30/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4022035223678646, Train acc: 0.8514957264957265\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4245727533140244, Train acc: 0.844017094017094\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.42065671984201825, Train acc: 0.8481125356125356\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4154667711347087, Train acc: 0.8506944444444444\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.41824678098544094, Train acc: 0.8504273504273504\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4196438950266254, Train acc: 0.8490028490028491\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.41463246729602454, Train acc: 0.8503510378510378\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.41549371442415267, Train acc: 0.8492254273504274\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4164156859275396, Train acc: 0.8489434947768281\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.41650173778717337, Train acc: 0.8498397435897436\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4161992491669403, Train acc: 0.8498445998445998\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4152119420468807, Train acc: 0.8499821937321937\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.41341331335155684, Train acc: 0.850180802103879\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4144239259226491, Train acc: 0.8497405372405372\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4133638921244192, Train acc: 0.8503917378917379\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.41291540194477916, Train acc: 0.8508279914529915\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.41231483422208515, Train acc: 0.8508358471593765\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.41346535579091803, Train acc: 0.8507537986704653\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4146718504263429, Train acc: 0.8505398110661269\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4149658856356246, Train acc: 0.8506143162393163\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4162707684040797, Train acc: 0.850503663003663\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.41529625454785846, Train acc: 0.8511315073815073\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4155124133429931, Train acc: 0.8511937941285768\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4156013436143172, Train acc: 0.8512286324786325\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.41631047598062415, Train acc: 0.8504700854700855\n",
      "Val loss: 0.4925735592842102, Val acc: 0.862\n",
      "Epoch 31/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.367132392194536, Train acc: 0.8637820512820513\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.3848241242205995, Train acc: 0.8565705128205128\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.39176233575554653, Train acc: 0.8571937321937322\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.400109016822062, Train acc: 0.8547008547008547\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.3979791562654014, Train acc: 0.8557692307692307\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3922448027507532, Train acc: 0.8584401709401709\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.3904583738327609, Train acc: 0.8582875457875457\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3978313088066812, Train acc: 0.8551014957264957\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.40082691009850696, Train acc: 0.8549382716049383\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4003244059144432, Train acc: 0.8547542735042735\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4011727158376757, Train acc: 0.853923853923854\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.40277782269352863, Train acc: 0.8534989316239316\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.40413443581553216, Train acc: 0.853344838921762\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.40410806216363215, Train acc: 0.8536324786324786\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4038291469598428, Train acc: 0.8540242165242166\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4052578070654701, Train acc: 0.8537326388888888\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4061521328948232, Train acc: 0.853789592760181\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.40660093823608795, Train acc: 0.8535434472934473\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.40755429728022846, Train acc: 0.8528733693207378\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4080274468860947, Train acc: 0.8526976495726496\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.40660644865279816, Train acc: 0.8531491656491657\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.40742486353063406, Train acc: 0.8528554778554779\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.40772898405906616, Train acc: 0.8529357116313638\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4076422249083739, Train acc: 0.8532986111111112\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4080581306277687, Train acc: 0.8530769230769231\n",
      "Val loss: 0.5086504817008972, Val acc: 0.854\n",
      "Epoch 32/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4072340991761949, Train acc: 0.8627136752136753\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4277312124195771, Train acc: 0.8448183760683761\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.41684498075406434, Train acc: 0.8449074074074074\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4138063929306391, Train acc: 0.8461538461538461\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4082301052717062, Train acc: 0.8508547008547008\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4101060780237543, Train acc: 0.8513176638176638\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.41154325308598877, Train acc: 0.850503663003663\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4126816243968076, Train acc: 0.8520299145299145\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.41068301241622013, Train acc: 0.8524453941120608\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.40910957966986883, Train acc: 0.8521901709401709\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.41218374524245266, Train acc: 0.8506701631701632\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4100213153579636, Train acc: 0.8514512108262108\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4084539412374484, Train acc: 0.8510848126232742\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.40780673312507704, Train acc: 0.8513812576312576\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4071623077674469, Train acc: 0.8508903133903134\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4073090616526066, Train acc: 0.8512286324786325\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4066613916815256, Train acc: 0.8522812971342383\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4056249823198359, Train acc: 0.8527421652421653\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.40568077825663384, Train acc: 0.8524516419253262\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4065899789030863, Train acc: 0.8520299145299145\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4077678943840551, Train acc: 0.8517246642246642\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.408064907561429, Train acc: 0.852248445998446\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4079636304709536, Train acc: 0.8523086213303604\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4072530888607381, Train acc: 0.8526308760683761\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.406778867442129, Train acc: 0.8529273504273505\n",
      "Val loss: 0.49570000171661377, Val acc: 0.854\n",
      "Epoch 33/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.38237552574047673, Train acc: 0.8547008547008547\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.40069823186749065, Train acc: 0.8517628205128205\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.40148875270134365, Train acc: 0.8516737891737892\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4043430373008944, Train acc: 0.8524305555555556\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.40593786347880323, Train acc: 0.8521367521367521\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.408511676350635, Train acc: 0.8530092592592593\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.40775688290959894, Train acc: 0.8547771672771672\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4054574389408669, Train acc: 0.8553018162393162\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.40201376808731193, Train acc: 0.8551163342830009\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4026831422032964, Train acc: 0.8547542735042735\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4033964413110378, Train acc: 0.8543123543123543\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.40396826882499065, Train acc: 0.85372150997151\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4044032199020491, Train acc: 0.854043392504931\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4033403937728741, Train acc: 0.8546626984126984\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4024728793258171, Train acc: 0.8547008547008547\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.40189368655292207, Train acc: 0.8548010149572649\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.40347808972750254, Train acc: 0.8542923579688285\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.40267215318737265, Train acc: 0.8543744064577398\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.40342431575281096, Train acc: 0.8538855150697255\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4031988704035807, Train acc: 0.8538728632478633\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.40440831333550437, Train acc: 0.8533781033781034\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4046263052794645, Train acc: 0.8532682595182595\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.40326041573626054, Train acc: 0.8539111854329245\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4037776646878241, Train acc: 0.8536324786324786\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4025720893636219, Train acc: 0.8542948717948718\n",
      "Val loss: 0.4921058714389801, Val acc: 0.844\n",
      "Epoch 34/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.38175928732778275, Train acc: 0.8701923076923077\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4072539057168696, Train acc: 0.8538995726495726\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4036079317628488, Train acc: 0.85505698005698\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3974237705970931, Train acc: 0.8589743589743589\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.3945013311174181, Train acc: 0.8606837606837607\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3938322365283966, Train acc: 0.8601317663817664\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.39223139474716523, Train acc: 0.861492673992674\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.39510052940911716, Train acc: 0.859909188034188\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3979834741138891, Train acc: 0.8592117758784426\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3973294953759919, Train acc: 0.8591880341880341\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3960680201962278, Train acc: 0.8592657342657343\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.39555478549198886, Train acc: 0.8589743589743589\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3937289080558679, Train acc: 0.8593852728468113\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3934410847791713, Train acc: 0.8596230158730159\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.39446117360240374, Train acc: 0.8590099715099715\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.39565691320249474, Train acc: 0.8588074252136753\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3973839492965518, Train acc: 0.8587858220211161\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3982737266280797, Train acc: 0.8582917853751187\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3995682687653221, Train acc: 0.8574561403508771\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.40025922226536476, Train acc: 0.8570245726495727\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.40070058136729625, Train acc: 0.8566849816849816\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4000709668024738, Train acc: 0.8568861693861693\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.39851093790879794, Train acc: 0.8573253437383872\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3987682707995134, Train acc: 0.8571937321937322\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.3980758536091218, Train acc: 0.8574358974358974\n",
      "Val loss: 0.5303933024406433, Val acc: 0.844\n",
      "Epoch 35/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3860366481364283, Train acc: 0.8584401709401709\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.40646481584025246, Train acc: 0.8528311965811965\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.39926176057581886, Train acc: 0.8554131054131054\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.40145489300449944, Train acc: 0.8549679487179487\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.40117318688167464, Train acc: 0.8548076923076923\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.40094428733415755, Train acc: 0.8551460113960114\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.395867722115783, Train acc: 0.8561507936507936\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.39339404221640056, Train acc: 0.8574385683760684\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3922578145848041, Train acc: 0.8574311490978158\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.39301803842760047, Train acc: 0.8574786324786324\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.39135903142328954, Train acc: 0.8583430458430459\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3934379782331338, Train acc: 0.8574163105413105\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.392270240643693, Train acc: 0.8581525312294543\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.39351220754394817, Train acc: 0.8579441391941391\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.39305677422006585, Train acc: 0.8584045584045584\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.3903214821301432, Train acc: 0.8587740384615384\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.39141609961922713, Train acc: 0.8580630970336852\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.39155925370766964, Train acc: 0.8581730769230769\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.39150926478632764, Train acc: 0.8579903283850653\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.39266039977311834, Train acc: 0.85758547008547\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3919127081628126, Train acc: 0.8576770451770451\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.39135992414699383, Train acc: 0.8581487956487956\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.39144489801769494, Train acc: 0.8580685618729097\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.39111414693953983, Train acc: 0.8580840455840456\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.39171932832807554, Train acc: 0.8580769230769231\n",
      "Val loss: 0.5066143870353699, Val acc: 0.86\n",
      "Epoch 36/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3759970601934653, Train acc: 0.8696581196581197\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.40709226785434616, Train acc: 0.8605769230769231\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.40219306913960695, Train acc: 0.8620014245014245\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4085342552130803, Train acc: 0.8577724358974359\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4021633876694573, Train acc: 0.8574786324786324\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3991840435271589, Train acc: 0.8581730769230769\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.39702663734391497, Train acc: 0.858592796092796\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3935877548084937, Train acc: 0.859375\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.39119465753492694, Train acc: 0.8596272554605888\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3919260145730188, Train acc: 0.8589209401709401\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3889857416287956, Train acc: 0.8594599844599845\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3859204856859271, Train acc: 0.8605324074074074\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3864635788247156, Train acc: 0.860905654174885\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.38557609947345994, Train acc: 0.86122557997558\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.38608891630699155, Train acc: 0.8612535612535612\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.38706063402370894, Train acc: 0.8605769230769231\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3887080924520845, Train acc: 0.8599170437405732\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.38714658207519215, Train acc: 0.8603395061728395\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3890717888650624, Train acc: 0.8596491228070176\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.38946247257483313, Train acc: 0.8595619658119659\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.38969604491483983, Train acc: 0.8597374847374848\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3901250235527962, Train acc: 0.8593871406371406\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3895383983606833, Train acc: 0.8592995169082126\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3908712362838734, Train acc: 0.8589298433048433\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.39050566002981274, Train acc: 0.8591880341880341\n",
      "Val loss: 0.48855167627334595, Val acc: 0.84\n",
      "Epoch 37/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3912383274008066, Train acc: 0.8563034188034188\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4016171613564858, Train acc: 0.8555021367521367\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3998340105908549, Train acc: 0.8568376068376068\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3905945159494877, Train acc: 0.8581730769230769\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.3855306828888054, Train acc: 0.8601495726495727\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3801946936392801, Train acc: 0.8625356125356125\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.38013489747713336, Train acc: 0.862942612942613\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3820513214271229, Train acc: 0.8630475427350427\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3850541910986219, Train acc: 0.8615859449192782\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.38411808657404195, Train acc: 0.8626068376068377\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3841885614332619, Train acc: 0.863490675990676\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.38345128064311806, Train acc: 0.8634704415954416\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.38369463044687135, Train acc: 0.8631245890861275\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.38241985981990567, Train acc: 0.8635531135531136\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.38543598733821144, Train acc: 0.8620014245014245\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.38563539992826873, Train acc: 0.8614115918803419\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.38637582947817983, Train acc: 0.8613624937154349\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.38719548234240225, Train acc: 0.8614672364672364\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3881472283526015, Train acc: 0.8605206927575348\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.3875989557299604, Train acc: 0.8609508547008548\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3876877085745068, Train acc: 0.8604497354497355\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3875373815039423, Train acc: 0.8602855477855478\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.38767987301151624, Train acc: 0.8600659606094389\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3876588924819779, Train acc: 0.8601317663817664\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.38756748074522385, Train acc: 0.8603205128205128\n",
      "Val loss: 0.5006182193756104, Val acc: 0.864\n",
      "Epoch 38/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.34509159242495513, Train acc: 0.8808760683760684\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.36522750078867644, Train acc: 0.8707264957264957\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3622351743471928, Train acc: 0.8700142450142451\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.37149956761899156, Train acc: 0.8644497863247863\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.3698071190179923, Train acc: 0.8655982905982906\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3698703549229182, Train acc: 0.8656517094017094\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.37439659687261734, Train acc: 0.865460927960928\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.37467893591532725, Train acc: 0.8656517094017094\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.37503910069296387, Train acc: 0.8650878442545109\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3749907933716845, Train acc: 0.8656517094017094\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3755661183367566, Train acc: 0.866064491064491\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.37285005762918383, Train acc: 0.8664084757834758\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3746476062352449, Train acc: 0.8656311637080868\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3754878482839345, Train acc: 0.8653846153846154\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.37696114140018777, Train acc: 0.8646011396011396\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.3755304422189728, Train acc: 0.8649839743589743\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3755991481480555, Train acc: 0.8645676219205631\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.37501069722238994, Train acc: 0.8651768755935423\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3757005119214469, Train acc: 0.8645692757534863\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.37519598770249857, Train acc: 0.8650373931623931\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.37544908924926623, Train acc: 0.86492673992674\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3750783811745226, Train acc: 0.86489898989899\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3762395429175412, Train acc: 0.8642697881828316\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3778491866260109, Train acc: 0.8636039886039886\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.3792070107238415, Train acc: 0.8631623931623932\n",
      "Val loss: 0.4850873053073883, Val acc: 0.852\n",
      "Epoch 39/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3625623049835364, Train acc: 0.8643162393162394\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.36031886873145896, Train acc: 0.8680555555555556\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3675539782562657, Train acc: 0.8655626780626781\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.36552470294026357, Train acc: 0.8660523504273504\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.37042783882437097, Train acc: 0.8661324786324787\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3751694708982976, Train acc: 0.8631588319088319\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.3755547070627866, Train acc: 0.8632478632478633\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.37447182117149425, Train acc: 0.8641826923076923\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.37371012631269546, Train acc: 0.8653846153846154\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3726721467354741, Train acc: 0.8651175213675214\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.37345350493634266, Train acc: 0.8651903651903652\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.37506223409874445, Train acc: 0.8649394586894587\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3763096639525236, Train acc: 0.8645216962524654\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.37488027753806485, Train acc: 0.8645833333333334\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3751140201374082, Train acc: 0.8643518518518518\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.37547837905924064, Train acc: 0.863815438034188\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3751458694775345, Train acc: 0.864096279537456\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.37454048102210363, Train acc: 0.8644349477682811\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.37479225191104615, Train acc: 0.8646817363922628\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.3748751286823207, Train acc: 0.8644497863247863\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.37527285711775354, Train acc: 0.8642908017908018\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.37530071529391185, Train acc: 0.864826146076146\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3751820753083126, Train acc: 0.8647807506503159\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.37480970471500397, Train acc: 0.8648504273504274\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.37425947721378927, Train acc: 0.8651068376068376\n",
      "Val loss: 0.49224796891212463, Val acc: 0.86\n",
      "Epoch 40/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3950474753211706, Train acc: 0.8616452991452992\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.3857537648744053, Train acc: 0.8635149572649573\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3841528558408433, Train acc: 0.8598646723646723\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3796882925188949, Train acc: 0.8600427350427351\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.3865124516380139, Train acc: 0.8579059829059829\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3841520651665508, Train acc: 0.8597756410256411\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.38600796779506225, Train acc: 0.8607295482295483\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3836926362899124, Train acc: 0.8614449786324786\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3821404934573321, Train acc: 0.8627730294396961\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3826553032407139, Train acc: 0.8628739316239317\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3823337292424588, Train acc: 0.8625679875679876\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3805779272032387, Train acc: 0.8630252849002849\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3797020284724737, Train acc: 0.8628780407626562\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.38022514709222593, Train acc: 0.8629807692307693\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3765484032170725, Train acc: 0.8644586894586894\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.376030257381658, Train acc: 0.8649172008547008\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.37520654030545025, Train acc: 0.8652275012569131\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3751246561557196, Train acc: 0.8652065527065527\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3756074097595717, Train acc: 0.8648785425101214\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.37505286396282095, Train acc: 0.8648771367521367\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.37651862151917764, Train acc: 0.8641381766381766\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3758409655947461, Train acc: 0.8646076146076146\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.37449327279858236, Train acc: 0.8651059085841695\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.37486684366402634, Train acc: 0.8650952635327636\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.3749054127243849, Train acc: 0.865534188034188\n",
      "Val loss: 0.4748179614543915, Val acc: 0.876\n",
      "Epoch 41/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3535919597006252, Train acc: 0.8733974358974359\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.3579443738055535, Train acc: 0.8715277777777778\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3525931796329653, Train acc: 0.8726851851851852\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3655577776237176, Train acc: 0.8689903846153846\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.36687385823227403, Train acc: 0.8686965811965812\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.36540481102708566, Train acc: 0.8694800569800569\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.36312424501128976, Train acc: 0.871031746031746\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.36426763443085247, Train acc: 0.8702590811965812\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3660343745997545, Train acc: 0.8692426400759734\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.36303442842366856, Train acc: 0.8697115384615385\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3607510910937512, Train acc: 0.8697552447552448\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.36251008313181055, Train acc: 0.8688123219373219\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3616506157199228, Train acc: 0.8692061143984221\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3627358210592625, Train acc: 0.8687042124542125\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3628469656229529, Train acc: 0.868554131054131\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.36289614263492137, Train acc: 0.8682558760683761\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3656456319942074, Train acc: 0.8674899446958271\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3659844261816113, Train acc: 0.8674620132953467\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.366157200965851, Train acc: 0.8671558704453441\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.36610328674746245, Train acc: 0.8674145299145299\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.36643737768561047, Train acc: 0.867546805046805\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3668348901574417, Train acc: 0.8671571484071484\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.36909932411919183, Train acc: 0.866662021553326\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3687488063340715, Train acc: 0.8665865384615384\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.36991863956499815, Train acc: 0.8660042735042736\n",
      "Val loss: 0.5079581141471863, Val acc: 0.862\n",
      "Epoch 42/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.35870373147165674, Train acc: 0.8766025641025641\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.3652188648016025, Train acc: 0.8723290598290598\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.36664052443456785, Train acc: 0.8712606837606838\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3761995418681803, Train acc: 0.8664529914529915\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.37411130969850426, Train acc: 0.8675213675213675\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.37456642667579854, Train acc: 0.8682336182336182\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.37212099146293487, Train acc: 0.8680555555555556\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.36723870356989086, Train acc: 0.8698584401709402\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.36745099984074026, Train acc: 0.8690645773979108\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.36956338207436423, Train acc: 0.8676282051282052\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.36789976707033717, Train acc: 0.8682012432012433\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3677196689427663, Train acc: 0.8679665242165242\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.36745708813936123, Train acc: 0.8682199211045365\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3667600550990147, Train acc: 0.867979242979243\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.36678786517420725, Train acc: 0.8677706552706552\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.36674028042202383, Train acc: 0.8678886217948718\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.36484682515203626, Train acc: 0.8687154348919055\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3643176217208787, Train acc: 0.868619420702754\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3640712753685675, Train acc: 0.8685053981106613\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.3637985653634, Train acc: 0.8683760683760684\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3656176390064942, Train acc: 0.8679029304029304\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.365167410029699, Train acc: 0.8682012432012433\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3660996343839439, Train acc: 0.8677768487551096\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.36668775722973496, Train acc: 0.8674100783475783\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.3674678149450029, Train acc: 0.8672649572649572\n",
      "Val loss: 0.5123181343078613, Val acc: 0.86\n",
      "Epoch 43/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3502485132179199, Train acc: 0.8712606837606838\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.3535987190488312, Train acc: 0.8736645299145299\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3510084361092657, Train acc: 0.8705484330484331\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.35375443366793996, Train acc: 0.8700587606837606\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.3508594513639935, Train acc: 0.8708333333333333\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.354340292688491, Train acc: 0.8702813390313391\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.35609161450330884, Train acc: 0.8686660561660562\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3539459557654575, Train acc: 0.8699252136752137\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.35464723653744656, Train acc: 0.870073599240266\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.35637486713946376, Train acc: 0.8697115384615385\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.35634398179037585, Train acc: 0.868978243978244\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.35806458300653515, Train acc: 0.8684116809116809\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3565319239074348, Train acc: 0.8693293885601578\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3600768983063899, Train acc: 0.8682844932844933\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.36188759611457844, Train acc: 0.868091168091168\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.36273808324407053, Train acc: 0.8681223290598291\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3642518570962235, Train acc: 0.867615635997989\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.36457776910460005, Train acc: 0.8675807217473884\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.36355126346157735, Train acc: 0.8678587494376968\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.36314067541756945, Train acc: 0.868215811965812\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3646382615364418, Train acc: 0.8679029304029304\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.36329086154421447, Train acc: 0.8686868686868687\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3635770854886603, Train acc: 0.8683342623560015\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3654144641484248, Train acc: 0.8682781339031339\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.36474517254747896, Train acc: 0.8683333333333333\n",
      "Val loss: 0.5009850859642029, Val acc: 0.862\n",
      "Epoch 44/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3562031894540175, Train acc: 0.8669871794871795\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.3484933254364719, Train acc: 0.8728632478632479\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3583983308463185, Train acc: 0.8696581196581197\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3618554760996475, Train acc: 0.8673878205128205\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.35779402401202764, Train acc: 0.8697649572649573\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3536996766977925, Train acc: 0.8719729344729344\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.35116696521930935, Train acc: 0.8726343101343101\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3547582964166107, Train acc: 0.8712606837606838\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.35482060819765215, Train acc: 0.8712606837606838\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.35382036291317553, Train acc: 0.871474358974359\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3576262445444503, Train acc: 0.8690268065268065\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3582956359915414, Train acc: 0.8687232905982906\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3620801369055253, Train acc: 0.8673159105851413\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3628117073923167, Train acc: 0.8675213675213675\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3639932670527034, Train acc: 0.8671652421652422\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.3638721523392532, Train acc: 0.8672876602564102\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.36576879322678674, Train acc: 0.8670500251382605\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.365272913071533, Train acc: 0.8676400759734093\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3639131879222002, Train acc: 0.868449167791273\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.3641218449633855, Train acc: 0.8680822649572649\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.36488813867790215, Train acc: 0.8678520553520553\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3656082333448374, Train acc: 0.867472804972805\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.36421819886767876, Train acc: 0.8682181345224823\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.363597973377213, Train acc: 0.8685229700854701\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.36400788746837875, Train acc: 0.8682264957264957\n",
      "Val loss: 0.5040988326072693, Val acc: 0.856\n",
      "Epoch 45/50\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3591340289920823, Train acc: 0.8643162393162394\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.34729024822003823, Train acc: 0.8717948717948718\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.35601485405976957, Train acc: 0.8701923076923077\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.352018779127771, Train acc: 0.8720619658119658\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.35399732081553875, Train acc: 0.8710470085470086\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.35332167391636093, Train acc: 0.8717058404558404\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.35522064002815257, Train acc: 0.8709554334554335\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3548165404528348, Train acc: 0.8719284188034188\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.35159624698204817, Train acc: 0.873338081671415\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3526787761423705, Train acc: 0.8729166666666667\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.35421973017493746, Train acc: 0.871989121989122\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3545449065227603, Train acc: 0.8723290598290598\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.35531796307415064, Train acc: 0.8727810650887574\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3553019600666796, Train acc: 0.8725961538461539\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3566435359237965, Train acc: 0.8721866096866097\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.35764298465154654, Train acc: 0.8719618055555556\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3568427066813617, Train acc: 0.8718577174459528\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.35623518097438167, Train acc: 0.8717058404558404\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3549593928874585, Train acc: 0.8720479082321188\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.3553475889663857, Train acc: 0.8722489316239316\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.357026631799464, Train acc: 0.8720238095238095\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.357560761023252, Train acc: 0.8717463092463092\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.35696968504282034, Train acc: 0.8719342251950948\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3580676848871469, Train acc: 0.8713942307692307\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.35886296348503005, Train acc: 0.8712179487179487\n",
      "Val loss: 0.5036423802375793, Val acc: 0.86\n",
      "Early stopping at epoch 45 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 1051.9686 [s]\n",
      "Epoch 1/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 1.5899314370929685, Train acc: 0.22382478632478633\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.5388180669556317, Train acc: 0.2831196581196581\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.4528463516819512, Train acc: 0.3449074074074074\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.3999667203324473, Train acc: 0.3874198717948718\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.3506048199458, Train acc: 0.4157051282051282\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 1.3109706522222937, Train acc: 0.44328703703703703\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 1.278417660130395, Train acc: 0.4606227106227106\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 1.2482912939710495, Train acc: 0.4780315170940171\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 1.2256795633212794, Train acc: 0.49115622032288697\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 1.2018816533251706, Train acc: 0.5026175213675214\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 1.1838849526370507, Train acc: 0.5114607614607615\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 1.1622647757289077, Train acc: 0.5221242877492878\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 1.1438596913643686, Train acc: 0.5308596318211702\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 1.1245630211736983, Train acc: 0.5393772893772893\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 1.106071894036399, Train acc: 0.5471153846153847\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 1.0897000355916655, Train acc: 0.5549879807692307\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 1.0749512554233909, Train acc: 0.5621543489190548\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 1.0601689422402287, Train acc: 0.5686431623931624\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 1.0461685506947878, Train acc: 0.5753205128205128\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 1.0341275594937496, Train acc: 0.5802884615384616\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 1.0216970598620032, Train acc: 0.5859533984533984\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 1.009663379414654, Train acc: 0.5918317793317793\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.99920300632682, Train acc: 0.5972454477889261\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.9875574094957096, Train acc: 0.6019853988603988\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.9768776677510678, Train acc: 0.606965811965812\n",
      "Val loss: 0.7000812888145447, Val acc: 0.726\n",
      "Epoch 2/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7145394825527811, Train acc: 0.7216880341880342\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.7109820987933722, Train acc: 0.7232905982905983\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.7075743906178705, Train acc: 0.7273860398860399\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.7123010902960076, Train acc: 0.7271634615384616\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.7090291525563623, Train acc: 0.7259615384615384\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.7013228454141536, Train acc: 0.7305911680911681\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.699448864866089, Train acc: 0.7335164835164835\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6950465254644808, Train acc: 0.7353098290598291\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.696841076851344, Train acc: 0.7349833808167141\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6931300370356975, Train acc: 0.7375\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6896890905029383, Train acc: 0.739559052059052\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6893372122421224, Train acc: 0.739494301994302\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6870832328808927, Train acc: 0.7407133464825773\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6870520068321181, Train acc: 0.7403083028083028\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6846647521369478, Train acc: 0.7415242165242165\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6841032466986495, Train acc: 0.7412192841880342\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6823252146120584, Train acc: 0.7421128707893414\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6803824506268429, Train acc: 0.7435897435897436\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6787060048134542, Train acc: 0.7446581196581197\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6771934335239422, Train acc: 0.7457264957264957\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6753951662868614, Train acc: 0.745980870980871\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6712919720264026, Train acc: 0.7477418414918415\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6689997587071967, Train acc: 0.7490942028985508\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6682569904470461, Train acc: 0.7493990384615384\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6668254518254191, Train acc: 0.7492948717948718\n",
      "Val loss: 0.5769230723381042, Val acc: 0.798\n",
      "Epoch 3/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5874171083808964, Train acc: 0.7767094017094017\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5891794666775272, Train acc: 0.7775106837606838\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5960402530312878, Train acc: 0.7752849002849003\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.592962074197001, Train acc: 0.7775106837606838\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5953001800255898, Train acc: 0.7784188034188034\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.598844066580646, Train acc: 0.7765313390313391\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6011896310635685, Train acc: 0.7746489621489622\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.605077651480579, Train acc: 0.7739049145299145\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6016991739740626, Train acc: 0.7766500474833808\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.602659558167315, Train acc: 0.7756410256410257\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6005173935710504, Train acc: 0.777534965034965\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5978578068634384, Train acc: 0.7786235754985755\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5986861244412255, Train acc: 0.7779421433267587\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5966849106526564, Train acc: 0.7785790598290598\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5932256390948242, Train acc: 0.7803062678062678\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5921366729009419, Train acc: 0.780448717948718\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.592598721321322, Train acc: 0.7801344896933132\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5920981696903252, Train acc: 0.7803300094966762\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5937629409848276, Train acc: 0.7799145299145299\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5919132760702035, Train acc: 0.7805288461538461\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5924609123714386, Train acc: 0.7809065934065934\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.590625322220533, Train acc: 0.7810800310800311\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5904200551668005, Train acc: 0.781377740616871\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5896019156003355, Train acc: 0.7817174145299145\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5893534549841514, Train acc: 0.7819017094017094\n",
      "Val loss: 0.651295006275177, Val acc: 0.79\n",
      "Epoch 4/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5529500577184889, Train acc: 0.7895299145299145\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5507040603930115, Train acc: 0.7911324786324786\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5370553076437056, Train acc: 0.7993233618233618\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5373858652658697, Train acc: 0.7994123931623932\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5464759249590401, Train acc: 0.7963675213675213\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5457073900625746, Train acc: 0.7970085470085471\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5472090015499438, Train acc: 0.7951770451770451\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5415929407916136, Train acc: 0.7986778846153846\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5423108341927655, Train acc: 0.7990265906932573\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5458550268831925, Train acc: 0.7980769230769231\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5448034467709648, Train acc: 0.7986111111111112\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5480220619419277, Train acc: 0.7973201566951567\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5456667643121244, Train acc: 0.7988987508218277\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5459769248498447, Train acc: 0.7982295482295483\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5453336198657667, Train acc: 0.7981837606837607\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5448334803685355, Train acc: 0.7991119123931624\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5448596550141405, Train acc: 0.7998994469582705\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5456496585984798, Train acc: 0.8002433523266856\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5449656853959253, Train acc: 0.8006072874493927\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.54563581520994, Train acc: 0.8009081196581197\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5447515667145894, Train acc: 0.8014092389092389\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5434451055479262, Train acc: 0.8018648018648019\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5410161115691503, Train acc: 0.802675585284281\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5383198013757369, Train acc: 0.8032629985754985\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5365130306640242, Train acc: 0.8040384615384616\n",
      "Val loss: 0.5429771542549133, Val acc: 0.84\n",
      "Epoch 5/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5025230559528383, Train acc: 0.8199786324786325\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4959751278098322, Train acc: 0.8231837606837606\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5027298042789484, Train acc: 0.8201566951566952\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5046298706219492, Train acc: 0.8201121794871795\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5017703422369101, Train acc: 0.8192307692307692\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4951701441891173, Train acc: 0.8201566951566952\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4915285367741544, Train acc: 0.8214285714285714\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4901936357506575, Train acc: 0.8222489316239316\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.49163569048655564, Train acc: 0.8218779677113011\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.48902300098258206, Train acc: 0.8222222222222222\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4917542561483457, Train acc: 0.822066822066822\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.48939860601540647, Train acc: 0.8225605413105413\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4885472221389403, Train acc: 0.822526298487837\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.48800937862332194, Train acc: 0.8231837606837606\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.48483151050969064, Train acc: 0.8249643874643875\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.48426683806678933, Train acc: 0.8251869658119658\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4856520019820013, Train acc: 0.8251948215183509\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4867760737383241, Train acc: 0.8250830959164293\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4860467774189382, Train acc: 0.8252923976608187\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.48708742677719674, Train acc: 0.8253472222222222\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4860108115437024, Train acc: 0.8258547008547008\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.48546767555229314, Train acc: 0.8261946386946387\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4862014048468752, Train acc: 0.8257850241545893\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4852115038538251, Train acc: 0.8258769586894587\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.48560103141344513, Train acc: 0.8255555555555556\n",
      "Val loss: 0.5221515893936157, Val acc: 0.822\n",
      "Epoch 6/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.42908010141462344, Train acc: 0.8376068376068376\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4637754888106615, Train acc: 0.8287927350427351\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.46232986142407795, Train acc: 0.8317307692307693\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.45340481142585093, Train acc: 0.8366720085470085\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.453754290072327, Train acc: 0.8367521367521368\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.45904588476460206, Train acc: 0.8354700854700855\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4547604515748292, Train acc: 0.8377594627594628\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.45217785546078515, Train acc: 0.8384748931623932\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4507456176294277, Train acc: 0.8391500474833808\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4498265281542499, Train acc: 0.8385683760683761\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4502868556852431, Train acc: 0.8388209013209014\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.45035007940684885, Train acc: 0.8388087606837606\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4503092985235297, Train acc: 0.8392504930966469\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4513619024834171, Train acc: 0.8395909645909646\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4512555853063055, Train acc: 0.8387820512820513\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45212975448260134, Train acc: 0.8388087606837606\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.45145238345782024, Train acc: 0.8394607843137255\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.45281591691105055, Train acc: 0.8391797245963912\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4534683797191753, Train acc: 0.838759559154296\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4539081067021968, Train acc: 0.8390758547008547\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.45281398947561813, Train acc: 0.8392094017094017\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.45226964492440547, Train acc: 0.8395736208236209\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4530106616970975, Train acc: 0.8390932738758826\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4537525700360133, Train acc: 0.8389200498575499\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.453659268832869, Train acc: 0.8388034188034188\n",
      "Val loss: 0.4992574453353882, Val acc: 0.848\n",
      "Epoch 7/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.46768056123684615, Train acc: 0.8317307692307693\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4348868060793377, Train acc: 0.84375\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.43988757921207666, Train acc: 0.8425925925925926\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.43850480777044326, Train acc: 0.84375\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4429622008568711, Train acc: 0.8447649572649573\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.43838410266805067, Train acc: 0.8468660968660968\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.43893939588259867, Train acc: 0.8463827838827839\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4390269093748787, Train acc: 0.8455528846153846\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4348572335415586, Train acc: 0.8474002849002849\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.43501843579877647, Train acc: 0.847542735042735\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.43430829633724904, Train acc: 0.8478049728049728\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4348079979318881, Train acc: 0.8473557692307693\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.43261971442980407, Train acc: 0.8483727810650887\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.43207549022440145, Train acc: 0.848519536019536\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4327477811344838, Train acc: 0.8479700854700855\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4322934985722009, Train acc: 0.8476896367521367\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.43106652652238886, Train acc: 0.8480392156862745\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.43215704235051033, Train acc: 0.8474299620132953\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.43197766170777074, Train acc: 0.8473346828609987\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.43100142560453497, Train acc: 0.8478365384615385\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4294251601811003, Train acc: 0.8483669108669109\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4293677554999152, Train acc: 0.8483634421134422\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.42760787303177056, Train acc: 0.8493125232255667\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.42651399588238903, Train acc: 0.8495370370370371\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4266622133132739, Train acc: 0.849551282051282\n",
      "Val loss: 0.5119876265525818, Val acc: 0.842\n",
      "Epoch 8/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.42320086705124277, Train acc: 0.8461538461538461\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.41944691118521565, Train acc: 0.8453525641025641\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4144968234023817, Train acc: 0.8484686609686609\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4197078959650209, Train acc: 0.8484241452991453\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.41948548996040963, Train acc: 0.8498931623931624\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4165757029845334, Train acc: 0.8520299145299145\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.41301510378409073, Train acc: 0.8527167277167277\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4107659177926297, Train acc: 0.8541666666666666\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.41339108090695603, Train acc: 0.8538698955365622\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.40997577023366066, Train acc: 0.8556089743589743\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4115161127334847, Train acc: 0.8554778554778555\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.40985934762151494, Train acc: 0.8559027777777778\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.408614470501584, Train acc: 0.8564677843523998\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.41045119673450353, Train acc: 0.8553495115995116\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4109027184057779, Train acc: 0.854985754985755\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.41215672781770557, Train acc: 0.8547676282051282\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.41248821911049466, Train acc: 0.8545123177476118\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4126123152797067, Train acc: 0.8547305318138652\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4100451130391481, Train acc: 0.8557411156095367\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4089555205506647, Train acc: 0.8557425213675214\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4091705118749847, Train acc: 0.8556674806674807\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4093985125920341, Train acc: 0.8556963869463869\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4082830139764265, Train acc: 0.8558389074693422\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4068572860014787, Train acc: 0.8562366452991453\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.40697804094761864, Train acc: 0.856025641025641\n",
      "Val loss: 0.49425262212753296, Val acc: 0.85\n",
      "Epoch 9/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.40548094062723666, Train acc: 0.8621794871794872\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.40932539637144816, Train acc: 0.8597756410256411\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.39589062580398343, Train acc: 0.8621794871794872\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4009468668562352, Train acc: 0.859642094017094\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4016668652685789, Train acc: 0.8600427350427351\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4029854148615714, Train acc: 0.8584401709401709\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.401458282332972, Train acc: 0.8590506715506715\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.40240405686199665, Train acc: 0.8587072649572649\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4020751615789881, Train acc: 0.8587962962962963\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.39895790477211657, Train acc: 0.8602029914529915\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.397559139428902, Train acc: 0.8613053613053613\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.39682306599752853, Train acc: 0.8615562678062678\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3972912513645443, Train acc: 0.8615220249835635\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3944844455482104, Train acc: 0.8625610500610501\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3937478830799078, Train acc: 0.8625356125356125\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.39377725177293277, Train acc: 0.8626135149572649\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3932823833057544, Train acc: 0.8629336349924586\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3939462685931018, Train acc: 0.8628027065527065\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.39373925990360953, Train acc: 0.8633884390463338\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.39204798584692496, Train acc: 0.8639423076923077\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3930183232503787, Train acc: 0.8637311762311762\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.39246431082811295, Train acc: 0.8639277389277389\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3911373208487003, Train acc: 0.8643859160163508\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.39007350555926223, Train acc: 0.86502849002849\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.38876620448043203, Train acc: 0.8654059829059829\n",
      "Val loss: 0.43790778517723083, Val acc: 0.874\n",
      "Epoch 10/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3811089526065904, Train acc: 0.8648504273504274\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.38263976760208607, Train acc: 0.8629807692307693\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.37094559469538874, Train acc: 0.8671652421652422\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.37485734792028225, Train acc: 0.8681891025641025\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.3800240020084585, Train acc: 0.867948717948718\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3755871643580263, Train acc: 0.8692129629629629\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.37400953457560004, Train acc: 0.8700396825396826\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3694245742323498, Train acc: 0.8719284188034188\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3710545990563016, Train acc: 0.8710232668566001\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3685431926455508, Train acc: 0.8727029914529915\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.36906747929838707, Train acc: 0.8718919968919969\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3656268416030964, Train acc: 0.8725961538461539\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3647518502125843, Train acc: 0.8722879684418146\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.36642289132116335, Train acc: 0.8718711843711844\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3667252207880686, Train acc: 0.8715811965811966\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.36580070165686435, Train acc: 0.8718282585470085\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3655828089795561, Train acc: 0.8719519859225742\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.36559209384993496, Train acc: 0.8721806742640076\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3659762244535844, Train acc: 0.8721041385515069\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.3654204124186793, Train acc: 0.872275641025641\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.363680913707785, Train acc: 0.8728632478632479\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.36177719427415955, Train acc: 0.8735916860916861\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3624273552031532, Train acc: 0.8730490523968785\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3625418472029481, Train acc: 0.8731971153846154\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.36378256743152937, Train acc: 0.872542735042735\n",
      "Val loss: 0.45474153757095337, Val acc: 0.866\n",
      "Tiempo total de entrenamiento: 234.8883 [s]\n",
      "Epoch 1/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 1.6077311599356496, Train acc: 0.22596153846153846\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.5128741870578537, Train acc: 0.31677350427350426\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.4157684886897053, Train acc: 0.37393162393162394\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.3510430936629956, Train acc: 0.41239316239316237\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.291628655918643, Train acc: 0.44294871794871793\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 1.2534837789854771, Train acc: 0.4614494301994302\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 1.2143156994247903, Train acc: 0.4806929181929182\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 1.1858543766359997, Train acc: 0.4941907051282051\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 1.159304377020594, Train acc: 0.50664767331434\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 1.1337524614527694, Train acc: 0.5186431623931624\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 1.1126721533306272, Train acc: 0.5293317793317793\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 1.0947163048547897, Train acc: 0.5383279914529915\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 1.078032529859618, Train acc: 0.5455703484549639\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 1.0614547264626903, Train acc: 0.5529990842490843\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 1.046787425238862, Train acc: 0.5606837606837607\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 1.0330256486677716, Train acc: 0.5669070512820513\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 1.0194900016679183, Train acc: 0.573340874811463\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 1.007198256375896, Train acc: 0.5795049857549858\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.998066345408217, Train acc: 0.5840643274853801\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.9871259544649694, Train acc: 0.5901442307692307\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.9761754019041105, Train acc: 0.5955433455433455\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.9663769202864069, Train acc: 0.6005730380730381\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.9581156869618612, Train acc: 0.605072463768116\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.948069198847751, Train acc: 0.6099314458689459\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.9382050189197573, Train acc: 0.6145299145299146\n",
      "Val loss: 0.7065708041191101, Val acc: 0.742\n",
      "Epoch 2/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7187268884263487, Train acc: 0.7366452991452992\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.7120918150131519, Train acc: 0.7374465811965812\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6995895103766367, Train acc: 0.7412749287749287\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6881038531597353, Train acc: 0.7462606837606838\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.6914258147661503, Train acc: 0.7454059829059829\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6918381025690978, Train acc: 0.7446581196581197\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6853105492425926, Train acc: 0.7451159951159951\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6853740217211919, Train acc: 0.7447916666666666\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6806593774503905, Train acc: 0.7479819563152896\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6841721624135971, Train acc: 0.7471153846153846\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6818202303052412, Train acc: 0.7476204351204351\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6822690921928468, Train acc: 0.7468839031339032\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6803027714145254, Train acc: 0.747370151216305\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6778218472542489, Train acc: 0.7479014041514042\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6744127136944366, Train acc: 0.7486111111111111\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6723586720947781, Train acc: 0.7494991987179487\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6692515291749322, Train acc: 0.7505027652086476\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.6676218286015143, Train acc: 0.7510386989553656\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6647427567907995, Train acc: 0.7521648672964463\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6619492317430484, Train acc: 0.7529647435897436\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6593677565049096, Train acc: 0.7539173789173789\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6571341784672396, Train acc: 0.754686285936286\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6546010077664153, Train acc: 0.7553186547751766\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6532913637732254, Train acc: 0.7560541310541311\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.651602007802735, Train acc: 0.7568589743589743\n",
      "Val loss: 0.5625020861625671, Val acc: 0.808\n",
      "Epoch 3/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.546618546685602, Train acc: 0.7932692307692307\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5644660760194827, Train acc: 0.7841880341880342\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5784592202246359, Train acc: 0.7811609686609686\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5747722305484817, Train acc: 0.7839209401709402\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.580764076877863, Train acc: 0.7830128205128205\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5809043018184497, Train acc: 0.7814280626780626\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5824458364019754, Train acc: 0.7812881562881563\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5851266667660739, Train acc: 0.7799145299145299\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5789276139433907, Train acc: 0.7827635327635327\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5770551085599467, Train acc: 0.7830662393162393\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5766021601780497, Train acc: 0.783556721056721\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5726177834844657, Train acc: 0.7847222222222222\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5722846740527344, Train acc: 0.7856262327416174\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5701258099650435, Train acc: 0.7865918803418803\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5685727703265655, Train acc: 0.7876780626780627\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5688651811299671, Train acc: 0.7877938034188035\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5690394253842251, Train acc: 0.7878645047762695\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5682741106759336, Train acc: 0.7876009021842355\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5653916845249797, Train acc: 0.7886864597390914\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5645519740624815, Train acc: 0.7895299145299145\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5645066708095878, Train acc: 0.7898351648351648\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5628520525939815, Train acc: 0.7908896658896659\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5607663616814919, Train acc: 0.7919221479004088\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.560689426133372, Train acc: 0.7923789173789174\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5596762729022238, Train acc: 0.792991452991453\n",
      "Val loss: 0.5604637265205383, Val acc: 0.818\n",
      "Epoch 4/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5299322001444988, Train acc: 0.7948717948717948\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5206740909916723, Train acc: 0.8007478632478633\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5238965373178492, Train acc: 0.8035968660968661\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5289560284815792, Train acc: 0.8034188034188035\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5259001222176429, Train acc: 0.8079059829059829\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.524101300234044, Train acc: 0.8088497150997151\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5261224799758786, Train acc: 0.8083028083028083\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5231465558943331, Train acc: 0.8078926282051282\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.5219738363851736, Train acc: 0.8075142450142451\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5238721644012337, Train acc: 0.8067307692307693\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5237754184489298, Train acc: 0.8070609945609946\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5246147159858476, Train acc: 0.8057336182336182\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.524031201155855, Train acc: 0.8060486522024983\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5238165386175789, Train acc: 0.8061660561660562\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5239379921784768, Train acc: 0.8072649572649573\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5226371418764322, Train acc: 0.8082598824786325\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.521650947263308, Train acc: 0.8092634489693313\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5214532344599395, Train acc: 0.8092948717948718\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.520059277701099, Train acc: 0.8096884840305892\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5193353978678202, Train acc: 0.8095352564102564\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5208523391003428, Train acc: 0.8092694342694343\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5198059615671126, Train acc: 0.8097562160062161\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5179090346048774, Train acc: 0.8108974358974359\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5151291764434162, Train acc: 0.8119880698005698\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5166271342375339, Train acc: 0.8114529914529914\n",
      "Val loss: 0.48515230417251587, Val acc: 0.85\n",
      "Epoch 5/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.4737188314907571, Train acc: 0.8247863247863247\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.47879840201164925, Train acc: 0.8231837606837606\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4739187352805056, Train acc: 0.8251424501424501\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4741576770081734, Train acc: 0.8247863247863247\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.47492063449234023, Train acc: 0.8236111111111111\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.47944803337468384, Train acc: 0.8213141025641025\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4842314984699454, Train acc: 0.8198260073260073\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.48366050014638495, Train acc: 0.819778311965812\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4829520850467999, Train acc: 0.818613485280152\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4840210661419436, Train acc: 0.8189636752136752\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.48308476506390124, Train acc: 0.8197843822843823\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.48262448270309005, Train acc: 0.8206463675213675\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.480817282189083, Train acc: 0.8217044707429323\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4824229206011991, Train acc: 0.8218101343101343\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4788022210327988, Train acc: 0.8237179487179487\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.47852528381408155, Train acc: 0.8242521367521367\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.4774137664457072, Train acc: 0.8249434389140271\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4767205099109118, Train acc: 0.8250830959164293\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4775158802882755, Train acc: 0.8250393612235718\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4769724178995587, Train acc: 0.8250267094017094\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4764955135279911, Train acc: 0.8256766381766382\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4742890642905245, Train acc: 0.8266559829059829\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.47408083293604963, Train acc: 0.8268998513563731\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4746605338173205, Train acc: 0.8266559829059829\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.47211843606498505, Train acc: 0.8278846153846153\n",
      "Val loss: 0.6132166981697083, Val acc: 0.816\n",
      "Epoch 6/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.43462556485946363, Train acc: 0.8397435897435898\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4456385040703492, Train acc: 0.8397435897435898\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4535594951265898, Train acc: 0.8345797720797721\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4471935615198225, Train acc: 0.8374732905982906\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4465143369176449, Train acc: 0.8378205128205128\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4500165505286975, Train acc: 0.8352920227920227\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.44602028787208853, Train acc: 0.8361568986568987\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.45096783799270534, Train acc: 0.8344684829059829\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4513710822418318, Train acc: 0.8354107312440646\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4530912508503494, Train acc: 0.8348824786324787\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.45455961255899396, Train acc: 0.8347902097902098\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4527600499459057, Train acc: 0.8358707264957265\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.45513573482407305, Train acc: 0.8345660749506904\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4542229637590047, Train acc: 0.8345161782661783\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4535539378234294, Train acc: 0.8346866096866097\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45240216931670457, Train acc: 0.8353365384615384\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.45170080785575434, Train acc: 0.835501508295626\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4499885331227286, Train acc: 0.8362713675213675\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.44773586425181183, Train acc: 0.8369883040935673\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4475820821797491, Train acc: 0.8371527777777777\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4470822500882956, Train acc: 0.8374287749287749\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4452418284237431, Train acc: 0.8383595571095571\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.44451650091149647, Train acc: 0.8391861761426979\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.44490151542989786, Train acc: 0.8390981125356125\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.44276922185706274, Train acc: 0.8400427350427351\n",
      "Val loss: 0.5243015885353088, Val acc: 0.832\n",
      "Epoch 7/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.39268601080800736, Train acc: 0.8589743589743589\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.39854376177247774, Train acc: 0.8597756410256411\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4083611383991703, Train acc: 0.8552350427350427\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4161255633474415, Train acc: 0.8500267094017094\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4197813223569821, Train acc: 0.8483974358974359\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.42084541482760696, Train acc: 0.8475783475783476\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4204454605032091, Train acc: 0.8479090354090354\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4209667824399777, Train acc: 0.8482905982905983\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4205655129829816, Train acc: 0.8488841405508072\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.42153199880232667, Train acc: 0.8477564102564102\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.42048678189666583, Train acc: 0.8476592851592851\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.41968695426948815, Train acc: 0.8477564102564102\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4182293386682917, Train acc: 0.8487836949375411\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4159080018295073, Train acc: 0.8497405372405372\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.41695476300653567, Train acc: 0.8496794871794872\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.41857416389119995, Train acc: 0.8491252670940171\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.42045316789177506, Train acc: 0.8481649069884364\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.41929107338388194, Train acc: 0.8488841405508072\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4201089293803656, Train acc: 0.8487123256860098\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4182424531310287, Train acc: 0.8492788461538462\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.41909244241731947, Train acc: 0.8490791615791616\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.41861044825072796, Train acc: 0.8492132867132867\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4183841936348407, Train acc: 0.8492660720921591\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.41803138775544035, Train acc: 0.8495815527065527\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4180223284266953, Train acc: 0.8495726495726496\n",
      "Val loss: 0.49840953946113586, Val acc: 0.85\n",
      "Epoch 8/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3780947717336508, Train acc: 0.8707264957264957\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.38632051137268036, Train acc: 0.8632478632478633\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.40058436911245354, Train acc: 0.8580840455840456\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4059618934352174, Train acc: 0.8553685897435898\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4083152862313466, Train acc: 0.8552350427350427\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4046063232540745, Train acc: 0.8575498575498576\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4029297129852613, Train acc: 0.8582875457875457\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4008959197562634, Train acc: 0.859375\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4015745525067037, Train acc: 0.8583214624881291\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4054312761538686, Train acc: 0.8565705128205128\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4066843560157979, Train acc: 0.8556721056721057\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.40535862298225955, Train acc: 0.8559918091168092\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4019814395798244, Train acc: 0.8579881656804734\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4010688836296272, Train acc: 0.8579822954822954\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.40011607027185436, Train acc: 0.8584045584045584\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4008554099595302, Train acc: 0.8581396901709402\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.401337201980047, Train acc: 0.8579374057315233\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4008409971749836, Train acc: 0.8579950142450142\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.40195266990174566, Train acc: 0.8574842555105713\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4020728022584485, Train acc: 0.8574786324786324\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4015058329458529, Train acc: 0.8571937321937322\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.4019094336953046, Train acc: 0.8571289821289821\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.40363855689240735, Train acc: 0.8566982534373839\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4017268102463505, Train acc: 0.8573050213675214\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4028485777394639, Train acc: 0.856517094017094\n",
      "Val loss: 0.45539629459381104, Val acc: 0.85\n",
      "Epoch 9/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.3672192671105393, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.36358361238311243, Train acc: 0.8688568376068376\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3771883978367786, Train acc: 0.8659188034188035\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.37654071133663386, Train acc: 0.8675213675213675\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.37807561879675106, Train acc: 0.8667735042735043\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3808190012138411, Train acc: 0.8662749287749287\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.37741404733369277, Train acc: 0.8678266178266179\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3803571323947742, Train acc: 0.8656517094017094\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3831277145842287, Train acc: 0.8634852801519468\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3829568228453525, Train acc: 0.8629807692307693\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3847945123583819, Train acc: 0.8623251748251748\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3847540787001939, Train acc: 0.8623575498575499\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.38411342250617964, Train acc: 0.8634122287968442\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.38518143966407387, Train acc: 0.8631715506715507\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3848518524828375, Train acc: 0.8633190883190883\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.3849652227079973, Train acc: 0.8634815705128205\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3862982943772056, Train acc: 0.8625251382604324\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3853904422512322, Train acc: 0.8627730294396961\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.38474700957942237, Train acc: 0.8630510571300045\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.38486308803081387, Train acc: 0.8625267094017094\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3841973218542255, Train acc: 0.8632224257224257\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3835148381318234, Train acc: 0.8635878010878011\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3853048901044217, Train acc: 0.8629691564474173\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.38513894689248135, Train acc: 0.8631365740740741\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.3850652426271102, Train acc: 0.8632264957264957\n",
      "Val loss: 0.4977702796459198, Val acc: 0.854\n",
      "Epoch 10/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.34913479718260276, Train acc: 0.875\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.37206333517455137, Train acc: 0.8677884615384616\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.36682840231859104, Train acc: 0.8705484330484331\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.37305425913829326, Train acc: 0.8697916666666666\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.36841367948641124, Train acc: 0.870940170940171\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.37272987068973035, Train acc: 0.8693910256410257\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.37467170446674464, Train acc: 0.8681318681318682\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3760841963414708, Train acc: 0.866920405982906\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3782526057528408, Train acc: 0.8655033238366572\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3792736382311226, Train acc: 0.8644764957264958\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3756392613782749, Train acc: 0.8661616161616161\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.37563139991171424, Train acc: 0.8667646011396012\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.37502214827138214, Train acc: 0.8669871794871795\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.37414038257038856, Train acc: 0.8671398046398047\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.375468468774333, Train acc: 0.8668091168091168\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.3748754215542163, Train acc: 0.8673878205128205\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3766811181004021, Train acc: 0.8662016088486677\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3751427611550148, Train acc: 0.8667497625830959\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3740216386757935, Train acc: 0.8673526765632029\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.374252643633602, Train acc: 0.8675480769230769\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3742504292855697, Train acc: 0.8673941798941799\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3733504056314002, Train acc: 0.8678370240870241\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.37291771840596233, Train acc: 0.8679394277220365\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.3730349112103064, Train acc: 0.8678107193732194\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.3722077353617065, Train acc: 0.8682478632478633\n",
      "Val loss: 0.5030882954597473, Val acc: 0.848\n",
      "Tiempo total de entrenamiento: 232.3787 [s]\n",
      "Epoch 1/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 1.5647240174122345, Train acc: 0.26175213675213677\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 1.5192525977762337, Train acc: 0.3020833333333333\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 1.4909463146133641, Train acc: 0.33084045584045585\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 1.4571769763516564, Train acc: 0.3581730769230769\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 1.413419342856122, Train acc: 0.3875\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 1.366769350831665, Train acc: 0.4133725071225071\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 1.32187102216504, Train acc: 0.43612637362637363\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 1.2789115339008152, Train acc: 0.46000267094017094\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 1.2400407634342951, Train acc: 0.4789292497625831\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 1.206191257508392, Train acc: 0.4967948717948718\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 1.1765014378016738, Train acc: 0.5121406371406372\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 1.1516610832508134, Train acc: 0.5250178062678063\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 1.126698755085429, Train acc: 0.5375575279421433\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 1.1032065848509471, Train acc: 0.5500228937728938\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 1.0840328450556154, Train acc: 0.5591524216524216\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 1.0646627869011245, Train acc: 0.5682425213675214\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 1.0480198554054105, Train acc: 0.5761689291101055\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 1.0330167771064998, Train acc: 0.5833630104463438\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 1.0175125263480522, Train acc: 0.591374269005848\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 1.0050968017333592, Train acc: 0.5970619658119658\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.9924801496237723, Train acc: 0.6034035409035409\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.9796295057875107, Train acc: 0.6097999222999223\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.9673909716825865, Train acc: 0.6152220364176886\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.9556700871392363, Train acc: 0.6207710113960114\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.9461308016430618, Train acc: 0.6252564102564102\n",
      "Val loss: 0.7117549180984497, Val acc: 0.734\n",
      "Epoch 2/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.7028592070962629, Train acc: 0.7542735042735043\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.6930794588520995, Train acc: 0.749465811965812\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.6992625358097914, Train acc: 0.7423433048433048\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.6876018506466833, Train acc: 0.7457264957264957\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.673664069481385, Train acc: 0.7513888888888889\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.6676488480294532, Train acc: 0.7528490028490028\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.6646172328054978, Train acc: 0.7542735042735043\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.6627121796497167, Train acc: 0.7542067307692307\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.6649464679720961, Train acc: 0.7534425451092118\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.6623577953149111, Train acc: 0.7550747863247863\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.6615903711448645, Train acc: 0.7557303807303807\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.6591016910864077, Train acc: 0.7565883190883191\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.6584871004358238, Train acc: 0.7572320841551611\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.6570983473689128, Train acc: 0.7577838827838828\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.6535623005142919, Train acc: 0.7586538461538461\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.6502692592131276, Train acc: 0.7603498931623932\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.6477450717178764, Train acc: 0.7620035193564605\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.644988454432569, Train acc: 0.7633843779677113\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.6416490339999579, Train acc: 0.7649291497975709\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.6401203377761393, Train acc: 0.7653846153846153\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.6382468932095611, Train acc: 0.766407203907204\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.6360539296220789, Train acc: 0.7668026418026418\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.6355937364286581, Train acc: 0.7672798216276477\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.6344744908090076, Train acc: 0.7674946581196581\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.6325274095423201, Train acc: 0.7681837606837607\n",
      "Val loss: 0.5612433552742004, Val acc: 0.788\n",
      "Epoch 3/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.6041542217772231, Train acc: 0.782051282051282\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.592108925884096, Train acc: 0.780982905982906\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.5824597075666458, Train acc: 0.7854344729344729\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5723551109465014, Train acc: 0.7908653846153846\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5735348686321169, Train acc: 0.7896367521367521\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.5754350958620211, Train acc: 0.7901531339031339\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.5728062736336711, Train acc: 0.7908272283272283\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.5701695914404132, Train acc: 0.7924011752136753\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.566352596430414, Train acc: 0.7943376068376068\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.5650168162189487, Train acc: 0.7950320512820512\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.5647158437663952, Train acc: 0.7960858585858586\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.5635501976865225, Train acc: 0.7963853276353277\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5610623681803897, Train acc: 0.7971318211702827\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.5591922462423206, Train acc: 0.797657203907204\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5597894409697959, Train acc: 0.7975427350427351\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5586623011443478, Train acc: 0.7979433760683761\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5583459298445269, Train acc: 0.7973856209150327\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5562822476722803, Train acc: 0.7985814339981007\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5556867503524631, Train acc: 0.7991734143049932\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5538793216785814, Train acc: 0.7996794871794872\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5534228597762724, Train acc: 0.799908424908425\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5529974729731598, Train acc: 0.7999708624708625\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5543251580761871, Train acc: 0.7990523968784838\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5524234997421673, Train acc: 0.7997462606837606\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.5507803136874468, Train acc: 0.8002136752136753\n",
      "Val loss: 0.556502640247345, Val acc: 0.82\n",
      "Epoch 4/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.5276644921455628, Train acc: 0.8125\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.5123834009799693, Train acc: 0.8151709401709402\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.507439076454721, Train acc: 0.8133903133903134\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.5035257375616039, Train acc: 0.8161057692307693\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.5010955670195767, Train acc: 0.8188034188034188\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4998204794916672, Train acc: 0.8198005698005698\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.49909548839879414, Train acc: 0.8195970695970696\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4994467858893749, Train acc: 0.8193108974358975\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4981045972799983, Train acc: 0.8196225071225072\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4988956777904278, Train acc: 0.8205662393162393\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.49431117012135817, Train acc: 0.8224553224553225\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.49926511240205024, Train acc: 0.8209134615384616\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.5016168613503592, Train acc: 0.8202251808021038\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4993687219444721, Train acc: 0.82123778998779\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.5007611567181061, Train acc: 0.8205128205128205\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.5014028473773128, Train acc: 0.820846688034188\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.5011470987568734, Train acc: 0.8207956259426847\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.5005684052288872, Train acc: 0.8203644349477682\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.5007142876980812, Train acc: 0.8205128205128205\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.5012574527762894, Train acc: 0.8202991452991453\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.5008048780192725, Train acc: 0.8203601953601953\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.5009746196380909, Train acc: 0.8202457264957265\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.5016351366344326, Train acc: 0.8200018580453363\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.5004606442525983, Train acc: 0.8202457264957265\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.49963729626093156, Train acc: 0.8203846153846154\n",
      "Val loss: 0.5177980065345764, Val acc: 0.824\n",
      "Epoch 5/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.47075734625005317, Train acc: 0.8183760683760684\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.46417447978742105, Train acc: 0.8250534188034188\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.466180715104963, Train acc: 0.823539886039886\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.46726103405603486, Train acc: 0.8247863247863247\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.46683061622146865, Train acc: 0.826602564102564\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.4629522246438638, Train acc: 0.8283475783475783\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.4678598921677787, Train acc: 0.8286019536019537\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4647851629126976, Train acc: 0.8306623931623932\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.46731695237393506, Train acc: 0.8294753086419753\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.4633563039999487, Train acc: 0.8308226495726496\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4595206032167735, Train acc: 0.8329448329448329\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.4568263816652538, Train acc: 0.8340010683760684\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.45637716709671056, Train acc: 0.8342784352399737\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4582843122985646, Train acc: 0.8336385836385837\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4603422968911055, Train acc: 0.8330128205128206\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.45838201858807737, Train acc: 0.8333667200854701\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.459476080727321, Train acc: 0.8328305681246858\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.45946596060231226, Train acc: 0.8331849477682811\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.46087695482332347, Train acc: 0.832995951417004\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4618208636179503, Train acc: 0.8325320512820513\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.45930830016656404, Train acc: 0.8333333333333334\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.45910550572959347, Train acc: 0.8335275835275835\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.4574527509935513, Train acc: 0.8341462281679674\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4573182240427018, Train acc: 0.8340010683760684\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.45761405480022616, Train acc: 0.8342521367521367\n",
      "Val loss: 0.5049992799758911, Val acc: 0.832\n",
      "Epoch 6/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.45263685311517143, Train acc: 0.8258547008547008\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4300432837385143, Train acc: 0.8410790598290598\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4314456740102367, Train acc: 0.843482905982906\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.43333987456260836, Train acc: 0.8426816239316239\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.4286919159703275, Train acc: 0.8438034188034188\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.43049805047271766, Train acc: 0.8438390313390314\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.42675231232038346, Train acc: 0.8447802197802198\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.4275227843497235, Train acc: 0.844551282051282\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.4274374929001132, Train acc: 0.8452041785375118\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.42722024663200237, Train acc: 0.8452457264957265\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.42345768171011616, Train acc: 0.8474164724164724\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.425301848801962, Train acc: 0.8469996438746439\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.4290966243041651, Train acc: 0.8458251150558843\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.42754352207367236, Train acc: 0.8466880341880342\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.42712524447940353, Train acc: 0.846937321937322\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.4265901622278855, Train acc: 0.8469551282051282\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.42607138354037205, Train acc: 0.8471593765711413\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.42594575754774544, Train acc: 0.8469254510921178\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.4271030530879828, Train acc: 0.8462663067926226\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.4264800385548136, Train acc: 0.8465544871794872\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.42685550323766713, Train acc: 0.8462301587301587\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.42586226198495175, Train acc: 0.8467851592851593\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.42515893017894624, Train acc: 0.8472918989223337\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.42571318987889634, Train acc: 0.8471999643874644\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.42686063366440624, Train acc: 0.8468162393162393\n",
      "Val loss: 0.5356758236885071, Val acc: 0.814\n",
      "Epoch 7/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.42485180388913196, Train acc: 0.844017094017094\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.4176219366490841, Train acc: 0.8466880341880342\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.4129196818058307, Train acc: 0.8516737891737892\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.4075591253100807, Train acc: 0.8559027777777778\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.41144049938163185, Train acc: 0.8532051282051282\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.40957127813982147, Train acc: 0.8530092592592593\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.406820813814799, Train acc: 0.8536324786324786\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.40515958572713995, Train acc: 0.8547676282051282\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.40852014118308466, Train acc: 0.8540479582146249\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.409701883003243, Train acc: 0.8536324786324786\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.4094265466432935, Train acc: 0.8529040404040404\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.40925054591244614, Train acc: 0.8526976495726496\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.409492746051854, Train acc: 0.8522353714661407\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.4104517986645447, Train acc: 0.8526022588522588\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.4092016434079392, Train acc: 0.8525997150997151\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.40892983836312896, Train acc: 0.8524973290598291\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.40819560392357523, Train acc: 0.8528154851684263\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.4065654585677476, Train acc: 0.8532169990503324\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.40601542097186644, Train acc: 0.8535200179937023\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.40554923026575745, Train acc: 0.853659188034188\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.4048643370921237, Train acc: 0.8539377289377289\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.40496559193750037, Train acc: 0.854190947940948\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.40662819170505526, Train acc: 0.8531215161649944\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.4059304067018598, Train acc: 0.8533431267806267\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.4049574540912086, Train acc: 0.8538461538461538\n",
      "Val loss: 0.49626874923706055, Val acc: 0.85\n",
      "Epoch 8/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.41779568523932725, Train acc: 0.8509615384615384\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.40758225788227004, Train acc: 0.8549679487179487\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.3976054094689354, Train acc: 0.8566595441595442\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.39194311648129654, Train acc: 0.8603098290598291\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.38749527260661126, Train acc: 0.861965811965812\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3849241450015042, Train acc: 0.8628917378917379\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.3804257451005107, Train acc: 0.8655372405372406\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.380986564900153, Train acc: 0.8643830128205128\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.379481248600021, Train acc: 0.8647910731244065\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.3800459363585354, Train acc: 0.8645833333333334\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.379798975944658, Train acc: 0.8639763014763014\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3797028115937258, Train acc: 0.8636485042735043\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.381012945936198, Train acc: 0.8625493096646942\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.38312397324576775, Train acc: 0.8613018925518926\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.385667254922227, Train acc: 0.8605769230769231\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.3850281700058084, Train acc: 0.8608440170940171\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.385106680501775, Train acc: 0.8604512317747612\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.38556144512652046, Train acc: 0.8603098290598291\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.38521708059910176, Train acc: 0.8604082321187584\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.38466835444331426, Train acc: 0.860176282051282\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.38486268222150055, Train acc: 0.8602971102971103\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.38617295119983797, Train acc: 0.8603098290598291\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.38711954104506097, Train acc: 0.8604375696767002\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.38682188414716484, Train acc: 0.8605546652421653\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.3864616485666006, Train acc: 0.8603846153846154\n",
      "Val loss: 0.4690355360507965, Val acc: 0.86\n",
      "Epoch 9/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.35435797057600105, Train acc: 0.8696581196581197\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.343815765542607, Train acc: 0.874465811965812\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.36287262016891414, Train acc: 0.8696581196581197\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3627381252331866, Train acc: 0.8685897435897436\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.36351103141394436, Train acc: 0.8688034188034188\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3620499322121894, Train acc: 0.8701923076923077\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.36332233379775786, Train acc: 0.868513431013431\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.36221721592064726, Train acc: 0.8677884615384616\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3625729600567254, Train acc: 0.8675807217473884\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.36485485495983533, Train acc: 0.8666666666666667\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3680370469934364, Train acc: 0.8662101787101787\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.36587854286926424, Train acc: 0.8664084757834758\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.3666853278527559, Train acc: 0.8661242603550295\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3658248477334541, Train acc: 0.8667200854700855\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.3645577422425448, Train acc: 0.8679131054131054\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.36675459225502854, Train acc: 0.8678552350427351\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3690840252522939, Train acc: 0.8669243338360986\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.36924502520281594, Train acc: 0.8668981481481481\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.36726451494232043, Train acc: 0.8678587494376968\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.36788097403497777, Train acc: 0.867681623931624\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.3686121010650245, Train acc: 0.8673941798941799\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.36877161583326984, Train acc: 0.8678127428127428\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.37029788897288685, Train acc: 0.8671729840208101\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.36908345165240586, Train acc: 0.8676326566951567\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.36842920309585386, Train acc: 0.8676923076923077\n",
      "Val loss: 0.5474680662155151, Val acc: 0.838\n",
      "Epoch 10/10\n",
      "Iteration 117 - Batch 117/3030 - Train loss: 0.32739867040744197, Train acc: 0.8840811965811965\n",
      "Iteration 234 - Batch 234/3030 - Train loss: 0.3412642144303546, Train acc: 0.8766025641025641\n",
      "Iteration 351 - Batch 351/3030 - Train loss: 0.349792555356637, Train acc: 0.8726851851851852\n",
      "Iteration 468 - Batch 468/3030 - Train loss: 0.3466081525493636, Train acc: 0.875534188034188\n",
      "Iteration 585 - Batch 585/3030 - Train loss: 0.35020303800065294, Train acc: 0.8742521367521368\n",
      "Iteration 702 - Batch 702/3030 - Train loss: 0.3490448410641349, Train acc: 0.874732905982906\n",
      "Iteration 819 - Batch 819/3030 - Train loss: 0.3489172771363832, Train acc: 0.8754578754578755\n",
      "Iteration 936 - Batch 936/3030 - Train loss: 0.3527193201156572, Train acc: 0.8743990384615384\n",
      "Iteration 1053 - Batch 1053/3030 - Train loss: 0.3519229331855987, Train acc: 0.8743471035137702\n",
      "Iteration 1170 - Batch 1170/3030 - Train loss: 0.35098361641678033, Train acc: 0.874732905982906\n",
      "Iteration 1287 - Batch 1287/3030 - Train loss: 0.3506874836251869, Train acc: 0.8753399378399378\n",
      "Iteration 1404 - Batch 1404/3030 - Train loss: 0.3535927878639679, Train acc: 0.8745548433048433\n",
      "Iteration 1521 - Batch 1521/3030 - Train loss: 0.35192947091631477, Train acc: 0.8748767258382643\n",
      "Iteration 1638 - Batch 1638/3030 - Train loss: 0.3550963639480691, Train acc: 0.8739316239316239\n",
      "Iteration 1755 - Batch 1755/3030 - Train loss: 0.35643088640777815, Train acc: 0.8738603988603989\n",
      "Iteration 1872 - Batch 1872/3030 - Train loss: 0.3552466013078761, Train acc: 0.8741653311965812\n",
      "Iteration 1989 - Batch 1989/3030 - Train loss: 0.3550571052946596, Train acc: 0.8747486173956762\n",
      "Iteration 2106 - Batch 2106/3030 - Train loss: 0.3556039147556592, Train acc: 0.8748219373219374\n",
      "Iteration 2223 - Batch 2223/3030 - Train loss: 0.3539489359836615, Train acc: 0.8755060728744939\n",
      "Iteration 2340 - Batch 2340/3030 - Train loss: 0.3546737585375961, Train acc: 0.8749465811965812\n",
      "Iteration 2457 - Batch 2457/3030 - Train loss: 0.35426324481083143, Train acc: 0.8747710622710623\n",
      "Iteration 2574 - Batch 2574/3030 - Train loss: 0.3550116415066817, Train acc: 0.8740530303030303\n",
      "Iteration 2691 - Batch 2691/3030 - Train loss: 0.3541418971166441, Train acc: 0.874187105165366\n",
      "Iteration 2808 - Batch 2808/3030 - Train loss: 0.35349652194418013, Train acc: 0.8744435541310541\n",
      "Iteration 2925 - Batch 2925/3030 - Train loss: 0.35423083821231993, Train acc: 0.8741880341880341\n",
      "Val loss: 0.43704307079315186, Val acc: 0.864\n",
      "Tiempo total de entrenamiento: 236.7524 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.6038095370317116, Train acc: 0.23559415494899366\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.5950736291388161, Train acc: 0.25861593603529087\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.5851579517380805, Train acc: 0.2708390772906902\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.5699374217253466, Train acc: 0.2900813344361731\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.546796226909018, Train acc: 0.31213123793768954\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.5227787110880229, Train acc: 0.33036945133719325\n",
      "Val loss: 1.2087346315383911, Val acc: 0.558\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.2997357733229287, Train acc: 0.4665012406947891\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.2752394133653395, Train acc: 0.47325613454645715\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.252493602937443, Train acc: 0.48396287105964525\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.2326037345024257, Train acc: 0.48842018196856907\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.2163594252023942, Train acc: 0.494816652881169\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.2024350936426396, Train acc: 0.5\n",
      "Val loss: 0.9598149657249451, Val acc: 0.634\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.115908704761766, Train acc: 0.5191618417424869\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.1014232556534629, Train acc: 0.532671629445823\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.0959153078560135, Train acc: 0.5340961308703244\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.0916220085997868, Train acc: 0.5361869313482217\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.0883967938586179, Train acc: 0.5370829886958919\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.0830597624479876, Train acc: 0.5391278375149343\n",
      "Val loss: 0.882258415222168, Val acc: 0.682\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.059164233187325, Train acc: 0.5479735318444996\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.0451974237067068, Train acc: 0.55479735318445\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.0430075538124455, Train acc: 0.5526606010476979\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.0401499180966973, Train acc: 0.5538668320926385\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.0361709130116, Train acc: 0.556189688447753\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.0318432405293836, Train acc: 0.5587262200165426\n",
      "Val loss: 0.8777435421943665, Val acc: 0.716\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.0079320974839039, Train acc: 0.5754066721808657\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9998166280933934, Train acc: 0.5820237110559692\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.000806194255155, Train acc: 0.5809668229023067\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9957148613583329, Train acc: 0.5839881444720154\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9908071303978945, Train acc: 0.5861317893575958\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9860230033893531, Train acc: 0.5868026835768771\n",
      "Val loss: 0.8159047365188599, Val acc: 0.726\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9440362433083037, Train acc: 0.5985663082437276\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.954275274123901, Train acc: 0.5929142542045768\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9538040286795026, Train acc: 0.5967282418895322\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9546130333955472, Train acc: 0.5980493520816101\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9529011038633494, Train acc: 0.5987593052109181\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9526007155916969, Train acc: 0.6010706736513188\n",
      "Val loss: 0.8073342442512512, Val acc: 0.724\n",
      "Epoch 7/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9282016158103943, Train acc: 0.6171767300799559\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9340089271720659, Train acc: 0.6179349324510615\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.926904377434668, Train acc: 0.621496186012315\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9252959022409896, Train acc: 0.6202095395643783\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9246416354790712, Train acc: 0.619933829611249\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9202593255076993, Train acc: 0.6218867751125816\n",
      "Val loss: 0.7921387553215027, Val acc: 0.722\n",
      "Epoch 8/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9156756558988848, Train acc: 0.6229666390956714\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9044740829202864, Train acc: 0.6292390405293631\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9074626262710984, Train acc: 0.6277915632754343\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8973651025285069, Train acc: 0.6315136476426799\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8932361242098686, Train acc: 0.6332506203473945\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8917137574606132, Train acc: 0.6329611248966087\n",
      "Val loss: 0.8011319637298584, Val acc: 0.716\n",
      "Epoch 9/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8800572212944683, Train acc: 0.6403363661428177\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8689383000899584, Train acc: 0.6432313206506755\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8619556544173477, Train acc: 0.6429096590386914\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8673768895558822, Train acc: 0.6428177557209815\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8650076564560588, Train acc: 0.6435621725944306\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8666310555744715, Train acc: 0.6433002481389578\n",
      "Val loss: 0.7822937965393066, Val acc: 0.716\n",
      "Epoch 10/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8572805793876321, Train acc: 0.6406120760959471\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8518595394925175, Train acc: 0.6441273779983457\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.850585971123133, Train acc: 0.6448855803694513\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8526092085063967, Train acc: 0.6436448855803695\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8509039852354262, Train acc: 0.6454370002757099\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8494378371625884, Train acc: 0.6469763808473485\n",
      "Val loss: 0.7623077630996704, Val acc: 0.722\n",
      "Epoch 11/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8269973040645958, Train acc: 0.6691480562448304\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8226796314757094, Train acc: 0.6696305486628067\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.82545735048093, Train acc: 0.6655178751952946\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8206213866798286, Train acc: 0.6666666666666666\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8227245369528093, Train acc: 0.6655913978494624\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8212296794622372, Train acc: 0.6663909567135373\n",
      "Val loss: 0.7641769051551819, Val acc: 0.696\n",
      "Epoch 12/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8039835844284449, Train acc: 0.6713537358698649\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.801727790863086, Train acc: 0.6729390681003584\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8032515745217305, Train acc: 0.6716294458229942\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8040413103806667, Train acc: 0.6723876481940998\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8025632273437631, Train acc: 0.6726771436448856\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8002810469719759, Train acc: 0.6730769230769231\n",
      "Val loss: 0.7761346697807312, Val acc: 0.668\n",
      "Epoch 13/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7914177469718151, Train acc: 0.676867934932451\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7862575825972434, Train acc: 0.6814171491590847\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7822228987672051, Train acc: 0.6793493245106148\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7854598758057652, Train acc: 0.6789702233250621\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7872105763508723, Train acc: 0.678108629721533\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7880905044724119, Train acc: 0.678476242992372\n",
      "Val loss: 0.7743496298789978, Val acc: 0.67\n",
      "Epoch 14/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7885917893841735, Train acc: 0.6790736145574855\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7819300085051447, Train acc: 0.6816239316239316\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7775998753190381, Train acc: 0.6829335539012958\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7787942415756038, Train acc: 0.6832781913427075\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7806287605028887, Train acc: 0.6816101461262751\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7779025571723269, Train acc: 0.6821753515301903\n",
      "Val loss: 0.7813372015953064, Val acc: 0.626\n",
      "Epoch 15/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.776318109443045, Train acc: 0.684449958643507\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7715713766395537, Train acc: 0.6841742486903777\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7687636102706279, Train acc: 0.6849554268909107\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7715754720390352, Train acc: 0.6865867107802591\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7696199661646134, Train acc: 0.6877584780810587\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7679036617533773, Train acc: 0.6878963330576234\n",
      "Val loss: 0.8028311133384705, Val acc: 0.618\n",
      "Early stopping at epoch 15 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 136.2058 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.6040194034576416, Train acc: 0.23076923076923078\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.5899318135701692, Train acc: 0.24283154121863798\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.5646777251507142, Train acc: 0.2720797720797721\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.5316992985387134, Train acc: 0.3017990074441687\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.506474942427415, Train acc: 0.32486903777226356\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.4816706888696067, Train acc: 0.34415494899365867\n",
      "Val loss: 1.1874455213546753, Val acc: 0.542\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.2724601060916216, Train acc: 0.47932175351530193\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.2605432223560464, Train acc: 0.4811138682106424\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.2401741748182182, Train acc: 0.4872254388383421\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.2216513493122199, Train acc: 0.49248690377722637\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.2116324904637459, Train acc: 0.4952577888061759\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.200626505575968, Train acc: 0.5001148791471373\n",
      "Val loss: 1.0030313730239868, Val acc: 0.64\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.1209890419601376, Train acc: 0.5263303005238489\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.1050173637703953, Train acc: 0.532464846980976\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.1019174826790465, Train acc: 0.5329013877400974\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.0971181646116779, Train acc: 0.5353253377446926\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.0923629832063986, Train acc: 0.536641852770885\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.0876158459743543, Train acc: 0.5402766289863064\n",
      "Val loss: 0.919346034526825, Val acc: 0.686\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.0376456604044662, Train acc: 0.5554177005789909\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.0336522238376813, Train acc: 0.5591397849462365\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.0295559257863254, Train acc: 0.5670434702692767\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.03026011127692, Train acc: 0.566411634960022\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.0273200144115677, Train acc: 0.566914805624483\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.0231412802836155, Train acc: 0.5680773825935116\n",
      "Val loss: 0.8836559057235718, Val acc: 0.694\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9888783061606252, Train acc: 0.5883650399779432\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9945395703499134, Train acc: 0.5854011579818031\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9889735499678174, Train acc: 0.5854241338112306\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.98708770213983, Train acc: 0.58915770609319\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9860834710618369, Train acc: 0.5899917287014061\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9831512745289381, Train acc: 0.5917195110743498\n",
      "Val loss: 0.8640909790992737, Val acc: 0.714\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9635827597389873, Train acc: 0.5970499035015164\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9658698409031599, Train acc: 0.6010476978218914\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9591756048705163, Train acc: 0.6014612627515853\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9577353187860587, Train acc: 0.6036324786324786\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9528333394955366, Train acc: 0.6065894678797905\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.952803364676288, Train acc: 0.6067457035198971\n",
      "Val loss: 0.8544392585754395, Val acc: 0.702\n",
      "Epoch 7/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9346326722039117, Train acc: 0.6120760959470637\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9404732704672039, Train acc: 0.6061483319547836\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9358069604618258, Train acc: 0.6095028030511902\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9336665009076779, Train acc: 0.6124896608767576\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9321689358124366, Train acc: 0.6145574855252275\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9303944261155577, Train acc: 0.6164874551971327\n",
      "Val loss: 0.8565499186515808, Val acc: 0.718\n",
      "Epoch 8/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9079550402796167, Train acc: 0.6226909291425421\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9165971880285149, Train acc: 0.6241384063964709\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9146970240478842, Train acc: 0.6269184817571914\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9120513111607641, Train acc: 0.6288254755996692\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9073112199449131, Train acc: 0.6298318169285911\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9042666794737519, Train acc: 0.6316515026192445\n",
      "Val loss: 0.8200979232788086, Val acc: 0.722\n",
      "Epoch 9/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9005085836108934, Train acc: 0.6379928315412187\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8951124620233846, Train acc: 0.6394403087951475\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.891711615429305, Train acc: 0.6430934656741109\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8919308225059102, Train acc: 0.6423007995588641\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8873577544831822, Train acc: 0.6428453267162945\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8866432762553549, Train acc: 0.6428866832092639\n",
      "Val loss: 0.8245503306388855, Val acc: 0.716\n",
      "Epoch 10/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.881218886273539, Train acc: 0.6480562448304383\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.877357752659382, Train acc: 0.6444720154397574\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8716455889902903, Train acc: 0.6459424685231137\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8721435602404114, Train acc: 0.647125723738627\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8728500349908812, Train acc: 0.647780534877309\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8718315127738181, Train acc: 0.6478494623655914\n",
      "Val loss: 0.8462989330291748, Val acc: 0.678\n",
      "Epoch 11/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8724038137329949, Train acc: 0.6415770609318996\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8608931952562088, Train acc: 0.6469534050179212\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8569959923412725, Train acc: 0.6511809576325706\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.85997778049901, Train acc: 0.6509856630824373\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8590472052239965, Train acc: 0.6516404742211194\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8593485081127906, Train acc: 0.6519391600036761\n",
      "Val loss: 0.8655449151992798, Val acc: 0.65\n",
      "Epoch 12/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8345503557441581, Train acc: 0.6645988420181969\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8358589466820415, Train acc: 0.6646677695064792\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8386287950722241, Train acc: 0.6614741292160647\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8385504780926256, Train acc: 0.6601185552798456\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8400637139621963, Train acc: 0.6607113316790736\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8367931991391032, Train acc: 0.6627148240051466\n",
      "Val loss: 0.8869931101799011, Val acc: 0.572\n",
      "Epoch 13/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.813979402057126, Train acc: 0.6775572098152743\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8174766640887301, Train acc: 0.6716294458229942\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.817639906521876, Train acc: 0.672548479000092\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8165968195495442, Train acc: 0.6735938792390406\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8118484050799639, Train acc: 0.6760132340777502\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.811911890649388, Train acc: 0.6762016358790552\n",
      "Val loss: 0.8794404864311218, Val acc: 0.616\n",
      "Early stopping at epoch 13 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 117.4377 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.6069324352802374, Train acc: 0.21491590846429556\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.5934551246145852, Train acc: 0.24234904880066171\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.5826890679166528, Train acc: 0.2519988971601875\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.571394843678189, Train acc: 0.26761097325613453\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.5581460257880708, Train acc: 0.2821064240419079\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.5445338017240888, Train acc: 0.2992142266335815\n",
      "Val loss: 1.3794962167739868, Val acc: 0.49\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.3982075901113005, Train acc: 0.42404190791287566\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.3634942550944467, Train acc: 0.4448580093741384\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.3291250338241924, Train acc: 0.458965168642588\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.2990518716665416, Train acc: 0.4699820788530466\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.2750588612678724, Train acc: 0.477695064791839\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.2503555147056906, Train acc: 0.48701865637349506\n",
      "Val loss: 0.9416705965995789, Val acc: 0.642\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.0830177894005408, Train acc: 0.5481113868210642\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.0740265591531737, Train acc: 0.553556658395368\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.0731014780848793, Train acc: 0.556566492050363\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.0668396631367185, Train acc: 0.5602081610146127\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.0615816018520257, Train acc: 0.5606286186931349\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.0549849621590726, Train acc: 0.5640336366142817\n",
      "Val loss: 0.8914467692375183, Val acc: 0.654\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.0031178405142238, Train acc: 0.583264405845051\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.0051164637264023, Train acc: 0.5822994210090985\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.0057399605753754, Train acc: 0.5849186655638269\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.000353980013448, Train acc: 0.5877791563275434\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.9980800311789554, Train acc: 0.588061759029501\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9965923140701066, Train acc: 0.5894219281316055\n",
      "Val loss: 0.8335010409355164, Val acc: 0.698\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9738222742692019, Train acc: 0.594017094017094\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9659160087760698, Train acc: 0.6007030603804797\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9621784535907952, Train acc: 0.604769782189137\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9656111391691061, Train acc: 0.6039771160738903\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.963775252786457, Train acc: 0.6046319272125724\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9594127839786715, Train acc: 0.6061483319547836\n",
      "Val loss: 0.7914999723434448, Val acc: 0.712\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9287460003143702, Train acc: 0.6261373035566584\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9424343516683986, Train acc: 0.6172456575682382\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9400230697077564, Train acc: 0.6185552798456024\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9348536785851177, Train acc: 0.6209677419354839\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.928560071113782, Train acc: 0.6236559139784946\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.9230940013359754, Train acc: 0.626114327727231\n",
      "Val loss: 0.7766407132148743, Val acc: 0.738\n",
      "Epoch 7/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9013232002910386, Train acc: 0.6335814722911497\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9035287282915196, Train acc: 0.6291701130410808\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8955097487169793, Train acc: 0.6329381490671814\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8916804337093973, Train acc: 0.6362351805900193\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8937052279456049, Train acc: 0.6347945960849186\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8905701233960285, Train acc: 0.6361777410164507\n",
      "Val loss: 0.7771605253219604, Val acc: 0.752\n",
      "Epoch 8/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8749150552301326, Train acc: 0.6408877860490764\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.87991750265798, Train acc: 0.6435759580920871\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8719238214343362, Train acc: 0.649802407866924\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8675090243928453, Train acc: 0.6511235180590019\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8634578280978733, Train acc: 0.6530741659773918\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8640740563896646, Train acc: 0.6532947339398952\n",
      "Val loss: 0.7722406387329102, Val acc: 0.74\n",
      "Epoch 9/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8445713255140517, Train acc: 0.6561896884477529\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8403348273191696, Train acc: 0.6614971050454922\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8395982160527482, Train acc: 0.6607848543332414\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8374294428489147, Train acc: 0.6625310173697271\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8373323658592681, Train acc: 0.6619244554728426\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8355788897584986, Train acc: 0.662576969028582\n",
      "Val loss: 0.7291459441184998, Val acc: 0.758\n",
      "Epoch 10/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8288241828608717, Train acc: 0.6668045216432313\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8271530515108353, Train acc: 0.6668045216432313\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8167613043404712, Train acc: 0.6708023159636063\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8138810403836079, Train acc: 0.6729046043562172\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8162251359377152, Train acc: 0.6724290046870692\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8126739375611656, Train acc: 0.6745473761602794\n",
      "Val loss: 0.7446342706680298, Val acc: 0.77\n",
      "Epoch 11/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8127313745327485, Train acc: 0.663358147229115\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7970361798746973, Train acc: 0.6721119382409705\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.794499338776977, Train acc: 0.6750758202371105\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.792477048933506, Train acc: 0.6771781086297215\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7946111961307689, Train acc: 0.6783843396746623\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7941847788302647, Train acc: 0.6802683576877125\n",
      "Val loss: 0.7579883933067322, Val acc: 0.758\n",
      "Epoch 12/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7874707849616678, Train acc: 0.6825199889716018\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7816611953780183, Train acc: 0.6898952302178109\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7828021929135011, Train acc: 0.6899182060472383\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7871938135761481, Train acc: 0.6871725944306589\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7867809709830161, Train acc: 0.6862145023435346\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7837016856313771, Train acc: 0.6885166804521643\n",
      "Val loss: 0.7355879545211792, Val acc: 0.79\n",
      "Epoch 13/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7764944943607363, Train acc: 0.6873449131513648\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7733067488059019, Train acc: 0.692652329749104\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7675142902084905, Train acc: 0.695616211745244\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7689371728616902, Train acc: 0.6944099807003032\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7665319137084179, Train acc: 0.6962779156327543\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.763843519225759, Train acc: 0.6962135833103575\n",
      "Val loss: 0.7260963320732117, Val acc: 0.764\n",
      "Epoch 14/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7584645447567997, Train acc: 0.6986490212296664\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7590000575933701, Train acc: 0.6996829335539013\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7581376825642382, Train acc: 0.6981435529822626\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7552783237053797, Train acc: 0.6989247311827957\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7517493287212829, Train acc: 0.7010201268265784\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7512069980048726, Train acc: 0.7006479183898539\n",
      "Val loss: 0.7528951168060303, Val acc: 0.764\n",
      "Epoch 15/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7411675412430723, Train acc: 0.6979597463468431\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7401627469011861, Train acc: 0.7031293079680176\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7359630080879244, Train acc: 0.7044389302453818\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7309777085852419, Train acc: 0.7077819134270747\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7311528971052578, Train acc: 0.7085470085470086\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7299323841462447, Train acc: 0.7094246852311369\n",
      "Val loss: 0.7065801024436951, Val acc: 0.79\n",
      "Epoch 16/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7102119357157977, Train acc: 0.7189137027846705\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7109069923559824, Train acc: 0.7155362558588365\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.711426633630383, Train acc: 0.7170296847716202\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7149752121832635, Train acc: 0.7157775020678246\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7174956373679332, Train acc: 0.7173697270471464\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7159439919256417, Train acc: 0.7181325245841375\n",
      "Val loss: 0.7552000284194946, Val acc: 0.748\n",
      "Epoch 17/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7123746003350641, Train acc: 0.7223600771987869\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7120199428919034, Train acc: 0.724083264405845\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7048712789672732, Train acc: 0.7259902582483228\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7040494488727334, Train acc: 0.7252550317066446\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7010947510218009, Train acc: 0.7261373035566584\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7008053133545438, Train acc: 0.7254618141714916\n",
      "Val loss: 0.7271990180015564, Val acc: 0.682\n",
      "Epoch 18/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7035996501262372, Train acc: 0.7176730079955886\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7013798482142962, Train acc: 0.7185690653432589\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6993426085367501, Train acc: 0.7214869956805441\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6947241863633833, Train acc: 0.7226357871519162\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6941641911991641, Train acc: 0.7225530741659774\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.693461609234837, Train acc: 0.7235777961584413\n",
      "Val loss: 0.7168774604797363, Val acc: 0.724\n",
      "Epoch 19/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6895752867572328, Train acc: 0.7311827956989247\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6910994023594081, Train acc: 0.7340088227185001\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6834049803918583, Train acc: 0.7389945777042551\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6811448911316375, Train acc: 0.7403501516404742\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6778273052639432, Train acc: 0.7409704990350152\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6775287527728964, Train acc: 0.7407177649113132\n",
      "Val loss: 0.7212805151939392, Val acc: 0.748\n",
      "Epoch 20/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6752207908365462, Train acc: 0.7389026743865453\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6671136524687465, Train acc: 0.7405569341053212\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6704572128434466, Train acc: 0.7427626137303557\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6762478675088311, Train acc: 0.7404535428728977\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6744800201338581, Train acc: 0.7408877860490763\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6720038679165717, Train acc: 0.7425098796066538\n",
      "Val loss: 0.7137232422828674, Val acc: 0.736\n",
      "Early stopping at epoch 20 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 182.2164 [s]\n",
      "Epoch 1/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.5704118492256882, Train acc: 0.297008547008547\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.4355773189638414, Train acc: 0.3612446581196581\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.326832836339956, Train acc: 0.41639957264957267\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.2399352524015639, Train acc: 0.46140491452991456\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.1743958482375512, Train acc: 0.4933226495726496\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.1266480386257172, Train acc: 0.5197204415954416\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.085619497066307, Train acc: 0.5400641025641025\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0533966462836306, Train acc: 0.5569577991452992\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0242193642391666, Train acc: 0.5705128205128205\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.0021262958009018, Train acc: 0.5821581196581197\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.981991229450045, Train acc: 0.5919046231546231\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9641787622474197, Train acc: 0.6010950854700855\n",
      "Val loss: 0.6418903470039368, Val acc: 0.736\n",
      "Epoch 2/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7294703897757407, Train acc: 0.7323717948717948\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7270058483904244, Train acc: 0.7294337606837606\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7263680113177015, Train acc: 0.7272079772079773\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7156563857172289, Train acc: 0.7314369658119658\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7069826423102974, Train acc: 0.7359508547008548\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7026764804396535, Train acc: 0.7358885327635327\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6973152491781447, Train acc: 0.7386294261294262\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6933885735706387, Train acc: 0.7398838141025641\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6890695219592378, Train acc: 0.7415420227920227\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6852397774019812, Train acc: 0.7421207264957265\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6808657833283135, Train acc: 0.7437597125097125\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6790963163190757, Train acc: 0.7442574786324786\n",
      "Val loss: 0.5934966802597046, Val acc: 0.792\n",
      "Epoch 3/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6219754820196037, Train acc: 0.7705662393162394\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6133723759498352, Train acc: 0.7732371794871795\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6140225290233253, Train acc: 0.7748397435897436\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6051863063222322, Train acc: 0.7787126068376068\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6018628398577373, Train acc: 0.777991452991453\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6039630489216911, Train acc: 0.7767984330484331\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6038559905193082, Train acc: 0.7763659951159951\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5988025145016165, Train acc: 0.7778445512820513\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5918449345845216, Train acc: 0.7814280626780626\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.589171102273668, Train acc: 0.7818910256410256\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5895529932283855, Train acc: 0.7822455322455323\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5852379071343149, Train acc: 0.783676103988604\n",
      "Val loss: 0.5561549067497253, Val acc: 0.804\n",
      "Epoch 4/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.550815761471406, Train acc: 0.7978098290598291\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5624336324568487, Train acc: 0.7959401709401709\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5465115208995988, Train acc: 0.8038639601139601\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5388496151337256, Train acc: 0.8055555555555556\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.532783126219725, Train acc: 0.8091880341880342\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5337692834967561, Train acc: 0.8091168091168092\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.529697058583645, Train acc: 0.8104014041514042\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5280438625436817, Train acc: 0.8113982371794872\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5272068082437217, Train acc: 0.8116690408357075\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5236239649801173, Train acc: 0.8125534188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.52292109998095, Train acc: 0.8127185314685315\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5220815846723029, Train acc: 0.8131454772079773\n",
      "Val loss: 0.5432958602905273, Val acc: 0.832\n",
      "Epoch 5/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5019155498753246, Train acc: 0.813034188034188\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4898780729526129, Train acc: 0.8209134615384616\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4871742325460809, Train acc: 0.8238960113960114\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4839205163029524, Train acc: 0.8263888888888888\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.48726433349980247, Train acc: 0.8240384615384615\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.48617079217209774, Train acc: 0.8242966524216524\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4861034996925838, Train acc: 0.8244429181929182\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.48689834241811025, Train acc: 0.8250534188034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4847767597731249, Train acc: 0.8252908357075024\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4840827610884976, Train acc: 0.8258279914529915\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4829282595514788, Train acc: 0.8262917637917638\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4828196655311476, Train acc: 0.8266114672364673\n",
      "Val loss: 0.5305251479148865, Val acc: 0.824\n",
      "Epoch 6/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4617383491534453, Train acc: 0.8330662393162394\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.46166687331393236, Train acc: 0.8323985042735043\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.45353899785765894, Train acc: 0.8383190883190883\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4455074428493141, Train acc: 0.8410122863247863\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4489280023635962, Train acc: 0.8393696581196581\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4479452694137382, Train acc: 0.8406339031339032\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.44902902145634643, Train acc: 0.8403540903540904\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44779863295296574, Train acc: 0.8409121260683761\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.44595590476495023, Train acc: 0.8414351851851852\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4445731949411396, Train acc: 0.8421207264957264\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4452071159834906, Train acc: 0.8418803418803419\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.44337047882208286, Train acc: 0.8425480769230769\n",
      "Val loss: 0.5226892232894897, Val acc: 0.826\n",
      "Epoch 7/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42303899185270327, Train acc: 0.8512286324786325\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.41759667536004996, Train acc: 0.8492254273504274\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42086360673619133, Train acc: 0.8506054131054132\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.42007832079489005, Train acc: 0.8505608974358975\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4229294123175817, Train acc: 0.8498931623931624\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.42256735002043583, Train acc: 0.8493589743589743\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42543982000286906, Train acc: 0.8482905982905983\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4247092645153658, Train acc: 0.8490584935897436\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4248519804813357, Train acc: 0.8487951092117759\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.42680084449995276, Train acc: 0.8486645299145299\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.42475138600588586, Train acc: 0.8494318181818182\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.42119064643989684, Train acc: 0.8508725071225072\n",
      "Val loss: 0.5142149329185486, Val acc: 0.826\n",
      "Epoch 8/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3873035120658385, Train acc: 0.8651175213675214\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.39585072582221437, Train acc: 0.8615117521367521\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.39994547654081275, Train acc: 0.8619123931623932\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.39883281285755146, Train acc: 0.8611111111111112\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.40354093143827896, Train acc: 0.8598290598290599\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.40186754344535347, Train acc: 0.8597756410256411\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4024468827287647, Train acc: 0.8583638583638583\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4036396575741406, Train acc: 0.8579059829059829\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4022329350383307, Train acc: 0.8588259734093068\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.40396588157512187, Train acc: 0.8577991452991452\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4042955317882576, Train acc: 0.858537296037296\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.40230820152876723, Train acc: 0.8590856481481481\n",
      "Val loss: 0.5234166383743286, Val acc: 0.84\n",
      "Epoch 9/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3717072630285198, Train acc: 0.8712606837606838\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.37504990328835625, Train acc: 0.8700587606837606\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3882046639070212, Train acc: 0.8653846153846154\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3879938366321417, Train acc: 0.8664529914529915\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3866259845148804, Train acc: 0.8669871794871795\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3899766211198945, Train acc: 0.8656962250712251\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3881335104923691, Train acc: 0.8657661782661783\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3909216464425509, Train acc: 0.864082532051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.39166442189517753, Train acc: 0.8641381766381766\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.39354001805823075, Train acc: 0.8635416666666667\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.39234757639334567, Train acc: 0.8637334887334888\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.38970783468537523, Train acc: 0.8650730056980057\n",
      "Val loss: 0.4602349102497101, Val acc: 0.83\n",
      "Epoch 10/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.36586509657721233, Train acc: 0.874465811965812\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3656578758397164, Train acc: 0.874198717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.36596624137159767, Train acc: 0.8725961538461539\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3698270902929143, Train acc: 0.8718616452991453\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3739095912784593, Train acc: 0.8715277777777778\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.37495907650798815, Train acc: 0.8702368233618234\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3725502821861315, Train acc: 0.8705738705738706\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3745072795094078, Train acc: 0.8702256944444444\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.37473910766789037, Train acc: 0.8700142450142451\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.37304222540468235, Train acc: 0.8698985042735042\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.375438914907099, Train acc: 0.8689539627039627\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.37256924644048905, Train acc: 0.8701032763532763\n",
      "Val loss: 0.5013524293899536, Val acc: 0.846\n",
      "Epoch 11/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.35766116911784196, Train acc: 0.875534188034188\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3578707704591191, Train acc: 0.875534188034188\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3612394029874238, Train acc: 0.8766025641025641\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.36126220336932147, Train acc: 0.8761351495726496\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.36055420577908176, Train acc: 0.8763354700854701\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.35957014335165505, Train acc: 0.8762464387464387\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.36061244414748583, Train acc: 0.8755723443223443\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.36233181801314157, Train acc: 0.8751001602564102\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3599086059372366, Train acc: 0.8757419278252612\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.35768317417520235, Train acc: 0.8767628205128205\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3577122089960312, Train acc: 0.8762869075369075\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.359432248650729, Train acc: 0.8754896723646723\n",
      "Val loss: 0.5031524896621704, Val acc: 0.842\n",
      "Epoch 12/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3349739588860773, Train acc: 0.8846153846153846\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3348601721227169, Train acc: 0.8864850427350427\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.34607814326670094, Train acc: 0.8804309116809117\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.33595405468064493, Train acc: 0.8843482905982906\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.33970007865856855, Train acc: 0.8836004273504273\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.33954207884662513, Train acc: 0.8827902421652422\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.34213354313002403, Train acc: 0.8820589133089133\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3447446197860389, Train acc: 0.8806089743589743\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.34615324680310483, Train acc: 0.8801638176638177\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.34610393900647124, Train acc: 0.879780982905983\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3450021459384537, Train acc: 0.8804147241647242\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3462840117530053, Train acc: 0.8797186609686609\n",
      "Val loss: 0.5121394395828247, Val acc: 0.832\n",
      "Epoch 13/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.31728921913438374, Train acc: 0.8904914529914529\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.32962343574334413, Train acc: 0.8862179487179487\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3310277589790502, Train acc: 0.8848824786324786\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.332135228249125, Train acc: 0.8852831196581197\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.33343121903574363, Train acc: 0.8863247863247863\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3333748156401167, Train acc: 0.8853721509971509\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.338402259089644, Train acc: 0.8832036019536019\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3378773234689083, Train acc: 0.883079594017094\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3359634525727229, Train acc: 0.8832502374169041\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3378931695834184, Train acc: 0.8828258547008547\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.33924895195578453, Train acc: 0.8827700077700078\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3375807291538882, Train acc: 0.8833912037037037\n",
      "Val loss: 0.4786081910133362, Val acc: 0.862\n",
      "Epoch 14/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.32068710812391377, Train acc: 0.8851495726495726\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.32620207570556903, Train acc: 0.8859508547008547\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.325597232839151, Train acc: 0.8867521367521367\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.32741574845953375, Train acc: 0.8868189102564102\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.32698680298705385, Train acc: 0.8872863247863247\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3274415563845057, Train acc: 0.8868856837606838\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.32356367431244426, Train acc: 0.8882783882783882\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.32564225336775565, Train acc: 0.887252938034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3274119624484185, Train acc: 0.8869895536562203\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3260718422758783, Train acc: 0.8875267094017094\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3261110642932141, Train acc: 0.8875291375291375\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3261149831890975, Train acc: 0.8877092236467237\n",
      "Val loss: 0.5244476795196533, Val acc: 0.818\n",
      "Early stopping at epoch 14 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 185.6231 [s]\n",
      "Epoch 1/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.5850504198644915, Train acc: 0.25\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.5202768338032258, Train acc: 0.3110309829059829\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.3928467159257656, Train acc: 0.3837250712250712\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.2878927774408944, Train acc: 0.43796741452991456\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.208101517114884, Train acc: 0.47841880341880344\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.1476704330525846, Train acc: 0.5111289173789174\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.0934863017737793, Train acc: 0.5372786935286935\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0482952935764422, Train acc: 0.5591613247863247\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0125565382546735, Train acc: 0.5763592117758785\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9847419061467179, Train acc: 0.5912660256410256\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9606449674661528, Train acc: 0.6043851981351981\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9343344656448079, Train acc: 0.6168536324786325\n",
      "Val loss: 0.6632192134857178, Val acc: 0.74\n",
      "Epoch 2/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6346977666402475, Train acc: 0.7676282051282052\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6425091892990291, Train acc: 0.7665598290598291\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6303652724479338, Train acc: 0.7717236467236467\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6285471829593691, Train acc: 0.7720352564102564\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6227639557459416, Train acc: 0.7762286324786325\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6126567368578707, Train acc: 0.7794693732193733\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6146254306238358, Train acc: 0.7785027472527473\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6080439486501054, Train acc: 0.7810162927350427\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6020066108260965, Train acc: 0.7832977207977208\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5960340679838108, Train acc: 0.7845619658119658\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5927635652780904, Train acc: 0.7851592851592851\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5889506284306701, Train acc: 0.7865918803418803\n",
      "Val loss: 0.5624592900276184, Val acc: 0.804\n",
      "Epoch 3/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5234207527505027, Train acc: 0.8108974358974359\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5263376466484151, Train acc: 0.8104967948717948\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5224923719028463, Train acc: 0.811698717948718\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5212964069766876, Train acc: 0.8110977564102564\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5189204236126354, Train acc: 0.8125534188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5128117464588918, Train acc: 0.8156160968660968\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5120675818169074, Train acc: 0.8157432844932845\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5055434159361399, Train acc: 0.8189436431623932\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5018151020250882, Train acc: 0.8198005698005698\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5006006157041615, Train acc: 0.8199786324786325\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4995816486953485, Train acc: 0.8202700077700078\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5011385606967995, Train acc: 0.8198228276353277\n",
      "Val loss: 0.5228884220123291, Val acc: 0.826\n",
      "Epoch 4/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4971590128719297, Train acc: 0.8221153846153846\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.48318897728991306, Train acc: 0.8269230769230769\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4768269210629314, Train acc: 0.8290598290598291\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4733604557143572, Train acc: 0.8306623931623932\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.46898366906958766, Train acc: 0.8325320512820513\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4677972626630907, Train acc: 0.8343126780626781\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4670268993219759, Train acc: 0.8338675213675214\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4677695782186511, Train acc: 0.8341346153846154\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4651663728546553, Train acc: 0.8353216999050332\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4633734763751173, Train acc: 0.8352564102564103\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4615210661058196, Train acc: 0.8362713675213675\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4570653270027916, Train acc: 0.8379184472934473\n",
      "Val loss: 0.4844018518924713, Val acc: 0.844\n",
      "Epoch 5/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.44876724577102906, Train acc: 0.8381410256410257\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.42886440987643015, Train acc: 0.8426816239316239\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42749688409247627, Train acc: 0.8456196581196581\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4273199609870839, Train acc: 0.8461538461538461\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.42161694230177466, Train acc: 0.849465811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4277315462416733, Train acc: 0.8462873931623932\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42545898300533036, Train acc: 0.8472985347985348\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4253190839742748, Train acc: 0.8479901175213675\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4262943312681412, Train acc: 0.8471628679962013\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4231847332838254, Train acc: 0.8485576923076923\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.42282575137316947, Train acc: 0.8484848484848485\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.42235774417378624, Train acc: 0.8482015669515669\n",
      "Val loss: 0.5370948314666748, Val acc: 0.816\n",
      "Epoch 6/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4109802487441617, Train acc: 0.8538995726495726\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3927893493738439, Train acc: 0.8607104700854701\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.40319326978463393, Train acc: 0.8556801994301995\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.40360945265771997, Train acc: 0.8569043803418803\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4062654902537664, Train acc: 0.8563034188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.40425453765949293, Train acc: 0.8578169515669516\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4028312008605044, Train acc: 0.8591269841269841\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.40291671421474373, Train acc: 0.8586738782051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.40191755009371105, Train acc: 0.8584995251661919\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4031856638307755, Train acc: 0.8578792735042735\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4043150699256739, Train acc: 0.8569104506604507\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.40348058329242414, Train acc: 0.8562811609686609\n",
      "Val loss: 0.4610327184200287, Val acc: 0.844\n",
      "Epoch 7/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.36910490360524917, Train acc: 0.8677884615384616\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3782491662308701, Train acc: 0.8635149572649573\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.37628630206965313, Train acc: 0.86502849002849\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.37651373605188143, Train acc: 0.8645165598290598\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3822909635101628, Train acc: 0.8621260683760684\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.38206932954808587, Train acc: 0.8631143162393162\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3817225559290512, Train acc: 0.8618742368742369\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3862298977457815, Train acc: 0.8599759615384616\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3863579495637505, Train acc: 0.860903371320038\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.38661142469853416, Train acc: 0.8605235042735043\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.38353729423456817, Train acc: 0.8616452991452992\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.38263011308243644, Train acc: 0.8623575498575499\n",
      "Val loss: 0.44406548142433167, Val acc: 0.856\n",
      "Epoch 8/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.36503462353323257, Train acc: 0.8693910256410257\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3496336307790544, Train acc: 0.8763354700854701\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.35620836206735706, Train acc: 0.8736645299145299\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.36218551965032375, Train acc: 0.8735977564102564\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.36634109541136994, Train acc: 0.871207264957265\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3614331814394123, Train acc: 0.8720174501424501\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3596564439217468, Train acc: 0.8720238095238095\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.36508328473577517, Train acc: 0.870292467948718\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.364108570553327, Train acc: 0.8706374643874644\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.366145378876573, Train acc: 0.869951923076923\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.365323183167995, Train acc: 0.8702165889665889\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.36594498232856093, Train acc: 0.8697916666666666\n",
      "Val loss: 0.4450136423110962, Val acc: 0.838\n",
      "Epoch 9/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3419831909685053, Train acc: 0.8835470085470085\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.34861664212921745, Train acc: 0.8802083333333334\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3502793480612953, Train acc: 0.8756232193732194\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.34880905631834114, Train acc: 0.8762019230769231\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3534709209305608, Train acc: 0.8750534188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3485572161816294, Train acc: 0.8770477207977208\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3481424755902372, Train acc: 0.8766788766788767\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.34994908531283975, Train acc: 0.8762686965811965\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3530874129204329, Train acc: 0.8749703228869895\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.35465691786291254, Train acc: 0.8737980769230769\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3558163130663121, Train acc: 0.8733974358974359\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3533112856578029, Train acc: 0.8738871082621082\n",
      "Val loss: 0.46417900919914246, Val acc: 0.848\n",
      "Epoch 10/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.31344028033761895, Train acc: 0.8883547008547008\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3138598444688524, Train acc: 0.8892895299145299\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3195589698660068, Train acc: 0.886039886039886\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3273895015446549, Train acc: 0.8837473290598291\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.33036049550924546, Train acc: 0.8827457264957265\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.33249776618305776, Train acc: 0.8812321937321937\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.33408208544364926, Train acc: 0.8807234432234432\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3351841438561678, Train acc: 0.8807759081196581\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.33541555503862647, Train acc: 0.8801638176638177\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3369719653366468, Train acc: 0.8795405982905983\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.33666017084804356, Train acc: 0.8797591297591297\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3376101967361238, Train acc: 0.8790731837606838\n",
      "Val loss: 0.40521952509880066, Val acc: 0.862\n",
      "Epoch 11/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.31110300671341073, Train acc: 0.8918269230769231\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3189714566892029, Train acc: 0.8876869658119658\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3189295680604429, Train acc: 0.8878205128205128\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3253226757972923, Train acc: 0.8859508547008547\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.32097599692324286, Train acc: 0.8878205128205128\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3244622027729651, Train acc: 0.8863514957264957\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3240892414153714, Train acc: 0.8862942612942613\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3234697108228619, Train acc: 0.886184561965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.32230265318271895, Train acc: 0.8862179487179487\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.32226584816717696, Train acc: 0.8858974358974359\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.32303505277415995, Train acc: 0.8855380730380731\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.32209176873719253, Train acc: 0.8861511752136753\n",
      "Val loss: 0.4228435456752777, Val acc: 0.858\n",
      "Epoch 12/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.29467483020873153, Train acc: 0.8955662393162394\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.31060538780039704, Train acc: 0.8879540598290598\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.313825888097541, Train acc: 0.8864850427350427\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3120626150272213, Train acc: 0.8875534188034188\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3105202775926162, Train acc: 0.8881410256410256\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3121568284918185, Train acc: 0.8876869658119658\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.31239056442119406, Train acc: 0.8883928571428571\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.31347881850157666, Train acc: 0.8887553418803419\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.31454297480972065, Train acc: 0.8889482431149098\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.31339768376360594, Train acc: 0.8891292735042735\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3118831730435408, Train acc: 0.8895687645687645\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.31095360272014755, Train acc: 0.8893563034188035\n",
      "Val loss: 0.4428495168685913, Val acc: 0.86\n",
      "Epoch 13/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2979246446719536, Train acc: 0.8918269230769231\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3131183102472216, Train acc: 0.8878205128205128\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3070747460055555, Train acc: 0.8869301994301995\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.30840697841575515, Train acc: 0.8882879273504274\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3079632276080103, Train acc: 0.8893696581196581\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.304189106279671, Train acc: 0.8909811253561254\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.30450244579505137, Train acc: 0.8906440781440782\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.30286392322781247, Train acc: 0.8919604700854701\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3015317907565736, Train acc: 0.8926282051282052\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.303488646992124, Train acc: 0.8919070512820513\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3034372471863434, Train acc: 0.8918269230769231\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3047662646857295, Train acc: 0.8912482193732194\n",
      "Val loss: 0.46677082777023315, Val acc: 0.852\n",
      "Epoch 14/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2673068946370712, Train acc: 0.8995726495726496\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2773354740288013, Train acc: 0.8952991452991453\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.27693712450245506, Train acc: 0.8967236467236467\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.27795111343391943, Train acc: 0.8968349358974359\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.28433852223759026, Train acc: 0.8956196581196582\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2899560215657423, Train acc: 0.8944088319088319\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.28945656634312267, Train acc: 0.8949557387057387\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.29062053629666823, Train acc: 0.8946647970085471\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2911735713970118, Train acc: 0.8946462488129154\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2920760232509456, Train acc: 0.8941773504273505\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2913470005047766, Train acc: 0.8942064879564879\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.29106324737589073, Train acc: 0.8944310897435898\n",
      "Val loss: 0.46959662437438965, Val acc: 0.842\n",
      "Epoch 15/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.27344258725006354, Train acc: 0.8969017094017094\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.26985865497054196, Train acc: 0.9031784188034188\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.27402710974046646, Train acc: 0.9007300569800569\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2805506237503937, Train acc: 0.8989049145299145\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2829931806613747, Train acc: 0.8976495726495727\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.27843800625973447, Train acc: 0.8998397435897436\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.27850887110236217, Train acc: 0.9001068376068376\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.27760656495204467, Train acc: 0.8999399038461539\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2766620168021941, Train acc: 0.9011455365622032\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2771902700137888, Train acc: 0.9014155982905983\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2771218083943613, Train acc: 0.9014665889665889\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.27905356165478035, Train acc: 0.9005519943019943\n",
      "Val loss: 0.42647695541381836, Val acc: 0.85\n",
      "Early stopping at epoch 15 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 196.6882 [s]\n",
      "Epoch 1/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.5620831893040583, Train acc: 0.27163461538461536\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.4204281517583082, Train acc: 0.3736645299145299\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.2806412187057343, Train acc: 0.44907407407407407\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.2007193185834804, Train acc: 0.492321047008547\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.1296192835538814, Train acc: 0.527457264957265\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.0810837307546892, Train acc: 0.5526175213675214\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.0364672077298893, Train acc: 0.574824481074481\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9974393355859141, Train acc: 0.5942174145299145\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.967816739963104, Train acc: 0.6087369420702754\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9440598966983649, Train acc: 0.6195779914529914\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9207976028357908, Train acc: 0.6308275058275058\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9022817619335957, Train acc: 0.6396233974358975\n",
      "Val loss: 0.725271999835968, Val acc: 0.724\n",
      "Epoch 2/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6431111581305153, Train acc: 0.7601495726495726\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6448953072739463, Train acc: 0.7606837606837606\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6435579399437646, Train acc: 0.7632656695156695\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6371780494466807, Train acc: 0.7656917735042735\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6385227132556784, Train acc: 0.7658653846153847\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6324007490336725, Train acc: 0.7666488603988604\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6272322822257567, Train acc: 0.7684294871794872\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6240393365613925, Train acc: 0.7706663995726496\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6218458174640297, Train acc: 0.7715159069325735\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.617695118066592, Train acc: 0.7726228632478632\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6137285139570888, Train acc: 0.7740141802641802\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.612543214515828, Train acc: 0.7753961894586895\n",
      "Val loss: 0.5255487561225891, Val acc: 0.808\n",
      "Epoch 3/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5734032084289779, Train acc: 0.7868589743589743\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5698419523417441, Train acc: 0.7877938034188035\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5758341341531514, Train acc: 0.7862357549857549\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5687575462218534, Train acc: 0.7911992521367521\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5557336162552874, Train acc: 0.7955128205128205\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5515197888016701, Train acc: 0.7983440170940171\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5484203032157651, Train acc: 0.7992216117216118\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5442149819503738, Train acc: 0.8007478632478633\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5434252038131072, Train acc: 0.8011633428300095\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5396173198763122, Train acc: 0.8016025641025641\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5393598576849898, Train acc: 0.8015491452991453\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5372572840533705, Train acc: 0.8029959045584045\n",
      "Val loss: 0.5143378973007202, Val acc: 0.846\n",
      "Epoch 4/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4780435984970158, Train acc: 0.8317307692307693\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4954639165065227, Train acc: 0.8235844017094017\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4897779832962911, Train acc: 0.8246082621082621\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.49147922590247584, Train acc: 0.8236511752136753\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.48838896804895154, Train acc: 0.8248931623931623\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.48651216066630815, Train acc: 0.82380698005698\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4861684381161272, Train acc: 0.8246336996336996\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4834875946498325, Train acc: 0.8249532585470085\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4828029977969634, Train acc: 0.8246972934472935\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.480989862469017, Train acc: 0.8255608974358974\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.48141655580610293, Train acc: 0.8261703574203574\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.48050378101673563, Train acc: 0.8265001780626781\n",
      "Val loss: 0.5514942407608032, Val acc: 0.814\n",
      "Epoch 5/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4492682278411001, Train acc: 0.84375\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.45236884395026755, Train acc: 0.8366720085470085\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4475818294660318, Train acc: 0.8393874643874644\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.45102495805193216, Train acc: 0.8386752136752137\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4506808218538252, Train acc: 0.8387820512820513\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4477266466982684, Train acc: 0.8395210113960114\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4434814523719024, Train acc: 0.8415750915750916\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4432613113258257, Train acc: 0.8417801816239316\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.44178868126184284, Train acc: 0.8423848528015194\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.43969245659362555, Train acc: 0.842948717948718\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4391322213788781, Train acc: 0.8431186868686869\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.43784874008485564, Train acc: 0.84375\n",
      "Val loss: 0.4648764133453369, Val acc: 0.844\n",
      "Epoch 6/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42063989967871934, Train acc: 0.8448183760683761\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.41155153178633785, Train acc: 0.8533653846153846\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.410385227165161, Train acc: 0.8535434472934473\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4073599729941696, Train acc: 0.8539663461538461\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.407253001528418, Train acc: 0.854594017094017\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.40964093472161184, Train acc: 0.8532763532763533\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4093578803706664, Train acc: 0.8536324786324786\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.41174563663637537, Train acc: 0.8533653846153846\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.41186358088385117, Train acc: 0.8536028015194682\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4108962106271687, Train acc: 0.8539529914529914\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4098639786185548, Train acc: 0.8546280108780109\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4085767975294607, Train acc: 0.8554798789173789\n",
      "Val loss: 0.5187837481498718, Val acc: 0.832\n",
      "Epoch 7/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.38809533894826204, Train acc: 0.8691239316239316\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.39188149033321273, Train acc: 0.8649839743589743\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4060736471558908, Train acc: 0.8580840455840456\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.40371882097206563, Train acc: 0.8590411324786325\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4018886113013977, Train acc: 0.8592414529914529\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3998453583876122, Train acc: 0.859909188034188\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.39893579795065087, Train acc: 0.8593177655677655\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3969845137296197, Train acc: 0.8592080662393162\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3963948333846425, Train acc: 0.8598053181386515\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3955555821713219, Train acc: 0.86116452991453\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.39476922236631895, Train acc: 0.8616210178710179\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.39416957956518545, Train acc: 0.8614004629629629\n",
      "Val loss: 0.4126542806625366, Val acc: 0.878\n",
      "Epoch 8/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.36640039156390053, Train acc: 0.8782051282051282\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3689287219546799, Train acc: 0.8709935897435898\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3656517702799577, Train acc: 0.8713497150997151\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3611084104826053, Train acc: 0.8731303418803419\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.35979050374948063, Train acc: 0.8738782051282051\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.36304581640792366, Train acc: 0.8725961538461539\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3620045309174483, Train acc: 0.8728250915750916\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3613420293753983, Train acc: 0.8735977564102564\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.36356507462483867, Train acc: 0.8726851851851852\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.36391843306139493, Train acc: 0.8723023504273504\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3672206937404177, Train acc: 0.870920745920746\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.36647336098214245, Train acc: 0.8711716524216524\n",
      "Val loss: 0.4441922903060913, Val acc: 0.866\n",
      "Epoch 9/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.35673250601841855, Train acc: 0.874198717948718\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.37057238919103247, Train acc: 0.8711271367521367\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3639134201314035, Train acc: 0.8736645299145299\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.36689508830507594, Train acc: 0.8715945512820513\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.36451028320524426, Train acc: 0.870940170940171\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.36097394228193835, Train acc: 0.8719729344729344\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.35806261397834516, Train acc: 0.8727106227106227\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.35650758918088216, Train acc: 0.8733640491452992\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.35737151692905317, Train acc: 0.8733084045584045\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3556250185602241, Train acc: 0.8739850427350427\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3603026402452794, Train acc: 0.8725718725718725\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3599629081318691, Train acc: 0.8724626068376068\n",
      "Val loss: 0.40578263998031616, Val acc: 0.866\n",
      "Epoch 10/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3378174983156033, Train acc: 0.8808760683760684\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.34695030834812385, Train acc: 0.875801282051282\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.34180041309180764, Train acc: 0.8779380341880342\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3433183176904662, Train acc: 0.8797409188034188\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.340391724754093, Train acc: 0.8805021367521367\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.34184953905748167, Train acc: 0.8793625356125356\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3401199867801061, Train acc: 0.880303724053724\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3418507687110677, Train acc: 0.8803752670940171\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3416510130307953, Train acc: 0.8807573599240266\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.33991491118302714, Train acc: 0.8816506410256411\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3386054936544601, Train acc: 0.8819444444444444\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.33841376609972906, Train acc: 0.8818554131054132\n",
      "Val loss: 0.4356812536716461, Val acc: 0.856\n",
      "Epoch 11/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3176292612766608, Train acc: 0.8864850427350427\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3200880238133618, Train acc: 0.8842147435897436\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.32307932914322257, Train acc: 0.8839921652421653\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.322557244497614, Train acc: 0.8842147435897436\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3187661834888988, Train acc: 0.8861645299145299\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3211098396646501, Train acc: 0.8856392450142451\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3224547166509215, Train acc: 0.8851877289377289\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.32693881357614046, Train acc: 0.8834134615384616\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3304604156135607, Train acc: 0.8823896011396012\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3321155325215087, Train acc: 0.881517094017094\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3316879299901304, Train acc: 0.8817744755244755\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.330960864052601, Train acc: 0.8820779914529915\n",
      "Val loss: 0.3793011009693146, Val acc: 0.874\n",
      "Epoch 12/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2944166037516716, Train acc: 0.8950320512820513\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3161465396515579, Train acc: 0.8899572649572649\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.31181587029428903, Train acc: 0.8889779202279202\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3084056662936878, Train acc: 0.8895566239316239\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.30395088865079434, Train acc: 0.8912927350427351\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3072892936938933, Train acc: 0.8899127492877493\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3112239210533208, Train acc: 0.8886981074481074\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.30969788482349014, Train acc: 0.8896233974358975\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.30993854945414084, Train acc: 0.88963081671415\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.30948043797706437, Train acc: 0.8899305555555556\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.30982626104190447, Train acc: 0.8900301087801088\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.31007024456091503, Train acc: 0.8901353276353277\n",
      "Val loss: 0.4106614589691162, Val acc: 0.848\n",
      "Epoch 13/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.30614885152914584, Train acc: 0.8880876068376068\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3003053466478984, Train acc: 0.8926282051282052\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3014161112344163, Train acc: 0.8945868945868946\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3005650207782403, Train acc: 0.8950988247863247\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.30297543074075994, Train acc: 0.8928418803418804\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.30354882446661635, Train acc: 0.8926727207977208\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3030946200056245, Train acc: 0.8920177045177046\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.29957515510738403, Train acc: 0.8932291666666666\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2997515345910336, Train acc: 0.8934591642924976\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2991712223451871, Train acc: 0.8932425213675214\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2996933674027314, Train acc: 0.8933809246309247\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.30039441845568154, Train acc: 0.8930956196581197\n",
      "Val loss: 0.41121751070022583, Val acc: 0.878\n",
      "Epoch 14/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2751998439685911, Train acc: 0.905715811965812\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.28512058878301555, Train acc: 0.9021100427350427\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.284824346035634, Train acc: 0.9009081196581197\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.27916891738358474, Train acc: 0.9020432692307693\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.28281441935234597, Train acc: 0.900267094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2816419668251548, Train acc: 0.900863603988604\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.28490587475371887, Train acc: 0.8996871184371185\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.28805892266189825, Train acc: 0.8992721688034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2853431544547011, Train acc: 0.8994836182336182\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2867053899595625, Train acc: 0.8989850427350428\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.28628348717762686, Train acc: 0.8988442113442113\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.285453645257741, Train acc: 0.8990607193732194\n",
      "Val loss: 0.39203372597694397, Val acc: 0.882\n",
      "Epoch 15/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3003044166944475, Train acc: 0.8896901709401709\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2730439729893055, Train acc: 0.8999732905982906\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.27388595217610695, Train acc: 0.9009081196581197\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.26909554161481625, Train acc: 0.9027110042735043\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2685379249226843, Train acc: 0.9028311965811966\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2709451188198012, Train acc: 0.9024661680911681\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.274435103876614, Train acc: 0.9011370573870574\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.27631824112966913, Train acc: 0.9001402243589743\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.27913938700827323, Train acc: 0.89960232668566\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2768637859031685, Train acc: 0.9011485042735042\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.27487894750280534, Train acc: 0.9018550893550894\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2756591390027662, Train acc: 0.9017316595441596\n",
      "Val loss: 0.44720906019210815, Val acc: 0.866\n",
      "Epoch 16/40\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.25112288165041524, Train acc: 0.9145299145299145\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2484669811450518, Train acc: 0.9130608974358975\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.2591395183145321, Train acc: 0.9093660968660968\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.26190092767246514, Train acc: 0.9093215811965812\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2635739719192696, Train acc: 0.9084401709401709\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2647108206010697, Train acc: 0.9070067663817664\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.26630137308576896, Train acc: 0.9064407814407814\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2694808988051855, Train acc: 0.9052483974358975\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2694506010134392, Train acc: 0.9051519468186134\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2680633945358742, Train acc: 0.9056089743589744\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2660305661097945, Train acc: 0.9064442501942502\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.26557958155122197, Train acc: 0.9066728988603988\n",
      "Val loss: 0.4170994460582733, Val acc: 0.864\n",
      "Early stopping at epoch 16 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 212.8874 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.612678890554314, Train acc: 0.18536324786324787\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.5811748519921913, Train acc: 0.2326388888888889\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.532733751158429, Train acc: 0.2922008547008547\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.4835453723740375, Train acc: 0.3374732905982906\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.4485237955028176, Train acc: 0.3681623931623932\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.4115886592287623, Train acc: 0.3940972222222222\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.384734384378962, Train acc: 0.41613247863247865\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.358583914482186, Train acc: 0.43325988247863245\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.334385775999013, Train acc: 0.44661087369420704\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.3162366528286893, Train acc: 0.4568643162393162\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.2970220149258078, Train acc: 0.4673416860916861\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.2791216288536702, Train acc: 0.47671830484330485\n",
      "Val loss: 0.9699525833129883, Val acc: 0.614\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.067691786166949, Train acc: 0.5868055555555556\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.063267523406917, Train acc: 0.5856036324786325\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.060272700766213, Train acc: 0.5887642450142451\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.0550545359778607, Train acc: 0.5878739316239316\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.0448156515757243, Train acc: 0.5924679487179487\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.0376543699845968, Train acc: 0.5962873931623932\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.032308803955423, Train acc: 0.5978327228327228\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0267686734342167, Train acc: 0.5992254273504274\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0219901804117961, Train acc: 0.6016441120607787\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.016912276775409, Train acc: 0.6027510683760684\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.01212901965351, Train acc: 0.604458041958042\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.009006551994897, Train acc: 0.6051682692307693\n",
      "Val loss: 0.8629865646362305, Val acc: 0.666\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.9663892169284005, Train acc: 0.6161858974358975\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.9508915199683263, Train acc: 0.6254006410256411\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9490824098940249, Train acc: 0.6272257834757835\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9472662353108072, Train acc: 0.6262019230769231\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9405150641742934, Train acc: 0.6266025641025641\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9394110011578964, Train acc: 0.6265135327635327\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9374075772852543, Train acc: 0.6290064102564102\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9353070037487226, Train acc: 0.6292735042735043\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9331170699761691, Train acc: 0.6299857549857549\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9317836066596529, Train acc: 0.6307425213675214\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9279620208384551, Train acc: 0.632260101010101\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9259944936403861, Train acc: 0.6329015313390314\n",
      "Val loss: 0.8188791871070862, Val acc: 0.664\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.9163044214758098, Train acc: 0.6343482905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.9076291104921927, Train acc: 0.6354166666666666\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9080316459181642, Train acc: 0.6368411680911681\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.900910750668273, Train acc: 0.6390224358974359\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9017410555456439, Train acc: 0.6401175213675213\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8991863836250414, Train acc: 0.6398682336182336\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8956902920835912, Train acc: 0.6408348595848596\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8930318689244425, Train acc: 0.6422275641025641\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8926546111745373, Train acc: 0.6430436847103513\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8885695686197689, Train acc: 0.6452190170940171\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8850825126076634, Train acc: 0.6457847707847708\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8843358959100525, Train acc: 0.6461672008547008\n",
      "Val loss: 0.806334912776947, Val acc: 0.69\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8316171997123294, Train acc: 0.6773504273504274\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.845312633830258, Train acc: 0.6697382478632479\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8436744281878839, Train acc: 0.6694266381766382\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8507051113196927, Train acc: 0.6653979700854701\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8490925533649248, Train acc: 0.6646901709401709\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8457802077728799, Train acc: 0.6654202279202279\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8490445339854383, Train acc: 0.6659798534798534\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8510781961182753, Train acc: 0.6639289529914529\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8497856365479635, Train acc: 0.665093779677113\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8499211109346814, Train acc: 0.6645833333333333\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8496562203836701, Train acc: 0.6639957264957265\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8479391816801835, Train acc: 0.6643518518518519\n",
      "Val loss: 0.7729551196098328, Val acc: 0.684\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8362851800062717, Train acc: 0.6615918803418803\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.834944189613701, Train acc: 0.6688034188034188\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8347825852894036, Train acc: 0.6694266381766382\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8273729378851051, Train acc: 0.6738114316239316\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8280205120388259, Train acc: 0.6734508547008548\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.826621347767675, Train acc: 0.6730324074074074\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8243680258664747, Train acc: 0.6743360805860806\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8233115185275037, Train acc: 0.6742120726495726\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8229482044295141, Train acc: 0.6738188509021842\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8211853688089257, Train acc: 0.6739583333333333\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8206921703795083, Train acc: 0.6745337995337995\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8184479303721689, Train acc: 0.67619301994302\n",
      "Val loss: 0.7412623763084412, Val acc: 0.718\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8034172103955195, Train acc: 0.6762820512820513\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7964019654398291, Train acc: 0.6808226495726496\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7985865342990626, Train acc: 0.6841168091168092\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7983715863436716, Train acc: 0.6879006410256411\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7952266227485787, Train acc: 0.6891559829059829\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7976814717309088, Train acc: 0.6888799857549858\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7941974187872495, Train acc: 0.6905906593406593\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7941496697310199, Train acc: 0.6903378739316239\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7933322487280466, Train acc: 0.6899335232668566\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7911192432428018, Train acc: 0.6912393162393162\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7896801250195818, Train acc: 0.6911907536907537\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7894065707200273, Train acc: 0.6913506054131054\n",
      "Val loss: 0.7308076024055481, Val acc: 0.714\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.771697545153463, Train acc: 0.7059294871794872\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7699762577684517, Train acc: 0.7040598290598291\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7615057784437793, Train acc: 0.7073539886039886\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7579566248589091, Train acc: 0.7082665598290598\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7589378201553965, Train acc: 0.7063568376068377\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7614261536856323, Train acc: 0.7060185185185185\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7632886802320515, Train acc: 0.7059294871794872\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7616166886356142, Train acc: 0.7051615918803419\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7610398099916047, Train acc: 0.7062262583095916\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7631233403315911, Train acc: 0.7052884615384616\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7609424031836726, Train acc: 0.7060751748251748\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7619628066520745, Train acc: 0.7052617521367521\n",
      "Val loss: 0.6709264516830444, Val acc: 0.73\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7495990255449572, Train acc: 0.7203525641025641\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7536603948499403, Train acc: 0.7154113247863247\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7433955783178324, Train acc: 0.7208867521367521\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7429920848236125, Train acc: 0.7176816239316239\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7434637324422853, Train acc: 0.7165064102564103\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7394172173110168, Train acc: 0.7176816239316239\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7408912693842863, Train acc: 0.7174145299145299\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7429383533059531, Train acc: 0.7180488782051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7414498589543762, Train acc: 0.7178893637226971\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7419752840557668, Train acc: 0.7182425213675213\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7428173513586612, Train acc: 0.7180944055944056\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7425220320027778, Train acc: 0.7178596866096866\n",
      "Val loss: 0.6638857126235962, Val acc: 0.744\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7430891924434238, Train acc: 0.7166132478632479\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7349843799303739, Train acc: 0.7204861111111112\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7259559060773279, Train acc: 0.7263176638176638\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7223573786198584, Train acc: 0.7263621794871795\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7214580134448842, Train acc: 0.7263888888888889\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7154778856412638, Train acc: 0.728988603988604\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7133946463156386, Train acc: 0.7295100732600732\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7184122696709938, Train acc: 0.7279313568376068\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7184072078010182, Train acc: 0.7276531339031339\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7166762948036194, Train acc: 0.7283386752136752\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7155160135600395, Train acc: 0.7292395104895105\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7171989446054836, Train acc: 0.729255698005698\n",
      "Val loss: 0.6824149489402771, Val acc: 0.732\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7157482275596032, Train acc: 0.7318376068376068\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7198763015942696, Train acc: 0.7298344017094017\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7265427648341893, Train acc: 0.7275641025641025\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7243891983714879, Train acc: 0.7276308760683761\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7192344550393586, Train acc: 0.7301816239316239\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7140987416957518, Train acc: 0.7321047008547008\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7110411988000439, Train acc: 0.732791514041514\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7086313623202662, Train acc: 0.7343082264957265\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7073186471534704, Train acc: 0.7352207977207977\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7077285425021098, Train acc: 0.7349893162393163\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7051118150550262, Train acc: 0.7358682983682984\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7038245574913473, Train acc: 0.7360220797720798\n",
      "Val loss: 0.6485417485237122, Val acc: 0.74\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6958156872508873, Train acc: 0.7417200854700855\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6950081346126703, Train acc: 0.7394497863247863\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6938847614149762, Train acc: 0.7396723646723646\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6933233748134385, Train acc: 0.7388488247863247\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6959109864683233, Train acc: 0.7375\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6926121994046404, Train acc: 0.7376691595441596\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6912803007278395, Train acc: 0.7381333943833944\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6917751958100204, Train acc: 0.7394163995726496\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6899210526223196, Train acc: 0.7393162393162394\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6889879834702891, Train acc: 0.7405982905982906\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6864361137545377, Train acc: 0.740797397047397\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6841971804188867, Train acc: 0.7418981481481481\n",
      "Val loss: 0.6224126815795898, Val acc: 0.754\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6666904957885416, Train acc: 0.7451923076923077\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6716656553694326, Train acc: 0.7455929487179487\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6686726494392439, Train acc: 0.750267094017094\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6652162878049744, Train acc: 0.7520699786324786\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6701821462720887, Train acc: 0.7503205128205128\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6719651097840393, Train acc: 0.7496883903133903\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6731081151190841, Train acc: 0.7506105006105006\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6710882397671031, Train acc: 0.7521367521367521\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6699668905164442, Train acc: 0.7511574074074074\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6665265468705414, Train acc: 0.7525106837606838\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6690950067634256, Train acc: 0.7511655011655012\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6694246084983871, Train acc: 0.7511796652421653\n",
      "Val loss: 0.6101061105728149, Val acc: 0.756\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6648017221536392, Train acc: 0.7553418803418803\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6720624957074467, Train acc: 0.7528044871794872\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.661641842109865, Train acc: 0.7562321937321937\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6616198401420544, Train acc: 0.7540064102564102\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6604678888096769, Train acc: 0.7557692307692307\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6587738840923011, Train acc: 0.7568554131054132\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6542352161055228, Train acc: 0.7586996336996337\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6556845870919716, Train acc: 0.7579460470085471\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6587224754965656, Train acc: 0.7565289648622981\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6593808267361079, Train acc: 0.7561965811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6580653845013439, Train acc: 0.7562402874902875\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.656117911840606, Train acc: 0.7566773504273504\n",
      "Val loss: 0.592571496963501, Val acc: 0.756\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.644343670616802, Train acc: 0.7662927350427351\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6723842585188711, Train acc: 0.7530715811965812\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6672171740620224, Train acc: 0.7526709401709402\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6613436250223054, Train acc: 0.7533386752136753\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6593095840042473, Train acc: 0.755715811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6555909246121376, Train acc: 0.7578792735042735\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6499999331831204, Train acc: 0.7604548229548229\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6489982929430965, Train acc: 0.760917467948718\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6481198198378709, Train acc: 0.7615443969610636\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6470387916024933, Train acc: 0.762099358974359\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.645122165886487, Train acc: 0.7626262626262627\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6434170601417196, Train acc: 0.7631321225071225\n",
      "Val loss: 0.6199830174446106, Val acc: 0.754\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6474662033411173, Train acc: 0.7521367521367521\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6403702268233666, Train acc: 0.7561431623931624\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6397732870701032, Train acc: 0.7612179487179487\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6378414634710703, Train acc: 0.7621527777777778\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6369626677443838, Train acc: 0.7626602564102564\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6333291924152619, Train acc: 0.7630876068376068\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6336613825825981, Train acc: 0.7632020757020757\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6351219586009144, Train acc: 0.7612179487179487\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6342129801300064, Train acc: 0.7612476258309592\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.63388353046189, Train acc: 0.7624198717948718\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6333013944785171, Train acc: 0.7625048562548562\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6321914057191621, Train acc: 0.7637553418803419\n",
      "Val loss: 0.6017475128173828, Val acc: 0.764\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6211972664564084, Train acc: 0.7743055555555556\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.619892147107002, Train acc: 0.7727029914529915\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6189785077531114, Train acc: 0.7733262108262108\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6190593393567281, Train acc: 0.7723023504273504\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6207416810541071, Train acc: 0.7712606837606838\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6183817166718323, Train acc: 0.7722578347578347\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6200056703754397, Train acc: 0.771710927960928\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6188798957846613, Train acc: 0.7717347756410257\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6206652806242193, Train acc: 0.7712488129154795\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6218901411590413, Train acc: 0.7704326923076923\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6227320850636483, Train acc: 0.7702263014763014\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6229963540776163, Train acc: 0.7698094729344729\n",
      "Val loss: 0.5873897671699524, Val acc: 0.764\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6161936954555348, Train acc: 0.7689636752136753\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6222379736156545, Train acc: 0.7711004273504274\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6171592248555942, Train acc: 0.7719907407407407\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6124344265295399, Train acc: 0.772636217948718\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6122579442384916, Train acc: 0.7724358974358975\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6092676529899622, Train acc: 0.7748842592592593\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6104134618326276, Train acc: 0.7735424297924298\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6135886282715787, Train acc: 0.7727363782051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6103283512031931, Train acc: 0.7739494301994302\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6103297629290156, Train acc: 0.7743322649572649\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6096089223997484, Train acc: 0.7750339937839937\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6086841679557606, Train acc: 0.7755742521367521\n",
      "Val loss: 0.5672081112861633, Val acc: 0.782\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5910106528009105, Train acc: 0.7868589743589743\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5992025978799559, Train acc: 0.7805822649572649\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5993871576765664, Train acc: 0.7794693732193733\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5965470934652874, Train acc: 0.7783119658119658\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5949122235306308, Train acc: 0.7795940170940171\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5933730513961227, Train acc: 0.780448717948718\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.596902416921215, Train acc: 0.780448717948718\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.597212221314255, Train acc: 0.7811832264957265\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5996670747301405, Train acc: 0.7807751661918328\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5995056550472211, Train acc: 0.78125\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6013114743438356, Train acc: 0.7807886557886557\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.602844291220703, Train acc: 0.7806045227920227\n",
      "Val loss: 0.5884036421775818, Val acc: 0.766\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5849296840337607, Train acc: 0.7956730769230769\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5896639585749716, Train acc: 0.7840544871794872\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5937727015582245, Train acc: 0.7851673789173789\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.597621839079592, Train acc: 0.7832532051282052\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5982033256791596, Train acc: 0.781517094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5975029533490156, Train acc: 0.7822293447293447\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5943363756487221, Train acc: 0.7826617826617827\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5906624202576712, Train acc: 0.7833867521367521\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5889702323127223, Train acc: 0.7843364197530864\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5869492918889747, Train acc: 0.7856036324786325\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5890656883691574, Train acc: 0.7843337218337219\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5886975192807169, Train acc: 0.7843883547008547\n",
      "Val loss: 0.5718520879745483, Val acc: 0.788\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5810463886994582, Train acc: 0.7905982905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5826368293701074, Train acc: 0.7884615384615384\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5894326193210406, Train acc: 0.7844551282051282\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5850971060621942, Train acc: 0.7854567307692307\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5869496914311352, Train acc: 0.7864850427350427\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5823222344629785, Train acc: 0.7876602564102564\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5801623276047072, Train acc: 0.7880418192918193\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5803724627814486, Train acc: 0.7880275106837606\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5817490262098801, Train acc: 0.7872744539411206\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5811286487131038, Train acc: 0.7876869658119658\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5793422066665196, Train acc: 0.7884858197358198\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.58045242550621, Train acc: 0.7878160612535613\n",
      "Val loss: 0.5578843951225281, Val acc: 0.796\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5572479499710931, Train acc: 0.7988782051282052\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5615588536119869, Train acc: 0.7942040598290598\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5652350713724424, Train acc: 0.7924679487179487\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5667498876523768, Train acc: 0.7944043803418803\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5679638301714872, Train acc: 0.7929487179487179\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5707891875097895, Train acc: 0.7906428062678063\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5724452665582827, Train acc: 0.7898733211233211\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5704837091203429, Train acc: 0.7901642628205128\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5706880506683505, Train acc: 0.7903311965811965\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5709944855198901, Train acc: 0.7901175213675213\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5703046492763333, Train acc: 0.7903797591297591\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5674049360544933, Train acc: 0.7918002136752137\n",
      "Val loss: 0.5573837757110596, Val acc: 0.796\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5468416170686738, Train acc: 0.8060897435897436\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5542481678545984, Train acc: 0.8018162393162394\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5599018497939123, Train acc: 0.7994123931623932\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5606331669239917, Train acc: 0.796340811965812\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5622192186932279, Train acc: 0.7956730769230769\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5601663046285638, Train acc: 0.7973201566951567\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5578597242033089, Train acc: 0.7972756410256411\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5585473451572351, Train acc: 0.7968416132478633\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5588606223534768, Train acc: 0.7963853276353277\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5577191522233507, Train acc: 0.7961538461538461\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.554900285432574, Train acc: 0.7972999222999223\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5560556755552435, Train acc: 0.7964298433048433\n",
      "Val loss: 0.557723343372345, Val acc: 0.794\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5375170335810409, Train acc: 0.8031517094017094\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5510953690251733, Train acc: 0.796340811965812\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5543883504436227, Train acc: 0.7973646723646723\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5518698735942698, Train acc: 0.7990785256410257\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5516495680452412, Train acc: 0.8007478632478633\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5535113317122486, Train acc: 0.7999465811965812\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.552482471415848, Train acc: 0.8007478632478633\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5495463811561593, Train acc: 0.8009481837606838\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.547157258276473, Train acc: 0.8016381766381766\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5468690247617216, Train acc: 0.8021367521367522\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.547626912454903, Train acc: 0.8017433954933955\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5458091363586761, Train acc: 0.8022836538461539\n",
      "Val loss: 0.5471314787864685, Val acc: 0.78\n",
      "Epoch 25/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5725036578045951, Train acc: 0.7959401709401709\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5560778368614677, Train acc: 0.8010149572649573\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5549148303745819, Train acc: 0.8000356125356125\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5535896208742236, Train acc: 0.8001469017094017\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5511768108249729, Train acc: 0.8009615384615385\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5487838986932043, Train acc: 0.8015046296296297\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5467897437067113, Train acc: 0.8040293040293041\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5451724315937768, Train acc: 0.804420405982906\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5436416304858661, Train acc: 0.8046652421652422\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5417688111718904, Train acc: 0.8049679487179487\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5406830827672998, Train acc: 0.805239898989899\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5388347888179654, Train acc: 0.8062232905982906\n",
      "Val loss: 0.5201389789581299, Val acc: 0.792\n",
      "Epoch 26/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5354989334049388, Train acc: 0.8066239316239316\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5401075569610311, Train acc: 0.8010149572649573\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.536171593387582, Train acc: 0.8001246438746439\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.538279894229948, Train acc: 0.8000801282051282\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5400331938623363, Train acc: 0.8004273504273505\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.535894464818161, Train acc: 0.8037304131054132\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5328194545775252, Train acc: 0.804945054945055\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5301346222623291, Train acc: 0.8069577991452992\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5279174656496655, Train acc: 0.8080484330484331\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.527306567132473, Train acc: 0.8091613247863247\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5292109897318235, Train acc: 0.8083721833721834\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5280083767815024, Train acc: 0.8083377849002849\n",
      "Val loss: 0.5425947308540344, Val acc: 0.792\n",
      "Epoch 27/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.516888373810002, Train acc: 0.8084935897435898\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5228567222117358, Train acc: 0.8071581196581197\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5169329751505811, Train acc: 0.8125890313390314\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5159651445081601, Train acc: 0.8129674145299145\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5168678457920368, Train acc: 0.8144764957264957\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5140627074105787, Train acc: 0.8156160968660968\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5143219731084011, Train acc: 0.8149038461538461\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5169397732322543, Train acc: 0.8132345085470085\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5172366170179018, Train acc: 0.8127077397910731\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5201307526256285, Train acc: 0.8117788461538461\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5199673978760896, Train acc: 0.8120386557886557\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5178456935617659, Train acc: 0.8129451566951567\n",
      "Val loss: 0.5084497332572937, Val acc: 0.808\n",
      "Epoch 28/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4910329558655747, Train acc: 0.8258547008547008\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.49620365924560106, Train acc: 0.8229166666666666\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.498578085837371, Train acc: 0.8214031339031339\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.49731976728345084, Train acc: 0.8230502136752137\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4966439788030763, Train acc: 0.8222222222222222\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4974259248804673, Train acc: 0.8222934472934473\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.500990464855827, Train acc: 0.8207036019536019\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5022131607461816, Train acc: 0.8201789529914529\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5042274893249994, Train acc: 0.8192960588793922\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5071780915762115, Train acc: 0.818349358974359\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5069795307915342, Train acc: 0.8181332556332557\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5095647295984702, Train acc: 0.8174189814814815\n",
      "Val loss: 0.5224157571792603, Val acc: 0.806\n",
      "Epoch 29/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.49302274485429126, Train acc: 0.8181089743589743\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5039506898476527, Train acc: 0.8147702991452992\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5037294379581413, Train acc: 0.8184650997150997\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5016879483611665, Train acc: 0.8191105769230769\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4996648305246973, Train acc: 0.8208867521367521\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4998100278978674, Train acc: 0.8218482905982906\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5004270497586701, Train acc: 0.8210088522588522\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.49961902568928707, Train acc: 0.8219484508547008\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5014018924431017, Train acc: 0.8212250712250713\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5001772534388762, Train acc: 0.8214476495726496\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.499110688186933, Train acc: 0.8223096348096348\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4977964285729278, Train acc: 0.8226718304843305\n",
      "Val loss: 0.5113659501075745, Val acc: 0.804\n",
      "Epoch 30/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5164920342528921, Train acc: 0.8154380341880342\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.49043286216055226, Train acc: 0.8249198717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.49769606753292245, Train acc: 0.8214031339031339\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.48991201846645427, Train acc: 0.8225827991452992\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4935938428609799, Train acc: 0.8206730769230769\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.49661210200555644, Train acc: 0.8200676638176638\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.498583028222615, Train acc: 0.8195970695970696\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4960696737672019, Train acc: 0.8218149038461539\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4962453015877424, Train acc: 0.8225308641975309\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.49587165788452847, Train acc: 0.8228899572649573\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.49663618283394056, Train acc: 0.8218725718725719\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4970671228555023, Train acc: 0.8214253917378918\n",
      "Val loss: 0.4987974166870117, Val acc: 0.814\n",
      "Epoch 31/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5012498338762511, Train acc: 0.8175747863247863\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4890439251803944, Train acc: 0.8221153846153846\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4861149417708742, Train acc: 0.8233618233618234\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4938840949191497, Train acc: 0.8216479700854701\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.49064755427022266, Train acc: 0.8216346153846154\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4870098196500727, Train acc: 0.8244747150997151\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.48786905653529117, Train acc: 0.824557387057387\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.48829422534531, Train acc: 0.8252203525641025\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.48668686963497515, Train acc: 0.8258843779677113\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4873702108605295, Train acc: 0.8263087606837607\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4844706579174473, Train acc: 0.8270930458430459\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4850503246902231, Train acc: 0.8268785612535613\n",
      "Val loss: 0.49640733003616333, Val acc: 0.828\n",
      "Epoch 32/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5004877339825671, Train acc: 0.8237179487179487\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4935852073960834, Train acc: 0.8262553418803419\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4926390595670439, Train acc: 0.8245192307692307\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.49244646788534957, Train acc: 0.8241185897435898\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.49124587567952965, Train acc: 0.8248931623931623\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4853517007284355, Train acc: 0.8272792022792023\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.48327512986348514, Train acc: 0.8284493284493285\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4803887742897894, Train acc: 0.8299612713675214\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4795115888967813, Train acc: 0.8303359449192782\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.47904061966726924, Train acc: 0.8302350427350428\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.47959523054640146, Train acc: 0.8297882672882673\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.48127477313590866, Train acc: 0.8288372507122507\n",
      "Val loss: 0.4925056993961334, Val acc: 0.814\n",
      "Epoch 33/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.47833108226967674, Train acc: 0.8373397435897436\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4702906050106399, Train acc: 0.8340010683760684\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.47447260314243134, Train acc: 0.8325320512820513\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.47497406331265074, Train acc: 0.8310630341880342\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.477568805625296, Train acc: 0.8298076923076924\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4753246435794735, Train acc: 0.8302172364672364\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.47696976518310763, Train acc: 0.8296703296703297\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4764665282594088, Train acc: 0.8301615918803419\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4777555508434716, Train acc: 0.8303062678062678\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.47754149509546084, Train acc: 0.8303685897435897\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4753354116058572, Train acc: 0.8310751748251748\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.47321766940744175, Train acc: 0.8322649572649573\n",
      "Val loss: 0.48987889289855957, Val acc: 0.834\n",
      "Epoch 34/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.44326626133714986, Train acc: 0.8400106837606838\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4671670625734533, Train acc: 0.8314636752136753\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.47127854509910627, Train acc: 0.8315527065527065\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4691892491382921, Train acc: 0.8311298076923077\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.47052555945184493, Train acc: 0.8300213675213676\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4699500585001418, Train acc: 0.8306178774928775\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4695122393168809, Train acc: 0.8307387057387058\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4687500159996442, Train acc: 0.8311631944444444\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4671431172494082, Train acc: 0.831582383665717\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4662578975797718, Train acc: 0.8324252136752137\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.466663507384298, Train acc: 0.83250777000777\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4677012192027012, Train acc: 0.8323539886039886\n",
      "Val loss: 0.5049706101417542, Val acc: 0.802\n",
      "Epoch 35/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.46887164441948265, Train acc: 0.8311965811965812\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.47208820442613375, Train acc: 0.8338675213675214\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4702192218470098, Train acc: 0.834045584045584\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.47182636017091256, Train acc: 0.8332665598290598\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.47528396220798164, Train acc: 0.8321047008547009\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.47239458378393767, Train acc: 0.8330217236467237\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4717173443718271, Train acc: 0.8329517704517705\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.470921109454372, Train acc: 0.8328659188034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4692855959179734, Train acc: 0.8337488129154795\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4669888267150292, Train acc: 0.8347489316239316\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4666493329181167, Train acc: 0.8345231157731158\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4645562743943216, Train acc: 0.8356481481481481\n",
      "Val loss: 0.47951745986938477, Val acc: 0.83\n",
      "Epoch 36/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4398484753492551, Train acc: 0.8397435897435898\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4461119008115214, Train acc: 0.8420138888888888\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.45113727339991816, Train acc: 0.8397435897435898\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.45518025650809973, Train acc: 0.8357371794871795\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4534752681724027, Train acc: 0.8375\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4558563252799531, Train acc: 0.8373397435897436\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.45587798401767954, Train acc: 0.838179181929182\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.45634416861730254, Train acc: 0.8377403846153846\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.45897773345481296, Train acc: 0.8365087844254511\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4577391626106368, Train acc: 0.8370993589743589\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4574610658088394, Train acc: 0.8367084304584305\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.45861930894715836, Train acc: 0.8358484686609686\n",
      "Val loss: 0.47568073868751526, Val acc: 0.824\n",
      "Epoch 37/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.46439062020717525, Train acc: 0.8346688034188035\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.46096032367557543, Train acc: 0.8331997863247863\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.45389102216799376, Train acc: 0.8367165242165242\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.45129580251299417, Train acc: 0.8385416666666666\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.46016012460757527, Train acc: 0.8374465811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4582753321553907, Train acc: 0.838630698005698\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4571223429816983, Train acc: 0.8397054334554335\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.456252075986475, Train acc: 0.8397435897435898\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4552679710153841, Train acc: 0.83909069325736\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.45306001604876966, Train acc: 0.8401709401709402\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4538433604237639, Train acc: 0.8398892773892774\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.45327372019022616, Train acc: 0.8401219729344729\n",
      "Val loss: 0.48555323481559753, Val acc: 0.82\n",
      "Epoch 38/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42512858639924955, Train acc: 0.8480235042735043\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.42978355581434363, Train acc: 0.8476228632478633\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.43101560237400893, Train acc: 0.8459757834757835\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.43693797188436884, Train acc: 0.8438835470085471\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.437964316081797, Train acc: 0.8436965811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.44210471299893495, Train acc: 0.8423700142450142\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.44366854503841774, Train acc: 0.8416514041514042\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44344404212429994, Train acc: 0.8410456730769231\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4448294230194626, Train acc: 0.8406635802469136\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.44527879163750217, Train acc: 0.8408386752136752\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4477287018563086, Train acc: 0.8400835275835276\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4472181720292976, Train acc: 0.8401664886039886\n",
      "Val loss: 0.497285932302475, Val acc: 0.822\n",
      "Epoch 39/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4535339465762815, Train acc: 0.8426816239316239\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4493628473490731, Train acc: 0.8422809829059829\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.45013410629879713, Train acc: 0.8423254985754985\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.448699644074226, Train acc: 0.8424145299145299\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4474234588125832, Train acc: 0.8423076923076923\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4499712100947684, Train acc: 0.8409900284900285\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4459474582466159, Train acc: 0.8425671550671551\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44418052414384407, Train acc: 0.8440504807692307\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4451024708570691, Train acc: 0.8440467711301045\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4449965804751612, Train acc: 0.8436431623931624\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.44491210733283465, Train acc: 0.8433857808857809\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4465544191998379, Train acc: 0.8427483974358975\n",
      "Val loss: 0.475114643573761, Val acc: 0.83\n",
      "Epoch 40/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.43412294703671056, Train acc: 0.8506944444444444\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4376512732133906, Train acc: 0.8465544871794872\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4407885487263019, Train acc: 0.8438390313390314\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4391777879343583, Train acc: 0.8454193376068376\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4462741727248216, Train acc: 0.8424679487179487\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4424313196217233, Train acc: 0.844551282051282\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.44226286142737004, Train acc: 0.843482905982906\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44260163958638143, Train acc: 0.8433493589743589\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4418472513514027, Train acc: 0.8432751661918328\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4413345659645195, Train acc: 0.8438301282051283\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4405170275475873, Train acc: 0.8441142191142191\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4395169160113885, Train acc: 0.8442619301994302\n",
      "Val loss: 0.4708355665206909, Val acc: 0.826\n",
      "Epoch 41/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.44284992454907834, Train acc: 0.8426816239316239\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4384758219632328, Train acc: 0.8448183760683761\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4273613910814296, Train acc: 0.8493589743589743\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.43402344870389015, Train acc: 0.8463541666666666\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4297613697174268, Train acc: 0.8484508547008547\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.43003614596024875, Train acc: 0.8486467236467237\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4290595451353962, Train acc: 0.8481379731379731\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4300532127276827, Train acc: 0.8477564102564102\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4324806931818652, Train acc: 0.84707383665717\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.43235693638268696, Train acc: 0.8472222222222222\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.43365906493299766, Train acc: 0.8469551282051282\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.43578584500166595, Train acc: 0.8460202991452992\n",
      "Val loss: 0.45959168672561646, Val acc: 0.848\n",
      "Epoch 42/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42860863746231437, Train acc: 0.8418803418803419\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4187243359847965, Train acc: 0.8509615384615384\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4250152532616232, Train acc: 0.8488247863247863\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.42613551759312296, Train acc: 0.8494925213675214\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.42772343026267157, Train acc: 0.8495726495726496\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4267110558189558, Train acc: 0.8488247863247863\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4272380627700843, Train acc: 0.8474130036630036\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.42745484785837495, Train acc: 0.8475560897435898\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4290517224715306, Train acc: 0.8475783475783476\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.42935964077965827, Train acc: 0.8479433760683761\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4295251950297415, Train acc: 0.8485819735819736\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4307293112726718, Train acc: 0.8482905982905983\n",
      "Val loss: 0.4845690429210663, Val acc: 0.836\n",
      "Epoch 43/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4293008208529562, Train acc: 0.8533653846153846\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4201771548797942, Train acc: 0.8541666666666666\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4281562505624233, Train acc: 0.8516737891737892\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4348537305163013, Train acc: 0.8476228632478633\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.43354700589791323, Train acc: 0.8488782051282051\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.43024393351499174, Train acc: 0.8494480056980057\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42792806735260003, Train acc: 0.8497405372405372\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4254044784535455, Train acc: 0.8501936431623932\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.42677857752991993, Train acc: 0.8502789648622981\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4283732438444072, Train acc: 0.8498397435897436\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4285565232387697, Train acc: 0.849504662004662\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.42847531848186443, Train acc: 0.8490696225071225\n",
      "Val loss: 0.4967186450958252, Val acc: 0.83\n",
      "Epoch 44/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42048571481663954, Train acc: 0.8536324786324786\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4189507279258508, Train acc: 0.8517628205128205\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4211444098428104, Train acc: 0.8502492877492878\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4250518416142107, Train acc: 0.8479567307692307\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4248172481217955, Train acc: 0.8479166666666667\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.42288644651104923, Train acc: 0.8497596153846154\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4260222816223481, Train acc: 0.8492063492063492\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4271207554265857, Train acc: 0.8490251068376068\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4249866912804777, Train acc: 0.8499821937321937\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4234580925323515, Train acc: 0.8504006410256411\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4256291697735275, Train acc: 0.8498931623931624\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4245947837309535, Train acc: 0.8501825142450142\n",
      "Val loss: 0.4704245328903198, Val acc: 0.838\n",
      "Epoch 45/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42215318914152616, Train acc: 0.8530982905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.43173248823891336, Train acc: 0.8456196581196581\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42263676090288027, Train acc: 0.8476673789173789\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4224394480260009, Train acc: 0.8508947649572649\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.42593318106781725, Train acc: 0.8496260683760684\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.42263748059500317, Train acc: 0.8506944444444444\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.41988001691043303, Train acc: 0.8516101953601953\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4212264503455824, Train acc: 0.8505942841880342\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.421138650027492, Train acc: 0.8506350902184235\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.42269973154378754, Train acc: 0.8499732905982906\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4225458920453534, Train acc: 0.8501602564102564\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4213352588050471, Train acc: 0.8506721866096866\n",
      "Val loss: 0.4650838077068329, Val acc: 0.836\n",
      "Epoch 46/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3990812922517459, Train acc: 0.8565705128205128\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4120731030582872, Train acc: 0.8513621794871795\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4100758003076257, Train acc: 0.8532763532763533\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.40849649398309046, Train acc: 0.8539663461538461\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.41184794725770624, Train acc: 0.8514957264957265\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.41539609080918155, Train acc: 0.8512286324786325\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4158076604371106, Train acc: 0.8518391330891331\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4165114059041326, Train acc: 0.8516292735042735\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4191269545487755, Train acc: 0.8506944444444444\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.41980956174496914, Train acc: 0.8509615384615384\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4169087495362471, Train acc: 0.8521270396270396\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4155162230845106, Train acc: 0.8526086182336182\n",
      "Val loss: 0.47261783480644226, Val acc: 0.836\n",
      "Early stopping at epoch 46 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 596.7728 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.6103991107044058, Train acc: 0.20352564102564102\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.5864970123665965, Train acc: 0.23143696581196582\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.5539757339363425, Train acc: 0.27697649572649574\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.503839148924901, Train acc: 0.31850961538461536\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.4500446236031688, Train acc: 0.35689102564102565\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.4089826226574063, Train acc: 0.3847934472934473\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.3709003431893094, Train acc: 0.40739468864468864\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.3394319613137815, Train acc: 0.42588141025641024\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.3153346658092617, Train acc: 0.44094254510921177\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.2926278323699267, Train acc: 0.4552350427350427\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.2727717912373102, Train acc: 0.4670503108003108\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.2561197242675683, Train acc: 0.4777866809116809\n",
      "Val loss: 0.9435245394706726, Val acc: 0.624\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.0273468244788992, Train acc: 0.6036324786324786\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.0226632975614989, Train acc: 0.6069711538461539\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.0148856009852851, Train acc: 0.6107549857549858\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.0065970126634989, Train acc: 0.6133814102564102\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.0061964458889432, Train acc: 0.6113782051282052\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9974415326050544, Train acc: 0.6143607549857549\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9932979608047867, Train acc: 0.6161477411477412\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9886379589637121, Train acc: 0.6175213675213675\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9847144401311195, Train acc: 0.618886514719848\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9795001084987934, Train acc: 0.6212873931623931\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9783875906106198, Train acc: 0.6220134032634033\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9749152584378197, Train acc: 0.6236867877492878\n",
      "Val loss: 0.8520064353942871, Val acc: 0.672\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.932589737778036, Train acc: 0.6474358974358975\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.9462731714941498, Train acc: 0.640625\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9385766735104074, Train acc: 0.6392450142450142\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.932763974253948, Train acc: 0.6392895299145299\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.925992093126998, Train acc: 0.6435363247863248\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9216142486133466, Train acc: 0.6448094729344729\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9191400937836103, Train acc: 0.6450702075702076\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9166596245943991, Train acc: 0.6477363782051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.912284852036497, Train acc: 0.6494242640075973\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9100153954110594, Train acc: 0.6496794871794872\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9074053963025411, Train acc: 0.6504224941724942\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9052727138809329, Train acc: 0.6505297364672364\n",
      "Val loss: 0.8103339076042175, Val acc: 0.69\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8713028683112218, Train acc: 0.6634615384615384\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8608157055245506, Train acc: 0.6665331196581197\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8709759640048372, Train acc: 0.6595441595441596\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8716206819965289, Train acc: 0.6614583333333334\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8724319347459026, Train acc: 0.661965811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.871829898820983, Train acc: 0.6635060541310541\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8677754623039884, Train acc: 0.6644154456654456\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8672345564979264, Train acc: 0.664596688034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8687547508411371, Train acc: 0.6648860398860399\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8678440760088783, Train acc: 0.6657051282051282\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8672630647725622, Train acc: 0.6663995726495726\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.865311145761244, Train acc: 0.6670673076923077\n",
      "Val loss: 0.7947623133659363, Val acc: 0.702\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8433862807404282, Train acc: 0.6709401709401709\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.838282478416068, Train acc: 0.6738782051282052\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8371931221919862, Train acc: 0.6780626780626781\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8352801546963871, Train acc: 0.6786858974358975\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8350232290915954, Train acc: 0.6799679487179487\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8336300835904912, Train acc: 0.6797097578347578\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8318531459722763, Train acc: 0.6797542735042735\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8352781330099982, Train acc: 0.6784855769230769\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8323322329935525, Train acc: 0.6788342830009497\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8357043284381557, Train acc: 0.6774305555555555\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8348320415062597, Train acc: 0.6775203962703963\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8329562203921483, Train acc: 0.6779513888888888\n",
      "Val loss: 0.7492260336875916, Val acc: 0.714\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8116721340224274, Train acc: 0.6952457264957265\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.826133227119079, Train acc: 0.6944444444444444\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8242162390482052, Train acc: 0.6905270655270656\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8248635744437193, Train acc: 0.6848958333333334\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.818542188558823, Train acc: 0.6884615384615385\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8144294553757393, Train acc: 0.6892806267806267\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8101883740943284, Train acc: 0.6899038461538461\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.80882309214809, Train acc: 0.6889356303418803\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8086543565745141, Train acc: 0.6896070750237417\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8080679031518789, Train acc: 0.6907051282051282\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8058102722768183, Train acc: 0.69131216006216\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8053894278585402, Train acc: 0.6914396367521367\n",
      "Val loss: 0.7268471121788025, Val acc: 0.728\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7844022734042926, Train acc: 0.7011217948717948\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7926660541795257, Train acc: 0.6976495726495726\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7933567096704771, Train acc: 0.6957799145299145\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7918150584794518, Train acc: 0.6959802350427351\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7933268216940073, Train acc: 0.6955662393162393\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7910334570625229, Train acc: 0.6967147435897436\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7867589384790451, Train acc: 0.6984126984126984\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7849093738975178, Train acc: 0.6993856837606838\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7827293195276179, Train acc: 0.7002908357075024\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7823764862667801, Train acc: 0.7007211538461539\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7811218701366686, Train acc: 0.7013646076146076\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.77930166192374, Train acc: 0.7021456552706553\n",
      "Val loss: 0.7121403217315674, Val acc: 0.724\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7507396572166019, Train acc: 0.7176816239316239\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7535798385357245, Train acc: 0.7136752136752137\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7513079346924426, Train acc: 0.7138532763532763\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7542749528701489, Train acc: 0.7146100427350427\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7565519496925875, Train acc: 0.7143696581196581\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7586190617746777, Train acc: 0.7133190883190883\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7576105523677099, Train acc: 0.7125686813186813\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7564169391671307, Train acc: 0.7124732905982906\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7581301222556224, Train acc: 0.7107965337132004\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7576507570651861, Train acc: 0.7103899572649572\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7544012765321235, Train acc: 0.7123883061383062\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7544731960082666, Train acc: 0.7121616809116809\n",
      "Val loss: 0.6830297708511353, Val acc: 0.732\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7586431803866329, Train acc: 0.71875\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7477305884289945, Train acc: 0.7176816239316239\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.743321418762207, Train acc: 0.7189280626780626\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7387941092507452, Train acc: 0.7220219017094017\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7382500680593344, Train acc: 0.7218482905982906\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7349010137496171, Train acc: 0.7215544871794872\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7318339948252444, Train acc: 0.723519536019536\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7313177223261605, Train acc: 0.7224559294871795\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7316191850224791, Train acc: 0.7218957739791073\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7292424788841835, Train acc: 0.7231570512820513\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7302403355765547, Train acc: 0.7231449106449106\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7303164719771116, Train acc: 0.7230012464387464\n",
      "Val loss: 0.6702863574028015, Val acc: 0.742\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.739075732536805, Train acc: 0.7211538461538461\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.730075578404288, Train acc: 0.7262286324786325\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.726662262722298, Train acc: 0.7270299145299145\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7239276696092043, Train acc: 0.7292334401709402\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7189501616180453, Train acc: 0.7297008547008547\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7223014271581955, Train acc: 0.7280092592592593\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7203335347920838, Train acc: 0.7292048229548229\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7173417641375309, Train acc: 0.729934561965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7145305680073903, Train acc: 0.730650522317189\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7137775569135307, Train acc: 0.7312232905982906\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7107426649901695, Train acc: 0.7324932012432013\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7098064861363835, Train acc: 0.7331285612535613\n",
      "Val loss: 0.6523529291152954, Val acc: 0.758\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6795272289687752, Train acc: 0.749198717948718\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6890517152272738, Train acc: 0.7423878205128205\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6840075166986199, Train acc: 0.7438568376068376\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6820963932535588, Train acc: 0.7436565170940171\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6793793006839915, Train acc: 0.7435897435897436\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6808665475869111, Train acc: 0.7435452279202279\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6784430971512427, Train acc: 0.7442765567765568\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6796113205834841, Train acc: 0.7436565170940171\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6817740817525109, Train acc: 0.7437381291547959\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6823599146981525, Train acc: 0.744150641025641\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6831476304683004, Train acc: 0.7445609945609946\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6836511284282744, Train acc: 0.7448139245014245\n",
      "Val loss: 0.6004538536071777, Val acc: 0.758\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6653876936333811, Train acc: 0.750801282051282\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6596391757584026, Train acc: 0.7521367521367521\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6641701812248284, Train acc: 0.749198717948718\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6680219514120338, Train acc: 0.750801282051282\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6666569728117723, Train acc: 0.752724358974359\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6660813107619598, Train acc: 0.7529825498575499\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6677093072834178, Train acc: 0.7519459706959707\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6693757103167028, Train acc: 0.7528044871794872\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6676263057211979, Train acc: 0.7539470560303894\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6686692453602441, Train acc: 0.7524038461538461\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6673893560109068, Train acc: 0.7525980963480964\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6663303510458382, Train acc: 0.7525819088319088\n",
      "Val loss: 0.5948695540428162, Val acc: 0.768\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6379804677433438, Train acc: 0.7711004273504274\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6499343774257562, Train acc: 0.7645566239316239\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6436104535037636, Train acc: 0.7687856125356125\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6410904616499559, Train acc: 0.7668269230769231\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6414831455446716, Train acc: 0.767147435897436\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6407827523104486, Train acc: 0.765090811965812\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6412769421771333, Train acc: 0.7643467643467643\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6421792684520922, Train acc: 0.7636551816239316\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6431646714409759, Train acc: 0.7628205128205128\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6459345618119606, Train acc: 0.7617521367521367\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6444182051598979, Train acc: 0.7623348873348873\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6439305936806222, Train acc: 0.7625311609686609\n",
      "Val loss: 0.5866663455963135, Val acc: 0.77\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6391088677267743, Train acc: 0.7566773504273504\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.635158049245166, Train acc: 0.7616185897435898\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6352646676903098, Train acc: 0.7637998575498576\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.637181821732949, Train acc: 0.7634214743589743\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6353392867960481, Train acc: 0.7651709401709401\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6339702185062941, Train acc: 0.7665153133903134\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6338269714907412, Train acc: 0.7670177045177046\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6329023531104765, Train acc: 0.7671274038461539\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6307443067332392, Train acc: 0.7675985280151947\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6309125696491992, Train acc: 0.7676014957264957\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6299060935555453, Train acc: 0.7679195804195804\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6297710899613861, Train acc: 0.7677394943019943\n",
      "Val loss: 0.5692964792251587, Val acc: 0.786\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5997788783831474, Train acc: 0.7791132478632479\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5981152395152638, Train acc: 0.7844551282051282\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.608595099119719, Train acc: 0.7784900284900285\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6067496838732662, Train acc: 0.7781784188034188\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6098384910669082, Train acc: 0.777991452991453\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6074393342787724, Train acc: 0.7795584045584045\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6074246222719605, Train acc: 0.7785027472527473\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6086961906244103, Train acc: 0.7783119658119658\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6077852191974861, Train acc: 0.7789055080721747\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6089893688758214, Train acc: 0.7780448717948718\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6085137274252321, Train acc: 0.7777534965034965\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6086318700699045, Train acc: 0.7778668091168092\n",
      "Val loss: 0.5662295818328857, Val acc: 0.78\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5789818753544081, Train acc: 0.7871260683760684\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5940044778279769, Train acc: 0.781517094017094\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5963953570422963, Train acc: 0.7816951566951567\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5965734424117284, Train acc: 0.7816506410256411\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5956538525911478, Train acc: 0.7826388888888889\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5929738899156918, Train acc: 0.7830751424501424\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5926850494884309, Train acc: 0.7826236263736264\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5922886512091017, Train acc: 0.7828525641025641\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5931987234613268, Train acc: 0.7823183760683761\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5955398263839575, Train acc: 0.7814102564102564\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.594876876290491, Train acc: 0.7821969696969697\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5940443272052327, Train acc: 0.7828748219373219\n",
      "Val loss: 0.5424444079399109, Val acc: 0.794\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6024111967820388, Train acc: 0.7719017094017094\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.611529272973028, Train acc: 0.7721688034188035\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.599000769632834, Train acc: 0.7753739316239316\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5924673202710274, Train acc: 0.7810496794871795\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5870640482148554, Train acc: 0.7837606837606838\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5859554392150325, Train acc: 0.7841435185185185\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5889365272062019, Train acc: 0.7835775335775336\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5870843442777792, Train acc: 0.7846554487179487\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5874231009967873, Train acc: 0.7845144824311491\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5856132581320583, Train acc: 0.7849091880341881\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5839306640828776, Train acc: 0.7849407536907537\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5861910130733099, Train acc: 0.7849225427350427\n",
      "Val loss: 0.5312625765800476, Val acc: 0.794\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5799618047526759, Train acc: 0.7940705128205128\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5792899252767236, Train acc: 0.7939369658119658\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5827188824656343, Train acc: 0.7934472934472935\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5759179895887008, Train acc: 0.7948050213675214\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5765484434417171, Train acc: 0.7929487179487179\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5761533223750585, Train acc: 0.7919782763532763\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5774766774040729, Train acc: 0.7907509157509157\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5782936190883828, Train acc: 0.7908987713675214\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5768337686743379, Train acc: 0.7911324786324786\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5777176820314848, Train acc: 0.7911858974358974\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5748112913056131, Train acc: 0.7922008547008547\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5737592514147956, Train acc: 0.7921340811965812\n",
      "Val loss: 0.5321766138076782, Val acc: 0.786\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.558055572784864, Train acc: 0.7924679487179487\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.557853512529634, Train acc: 0.7956730769230769\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5658656310321938, Train acc: 0.7930911680911681\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5709526352265961, Train acc: 0.7906650641025641\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5713304740750892, Train acc: 0.7912393162393162\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.569134147206263, Train acc: 0.7925124643874644\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5691374097478841, Train acc: 0.7923534798534798\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5660312167790711, Train acc: 0.7946380876068376\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5633454535648241, Train acc: 0.7953169515669516\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.564004797864164, Train acc: 0.7950320512820512\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5658176224895847, Train acc: 0.7945075757575758\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5671733752253898, Train acc: 0.794181801994302\n",
      "Val loss: 0.522369921207428, Val acc: 0.798\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5519758093560863, Train acc: 0.7975427350427351\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5578614027581663, Train acc: 0.7990117521367521\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5547022376305017, Train acc: 0.8004807692307693\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5540479796691837, Train acc: 0.7999465811965812\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5551911943488651, Train acc: 0.7981837606837607\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5511475664343249, Train acc: 0.7998575498575499\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5543092791268531, Train acc: 0.7991834554334555\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5528186663475811, Train acc: 0.7999465811965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5570257264777579, Train acc: 0.7989969135802469\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5560647657539091, Train acc: 0.7988782051282052\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5546798359634529, Train acc: 0.7996552059052059\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5533332977963988, Train acc: 0.8007478632478633\n",
      "Val loss: 0.5045561790466309, Val acc: 0.798\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5288826138035864, Train acc: 0.8103632478632479\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5361360380919571, Train acc: 0.8054220085470085\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5433016529365143, Train acc: 0.8007478632478633\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5458637610650979, Train acc: 0.8019497863247863\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5471997191508611, Train acc: 0.8005341880341881\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5465645352617289, Train acc: 0.8003472222222222\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5474411763150759, Train acc: 0.7991071428571429\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5484932861489873, Train acc: 0.7984107905982906\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5471694896392106, Train acc: 0.7992936847103513\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5457915549476942, Train acc: 0.7992521367521368\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5449956054129893, Train acc: 0.7998494560994561\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5448909674550902, Train acc: 0.8000578703703703\n",
      "Val loss: 0.49744322896003723, Val acc: 0.81\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5371459867709723, Train acc: 0.8036858974358975\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5388475639952554, Train acc: 0.8066239316239316\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5439463019201219, Train acc: 0.8050213675213675\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5392688412187446, Train acc: 0.8062232905982906\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5339346044593387, Train acc: 0.8086538461538462\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5337980668089668, Train acc: 0.8094729344729344\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5330828613122887, Train acc: 0.8092567155067155\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5325456575100493, Train acc: 0.8084935897435898\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5357105948193687, Train acc: 0.8070987654320988\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5364165678492978, Train acc: 0.806517094017094\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5340972763443512, Train acc: 0.8070367132867133\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5355217753991782, Train acc: 0.8067574786324786\n",
      "Val loss: 0.5140594244003296, Val acc: 0.804\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5396904178664216, Train acc: 0.8074252136752137\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.543887746002939, Train acc: 0.8044871794871795\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5441778839228839, Train acc: 0.8050213675213675\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5407792015208138, Train acc: 0.8053552350427351\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5380174431535932, Train acc: 0.8066773504273504\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5312425518392497, Train acc: 0.8088497150997151\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5331035137394846, Train acc: 0.808531746031746\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5308149847337323, Train acc: 0.8089610042735043\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5292073831117844, Train acc: 0.8094729344729344\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5271224555169415, Train acc: 0.8106570512820512\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5283514648305416, Train acc: 0.8098776223776224\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5273114832229594, Train acc: 0.8095842236467237\n",
      "Val loss: 0.49825242161750793, Val acc: 0.824\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5384876542111747, Train acc: 0.8052884615384616\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5321503203903508, Train acc: 0.8087606837606838\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5268803643874633, Train acc: 0.8116096866096866\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5227039698033761, Train acc: 0.8149706196581197\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5179391984756176, Train acc: 0.8156517094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5146466752572617, Train acc: 0.8165064102564102\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5170323823980069, Train acc: 0.8151327838827839\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5149740880498519, Train acc: 0.8162059294871795\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5149167888184898, Train acc: 0.8154973884140551\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5155504256232172, Train acc: 0.8149305555555556\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5153095744197093, Train acc: 0.815413752913753\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5159044683382892, Train acc: 0.8150373931623932\n",
      "Val loss: 0.48565873503685, Val acc: 0.822\n",
      "Epoch 25/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.49425675102278716, Train acc: 0.8263888888888888\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4988442911551549, Train acc: 0.8241185897435898\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.496542422859757, Train acc: 0.8240740740740741\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5026828810317904, Train acc: 0.8215811965811965\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5032906389134562, Train acc: 0.8215811965811965\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5044299029344507, Train acc: 0.8221599002849003\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5058245888305088, Train acc: 0.8207799145299145\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5056008969266446, Train acc: 0.8199118589743589\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5053671807446937, Train acc: 0.8200379867046533\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5039512471383454, Train acc: 0.8204059829059829\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5046648857499985, Train acc: 0.8201728826728827\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5056205476047816, Train acc: 0.8189547720797721\n",
      "Val loss: 0.47308674454689026, Val acc: 0.826\n",
      "Epoch 26/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.508172573060052, Train acc: 0.8253205128205128\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.498088742295901, Train acc: 0.828125\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4929384796792625, Train acc: 0.8279024216524217\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.49827995020737, Train acc: 0.8243856837606838\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.49888241392934424, Train acc: 0.8232371794871794\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.49728584253465347, Train acc: 0.8233618233618234\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4969696158017868, Train acc: 0.8238705738705738\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.49801623539473766, Train acc: 0.823417467948718\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5021397107884868, Train acc: 0.8216999050332384\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5037834653487572, Train acc: 0.820806623931624\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5034661538024789, Train acc: 0.8207556332556333\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5028907298044092, Train acc: 0.8212250712250713\n",
      "Val loss: 0.4893264174461365, Val acc: 0.824\n",
      "Epoch 27/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5210714402616533, Train acc: 0.8162393162393162\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5053044514268892, Train acc: 0.8189102564102564\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.49971948827263973, Train acc: 0.8216702279202279\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4932468534471133, Train acc: 0.8233840811965812\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4921137909349213, Train acc: 0.8241987179487179\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.498089173474373, Train acc: 0.8223824786324786\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.499444518588547, Train acc: 0.8235653235653235\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.49550462062032813, Train acc: 0.8250534188034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.49416491983622907, Train acc: 0.8250830959164293\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4946578396563856, Train acc: 0.8247863247863247\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.49420211843353146, Train acc: 0.8247377622377622\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.49373134946254243, Train acc: 0.8246750356125356\n",
      "Val loss: 0.46857476234436035, Val acc: 0.814\n",
      "Epoch 28/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4643829126133878, Train acc: 0.8319978632478633\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.47151887429575634, Train acc: 0.8307959401709402\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4691416814795926, Train acc: 0.8317307692307693\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.47548550695308256, Train acc: 0.828392094017094\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4783254448929404, Train acc: 0.8281517094017095\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.47974886448040305, Train acc: 0.8272346866096866\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.47899317528520313, Train acc: 0.8288308913308914\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4805862579940476, Train acc: 0.8275240384615384\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.480996712646027, Train acc: 0.8280211301044634\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.48274067031522083, Train acc: 0.8269230769230769\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.48403406290231793, Train acc: 0.8271416083916084\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4847001366402179, Train acc: 0.8269898504273504\n",
      "Val loss: 0.48041749000549316, Val acc: 0.824\n",
      "Epoch 29/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5138685942078248, Train acc: 0.8207799145299145\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4996214939806706, Train acc: 0.8242521367521367\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4931348930099751, Train acc: 0.8268340455840456\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4868487386017019, Train acc: 0.8274572649572649\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4897621581569696, Train acc: 0.8262820512820512\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.48567787391210554, Train acc: 0.8270566239316239\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4854766163470108, Train acc: 0.8271901709401709\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.48581773626148445, Train acc: 0.8267227564102564\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4853656546045572, Train acc: 0.8271901709401709\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.48165316796328267, Train acc: 0.8279647435897436\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.48290432542099354, Train acc: 0.8270444832944833\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4802388023754895, Train acc: 0.8280137108262108\n",
      "Val loss: 0.46703439950942993, Val acc: 0.842\n",
      "Epoch 30/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.45520389436656594, Train acc: 0.8400106837606838\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4737035431540929, Train acc: 0.8298611111111112\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4767578892048947, Train acc: 0.8309294871794872\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4788398406126051, Train acc: 0.8309294871794872\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4781305103220491, Train acc: 0.8289529914529915\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.47200197030251523, Train acc: 0.8303507834757835\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4695544308176553, Train acc: 0.8315399877899878\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4714151620068866, Train acc: 0.8303285256410257\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.472685470129916, Train acc: 0.8294753086419753\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.47249714511836693, Train acc: 0.8298344017094017\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.47203204744345656, Train acc: 0.8297397047397047\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.47207863291359353, Train acc: 0.8301949786324786\n",
      "Val loss: 0.4723375737667084, Val acc: 0.83\n",
      "Epoch 31/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4670281166959013, Train acc: 0.8384081196581197\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.470560654042623, Train acc: 0.8365384615384616\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4657710957900751, Train acc: 0.8356481481481481\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4711544959782026, Train acc: 0.8320646367521367\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.46902728630946233, Train acc: 0.833173076923077\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.47120637809618926, Train acc: 0.8324875356125356\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4679496798104856, Train acc: 0.8338675213675214\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.46735629077968943, Train acc: 0.8344684829059829\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4665913890484731, Train acc: 0.8349655745489079\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4652271465231211, Train acc: 0.8352564102564103\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4638871707560577, Train acc: 0.835348679098679\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.46506451923623043, Train acc: 0.8350471866096866\n",
      "Val loss: 0.4666666090488434, Val acc: 0.834\n",
      "Epoch 32/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.49276912110483545, Train acc: 0.8231837606837606\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4759192678790826, Train acc: 0.8290598290598291\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4722330739185681, Train acc: 0.8279024216524217\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4701883979141712, Train acc: 0.8305288461538461\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.46699308643483706, Train acc: 0.8321047008547009\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4636413804825894, Train acc: 0.833511396011396\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.46408885817460815, Train acc: 0.8331807081807082\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4611127205575124, Train acc: 0.8344017094017094\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4612371703894616, Train acc: 0.8344017094017094\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4608109136040394, Train acc: 0.8344017094017094\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.45942116711106334, Train acc: 0.8351544289044289\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.46014507090476503, Train acc: 0.8352697649572649\n",
      "Val loss: 0.4612340033054352, Val acc: 0.838\n",
      "Epoch 33/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.45017233822080827, Train acc: 0.8373397435897436\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.45433338877991736, Train acc: 0.8369391025641025\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4542217233836481, Train acc: 0.8373397435897436\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.45882359022895497, Train acc: 0.8353365384615384\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4610698903982456, Train acc: 0.8352564102564103\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.45822380860986195, Train acc: 0.8368945868945868\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.45533227670360105, Train acc: 0.8368818681318682\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4558018298151019, Train acc: 0.8371728098290598\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.45665010281352914, Train acc: 0.8372210351377019\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4557542158051943, Train acc: 0.838034188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4547106797496478, Train acc: 0.8382867132867133\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4541177308170969, Train acc: 0.8385194088319088\n",
      "Val loss: 0.47486957907676697, Val acc: 0.842\n",
      "Epoch 34/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.43532261965621233, Train acc: 0.843215811965812\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4379720966785382, Train acc: 0.8412126068376068\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4391524816765065, Train acc: 0.8409009971509972\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.44021029088996416, Train acc: 0.8421474358974359\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4387209929080091, Train acc: 0.8425747863247863\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.44494434256117227, Train acc: 0.8406339031339032\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4472391696803736, Train acc: 0.8399725274725275\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4481122714475307, Train acc: 0.8394764957264957\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4488218481823137, Train acc: 0.838823599240266\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.44837690042252215, Train acc: 0.8387820512820513\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.44845564697565915, Train acc: 0.8393550893550894\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4493667229617338, Train acc: 0.8390535968660968\n",
      "Val loss: 0.449195921421051, Val acc: 0.838\n",
      "Epoch 35/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.43473229576379824, Train acc: 0.8408119658119658\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.445910953813129, Train acc: 0.8368055555555556\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4378109600214537, Train acc: 0.8396545584045584\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4450408755523017, Train acc: 0.8392761752136753\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4383736166943852, Train acc: 0.8433760683760684\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.43918971589997285, Train acc: 0.8438390313390314\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4404327764252021, Train acc: 0.8426816239316239\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4398372207537421, Train acc: 0.8434495192307693\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4401979195369728, Train acc: 0.8428596866096866\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4410168869373126, Train acc: 0.8420138888888888\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4424504766467938, Train acc: 0.8417832167832168\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.44174785653070847, Train acc: 0.8421919515669516\n",
      "Val loss: 0.4686989486217499, Val acc: 0.84\n",
      "Epoch 36/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.40034077959692377, Train acc: 0.8648504273504274\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.42421905849224484, Train acc: 0.8552350427350427\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42523232892028284, Train acc: 0.8535434472934473\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.43138200936154425, Train acc: 0.8504941239316239\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4356970882313883, Train acc: 0.8487713675213675\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4327640748049459, Train acc: 0.8481125356125356\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42973414292702306, Train acc: 0.8488247863247863\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.43228265308798886, Train acc: 0.8476896367521367\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.43338189783514053, Train acc: 0.8470441595441596\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.43449962256937963, Train acc: 0.8467681623931624\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4348093389353274, Train acc: 0.8473679098679099\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4358354442012616, Train acc: 0.8465990028490028\n",
      "Val loss: 0.4511134624481201, Val acc: 0.83\n",
      "Epoch 37/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.43545492795797497, Train acc: 0.8458867521367521\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4380324498519429, Train acc: 0.8422809829059829\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4333150996654122, Train acc: 0.8474002849002849\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.43413884261161345, Train acc: 0.8465544871794872\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.43416178128403476, Train acc: 0.8456196581196581\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4308208206992204, Train acc: 0.8465099715099715\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4317555486500918, Train acc: 0.8464972527472527\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.43166121789532846, Train acc: 0.8470219017094017\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4318265467174599, Train acc: 0.8472222222222222\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4317184823063704, Train acc: 0.8465010683760684\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.43110407606380896, Train acc: 0.8464452214452215\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4317832479195038, Train acc: 0.8461315883190883\n",
      "Val loss: 0.4444245994091034, Val acc: 0.85\n",
      "Epoch 38/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.436079455109743, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4384864946970573, Train acc: 0.842948717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4327459658397908, Train acc: 0.8457977207977208\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4336569857520935, Train acc: 0.8454193376068376\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.42944521993143947, Train acc: 0.8481837606837607\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.427082170363505, Train acc: 0.8481125356125356\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4284124801956545, Train acc: 0.8476037851037851\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.42696315540462476, Train acc: 0.8485910790598291\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4286662341296277, Train acc: 0.8474299620132953\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4282174649465288, Train acc: 0.8475160256410257\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4282048297083554, Train acc: 0.8472465034965035\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4279875983868824, Train acc: 0.8475115740740741\n",
      "Val loss: 0.44571149349212646, Val acc: 0.846\n",
      "Epoch 39/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42805868132501584, Train acc: 0.8504273504273504\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.43109528987835616, Train acc: 0.8472222222222222\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42770866005339175, Train acc: 0.8474893162393162\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4255668932938168, Train acc: 0.8478899572649573\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.42486943434446284, Train acc: 0.8493589743589743\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.42685985204331217, Train acc: 0.8490028490028491\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42632773806032825, Train acc: 0.8489774114774115\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4293375178916842, Train acc: 0.84765625\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4292979672452097, Train acc: 0.8477267331433999\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.42822952090929717, Train acc: 0.8477564102564102\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4259126210279861, Train acc: 0.8486790986790986\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.42421631316556213, Train acc: 0.8495592948717948\n",
      "Val loss: 0.45094943046569824, Val acc: 0.842\n",
      "Epoch 40/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4080093002472168, Train acc: 0.8504273504273504\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4166959366864628, Train acc: 0.8462873931623932\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4211155325598866, Train acc: 0.8461538461538461\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.41892847975986636, Train acc: 0.8482905982905983\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.41882066515266386, Train acc: 0.8497863247863248\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.41665062188380464, Train acc: 0.8506499287749287\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.41579663545184375, Train acc: 0.8511141636141636\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.41480123878130293, Train acc: 0.8519631410256411\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4142933536218329, Train acc: 0.8515550807217473\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.41473803378195845, Train acc: 0.8511485042735043\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4158476894748887, Train acc: 0.8508644133644133\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4155537013728649, Train acc: 0.8511396011396012\n",
      "Val loss: 0.4290381073951721, Val acc: 0.834\n",
      "Epoch 41/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3886316225060031, Train acc: 0.8643162393162394\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.40716546500085765, Train acc: 0.8538995726495726\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4098632503353972, Train acc: 0.853454415954416\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4106123678577252, Train acc: 0.8516960470085471\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.41210643636365224, Train acc: 0.8518162393162393\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4142830849670277, Train acc: 0.8506944444444444\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4136122635978482, Train acc: 0.8508852258852259\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4146341766652643, Train acc: 0.8505942841880342\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4136863967716864, Train acc: 0.8507834757834758\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4131153145852761, Train acc: 0.8513354700854701\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4133036094771954, Train acc: 0.8517628205128205\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.41396956483798997, Train acc: 0.8516737891737892\n",
      "Val loss: 0.44302865862846375, Val acc: 0.84\n",
      "Epoch 42/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.40720874631506765, Train acc: 0.8589743589743589\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4084322793234108, Train acc: 0.858573717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4044236747118143, Train acc: 0.8577279202279202\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.40844686252948564, Train acc: 0.8549679487179487\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.40993180534778495, Train acc: 0.8526175213675213\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.41170546777567635, Train acc: 0.8525195868945868\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4114506147137783, Train acc: 0.852754884004884\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.41370521531973636, Train acc: 0.8521300747863247\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4101836722121279, Train acc: 0.8537808641975309\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4093512562987132, Train acc: 0.8536858974358974\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.40923752585352163, Train acc: 0.853486790986791\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4095761548526097, Train acc: 0.8533431267806267\n",
      "Val loss: 0.42612501978874207, Val acc: 0.844\n",
      "Epoch 43/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.40039710216542596, Train acc: 0.8563034188034188\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.404225583641957, Train acc: 0.8543002136752137\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4067193198832352, Train acc: 0.8563034188034188\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4160077111461224, Train acc: 0.8522302350427351\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4138315333642511, Train acc: 0.8535790598290598\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4083043570892933, Train acc: 0.8551014957264957\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4070515913095084, Train acc: 0.8556929181929182\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.407593616650591, Train acc: 0.8555355235042735\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.40635722956964204, Train acc: 0.8561253561253561\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4067373658156293, Train acc: 0.8561965811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.40616225830996416, Train acc: 0.8561820124320124\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4067821987450887, Train acc: 0.8554131054131054\n",
      "Val loss: 0.43906351923942566, Val acc: 0.846\n",
      "Epoch 44/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4071467776074369, Train acc: 0.8525641025641025\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.39984362037518084, Train acc: 0.8552350427350427\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4106537957816382, Train acc: 0.8514066951566952\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4108534883866962, Train acc: 0.8526308760683761\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.40649878003148954, Train acc: 0.854594017094017\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4055261117992578, Train acc: 0.8549679487179487\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4009373479855948, Train acc: 0.8561126373626373\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.40366491981041736, Train acc: 0.8557024572649573\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.40294836853530897, Train acc: 0.8559769705603039\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4031237666168783, Train acc: 0.8563568376068376\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4012508485075477, Train acc: 0.8568618881118881\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4014398844015819, Train acc: 0.8567263176638177\n",
      "Val loss: 0.43703460693359375, Val acc: 0.85\n",
      "Epoch 45/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3917106672739371, Train acc: 0.8571047008547008\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3949779939447713, Train acc: 0.8584401709401709\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3948811131749737, Train acc: 0.8587072649572649\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.396896190304532, Train acc: 0.8583733974358975\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3942880582605672, Train acc: 0.8584401709401709\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.39660486992862487, Train acc: 0.8578614672364673\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3949718352711972, Train acc: 0.8584783272283272\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3952855674916098, Train acc: 0.8583066239316239\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.39522066442092485, Train acc: 0.8587369420702754\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3964445022181568, Train acc: 0.8585202991452991\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3974631470631701, Train acc: 0.8580516705516705\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.39799504535023306, Train acc: 0.8576166310541311\n",
      "Val loss: 0.4173766076564789, Val acc: 0.856\n",
      "Epoch 46/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.43091995950438017, Train acc: 0.8501602564102564\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.41103451106792843, Train acc: 0.8533653846153846\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.40491441486567853, Train acc: 0.85505698005698\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.40195295857822794, Train acc: 0.8564369658119658\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.40189898808797203, Train acc: 0.8559294871794871\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.40264829085805476, Train acc: 0.8565259971509972\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.40184762452584627, Train acc: 0.8573717948717948\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4025405198096847, Train acc: 0.8566706730769231\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.40006825037067095, Train acc: 0.8576388888888888\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4005693540040754, Train acc: 0.8570245726495727\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3982679011602687, Train acc: 0.8581730769230769\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3970950877086984, Train acc: 0.8585959757834758\n",
      "Val loss: 0.43656378984451294, Val acc: 0.848\n",
      "Epoch 47/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4001081104461963, Train acc: 0.8635149572649573\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.39382467686365813, Train acc: 0.8620459401709402\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.395375707104505, Train acc: 0.8585292022792023\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3939473145187665, Train acc: 0.8594417735042735\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.39297623997315384, Train acc: 0.8594551282051283\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.39181861392835265, Train acc: 0.8592414529914529\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.39119728549095883, Train acc: 0.8590888278388278\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.39389749984933525, Train acc: 0.8586071047008547\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3931806744319534, Train acc: 0.8584401709401709\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3927971897662705, Train acc: 0.8581730769230769\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3905190990278215, Train acc: 0.85880439005439\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.39100331457018683, Train acc: 0.8588630698005698\n",
      "Val loss: 0.4380803406238556, Val acc: 0.854\n",
      "Epoch 48/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.37034736147038955, Train acc: 0.8659188034188035\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.37915938806075317, Train acc: 0.8667200854700855\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.38512499080679014, Train acc: 0.8630698005698005\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.37989369568088627, Train acc: 0.8635149572649573\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.38101731661038524, Train acc: 0.8642628205128206\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3799723788904838, Train acc: 0.8643607549857549\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3790717924489061, Train acc: 0.8655753968253969\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.38406910520429033, Train acc: 0.8635817307692307\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3838878669700844, Train acc: 0.8632775403608737\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3866257549032696, Train acc: 0.8620459401709402\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3887040558059084, Train acc: 0.8613296425796426\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3897019006853175, Train acc: 0.8618456196581197\n",
      "Val loss: 0.42600154876708984, Val acc: 0.858\n",
      "Epoch 49/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.39572895057181007, Train acc: 0.8627136752136753\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3860494040716917, Train acc: 0.8640491452991453\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.38843963910796364, Train acc: 0.8628027065527065\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.39006376328567666, Train acc: 0.8624465811965812\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3874688058709487, Train acc: 0.861965811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3867423953090468, Train acc: 0.8618233618233618\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.38568819577149566, Train acc: 0.8627899877899878\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3847809316128747, Train acc: 0.8629807692307693\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.38557348187379575, Train acc: 0.8630401234567902\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.38469435579629024, Train acc: 0.8634882478632478\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3846671062216985, Train acc: 0.8634663947163947\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.38604491224719417, Train acc: 0.8629807692307693\n",
      "Val loss: 0.4162972569465637, Val acc: 0.858\n",
      "Epoch 50/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.36894170857138103, Train acc: 0.8669871794871795\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.38558347749277055, Train acc: 0.8589743589743589\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.38359803058992425, Train acc: 0.8610220797720798\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.38672867111670667, Train acc: 0.859107905982906\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.385417084956271, Train acc: 0.862232905982906\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3862375398498145, Train acc: 0.8605769230769231\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.38785557180497526, Train acc: 0.8605769230769231\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.38680574496109515, Train acc: 0.8614115918803419\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3837146664715787, Train acc: 0.8622091642924976\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3819729590963604, Train acc: 0.8634081196581197\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.38222705862298895, Train acc: 0.8638791763791763\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3820776031737314, Train acc: 0.8640268874643875\n",
      "Val loss: 0.4245813190937042, Val acc: 0.848\n",
      "Tiempo total de entrenamiento: 649.9478 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.608023729079809, Train acc: 0.21741452991452992\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.5912577672901316, Train acc: 0.25854700854700857\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.548516372669796, Train acc: 0.29255698005698005\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.4988853264058757, Train acc: 0.3263888888888889\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.453921571552244, Train acc: 0.3578525641025641\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.4218198246935494, Train acc: 0.380386396011396\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.3929222471402534, Train acc: 0.40228174603174605\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.3636738625474465, Train acc: 0.4201388888888889\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.3391499653501049, Train acc: 0.4354522792022792\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.315756474295233, Train acc: 0.4480769230769231\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.296302746225904, Train acc: 0.45945027195027194\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.2763692976487668, Train acc: 0.4709312678062678\n",
      "Val loss: 0.9222588539123535, Val acc: 0.622\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.0253263487775102, Train acc: 0.6073717948717948\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.026251868559764, Train acc: 0.6111111111111112\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.0204706906932712, Train acc: 0.6112891737891738\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.0155992113117478, Train acc: 0.6117120726495726\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.0056631081124656, Train acc: 0.6138888888888889\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.0001091388216046, Train acc: 0.6167200854700855\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9948514034750988, Train acc: 0.6182844932844933\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9919278456741928, Train acc: 0.6204260149572649\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9870786904043395, Train acc: 0.6231006647673314\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9828980558957809, Train acc: 0.6239850427350427\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9806121634066152, Train acc: 0.6245872183372183\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.974323036918613, Train acc: 0.6275373931623932\n",
      "Val loss: 0.8157223463058472, Val acc: 0.694\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.9039002886185279, Train acc: 0.6522435897435898\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8986178302866781, Train acc: 0.6579861111111112\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9075038181750523, Train acc: 0.6534009971509972\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9041583243853006, Train acc: 0.6540464743589743\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8998871350899721, Train acc: 0.6566239316239316\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8976411736249245, Train acc: 0.6584312678062678\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.894642842048836, Train acc: 0.6595695970695971\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8892814509061158, Train acc: 0.6620259081196581\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8870754054299107, Train acc: 0.6630460588793922\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8876811253973561, Train acc: 0.662366452991453\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8869601190090179, Train acc: 0.6629030691530692\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8841350339883752, Train acc: 0.6637508903133903\n",
      "Val loss: 0.7812292575836182, Val acc: 0.706\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8535874052944347, Train acc: 0.6682692307692307\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8536979028811822, Train acc: 0.6730769230769231\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8527776672799363, Train acc: 0.6719195156695157\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8516882770718672, Train acc: 0.6730769230769231\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.847821462714774, Train acc: 0.6762820512820513\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8471432565538971, Train acc: 0.6764155982905983\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.849362564501745, Train acc: 0.676510989010989\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8469373635056182, Train acc: 0.6774839743589743\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.843178324578268, Train acc: 0.6790123456790124\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8417419744098288, Train acc: 0.6803151709401709\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8399873996021772, Train acc: 0.6809440559440559\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8378757922100885, Train acc: 0.6810897435897436\n",
      "Val loss: 0.7464260458946228, Val acc: 0.714\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8086039395923288, Train acc: 0.6853632478632479\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8046512806262726, Train acc: 0.6912393162393162\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8042081221725866, Train acc: 0.6925747863247863\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8029269615554402, Train acc: 0.694511217948718\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8009878679727896, Train acc: 0.6943376068376068\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7974075742511668, Train acc: 0.6959134615384616\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7983369314918005, Train acc: 0.6952075702075702\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8013999181934911, Train acc: 0.6935763888888888\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8056760934116947, Train acc: 0.69215930674264\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8060630883926, Train acc: 0.6924145299145299\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.802301851116416, Train acc: 0.6937402874902875\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8031006118128782, Train acc: 0.6924857549857549\n",
      "Val loss: 0.764420747756958, Val acc: 0.714\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7902086779602573, Train acc: 0.6915064102564102\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7897788255642622, Train acc: 0.6985844017094017\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7833758404961338, Train acc: 0.6992521367521367\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7777260625336924, Train acc: 0.7043269230769231\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.777604989006988, Train acc: 0.7034188034188035\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7712821248080316, Train acc: 0.7069088319088319\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7706189941573929, Train acc: 0.7075320512820513\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7733642818708705, Train acc: 0.7064302884615384\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7707664175816977, Train acc: 0.7080959164292497\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7699983233824754, Train acc: 0.7076121794871795\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7671962125920101, Train acc: 0.7089889277389277\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7672161280556962, Train acc: 0.7089788105413105\n",
      "Val loss: 0.7037482261657715, Val acc: 0.736\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7532785422781594, Train acc: 0.7142094017094017\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7405781759920284, Train acc: 0.7188835470085471\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7473959351367081, Train acc: 0.7171474358974359\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.749689584868586, Train acc: 0.7164129273504274\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7489759523134966, Train acc: 0.7161324786324786\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.746149961680089, Train acc: 0.7178151709401709\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.743467758099238, Train acc: 0.7192078754578755\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7440103943276609, Train acc: 0.7200186965811965\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7434046500825021, Train acc: 0.719877730294397\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7408039112885793, Train acc: 0.7209401709401709\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7394835283428361, Train acc: 0.7218094405594405\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.738470981327387, Train acc: 0.7226006054131054\n",
      "Val loss: 0.6560634970664978, Val acc: 0.736\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7390226609686501, Train acc: 0.7176816239316239\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7209350951971152, Train acc: 0.7279647435897436\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7104326854234407, Train acc: 0.7353098290598291\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7123105852012961, Train acc: 0.734107905982906\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7138873550117525, Train acc: 0.7347756410256411\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7140871789465603, Train acc: 0.7328614672364673\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7096152453704923, Train acc: 0.7347756410256411\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7110068630586323, Train acc: 0.733573717948718\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7118740352074079, Train acc: 0.7342117758784426\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.709218154529221, Train acc: 0.7346955128205128\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7096912510745175, Train acc: 0.7339500777000777\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7100784142217745, Train acc: 0.7348424145299145\n",
      "Val loss: 0.630660891532898, Val acc: 0.754\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6867452992333306, Train acc: 0.750534188034188\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.69545937794396, Train acc: 0.7449252136752137\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6973108927748481, Train acc: 0.7437678062678063\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6927657990883558, Train acc: 0.7455261752136753\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6923032907339243, Train acc: 0.7450320512820513\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6887018529013691, Train acc: 0.7454594017094017\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6860506532756983, Train acc: 0.7479014041514042\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6854938508417362, Train acc: 0.7483974358974359\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6829620872014834, Train acc: 0.7495251661918328\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6819309526019626, Train acc: 0.7498397435897436\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6813314391738607, Train acc: 0.749465811965812\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.678980210206957, Train acc: 0.7496216168091168\n",
      "Val loss: 0.6001895666122437, Val acc: 0.77\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6713999145560794, Train acc: 0.7638888888888888\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6707079354514424, Train acc: 0.7580128205128205\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.673790358410262, Train acc: 0.7570334757834758\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6744845532453977, Train acc: 0.7567441239316239\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6714185771269676, Train acc: 0.7566239316239316\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6637767760502307, Train acc: 0.7585024928774928\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6641341355039087, Train acc: 0.7585851648351648\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6614224846419107, Train acc: 0.7588474893162394\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6596528965863068, Train acc: 0.7595857075023742\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6606877354729889, Train acc: 0.7587606837606837\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6586462569820297, Train acc: 0.7599067599067599\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6574242080208922, Train acc: 0.7603276353276354\n",
      "Val loss: 0.5805820822715759, Val acc: 0.77\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6334034690999577, Train acc: 0.7649572649572649\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6473577375340666, Train acc: 0.7624198717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6480058476626024, Train acc: 0.7603276353276354\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6503283473798352, Train acc: 0.7618189102564102\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6471886321010752, Train acc: 0.7620192307692307\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6439769029022961, Train acc: 0.7639779202279202\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6439090356576428, Train acc: 0.7644612332112332\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6430731531964917, Train acc: 0.7650240384615384\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6429711256498172, Train acc: 0.7657585470085471\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6413817430918033, Train acc: 0.7666399572649573\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.638551392847934, Train acc: 0.7674339549339549\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6357200321054187, Train acc: 0.7689636752136753\n",
      "Val loss: 0.5747275948524475, Val acc: 0.772\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6221813969632499, Train acc: 0.7678952991452992\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6315709386880581, Train acc: 0.7649572649572649\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6306490199348526, Train acc: 0.7673611111111112\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6292360961182505, Train acc: 0.7692307692307693\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.622678085270091, Train acc: 0.7725427350427351\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6179385039626364, Train acc: 0.7734597578347578\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6167159994588812, Train acc: 0.7751831501831502\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6162945611609353, Train acc: 0.7751068376068376\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6170692393469561, Train acc: 0.7737713675213675\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6177820548033103, Train acc: 0.7743322649572649\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6166318124268716, Train acc: 0.7747183372183373\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6172756113939815, Train acc: 0.7739939458689459\n",
      "Val loss: 0.553642213344574, Val acc: 0.786\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6049323835943499, Train acc: 0.7785790598290598\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6027206067855542, Train acc: 0.7823183760683761\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6007767073449246, Train acc: 0.7829415954415955\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6050888608790871, Train acc: 0.7808493589743589\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6020077697741679, Train acc: 0.7808226495726496\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6010884874821388, Train acc: 0.7811609686609686\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6033768743749649, Train acc: 0.7803724053724054\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6008920445083044, Train acc: 0.78125\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5999501806039077, Train acc: 0.7810125830959165\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5998271545283814, Train acc: 0.7815705128205128\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5991747736606657, Train acc: 0.7814685314685315\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6000131470737634, Train acc: 0.7814503205128205\n",
      "Val loss: 0.54912269115448, Val acc: 0.786\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5835392654706271, Train acc: 0.7930021367521367\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5930544925678489, Train acc: 0.7889957264957265\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5849408635981063, Train acc: 0.7890847578347578\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5851693481652656, Train acc: 0.7872596153846154\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5806990740645644, Train acc: 0.7879807692307692\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5828606871797828, Train acc: 0.7880163817663818\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5861821236118437, Train acc: 0.787469474969475\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5837720294092965, Train acc: 0.7875934829059829\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5809923941649829, Train acc: 0.7884912155745489\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.582885058491658, Train acc: 0.7883547008547008\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5818598699041736, Train acc: 0.7886072261072261\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5833978271085313, Train acc: 0.7873041310541311\n",
      "Val loss: 0.5311519503593445, Val acc: 0.786\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5784577923452753, Train acc: 0.7964743589743589\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5661941430507562, Train acc: 0.7982104700854701\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5646962638933775, Train acc: 0.8003027065527065\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5675284588056752, Train acc: 0.797676282051282\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5723429249392615, Train acc: 0.7948717948717948\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5764622755369909, Train acc: 0.7941595441595442\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5742251634670555, Train acc: 0.7952152014652014\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5733649058538115, Train acc: 0.7963074252136753\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5741583404377995, Train acc: 0.7958214624881291\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5741344657845986, Train acc: 0.7951923076923076\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5744447279735971, Train acc: 0.7951874514374514\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5734923868904426, Train acc: 0.7955840455840456\n",
      "Val loss: 0.5344251394271851, Val acc: 0.78\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.567353583044476, Train acc: 0.7927350427350427\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5803009267800894, Train acc: 0.7897970085470085\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5734146162655279, Train acc: 0.7929131054131054\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5754577990016366, Train acc: 0.7938034188034188\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5745747322200709, Train acc: 0.7927350427350427\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.572854652661204, Train acc: 0.7914440883190883\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5667433652467343, Train acc: 0.7944902319902319\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5662190976560625, Train acc: 0.7944043803418803\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5656968847072815, Train acc: 0.7955246913580247\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5646107985423161, Train acc: 0.7963141025641025\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5625372034987463, Train acc: 0.7967414529914529\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5631444538781616, Train acc: 0.796607905982906\n",
      "Val loss: 0.4978346526622772, Val acc: 0.796\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5607011896422786, Train acc: 0.7975427350427351\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5624909659481456, Train acc: 0.7959401709401709\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5624372336769375, Train acc: 0.7971866096866097\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5546922540117023, Train acc: 0.7998130341880342\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5507423404699717, Train acc: 0.8012286324786325\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5499971998255817, Train acc: 0.8002136752136753\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5510898477537728, Train acc: 0.7989545177045178\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5488047059147786, Train acc: 0.8002804487179487\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5478584512516305, Train acc: 0.8009556030389364\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5483498845090214, Train acc: 0.8010416666666667\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5512052007720002, Train acc: 0.7996794871794872\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5502168355334518, Train acc: 0.80036948005698\n",
      "Val loss: 0.5019938945770264, Val acc: 0.806\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5245663755469852, Train acc: 0.811965811965812\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5373889517325622, Train acc: 0.8091613247863247\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5389879974629465, Train acc: 0.8099180911680912\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5329723157880143, Train acc: 0.8123664529914529\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5401978072702375, Train acc: 0.808974358974359\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5424566746734486, Train acc: 0.80818198005698\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5434873756362405, Train acc: 0.8071199633699634\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5418873239213076, Train acc: 0.8068910256410257\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5424082242412331, Train acc: 0.8070690883190883\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5406595366632837, Train acc: 0.806784188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5424834109362281, Train acc: 0.8062354312354313\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5412101287426602, Train acc: 0.8063790954415955\n",
      "Val loss: 0.49298444390296936, Val acc: 0.806\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.549301375181247, Train acc: 0.7991452991452992\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5413205905093087, Train acc: 0.8039529914529915\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5309044156658683, Train acc: 0.8084045584045584\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5319259546251378, Train acc: 0.8076255341880342\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5295060470063462, Train acc: 0.808974358974359\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5314038487581106, Train acc: 0.80818198005698\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5317699239840583, Train acc: 0.8082264957264957\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5285361893156655, Train acc: 0.8087606837606838\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5281535031562291, Train acc: 0.80917616334283\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5265486817711439, Train acc: 0.8094017094017094\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5278522849152416, Train acc: 0.8090763403263403\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5300106841890391, Train acc: 0.8086048789173789\n",
      "Val loss: 0.4788590371608734, Val acc: 0.824\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5250763289439373, Train acc: 0.8175747863247863\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5274013356011138, Train acc: 0.8155715811965812\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5172952808376051, Train acc: 0.8169515669515669\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5257891506338731, Train acc: 0.8120325854700855\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5243613130771196, Train acc: 0.8125534188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5232097099606807, Train acc: 0.8130787037037037\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.522041275678828, Train acc: 0.8137210012210012\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5232209615154654, Train acc: 0.8125667735042735\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5247618238046299, Train acc: 0.812232905982906\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5235887019298016, Train acc: 0.8125534188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5220056086801798, Train acc: 0.812767094017094\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.522396187688041, Train acc: 0.8126112891737892\n",
      "Val loss: 0.4759792387485504, Val acc: 0.818\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5156639139366965, Train acc: 0.8175747863247863\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5043252255672064, Train acc: 0.8181089743589743\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5015262467654003, Train acc: 0.8185541310541311\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5053192113454525, Train acc: 0.8187099358974359\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5073334390791053, Train acc: 0.8177884615384615\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5082598286349209, Train acc: 0.8174857549857549\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5064036756826699, Train acc: 0.8189102564102564\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5091017394079866, Train acc: 0.8177751068376068\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5091582166078763, Train acc: 0.8177825261158594\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5120355657660044, Train acc: 0.8168269230769231\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5120774292283587, Train acc: 0.8167006604506605\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5117452752980751, Train acc: 0.8173076923076923\n",
      "Val loss: 0.4879358410835266, Val acc: 0.81\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.507034889398477, Train acc: 0.8178418803418803\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5061815795608056, Train acc: 0.8237179487179487\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5025355550553384, Train acc: 0.8257656695156695\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5077967339028151, Train acc: 0.8223824786324786\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5054606659544839, Train acc: 0.8225961538461538\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5058852943812001, Train acc: 0.8213586182336182\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5066523120553211, Train acc: 0.8211233211233211\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5090960938419796, Train acc: 0.8198450854700855\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5089817104289788, Train acc: 0.8201566951566952\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5070144725660992, Train acc: 0.821073717948718\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5082529835350864, Train acc: 0.8207556332556333\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5073540817702088, Train acc: 0.8203125\n",
      "Val loss: 0.46077999472618103, Val acc: 0.812\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5107596875765384, Train acc: 0.8151709401709402\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5061259751136487, Train acc: 0.8186431623931624\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4960851067746127, Train acc: 0.8223824786324786\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4921023718630656, Train acc: 0.8238514957264957\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.49619105954964954, Train acc: 0.8224358974358974\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4951666504823924, Train acc: 0.8223379629629629\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.49771898755660426, Train acc: 0.8207036019536019\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4977408148284651, Train acc: 0.820579594017094\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4971639748170958, Train acc: 0.8209579772079773\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.49611783088781897, Train acc: 0.821607905982906\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.49550589084301055, Train acc: 0.821702602952603\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.49619184540547534, Train acc: 0.8217147435897436\n",
      "Val loss: 0.44483572244644165, Val acc: 0.834\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4951229057250879, Train acc: 0.8213141025641025\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4881412480504085, Train acc: 0.8226495726495726\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.48870562382063637, Train acc: 0.8213141025641025\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4899551216035317, Train acc: 0.8229834401709402\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.48804685898825656, Train acc: 0.8243589743589743\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.48688413898999194, Train acc: 0.8253205128205128\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4868111037291013, Train acc: 0.8266178266178266\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4896754724506894, Train acc: 0.8254540598290598\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.49018182109507175, Train acc: 0.8255282526115859\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.49061570240136904, Train acc: 0.8252136752136752\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.490174826150637, Train acc: 0.8257575757575758\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4887572794176235, Train acc: 0.8264111467236467\n",
      "Val loss: 0.442868709564209, Val acc: 0.826\n",
      "Epoch 25/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.497407927727088, Train acc: 0.8221153846153846\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.49102304162632704, Train acc: 0.8251869658119658\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4839404022421932, Train acc: 0.8280804843304843\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4855611536046888, Train acc: 0.8283253205128205\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4862818843788571, Train acc: 0.8269764957264957\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4843617036641493, Train acc: 0.8279469373219374\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4841684353730035, Train acc: 0.8285256410256411\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.48343379564710665, Train acc: 0.8290598290598291\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.48391697992343397, Train acc: 0.8292378917378918\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4824129323928784, Train acc: 0.8294604700854701\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.48232209175338836, Train acc: 0.8291812354312355\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4829116078289995, Train acc: 0.8291488603988604\n",
      "Val loss: 0.42940476536750793, Val acc: 0.83\n",
      "Epoch 26/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.46621475871811563, Train acc: 0.8349358974358975\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4573721543718607, Train acc: 0.8386752136752137\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.46966598681744687, Train acc: 0.8345797720797721\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4691865058918285, Train acc: 0.8374065170940171\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.47072079293748254, Train acc: 0.8361111111111111\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4697489413058656, Train acc: 0.8360042735042735\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.47052194392797564, Train acc: 0.8355463980463981\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4687328780722669, Train acc: 0.8354700854700855\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.46943890850315295, Train acc: 0.8352029914529915\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.46958083661957684, Train acc: 0.8351495726495727\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.471482889820831, Train acc: 0.834013209013209\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.47243552083982704, Train acc: 0.834045584045584\n",
      "Val loss: 0.43471473455429077, Val acc: 0.83\n",
      "Epoch 27/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.44075320406347257, Train acc: 0.843215811965812\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4548236128484082, Train acc: 0.8392094017094017\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.46007586646283793, Train acc: 0.8375178062678063\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4607914935829293, Train acc: 0.836204594017094\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4582414688462885, Train acc: 0.837232905982906\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.45971247082592076, Train acc: 0.8375178062678063\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4605466230011685, Train acc: 0.8365766178266179\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4605829839905103, Train acc: 0.8363715277777778\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4616544778049275, Train acc: 0.8360636277302944\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.46299689677026534, Train acc: 0.8357371794871795\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.46449051154799115, Train acc: 0.8353729603729604\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4644058235361111, Train acc: 0.8355368589743589\n",
      "Val loss: 0.4173813760280609, Val acc: 0.834\n",
      "Epoch 28/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.47056328893726707, Train acc: 0.8389423076923077\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.46725926223473674, Train acc: 0.8409455128205128\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.46385431595337695, Train acc: 0.8398326210826211\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4647480047220348, Train acc: 0.8376736111111112\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.46433821292514477, Train acc: 0.8363247863247864\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.466145787377473, Train acc: 0.8363603988603988\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4660368098153008, Train acc: 0.8351648351648352\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.46168450325026983, Train acc: 0.8367721688034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.46277564597039256, Train acc: 0.8361823361823362\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4611806361721112, Train acc: 0.8363514957264957\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.46103038814240127, Train acc: 0.8357614607614607\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.45948319452313274, Train acc: 0.8364271723646723\n",
      "Val loss: 0.421347439289093, Val acc: 0.836\n",
      "Epoch 29/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4826531710787716, Train acc: 0.8298611111111112\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4633801578838601, Train acc: 0.8389423076923077\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.46497301884696013, Train acc: 0.8385861823361823\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.46802108359133077, Train acc: 0.8366720085470085\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.46000601397110863, Train acc: 0.8390491452991453\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.45438786368933837, Train acc: 0.8402332621082621\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.45446142385075816, Train acc: 0.8403159340659341\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4544482931940474, Train acc: 0.8403445512820513\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4536390715373321, Train acc: 0.8406932573599241\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.45370527928711, Train acc: 0.8409989316239316\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.45253803330331044, Train acc: 0.8408848096348096\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4518032481846137, Train acc: 0.8408564814814815\n",
      "Val loss: 0.4011877477169037, Val acc: 0.84\n",
      "Epoch 30/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4429035125634609, Train acc: 0.8416132478632479\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4488962065969777, Train acc: 0.8397435897435898\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4503910922766411, Train acc: 0.8406339031339032\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.45037871296716553, Train acc: 0.8386084401709402\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4477600343971171, Train acc: 0.840491452991453\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.44832918069131694, Train acc: 0.8410345441595442\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4458903796397723, Train acc: 0.8421092796092796\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44308940357823146, Train acc: 0.8431490384615384\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4433744086301112, Train acc: 0.8432751661918328\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.44396241505940753, Train acc: 0.8434561965811965\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.44396813632956567, Train acc: 0.8429244366744367\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4427754329862418, Train acc: 0.8433048433048433\n",
      "Val loss: 0.392691045999527, Val acc: 0.844\n",
      "Epoch 31/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42192411473673636, Train acc: 0.8466880341880342\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4236612469276302, Train acc: 0.8489583333333334\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4284138005980399, Train acc: 0.8475783475783476\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4312987596146826, Train acc: 0.8456196581196581\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.43750613192971954, Train acc: 0.8430555555555556\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4398684464395046, Train acc: 0.8429042022792023\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.43852336145255155, Train acc: 0.8431776556776557\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.43992762562906385, Train acc: 0.8436832264957265\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.44041255489345515, Train acc: 0.8436312915479582\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.439832703036885, Train acc: 0.8439369658119659\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4397091251025852, Train acc: 0.843482905982906\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.43901407573786044, Train acc: 0.8441951566951567\n",
      "Val loss: 0.39972540736198425, Val acc: 0.852\n",
      "Epoch 32/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.442479255489814, Train acc: 0.8349358974358975\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.43597722028055763, Train acc: 0.8424145299145299\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4326336650002716, Train acc: 0.8457977207977208\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4309601983071392, Train acc: 0.8470886752136753\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.432893650755923, Train acc: 0.8460470085470085\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.43015348754207633, Train acc: 0.8475338319088319\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4284070723841333, Train acc: 0.8480616605616605\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.43144927887070894, Train acc: 0.847122061965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.433783331417177, Train acc: 0.8463912630579298\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.43213608471246867, Train acc: 0.8470352564102565\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.43136080415237754, Train acc: 0.8477078477078477\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.43112843205616347, Train acc: 0.8478454415954416\n",
      "Val loss: 0.3900112807750702, Val acc: 0.842\n",
      "Epoch 33/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4117585051263499, Train acc: 0.8525641025641025\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.41690630281073415, Train acc: 0.8514957264957265\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4120824992486894, Train acc: 0.8533653846153846\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4165221896882241, Train acc: 0.8522302350427351\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.41758696363013015, Train acc: 0.8535790598290598\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.41865895079326765, Train acc: 0.8518073361823362\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42029055987595054, Train acc: 0.8512667887667887\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4218524508815036, Train acc: 0.8509281517094017\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.42327393414514586, Train acc: 0.8498338081671415\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4232600290933226, Train acc: 0.8499465811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4244765747561414, Train acc: 0.849237567987568\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4235718236158886, Train acc: 0.8491141381766382\n",
      "Val loss: 0.3903023600578308, Val acc: 0.852\n",
      "Epoch 34/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.41228682172094655, Train acc: 0.8541666666666666\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4248868639015744, Train acc: 0.8529647435897436\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42925579871377373, Train acc: 0.8500712250712251\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4279316960810087, Train acc: 0.8498931623931624\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4312051434929554, Train acc: 0.8495726495726496\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4288875739225465, Train acc: 0.8492254273504274\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42487730059467976, Train acc: 0.8503891941391941\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.42522969500472146, Train acc: 0.8506610576923077\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.42263869351839184, Train acc: 0.850931861348528\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4244421431205721, Train acc: 0.8508547008547008\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.42357894569264887, Train acc: 0.8512043512043512\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4228365259812853, Train acc: 0.8513176638176638\n",
      "Val loss: 0.3783169686794281, Val acc: 0.856\n",
      "Epoch 35/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.39822936134460646, Train acc: 0.8584401709401709\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.40770730102418834, Train acc: 0.8549679487179487\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4124335128697235, Train acc: 0.854522792022792\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.41405028375422853, Train acc: 0.8536992521367521\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.41402779922016664, Train acc: 0.8540064102564102\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4148464812300144, Train acc: 0.8533653846153846\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.41412453848290937, Train acc: 0.852754884004884\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4130669412895655, Train acc: 0.8532318376068376\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4088032493564138, Train acc: 0.8552943969610636\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.40999295638921934, Train acc: 0.8543803418803418\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4121843807995551, Train acc: 0.8537781662781663\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4130304725336553, Train acc: 0.8536102207977208\n",
      "Val loss: 0.37129032611846924, Val acc: 0.858\n",
      "Epoch 36/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4279046435641427, Train acc: 0.8477564102564102\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.421166985685754, Train acc: 0.8521634615384616\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.417820159250345, Train acc: 0.8512286324786325\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4157228077419548, Train acc: 0.8526308760683761\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4149200038140656, Train acc: 0.8518696581196581\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.41456118303528877, Train acc: 0.8515402421652422\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4104218249901747, Train acc: 0.8539377289377289\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4142969351612095, Train acc: 0.8521968482905983\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4128116712695853, Train acc: 0.852326685660019\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.41133116100333694, Train acc: 0.8530715811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.40962101039769766, Train acc: 0.8543123543123543\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4095553364020977, Train acc: 0.8545450498575499\n",
      "Val loss: 0.37796759605407715, Val acc: 0.852\n",
      "Epoch 37/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3921263341465567, Train acc: 0.8624465811965812\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3952098439773943, Train acc: 0.8616452991452992\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.39967426925133437, Train acc: 0.8590633903133903\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4020752740275656, Train acc: 0.8582398504273504\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.40056222097741234, Train acc: 0.8587072649572649\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.40208561394756337, Train acc: 0.8577279202279202\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.40358022720494985, Train acc: 0.8568376068376068\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.40148775059029335, Train acc: 0.8578392094017094\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.40331640994531814, Train acc: 0.8567782526115859\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.40442670785591134, Train acc: 0.8567040598290598\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4053283809585079, Train acc: 0.855963480963481\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4021757032531194, Train acc: 0.8568376068376068\n",
      "Val loss: 0.3785346746444702, Val acc: 0.85\n",
      "Epoch 38/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.39951762926374745, Train acc: 0.8565705128205128\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3967669069384917, Train acc: 0.8573717948717948\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4030181996503107, Train acc: 0.8571937321937322\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4012162048910928, Train acc: 0.8561030982905983\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.397689886429371, Train acc: 0.8575320512820512\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4007278605021982, Train acc: 0.8571937321937322\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4031260715881692, Train acc: 0.8572191697191697\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4035851406490701, Train acc: 0.8566038995726496\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.40372912106011327, Train acc: 0.8566595441595442\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.40143074477967033, Train acc: 0.8572649572649572\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4003628010006959, Train acc: 0.8574689199689199\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.402474934887937, Train acc: 0.8564814814814815\n",
      "Val loss: 0.3605175316333771, Val acc: 0.844\n",
      "Epoch 39/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.402805604868465, Train acc: 0.8514957264957265\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3989481327370701, Train acc: 0.8525641025641025\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.39125372044890694, Train acc: 0.8594195156695157\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3937918403082424, Train acc: 0.8581730769230769\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3918210140150836, Train acc: 0.8587606837606837\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3915116599949337, Train acc: 0.8595085470085471\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.391863972348899, Train acc: 0.8594703907203908\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3923299563013845, Train acc: 0.8595419337606838\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3915538619160086, Train acc: 0.8603988603988604\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3943942476159487, Train acc: 0.8595085470085471\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.39426256757354033, Train acc: 0.8594357031857032\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.394410244977245, Train acc: 0.8595085470085471\n",
      "Val loss: 0.38573595881462097, Val acc: 0.856\n",
      "Epoch 40/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.39769212634135515, Train acc: 0.8592414529914529\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3882090426408328, Train acc: 0.8627136752136753\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.39302474277311583, Train acc: 0.8603988603988604\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.38978415683039236, Train acc: 0.8608440170940171\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3919843175472357, Train acc: 0.8611111111111112\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3945856658672845, Train acc: 0.8609775641025641\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.39205333665104286, Train acc: 0.8619123931623932\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3891295737180954, Train acc: 0.8628806089743589\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3887144184520102, Train acc: 0.8628620607787274\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.39011295217479397, Train acc: 0.8623130341880342\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.39349452776091914, Train acc: 0.8611353923853924\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3938071563776232, Train acc: 0.8606882122507122\n",
      "Val loss: 0.3765500485897064, Val acc: 0.858\n",
      "Epoch 41/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3936324018953193, Train acc: 0.8568376068376068\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3895440925008211, Train acc: 0.8600427350427351\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3844180576651864, Train acc: 0.8623575498575499\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.38677808550051135, Train acc: 0.8631143162393162\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3886826088413214, Train acc: 0.8628205128205129\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3897283548367159, Train acc: 0.8634259259259259\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3877882637329154, Train acc: 0.8642017704517705\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3876253796709526, Train acc: 0.8640157585470085\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3876684649024028, Train acc: 0.86369301994302\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.38788817044124646, Train acc: 0.8635149572649573\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3878168212286057, Train acc: 0.8634663947163947\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.38981520938036957, Train acc: 0.8624243233618234\n",
      "Val loss: 0.37249618768692017, Val acc: 0.844\n",
      "Epoch 42/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3891758331630984, Train acc: 0.8592414529914529\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.39114473174270403, Train acc: 0.8617788461538461\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.38678272174634143, Train acc: 0.8622685185185185\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3874382508488802, Train acc: 0.8618456196581197\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.38980163185514954, Train acc: 0.8612713675213676\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.38825343424013875, Train acc: 0.8624910968660968\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.38918084739474296, Train acc: 0.8620650183150184\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.38604679339143455, Train acc: 0.8630475427350427\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3850275265213884, Train acc: 0.86369301994302\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.38704245349026134, Train acc: 0.8635149572649573\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.38845015887428647, Train acc: 0.8631021756021756\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3869330002647689, Train acc: 0.8638043091168092\n",
      "Val loss: 0.3636510968208313, Val acc: 0.854\n",
      "Epoch 43/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.38034119705359143, Train acc: 0.8605769230769231\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3901263692567491, Train acc: 0.8595085470085471\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3889169659879472, Train acc: 0.8603098290598291\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3851001364712277, Train acc: 0.8621794871794872\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3818245385574479, Train acc: 0.8635149572649573\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3821588140883167, Train acc: 0.8640936609686609\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3806207749051052, Train acc: 0.865460927960928\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3792663263555011, Train acc: 0.866653311965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.378947911764595, Train acc: 0.8663936372269706\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3806022420907632, Train acc: 0.8651976495726496\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.38037769450128034, Train acc: 0.8652389277389277\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3813424017917142, Train acc: 0.8650730056980057\n",
      "Val loss: 0.38271212577819824, Val acc: 0.85\n",
      "Early stopping at epoch 43 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 567.2115 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.4795272508238115, Train acc: 0.3034188034188034\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.3392566197957747, Train acc: 0.3859508547008547\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.2456074589677686, Train acc: 0.4373219373219373\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.1793006067602043, Train acc: 0.47716346153846156\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.1274118788221963, Train acc: 0.5072649572649572\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.087924993955172, Train acc: 0.5311164529914529\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.0560010809164782, Train acc: 0.5491452991452992\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0259354704210901, Train acc: 0.5660056089743589\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.003381321120013, Train acc: 0.5789114434947769\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9795085838462553, Train acc: 0.5915331196581196\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9596370824706026, Train acc: 0.6020541958041958\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9416590757943966, Train acc: 0.6118233618233618\n",
      "Val loss: 0.7562901377677917, Val acc: 0.728\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7518585331929035, Train acc: 0.7099358974358975\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7345123880694056, Train acc: 0.7160790598290598\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.731392745618467, Train acc: 0.7185719373219374\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7278019539438761, Train acc: 0.7216212606837606\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7177954686503125, Train acc: 0.725801282051282\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.711424624435922, Train acc: 0.7286324786324786\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7049249901512458, Train acc: 0.7308836996336996\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7025483877549314, Train acc: 0.7322382478632479\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6941403365938978, Train acc: 0.7343601614434948\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6887636756795085, Train acc: 0.7366452991452992\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6838023791085313, Train acc: 0.7388791763791763\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6790972566630087, Train acc: 0.7406071937321937\n",
      "Val loss: 0.6189103722572327, Val acc: 0.792\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6226415089052967, Train acc: 0.7628205128205128\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.619565736152168, Train acc: 0.7629540598290598\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6179167366775012, Train acc: 0.7626424501424501\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.613948213812123, Train acc: 0.7672275641025641\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6075264622767766, Train acc: 0.770673076923077\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6041315490661183, Train acc: 0.7716791310541311\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6003518260478682, Train acc: 0.7738095238095238\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5959201653480021, Train acc: 0.7754407051282052\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5965306461989484, Train acc: 0.7753442545109211\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5977644850810369, Train acc: 0.775534188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5947163018405947, Train acc: 0.7766122766122766\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5922219195851574, Train acc: 0.7776442307692307\n",
      "Val loss: 0.5964736342430115, Val acc: 0.814\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5423776532849695, Train acc: 0.8052884615384616\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5416885124057786, Train acc: 0.8020833333333334\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5296708395508279, Train acc: 0.8048433048433048\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5321300629621897, Train acc: 0.8033520299145299\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5301084427242605, Train acc: 0.8037927350427351\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5312387020329804, Train acc: 0.8035078347578347\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5326468696560761, Train acc: 0.8024648962148963\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5302270445017478, Train acc: 0.8032184829059829\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.530606444206899, Train acc: 0.8035968660968661\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5308256859198595, Train acc: 0.8037927350427351\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5300270092051994, Train acc: 0.8040743978243978\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.528211435287172, Train acc: 0.8048878205128205\n",
      "Val loss: 0.557175874710083, Val acc: 0.824\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.47407468758587146, Train acc: 0.8298611111111112\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4789449549001506, Train acc: 0.827323717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4936597785803667, Train acc: 0.8246082621082621\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.489209142441933, Train acc: 0.8253205128205128\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.48848633205788766, Train acc: 0.8259081196581196\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.49283373041709944, Train acc: 0.8241631054131054\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.49040960927149313, Train acc: 0.8242139804639804\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4913945171631809, Train acc: 0.8228165064102564\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4920921836681629, Train acc: 0.8233618233618234\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.49270445755404285, Train acc: 0.8232638888888889\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.49173766994601364, Train acc: 0.8231837606837606\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.49149630665566846, Train acc: 0.8234731125356125\n",
      "Val loss: 0.542831301689148, Val acc: 0.842\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.46599508388939065, Train acc: 0.8354700854700855\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.47435208531016976, Train acc: 0.8321314102564102\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.47420582353559315, Train acc: 0.8309294871794872\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4765347154158303, Train acc: 0.8291266025641025\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.471496692006914, Train acc: 0.8297542735042736\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4688721237507811, Train acc: 0.8295495014245015\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.467793122525253, Train acc: 0.829441391941392\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.46844376729498816, Train acc: 0.8301615918803419\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4641941950744034, Train acc: 0.8323836657169991\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.462866183816113, Train acc: 0.832184829059829\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4600219480351783, Train acc: 0.8339403651903652\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.45965499404892113, Train acc: 0.8340678418803419\n",
      "Val loss: 0.5001564621925354, Val acc: 0.846\n",
      "Epoch 7/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.42143980216266763, Train acc: 0.8472222222222222\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.42055180560574573, Train acc: 0.8490918803418803\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.418520197866649, Train acc: 0.8502492877492878\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4286482614966539, Train acc: 0.8467548076923077\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4333203712844441, Train acc: 0.8447649572649573\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4354085625279323, Train acc: 0.8450409544159544\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4320550986880639, Train acc: 0.8469551282051282\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.43065673708278907, Train acc: 0.8472556089743589\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4302472271739814, Train acc: 0.8479938271604939\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.42839102923997446, Train acc: 0.8484508547008547\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.427947286814567, Train acc: 0.8481449106449106\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.42765248116328675, Train acc: 0.848602207977208\n",
      "Val loss: 0.4992166757583618, Val acc: 0.826\n",
      "Epoch 8/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.37978182309585756, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.38495747746629083, Train acc: 0.8644497863247863\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3986574488126824, Train acc: 0.8580840455840456\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.39137584019579696, Train acc: 0.8616452991452992\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.39121857854163544, Train acc: 0.8604700854700855\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3963204207233129, Train acc: 0.8595530626780626\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3994885780353904, Train acc: 0.8586309523809523\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4000572739288402, Train acc: 0.8587740384615384\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3999825356614839, Train acc: 0.8587962962962963\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.40149549400258777, Train acc: 0.8580662393162393\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4027566405124644, Train acc: 0.8575417637917638\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.40207526746054745, Train acc: 0.8578614672364673\n",
      "Val loss: 0.5141980051994324, Val acc: 0.816\n",
      "Epoch 9/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.39985753748661435, Train acc: 0.8584401709401709\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.39450405289729434, Train acc: 0.8635149572649573\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.40131564990237906, Train acc: 0.8612001424501424\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4130211154428812, Train acc: 0.8567040598290598\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.40830060328938006, Train acc: 0.8574786324786324\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.40667254787649526, Train acc: 0.8571492165242165\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4048964173256696, Train acc: 0.8574481074481074\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.39901164575265, Train acc: 0.8588741987179487\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3954289285333408, Train acc: 0.8600724121557455\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.39678078351749313, Train acc: 0.8597756410256411\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3962450557342776, Train acc: 0.8597270784770785\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.39534085633450766, Train acc: 0.8605991809116809\n",
      "Val loss: 0.5190590620040894, Val acc: 0.854\n",
      "Epoch 10/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3746249873159278, Train acc: 0.8643162393162394\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.37159230429519957, Train acc: 0.8653846153846154\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3716161599398678, Train acc: 0.8655626780626781\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3670845413660137, Train acc: 0.8679220085470085\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3643718272065505, Train acc: 0.869017094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.36833976556220627, Train acc: 0.8676103988603988\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.37132027473241563, Train acc: 0.867979242979243\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.37055304695844143, Train acc: 0.8686231303418803\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.36777739121392017, Train acc: 0.8696581196581197\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.36571281930575006, Train acc: 0.8706997863247863\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.36564597118683534, Train acc: 0.8703622766122766\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.36698805128470957, Train acc: 0.8701700498575499\n",
      "Val loss: 0.5176345109939575, Val acc: 0.838\n",
      "Epoch 11/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3421423819202643, Train acc: 0.8790064102564102\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.350248768989347, Train acc: 0.8756677350427351\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.35378277615943865, Train acc: 0.8756232193732194\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3503928499726149, Train acc: 0.8779380341880342\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.35150753626456627, Train acc: 0.8777777777777778\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3487829165559718, Train acc: 0.8782496438746439\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.34534668986104494, Train acc: 0.8791971916971917\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3461760740774946, Train acc: 0.8790064102564102\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3498597778981341, Train acc: 0.8778786799620133\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.35181988200698144, Train acc: 0.8768696581196581\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.35159311273367533, Train acc: 0.8772581585081585\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.351766267748937, Train acc: 0.877426103988604\n",
      "Val loss: 0.5192766785621643, Val acc: 0.838\n",
      "Epoch 12/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.32237337007481826, Train acc: 0.8867521367521367\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3316030023762813, Train acc: 0.8836805555555556\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3356518503225427, Train acc: 0.8809650997150997\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.33671305341343594, Train acc: 0.8816105769230769\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3327341131420217, Train acc: 0.8831730769230769\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3323141580123847, Train acc: 0.8831463675213675\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3313944692995231, Train acc: 0.8835088522588522\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3322657843510437, Train acc: 0.8835470085470085\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3325125303929574, Train acc: 0.8840515194681862\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3333186536772638, Train acc: 0.8837873931623932\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.33366820889162574, Train acc: 0.8837898212898213\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3354249034910078, Train acc: 0.8830350783475783\n",
      "Val loss: 0.5042341947555542, Val acc: 0.838\n",
      "Early stopping at epoch 12 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 162.6732 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.597372921104105, Train acc: 0.24252136752136752\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.5466811116944013, Train acc: 0.28979700854700857\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.4586931025540386, Train acc: 0.3423254985754986\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.3751992978091934, Train acc: 0.38648504273504275\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.3120117087649483, Train acc: 0.4183226495726496\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.2620377707005905, Train acc: 0.4437767094017094\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.2238983118199311, Train acc: 0.46287393162393164\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.1854050097684574, Train acc: 0.4830729166666667\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.149999017332807, Train acc: 0.5025819088319088\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.1214381304306862, Train acc: 0.517948717948718\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.095570312027053, Train acc: 0.5315413752913752\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.070894924069402, Train acc: 0.5443376068376068\n",
      "Val loss: 0.7523842453956604, Val acc: 0.704\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7726096345318688, Train acc: 0.6944444444444444\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7606163533070148, Train acc: 0.7015224358974359\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.757501670692721, Train acc: 0.7037037037037037\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7472005032448688, Train acc: 0.7081997863247863\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7485797479111924, Train acc: 0.7092948717948718\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7511608280092902, Train acc: 0.70877849002849\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7431451251073052, Train acc: 0.7128357753357754\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7364631935826733, Train acc: 0.7164463141025641\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.73017950532896, Train acc: 0.7191358024691358\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7254094799359639, Train acc: 0.7210202991452992\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7209504063775952, Train acc: 0.7228049728049728\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.714614040236867, Train acc: 0.7254496082621082\n",
      "Val loss: 0.6525262594223022, Val acc: 0.796\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6435033289285806, Train acc: 0.7524038461538461\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6448466413550906, Train acc: 0.7489316239316239\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6545104372535336, Train acc: 0.7483974358974359\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6472122624007046, Train acc: 0.7528712606837606\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6396258012351826, Train acc: 0.7574786324786325\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.637257304371592, Train acc: 0.75997150997151\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6405050978337452, Train acc: 0.7601495726495726\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6334818843274544, Train acc: 0.7620526175213675\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.632118761992296, Train acc: 0.7623160018993352\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.625427345918794, Train acc: 0.7643162393162393\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6210919028580791, Train acc: 0.7665841103341103\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6151482574323304, Train acc: 0.7690972222222222\n",
      "Val loss: 0.6360231637954712, Val acc: 0.81\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5671338193182253, Train acc: 0.7919337606837606\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.558099758803335, Train acc: 0.7935363247863247\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5577150945819681, Train acc: 0.7923789173789174\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5578243132585134, Train acc: 0.7934695512820513\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5583751023070425, Train acc: 0.7927884615384615\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5569115593137904, Train acc: 0.7929576210826211\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5547954719581883, Train acc: 0.7944902319902319\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5515866932801456, Train acc: 0.7958733974358975\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5528602477690207, Train acc: 0.7950201804368471\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5517758933015359, Train acc: 0.7961271367521368\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5489948595575149, Train acc: 0.7973484848484849\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5459284356283156, Train acc: 0.7985220797720798\n",
      "Val loss: 0.7011087536811829, Val acc: 0.762\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5109068663456501, Train acc: 0.8149038461538461\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.516161881132513, Train acc: 0.8125\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.513878911606267, Train acc: 0.8140135327635327\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.512430306778759, Train acc: 0.8129674145299145\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5136514136679152, Train acc: 0.8129273504273504\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5115036531391307, Train acc: 0.8141915954415955\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5156517197288145, Train acc: 0.8121947496947497\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5155823612983665, Train acc: 0.8123330662393162\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5112126646501499, Train acc: 0.8141025641025641\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5067564049528704, Train acc: 0.8161057692307693\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5063342088088104, Train acc: 0.8163607226107226\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.503742472610922, Train acc: 0.8177528490028491\n",
      "Val loss: 0.6155770421028137, Val acc: 0.798\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5077352149364276, Train acc: 0.8197115384615384\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4968862322787953, Train acc: 0.8247863247863247\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.49666505602010635, Train acc: 0.8230947293447294\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4901759715225452, Train acc: 0.8258547008547008\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.48301060622573916, Train acc: 0.8300213675213676\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4805195976016868, Train acc: 0.8300836894586895\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4775156796669931, Train acc: 0.8307005494505495\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.47227934575997865, Train acc: 0.832298344017094\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.47024525672282247, Train acc: 0.8327694681861348\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4687043358754908, Train acc: 0.8335470085470086\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.46637919144567447, Train acc: 0.834814491064491\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.467812563120215, Train acc: 0.8341123575498576\n",
      "Val loss: 0.5659588575363159, Val acc: 0.83\n",
      "Epoch 7/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.44849230030662995, Train acc: 0.8458867521367521\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4415434642862051, Train acc: 0.8456196581196581\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4391755182773639, Train acc: 0.844284188034188\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4392930649730385, Train acc: 0.8440838675213675\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.44090926573317274, Train acc: 0.8435897435897436\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.43977848824612437, Train acc: 0.8434383903133903\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4399773612782195, Train acc: 0.8441315628815629\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44149283736816836, Train acc: 0.8438835470085471\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.43839804907413404, Train acc: 0.845411918328585\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4387333220268926, Train acc: 0.8450854700854701\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.43801681458857417, Train acc: 0.8451825951825952\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4368031575447983, Train acc: 0.8463986823361823\n",
      "Val loss: 0.5706961750984192, Val acc: 0.836\n",
      "Epoch 8/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4123072042169734, Train acc: 0.8568376068376068\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.41722887703496164, Train acc: 0.8544337606837606\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4195830881425798, Train acc: 0.8515847578347578\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.41512042137547434, Train acc: 0.8529647435897436\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4162079699146442, Train acc: 0.8527243589743589\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4169303578168069, Train acc: 0.8524750712250713\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4141437532846453, Train acc: 0.8536706349206349\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.41393661820608324, Train acc: 0.8536992521367521\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.41305800751629945, Train acc: 0.8539589268755935\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.41343658493879515, Train acc: 0.853659188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.41255501511259973, Train acc: 0.8542152292152292\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.41243869850035236, Train acc: 0.8546563390313391\n",
      "Val loss: 0.5379007458686829, Val acc: 0.852\n",
      "Epoch 9/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.40736621516382593, Train acc: 0.8530982905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4054501068770376, Train acc: 0.8580395299145299\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3976694932605466, Train acc: 0.8615562678062678\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.39699143043950075, Train acc: 0.8625801282051282\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3943282654015427, Train acc: 0.8630341880341881\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.39487552300010653, Train acc: 0.8634259259259259\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3963719986570187, Train acc: 0.8625992063492064\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3950890414814791, Train acc: 0.8624131944444444\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.39464925535921813, Train acc: 0.8620607787274454\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3967546817392875, Train acc: 0.8616452991452992\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3988131809264723, Train acc: 0.8608925796425796\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3985984926602864, Train acc: 0.8611556267806267\n",
      "Val loss: 0.5494431257247925, Val acc: 0.84\n",
      "Epoch 10/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.34928602502386796, Train acc: 0.8814102564102564\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.35781462729359287, Train acc: 0.8768696581196581\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3615221136231028, Train acc: 0.8756232193732194\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3653143077579319, Train acc: 0.8739983974358975\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.36803094268354597, Train acc: 0.8728098290598291\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.37100696958537793, Train acc: 0.8715277777777778\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3703153169007747, Train acc: 0.8716804029304029\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.37040910144280803, Train acc: 0.8710603632478633\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.37177579659979226, Train acc: 0.871141975308642\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3723785837013752, Train acc: 0.8707532051282051\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3736824345830669, Train acc: 0.8708236208236209\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.37746215112072196, Train acc: 0.8697248931623932\n",
      "Val loss: 0.5866851806640625, Val acc: 0.802\n",
      "Epoch 11/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3701105191666856, Train acc: 0.8693910256410257\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3666782872034953, Train acc: 0.8713942307692307\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.36539468022747934, Train acc: 0.8723290598290598\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.36565272618308026, Train acc: 0.8723290598290598\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.362978818337632, Train acc: 0.8730235042735043\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3632272083386567, Train acc: 0.8732638888888888\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3640902367976559, Train acc: 0.8726724664224664\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3626504551587451, Train acc: 0.8739650106837606\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3632689584170425, Train acc: 0.873872269705603\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.36417626755105126, Train acc: 0.8733707264957264\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3656486027064301, Train acc: 0.8729360916860917\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.36513474340314456, Train acc: 0.8729077635327636\n",
      "Val loss: 0.5149661302566528, Val acc: 0.84\n",
      "Epoch 12/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3345426129352333, Train acc: 0.8843482905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3451718207862642, Train acc: 0.8812767094017094\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3449836138349313, Train acc: 0.8805199430199431\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.34324535122539246, Train acc: 0.882545405982906\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3419404606279145, Train acc: 0.8840277777777777\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.34298441497518806, Train acc: 0.8831463675213675\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.34167735255863496, Train acc: 0.8830509768009768\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3432087590121943, Train acc: 0.8821113782051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3418422333775875, Train acc: 0.8829237891737892\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3419441788815535, Train acc: 0.8828258547008547\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.34256950206620435, Train acc: 0.8825514763014763\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3444470287197166, Train acc: 0.8816550925925926\n",
      "Val loss: 0.48081472516059875, Val acc: 0.838\n",
      "Epoch 13/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3280942755886632, Train acc: 0.8875534188034188\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3258765042464957, Train acc: 0.8882211538461539\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.33326360946282363, Train acc: 0.8845263532763533\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3368133815467103, Train acc: 0.8839476495726496\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.34081911960473427, Train acc: 0.882051282051282\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.34086047065181607, Train acc: 0.8818108974358975\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.34047116689447665, Train acc: 0.8822115384615384\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3403884303381937, Train acc: 0.8826121794871795\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.33856167781528473, Train acc: 0.8833986229819563\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3375965151649255, Train acc: 0.883573717948718\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.33763005471590796, Train acc: 0.883401320901321\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3385653832975106, Train acc: 0.8832799145299145\n",
      "Val loss: 0.48446035385131836, Val acc: 0.824\n",
      "Epoch 14/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3274043462215326, Train acc: 0.8848824786324786\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3255683665092175, Train acc: 0.8895566239316239\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3217048641623255, Train acc: 0.8888888888888888\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3260987815248151, Train acc: 0.8879540598290598\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3224796510468691, Train acc: 0.8899038461538461\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3210749074424009, Train acc: 0.8897346866096866\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.32109792037925006, Train acc: 0.8896901709401709\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.31951997303762114, Train acc: 0.8898904914529915\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.31843216363818216, Train acc: 0.8904320987654321\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.31805350284100087, Train acc: 0.8909722222222223\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3211625371368569, Train acc: 0.8895930458430459\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.32394023990242654, Train acc: 0.8884882478632479\n",
      "Val loss: 0.4927586615085602, Val acc: 0.854\n",
      "Epoch 15/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.30220687835135013, Train acc: 0.8998397435897436\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.29736599310213685, Train acc: 0.8994391025641025\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.30727767449753235, Train acc: 0.8941417378917379\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3061010658613637, Train acc: 0.8958333333333334\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3082591413305356, Train acc: 0.8942841880341881\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3080356283872216, Train acc: 0.8943643162393162\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.30644694523333194, Train acc: 0.8945741758241759\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.30785047775930446, Train acc: 0.8944644764957265\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3075349199190269, Train acc: 0.894616571699905\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.30864687179907774, Train acc: 0.8943108974358974\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3077874016928506, Train acc: 0.8949349261849262\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.30912391415019747, Train acc: 0.8948094729344729\n",
      "Val loss: 0.4425422251224518, Val acc: 0.848\n",
      "Epoch 16/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2987413784632316, Train acc: 0.8947649572649573\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2958863557149202, Train acc: 0.8966346153846154\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.29242205004236976, Train acc: 0.8976139601139601\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2978215955643572, Train acc: 0.8967681623931624\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2979496201643577, Train acc: 0.8975427350427351\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.30310484376388397, Train acc: 0.8953881766381766\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.30258059363371, Train acc: 0.8948794261294262\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.29910394320917183, Train acc: 0.8965344551282052\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.29847081345200877, Train acc: 0.8970797720797721\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.29745106032898283, Train acc: 0.897409188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.29673642104183323, Train acc: 0.8974116161616161\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.29756084848672915, Train acc: 0.8976584757834758\n",
      "Val loss: 0.47224313020706177, Val acc: 0.832\n",
      "Epoch 17/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.28680486432634866, Train acc: 0.9014423076923077\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.27952626186749363, Train acc: 0.9042467948717948\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.28474872483954133, Train acc: 0.9002849002849003\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2867192040460232, Train acc: 0.9005742521367521\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.28546235688731203, Train acc: 0.9012286324786325\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2876222261421361, Train acc: 0.9001068376068376\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2878810551213112, Train acc: 0.9004120879120879\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.28735364448581624, Train acc: 0.9010082799145299\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2885244085726745, Train acc: 0.9011455365622032\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2875890897252621, Train acc: 0.9018429487179487\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.28669139338192684, Train acc: 0.9019036519036518\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2853994424961656, Train acc: 0.9023993945868946\n",
      "Val loss: 0.5310543179512024, Val acc: 0.832\n",
      "Epoch 18/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2761043023604613, Train acc: 0.9051816239316239\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.28475338160100144, Train acc: 0.9013087606837606\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.29147710292427625, Train acc: 0.8999287749287749\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.28304236976063657, Train acc: 0.9022435897435898\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2770219492288227, Train acc: 0.9053418803418803\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.27538869938577004, Train acc: 0.9059383903133903\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2728558582718847, Train acc: 0.9068223443223443\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.272396613469618, Train acc: 0.9064503205128205\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.27409869186926883, Train acc: 0.9059235517568851\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.27101677242570965, Train acc: 0.9070779914529915\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2704690760211395, Train acc: 0.9076097513597513\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2735355247032332, Train acc: 0.9064725783475783\n",
      "Val loss: 0.5022318959236145, Val acc: 0.854\n",
      "Epoch 19/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2650840823721682, Train acc: 0.9089209401709402\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.26638653051331, Train acc: 0.9083867521367521\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.2708467204240482, Train acc: 0.906517094017094\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.27465876480007273, Train acc: 0.9047142094017094\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.26851044215070896, Train acc: 0.906784188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.26721692604790215, Train acc: 0.9069622507122507\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2672150695471318, Train acc: 0.9076617826617827\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.26455434230275643, Train acc: 0.9088541666666666\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2634505547462818, Train acc: 0.9090099715099715\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2632309175199933, Train acc: 0.909107905982906\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.26545081291964284, Train acc: 0.9084353146853147\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2660680017009526, Train acc: 0.9087206196581197\n",
      "Val loss: 0.48932644724845886, Val acc: 0.844\n",
      "Epoch 20/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.27600212681751984, Train acc: 0.9099893162393162\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2742821476621251, Train acc: 0.9097222222222222\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.27393580072073853, Train acc: 0.9082977207977208\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.27311739076374686, Train acc: 0.9079861111111112\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2695100573456695, Train acc: 0.9099358974358974\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.265033122836652, Train acc: 0.9109241452991453\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.26292571701991835, Train acc: 0.9113629426129426\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2598029846389197, Train acc: 0.9123597756410257\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.26259625718662655, Train acc: 0.9108796296296297\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.26455805111899333, Train acc: 0.9100160256410257\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2675178454987699, Train acc: 0.9088723776223776\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2667695809781891, Train acc: 0.9094551282051282\n",
      "Val loss: 0.468374639749527, Val acc: 0.838\n",
      "Early stopping at epoch 20 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 271.9739 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.5384188839513013, Train acc: 0.29246794871794873\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.4494470638087673, Train acc: 0.3645833333333333\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.361128442817264, Train acc: 0.41978276353276356\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.2799105915503624, Train acc: 0.45913461538461536\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.2227581679311572, Train acc: 0.4873931623931624\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.1764892480651876, Train acc: 0.5089031339031339\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.1343013938238915, Train acc: 0.5309447496947497\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0981817970800603, Train acc: 0.5490451388888888\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0663477853718872, Train acc: 0.564963200379867\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.037811468503414, Train acc: 0.5779380341880341\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.0110722434270631, Train acc: 0.5902534965034965\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9887087259791855, Train acc: 0.6013621794871795\n",
      "Val loss: 0.6440094113349915, Val acc: 0.764\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7232236686425332, Train acc: 0.7337072649572649\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6968986655657108, Train acc: 0.7410523504273504\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6963964792398306, Train acc: 0.739761396011396\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6901142159715677, Train acc: 0.7404513888888888\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6849014441681723, Train acc: 0.7438568376068376\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6813450599158252, Train acc: 0.7474180911680912\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6796104546286102, Train acc: 0.7473290598290598\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6780932894629291, Train acc: 0.7475627670940171\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6718355762256629, Train acc: 0.7498219373219374\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6659873086417842, Train acc: 0.7530448717948718\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6646987226644782, Train acc: 0.7534722222222222\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6589024046122858, Train acc: 0.7550525284900285\n",
      "Val loss: 0.5640448927879333, Val acc: 0.788\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6144287741591787, Train acc: 0.7780448717948718\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6023542870823134, Train acc: 0.780448717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6002382087571668, Train acc: 0.7798254985754985\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5943110068448079, Train acc: 0.782051282051282\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5819260560803943, Train acc: 0.7878205128205128\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5791490602484796, Train acc: 0.7893073361823362\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5760238950978851, Train acc: 0.790254884004884\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5727759342258557, Train acc: 0.7923344017094017\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5686537657226706, Train acc: 0.7931801994301995\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5677305433867325, Train acc: 0.7931089743589743\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5668884921347159, Train acc: 0.7935848873348873\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5630113616522051, Train acc: 0.7948495370370371\n",
      "Val loss: 0.5514318346977234, Val acc: 0.816\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5276471001979632, Train acc: 0.8071581196581197\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5200830859124151, Train acc: 0.8099626068376068\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5208804487672627, Train acc: 0.8112535612535613\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5204066096399075, Train acc: 0.8114316239316239\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5163022908390078, Train acc: 0.812767094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5173216329256014, Train acc: 0.8123664529914529\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5137939373251865, Train acc: 0.8132631257631258\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5167714205658079, Train acc: 0.8121995192307693\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5146703300794192, Train acc: 0.813301282051282\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5136726355960226, Train acc: 0.8141559829059829\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5131903358828493, Train acc: 0.8138840326340326\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.512928953940882, Train acc: 0.8140580484330484\n",
      "Val loss: 0.5541929602622986, Val acc: 0.81\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.46404710195512855, Train acc: 0.8287927350427351\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.461771884917194, Train acc: 0.8270566239316239\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4564457388005705, Train acc: 0.8305733618233618\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4629596057865355, Train acc: 0.827323717948718\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4659563021272676, Train acc: 0.8284722222222223\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.46918608165449566, Train acc: 0.8276798433048433\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.47255268367291664, Train acc: 0.8278006715506715\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4714359751321439, Train acc: 0.8280248397435898\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4708984859662744, Train acc: 0.8287630579297246\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.47079001318058394, Train acc: 0.8286858974358975\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4705940958839011, Train acc: 0.8285499222999223\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4724258066040923, Train acc: 0.8275240384615384\n",
      "Val loss: 0.5657393336296082, Val acc: 0.826\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4487944961103619, Train acc: 0.8341346153846154\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4530590794280044, Train acc: 0.8342681623931624\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.45864237394937424, Train acc: 0.8345797720797721\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.45417750171489185, Train acc: 0.8364049145299145\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.45383814117337906, Train acc: 0.8368589743589744\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4514490158809216, Train acc: 0.8376958689458689\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.45218349962298543, Train acc: 0.8376831501831502\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.45167156955243176, Train acc: 0.8383413461538461\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4512497629167234, Train acc: 0.8387048907882241\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.45042139140204485, Train acc: 0.8385416666666666\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.44851050302444173, Train acc: 0.8395250582750583\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4468538134472917, Train acc: 0.8396100427350427\n",
      "Val loss: 0.4983563721179962, Val acc: 0.83\n",
      "Epoch 7/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4300200471765975, Train acc: 0.8461538461538461\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.43082480151683855, Train acc: 0.8481570512820513\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4379693302418771, Train acc: 0.8433048433048433\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4342076578456113, Train acc: 0.8433493589743589\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4338107027559199, Train acc: 0.8439102564102564\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.43669210323411173, Train acc: 0.8431712962962963\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4373167408254994, Train acc: 0.8421474358974359\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4353449180976957, Train acc: 0.8431156517094017\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.43267306787335974, Train acc: 0.8442545109211775\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4302819700195239, Train acc: 0.8452991452991453\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4301771855252421, Train acc: 0.8447698135198135\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.42925418778574703, Train acc: 0.8453748219373219\n",
      "Val loss: 0.502763569355011, Val acc: 0.85\n",
      "Epoch 8/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.40466599472058123, Train acc: 0.8528311965811965\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.40607454729640585, Train acc: 0.8549679487179487\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.40514419017693937, Train acc: 0.8555021367521367\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4085661986699471, Train acc: 0.8526308760683761\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4120070356461737, Train acc: 0.8509081196581196\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.41276944841542135, Train acc: 0.8512731481481481\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.41105898675282504, Train acc: 0.8514957264957265\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.41004166775184053, Train acc: 0.8511284722222222\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4126287271027212, Train acc: 0.8507241215574549\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.41258507430171354, Train acc: 0.8516292735042735\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4157453911044659, Train acc: 0.8503545066045066\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4133282506100812, Train acc: 0.8513844373219374\n",
      "Val loss: 0.5447136759757996, Val acc: 0.826\n",
      "Epoch 9/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.40263367208660156, Train acc: 0.8581730769230769\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4037570477041424, Train acc: 0.8571047008547008\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.40311324248286734, Train acc: 0.8569266381766382\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3978530618433769, Train acc: 0.8583066239316239\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.39626999094954923, Train acc: 0.8596688034188035\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3909449249803171, Train acc: 0.8616452991452992\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.38998423155592765, Train acc: 0.8620650183150184\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.38940995258207506, Train acc: 0.8621794871794872\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.39367754991605863, Train acc: 0.8605472459639126\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3929300609625812, Train acc: 0.8611912393162393\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3912721428073647, Train acc: 0.8622037684537684\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3915730121298733, Train acc: 0.8615785256410257\n",
      "Val loss: 0.4588729441165924, Val acc: 0.856\n",
      "Epoch 10/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.37985624748672175, Train acc: 0.8648504273504274\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3806408067735342, Train acc: 0.8668536324786325\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.37551510350996276, Train acc: 0.8676994301994302\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.37328627017828137, Train acc: 0.8685229700854701\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.37061642289925845, Train acc: 0.869017094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.374153057659878, Train acc: 0.8682781339031339\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.37337416808902124, Train acc: 0.8680937118437119\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.37559139491337484, Train acc: 0.8676549145299145\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.37497025821977076, Train acc: 0.8677291073124407\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3727744301637778, Train acc: 0.8683493589743589\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.37352043021797393, Train acc: 0.8680555555555556\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.37429886939338386, Train acc: 0.8682558760683761\n",
      "Val loss: 0.47714635729789734, Val acc: 0.854\n",
      "Epoch 11/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3505395287886644, Train acc: 0.874732905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3627423670811531, Train acc: 0.874732905982906\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3691524594937295, Train acc: 0.8721509971509972\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.37198815314879274, Train acc: 0.8698584401709402\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.36918619887696374, Train acc: 0.8698183760683761\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3684359419231231, Train acc: 0.8692129629629629\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3664776463520978, Train acc: 0.8693147130647131\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.36621957185526943, Train acc: 0.8693576388888888\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.36528244598327314, Train acc: 0.8693910256410257\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.36594400586098685, Train acc: 0.869284188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3661326405873758, Train acc: 0.8695852758352758\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3657256072795714, Train acc: 0.8699697293447294\n",
      "Val loss: 0.5396865606307983, Val acc: 0.838\n",
      "Epoch 12/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3558940837143833, Train acc: 0.8717948717948718\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3632022047845217, Train acc: 0.8717948717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3675447025063031, Train acc: 0.8688568376068376\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.36406596597188556, Train acc: 0.8698584401709402\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.36038589309423397, Train acc: 0.8716880341880342\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3562713951891304, Train acc: 0.8736200142450142\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.35388168195883435, Train acc: 0.8749236874236874\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3535824277334743, Train acc: 0.875\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3530502868825101, Train acc: 0.8750296771130105\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3527624058545145, Train acc: 0.875\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.35283470835186476, Train acc: 0.8753642191142191\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.35220428495707673, Train acc: 0.8760683760683761\n",
      "Val loss: 0.5278701782226562, Val acc: 0.85\n",
      "Epoch 13/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.33396263051236796, Train acc: 0.8830128205128205\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3410349187687931, Train acc: 0.8816773504273504\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3425337351922296, Train acc: 0.8800747863247863\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3422992699102968, Train acc: 0.8798076923076923\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3378046074738869, Train acc: 0.8805555555555555\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3412240300953728, Train acc: 0.8792735042735043\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3420973820691435, Train acc: 0.8783959096459096\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.34075627096283895, Train acc: 0.878372061965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.34216785063685856, Train acc: 0.8776412630579298\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3455823295837284, Train acc: 0.8766559829059829\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.34621088309235226, Train acc: 0.8763840326340326\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.34531818947454973, Train acc: 0.8769141737891738\n",
      "Val loss: 0.511303722858429, Val acc: 0.842\n",
      "Epoch 14/30\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3361388332186601, Train acc: 0.8814102564102564\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3353658947679732, Train acc: 0.8812767094017094\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3264528150508377, Train acc: 0.885772792022792\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3298541827318378, Train acc: 0.8858173076923077\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3297872590394611, Train acc: 0.8861645299145299\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.330615472032601, Train acc: 0.8856392450142451\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3291750026227718, Train acc: 0.8846916971916972\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3309978325532861, Train acc: 0.8840144230769231\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3308216444298922, Train acc: 0.884110873694207\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.32977905453334, Train acc: 0.8844818376068376\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3309303538001084, Train acc: 0.8840083527583528\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.33022616295000684, Train acc: 0.8843928062678063\n",
      "Val loss: 0.49157723784446716, Val acc: 0.85\n",
      "Early stopping at epoch 14 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 181.1766 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.5761574781858003, Train acc: 0.2657843948166529\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.454570658186562, Train acc: 0.34608491866556385\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.3425459494957557, Train acc: 0.4045124528995497\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.255822952868592, Train acc: 0.4494761510890543\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.1922079977826177, Train acc: 0.48196856906534324\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.1428183863135823, Train acc: 0.5069846521459425\n",
      "Val loss: 0.7331824898719788, Val acc: 0.734\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8253861512893286, Train acc: 0.669699476151089\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8120648532341688, Train acc: 0.6772125723738627\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7955548877899463, Train acc: 0.6857825567502986\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7756564256090385, Train acc: 0.6955472842569617\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7656619223773989, Train acc: 0.7006065618968845\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7551832846511802, Train acc: 0.7049443984927856\n",
      "Val loss: 0.6592035889625549, Val acc: 0.77\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6781312734143347, Train acc: 0.741797628894403\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6815314424088877, Train acc: 0.7366280672732286\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6718080485308612, Train acc: 0.7409245473761603\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6659472204553776, Train acc: 0.7435552798456024\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6603876581558814, Train acc: 0.7457127102288392\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6520998159959446, Train acc: 0.7502297582942744\n",
      "Val loss: 0.6094557046890259, Val acc: 0.792\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.606801419176607, Train acc: 0.7722635787151916\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6002868378264272, Train acc: 0.7726082161566032\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.5913392332544354, Train acc: 0.7767208896241155\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.598841487406156, Train acc: 0.7743314033636615\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5959082604473472, Train acc: 0.7755169561621175\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5902292445684091, Train acc: 0.7771574303832368\n",
      "Val loss: 0.5639054775238037, Val acc: 0.794\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.5288400512475234, Train acc: 0.8085194375516956\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.5427116801341375, Train acc: 0.8029363110008272\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.5480511073882763, Train acc: 0.7986398308978954\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5510465184338073, Train acc: 0.7965605183347119\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5470443826200616, Train acc: 0.798786876206231\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5456386032140154, Train acc: 0.7984330484330484\n",
      "Val loss: 0.568119466304779, Val acc: 0.816\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.5251689127877227, Train acc: 0.8130686517783292\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.5181408053916744, Train acc: 0.811138682106424\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.5200667242463838, Train acc: 0.8104953588824556\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5165505910085307, Train acc: 0.8118968844775296\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5192022187842263, Train acc: 0.8117176730079956\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5140786722005263, Train acc: 0.8134362650491683\n",
      "Val loss: 0.5712431073188782, Val acc: 0.808\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.49039449447240585, Train acc: 0.8192721257237386\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4894646137443363, Train acc: 0.8182382133995038\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.49106806270417325, Train acc: 0.8176638176638177\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4884278916905069, Train acc: 0.8192031982354563\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.4855114721334898, Train acc: 0.8209263854425145\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.48515494148700666, Train acc: 0.8220292252550317\n",
      "Val loss: 0.5653105974197388, Val acc: 0.836\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4638123904537951, Train acc: 0.8351254480286738\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4629036260721011, Train acc: 0.8341604631927213\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4652397164337316, Train acc: 0.8332414300156236\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4670760050479673, Train acc: 0.8325406672180866\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.46325166696666653, Train acc: 0.8336641852770885\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.46324085567326967, Train acc: 0.8332873816744785\n",
      "Val loss: 0.5513762831687927, Val acc: 0.828\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4637763316814716, Train acc: 0.8330576233802041\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.45553203869579184, Train acc: 0.8350565205403915\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.44820195682707675, Train acc: 0.8388015807370646\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.448494153495273, Train acc: 0.8381237937689551\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.45168031117854973, Train acc: 0.8373035566583954\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.45007277042352917, Train acc: 0.8386867015899274\n",
      "Val loss: 0.5456290245056152, Val acc: 0.844\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4266904702043941, Train acc: 0.8407775020678246\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4274268243302647, Train acc: 0.8420181968569065\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4192825573631841, Train acc: 0.8460619428361363\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.43015451980834335, Train acc: 0.8432933553901296\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.4302046659919951, Train acc: 0.8443065894678798\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.43287657867809304, Train acc: 0.8439941181876666\n",
      "Val loss: 0.5228992700576782, Val acc: 0.862\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4251104371669965, Train acc: 0.8496002205679625\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4252721220254898, Train acc: 0.8495312930796802\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4246231458465598, Train acc: 0.8497380755445272\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4220974181388688, Train acc: 0.8502894954507858\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.42480590335833723, Train acc: 0.8496277915632754\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.4267938219838672, Train acc: 0.8484973807554452\n",
      "Val loss: 0.5573389530181885, Val acc: 0.846\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4020555176031895, Train acc: 0.8534601599117728\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4050751377502058, Train acc: 0.8540115798180314\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.41169530221200057, Train acc: 0.8531384982997886\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4083771597052741, Train acc: 0.8547008547008547\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.4109096967256986, Train acc: 0.853791011855528\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.409580539052303, Train acc: 0.8534601599117728\n",
      "Val loss: 0.5803344249725342, Val acc: 0.832\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.40250967697710055, Train acc: 0.8530465949820788\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.3931404050980878, Train acc: 0.8575958092087125\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.3984803709803823, Train acc: 0.8568605826670342\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.39573688117357403, Train acc: 0.8571822442790185\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3990018734565148, Train acc: 0.8572373862696443\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.39986660019454795, Train acc: 0.8576417608675673\n",
      "Val loss: 0.5085312724113464, Val acc: 0.84\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.3754604816691488, Train acc: 0.8650399779432038\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.37314906703610706, Train acc: 0.8682106424041908\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.37370937298505735, Train acc: 0.8673375608859479\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.37729013959566754, Train acc: 0.8659704990350152\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3818244686748227, Train acc: 0.8649848359525779\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.38267754377038393, Train acc: 0.8643736788898079\n",
      "Val loss: 0.5872946977615356, Val acc: 0.8\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.3920945609227205, Train acc: 0.8610421836228288\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.3897766631383162, Train acc: 0.86255858836504\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.3917506488300117, Train acc: 0.8632938149067181\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.3863389923149704, Train acc: 0.8647298042459333\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3785579524743251, Train acc: 0.8671629445822994\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.37632134195915995, Train acc: 0.8676592224979321\n",
      "Val loss: 0.5442167520523071, Val acc: 0.826\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.3562136343401721, Train acc: 0.8701406120760959\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.35665946167248946, Train acc: 0.8701406120760959\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.36306349317572395, Train acc: 0.870094660417241\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.3660165395937924, Train acc: 0.8689343810311553\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3651054287568117, Train acc: 0.8695340501792115\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.36353521116948195, Train acc: 0.8704163220292253\n",
      "Val loss: 0.5142828822135925, Val acc: 0.85\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.3563293781545427, Train acc: 0.8734491315136477\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.3536097504134871, Train acc: 0.8746208988144472\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.3530827513617328, Train acc: 0.8768495542689091\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.35453746361164457, Train acc: 0.8756548111386822\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.35589396348621094, Train acc: 0.8755169561621174\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.3582124844554313, Train acc: 0.8751493428912783\n",
      "Val loss: 0.5491604208946228, Val acc: 0.808\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.35509156747760934, Train acc: 0.8748276812792942\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.34928619243905074, Train acc: 0.8753791011855528\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.34781510385013376, Train acc: 0.8762981343626505\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.34000482849585706, Train acc: 0.880066170388751\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.33740294876261656, Train acc: 0.8821891370278467\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.34131955520974266, Train acc: 0.8806175902950096\n",
      "Val loss: 0.5266703963279724, Val acc: 0.858\n",
      "Early stopping at epoch 18 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 165.7895 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.519749307224893, Train acc: 0.28770333609043286\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.3544977890630054, Train acc: 0.38427074717397297\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.2454081989421464, Train acc: 0.4400790368532304\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.1802035833780582, Train acc: 0.4746002205679625\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.1273943768607246, Train acc: 0.5004962779156328\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.0889939740512447, Train acc: 0.521712158808933\n",
      "Val loss: 0.7529190182685852, Val acc: 0.72\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8252899799591455, Train acc: 0.66735594154949\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8196467219764351, Train acc: 0.6670802315963607\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8049388988065583, Train acc: 0.6741567870600129\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7967709626397516, Train acc: 0.6800385993934381\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7871487772872305, Train acc: 0.685663082437276\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7810406445013491, Train acc: 0.6887234629170113\n",
      "Val loss: 0.6365863084793091, Val acc: 0.768\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7215694489642086, Train acc: 0.7201543975737524\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7074548770219852, Train acc: 0.7291838985387372\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7015402344217327, Train acc: 0.7288852127561805\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6931851919517558, Train acc: 0.7300799558864075\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6906971946231321, Train acc: 0.7328921974083265\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6870529016538224, Train acc: 0.7355022516312839\n",
      "Val loss: 0.6308162808418274, Val acc: 0.784\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6589883828265035, Train acc: 0.7503446374414117\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6671101899228544, Train acc: 0.7441411634960022\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.652427904796057, Train acc: 0.7514934289127837\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6488716308123026, Train acc: 0.7529638819961401\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6407623589038849, Train acc: 0.7568513923352633\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6348752383662765, Train acc: 0.7594430658946788\n",
      "Val loss: 0.6162111163139343, Val acc: 0.78\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6165307997128903, Train acc: 0.7693686242073339\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6022673850385551, Train acc: 0.7782602701957541\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6007151320109679, Train acc: 0.778880617590295\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5971889863284225, Train acc: 0.7783636614281776\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.588094990579491, Train acc: 0.7823821339950372\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5858750049544875, Train acc: 0.7833609043286462\n",
      "Val loss: 0.6154749989509583, Val acc: 0.79\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.5578119199500124, Train acc: 0.7965260545905707\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.5496554111058896, Train acc: 0.7993520816101461\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.5479785357615208, Train acc: 0.7985938792390406\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.545475681941224, Train acc: 0.8002136752136753\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5435312000604776, Train acc: 0.8012958367797077\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5399385655452723, Train acc: 0.8027295285359801\n",
      "Val loss: 0.6586218476295471, Val acc: 0.772\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.5131429594296676, Train acc: 0.8138957816377171\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.5039634266470232, Train acc: 0.8189964157706093\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.5152523706101964, Train acc: 0.8154581380387832\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5134456863260677, Train acc: 0.8154811138682106\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.51423246661822, Train acc: 0.8151089054314861\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5139084788426714, Train acc: 0.8156878963330576\n",
      "Val loss: 0.6384291052818298, Val acc: 0.77\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.48932764061495787, Train acc: 0.825062034739454\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4945363362884929, Train acc: 0.8249241797628895\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4947048364541469, Train acc: 0.8235915816560978\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.494187963155345, Train acc: 0.8235111662531017\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.49605082077348334, Train acc: 0.8234353460159912\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.4951215540284445, Train acc: 0.824120025732929\n",
      "Val loss: 0.6468015313148499, Val acc: 0.786\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4897055157229432, Train acc: 0.8205128205128205\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.48532760289744437, Train acc: 0.8215467328370554\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4795036061621799, Train acc: 0.824694421468615\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.47675367823650694, Train acc: 0.8267852219465123\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.47633227107361853, Train acc: 0.8274331403363662\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.47553473628229564, Train acc: 0.8283016266887234\n",
      "Val loss: 0.6062236428260803, Val acc: 0.792\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4608322598485865, Train acc: 0.8349875930521092\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4615717797070487, Train acc: 0.8365039977943204\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4600957638432837, Train acc: 0.8376527892656925\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.45991304895689344, Train acc: 0.8374689826302729\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.45694598346693904, Train acc: 0.8379376895505928\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.46018732829481107, Train acc: 0.8357687712526423\n",
      "Val loss: 0.5532540082931519, Val acc: 0.84\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.46644360412899244, Train acc: 0.8337468982630273\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.462439738596097, Train acc: 0.8351254480286738\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4538368881173283, Train acc: 0.8395827589375976\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.450243816161767, Train acc: 0.8409498207885304\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.44737972994135994, Train acc: 0.8411634960022056\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.44759026135814156, Train acc: 0.8412370186563735\n",
      "Val loss: 0.6199026107788086, Val acc: 0.796\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.41648160570707077, Train acc: 0.8471188309897987\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.437626468256498, Train acc: 0.8409153570443894\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4342578039464787, Train acc: 0.8433048433048433\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4343621984569945, Train acc: 0.8441549489936586\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.43298756533708327, Train acc: 0.845409429280397\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.4317798450419366, Train acc: 0.8460159911772815\n",
      "Val loss: 0.6367964744567871, Val acc: 0.764\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.41985622659707683, Train acc: 0.8524951750758203\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4175829684887177, Train acc: 0.8515301902398676\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.41512275935068427, Train acc: 0.8527708850289496\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4150848442481624, Train acc: 0.8529776674937966\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.4176705724408484, Train acc: 0.8524400330851943\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.4185205046989639, Train acc: 0.8527938608583769\n",
      "Val loss: 0.6720176935195923, Val acc: 0.72\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4018249707853692, Train acc: 0.8571822442790185\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4046837048143403, Train acc: 0.8569065343258891\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.40446913055544903, Train acc: 0.8577336641852771\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.40380617078298175, Train acc: 0.8585263303005238\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.40270300698586, Train acc: 0.8586159360352908\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.40353437472782244, Train acc: 0.8583769874092455\n",
      "Val loss: 0.5701979994773865, Val acc: 0.818\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.383320557892832, Train acc: 0.8653156878963331\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.37573335186029094, Train acc: 0.8678660049627791\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.39104541745620575, Train acc: 0.8605367153754251\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.38963821014532674, Train acc: 0.8620071684587813\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3904277890665918, Train acc: 0.8611248966087676\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.38971106892722285, Train acc: 0.8618693134822167\n",
      "Val loss: 0.5555793642997742, Val acc: 0.814\n",
      "Early stopping at epoch 15 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 139.1086 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.5719558357173562, Train acc: 0.2893575958092087\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.4981162563348427, Train acc: 0.3498070030328095\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.4201990909386224, Train acc: 0.4006525135557394\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.3370976594523487, Train acc: 0.44361731458505654\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.268385557944958, Train acc: 0.4760132340777502\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.2100666762247385, Train acc: 0.5031936402904145\n",
      "Val loss: 0.7658504247665405, Val acc: 0.716\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8728109139662522, Train acc: 0.6510890543148607\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.839715514682297, Train acc: 0.6710090984284532\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8147539348683805, Train acc: 0.6831633121955702\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7928173934292589, Train acc: 0.6921009098428453\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7832266722988879, Train acc: 0.6963606286186932\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7714076520201147, Train acc: 0.7029225255031707\n",
      "Val loss: 0.6486824750900269, Val acc: 0.756\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6763294058987218, Train acc: 0.7554452715743039\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6719155849045159, Train acc: 0.7520678246484698\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6716287682029257, Train acc: 0.749839169194008\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.659341629817445, Train acc: 0.7541701130410808\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6541521519677251, Train acc: 0.7561345464571271\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6517493226844021, Train acc: 0.7561115706276996\n",
      "Val loss: 0.6156731843948364, Val acc: 0.764\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.625232650174035, Train acc: 0.7670250896057348\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6103263750799701, Train acc: 0.7725392886683209\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6085880057424562, Train acc: 0.7730447569157246\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5965218474594955, Train acc: 0.7763647642679901\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5943942269198915, Train acc: 0.7770057899090157\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5914765494076955, Train acc: 0.7775939711423583\n",
      "Val loss: 0.627440333366394, Val acc: 0.786\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.5800834331247542, Train acc: 0.7842569616763165\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.5706896896545703, Train acc: 0.7889440308795147\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.558997334068657, Train acc: 0.7917011304108078\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5607046279897038, Train acc: 0.791528811690102\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5548931369924138, Train acc: 0.7942376619795974\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5489607767077254, Train acc: 0.7975140152559508\n",
      "Val loss: 0.5574712157249451, Val acc: 0.806\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.5103099096534599, Train acc: 0.808795147504825\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.5114474602234669, Train acc: 0.8078990901571547\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.5089154148373509, Train acc: 0.8115522470361181\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5043816968925998, Train acc: 0.8127929418251999\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5080240843642471, Train acc: 0.8108905431486076\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5037652738498487, Train acc: 0.8127929418251999\n",
      "Val loss: 0.568327784538269, Val acc: 0.796\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4947180139203357, Train acc: 0.8094844223876482\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.4881939591250868, Train acc: 0.8160325337744693\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.48248054414053587, Train acc: 0.8189504641117544\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4818763642802707, Train acc: 0.8202715743038324\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.4815176675971757, Train acc: 0.8216432313206506\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.4799568068522673, Train acc: 0.821730539472475\n",
      "Val loss: 0.5216392278671265, Val acc: 0.83\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4555314039317971, Train acc: 0.8319547835676868\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.45679018429966056, Train acc: 0.8287151916184174\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.45630641302831493, Train acc: 0.8292895873541035\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4521142406405037, Train acc: 0.8316446098704163\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.45323267917857213, Train acc: 0.8309070857457954\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.4532878467882121, Train acc: 0.8312195570260087\n",
      "Val loss: 0.5191513299942017, Val acc: 0.842\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4534182106582528, Train acc: 0.8314033636614282\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.44088963839488154, Train acc: 0.8349875930521092\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.44099367157346503, Train acc: 0.8380663541953864\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.43857465754462105, Train acc: 0.838537358698649\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.43656577315085976, Train acc: 0.839426523297491\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.43520208997943804, Train acc: 0.8408004778972521\n",
      "Val loss: 0.5306448936462402, Val acc: 0.826\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4297878031547253, Train acc: 0.84063964709126\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.43055697976269275, Train acc: 0.8409842845326716\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4248355685221164, Train acc: 0.8444995864350703\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.42115653857079327, Train acc: 0.8458436724565757\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.42049631894144235, Train acc: 0.8468982630272953\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.42127801367530116, Train acc: 0.8467512177189597\n",
      "Val loss: 0.5200396776199341, Val acc: 0.832\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.4239577600079724, Train acc: 0.8479459608491866\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.41792859990372616, Train acc: 0.8469120485249517\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.4156342686666043, Train acc: 0.8491866556382686\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.4130700110242917, Train acc: 0.8505996691480563\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.40897579241512166, Train acc: 0.8512544802867383\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.4084873516165633, Train acc: 0.8512085286278834\n",
      "Val loss: 0.48299071192741394, Val acc: 0.868\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.3901345872471475, Train acc: 0.8569065343258891\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.39028933797127163, Train acc: 0.8581472291149711\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.39452224579292144, Train acc: 0.8552522746071133\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.39548264306961983, Train acc: 0.855183347118831\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.39376485653412646, Train acc: 0.8558588365039977\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.3932352973907082, Train acc: 0.8565389210550501\n",
      "Val loss: 0.5632138252258301, Val acc: 0.828\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.39043333400518465, Train acc: 0.8577336641852771\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.3939549586074984, Train acc: 0.8573200992555832\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.38781869301089533, Train acc: 0.8598474404926018\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.388671371098767, Train acc: 0.8592156051833472\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3873406408688961, Train acc: 0.8603804797353184\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.38397087619515224, Train acc: 0.8616625310173698\n",
      "Val loss: 0.4972147047519684, Val acc: 0.864\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.3672084907690684, Train acc: 0.8688999172870141\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.362449739414912, Train acc: 0.869933829611249\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.36885454601202255, Train acc: 0.8664644793677052\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.37250201505982977, Train acc: 0.865729252826027\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.37140580127891315, Train acc: 0.8658395368072788\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.3715402441698941, Train acc: 0.8665334068559875\n",
      "Val loss: 0.5233672857284546, Val acc: 0.852\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.3552900384633969, Train acc: 0.8749655362558588\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.3674828421611052, Train acc: 0.8694513371932727\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.36320685622868715, Train acc: 0.8705082253469351\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.3656788827835495, Train acc: 0.8694858009374138\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3650737950435052, Train acc: 0.8695064791838986\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.36468614697286544, Train acc: 0.8699108537818215\n",
      "Val loss: 0.5248725414276123, Val acc: 0.834\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.34620610185158557, Train acc: 0.873311276537083\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.3474187045397922, Train acc: 0.8757926661152468\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.3512960373506247, Train acc: 0.875287197867843\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.35436434945107526, Train acc: 0.8741728701406121\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.3535277004680063, Train acc: 0.8748276812792942\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.354010832827655, Train acc: 0.8746668504733021\n",
      "Val loss: 0.5052365064620972, Val acc: 0.838\n",
      "Early stopping at epoch 16 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 153.3392 [s]\n",
      "Epoch 1/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.6127091829593365, Train acc: 0.19417735042735043\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.6086013067481864, Train acc: 0.20739850427350429\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.5877251254866944, Train acc: 0.23726851851851852\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.563853462282409, Train acc: 0.2670940170940171\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.541508026000781, Train acc: 0.294017094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.5174838443766971, Train acc: 0.3196225071225071\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.49647594895555, Train acc: 0.33878968253968256\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.4764939027591648, Train acc: 0.3535657051282051\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.456689124037171, Train acc: 0.36752136752136755\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.437149250965852, Train acc: 0.3804220085470085\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.41976528086214, Train acc: 0.3924825174825175\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.401374407218732, Train acc: 0.4030448717948718\n",
      "Val loss: 1.0774450302124023, Val acc: 0.57\n",
      "Epoch 2/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.1850915597035334, Train acc: 0.5371260683760684\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.1689403299083057, Train acc: 0.5405982905982906\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.1628129122264026, Train acc: 0.5456730769230769\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.1550504232828434, Train acc: 0.5502804487179487\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.1484906072290535, Train acc: 0.5521367521367522\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.1417434534795603, Train acc: 0.5543091168091168\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.1307168212129084, Train acc: 0.5596001221001221\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.1206780473391216, Train acc: 0.5641693376068376\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.116024753816447, Train acc: 0.5653786799620133\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.113060287379811, Train acc: 0.5663461538461538\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.10900040838083, Train acc: 0.5675747863247863\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.1023872169079605, Train acc: 0.5696225071225072\n",
      "Val loss: 0.8896596431732178, Val acc: 0.65\n",
      "Epoch 3/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.0005129585918198, Train acc: 0.6127136752136753\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.0261192056867812, Train acc: 0.5998931623931624\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.018893634831464, Train acc: 0.6051460113960114\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.0193667660156887, Train acc: 0.6051682692307693\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.0218849473529392, Train acc: 0.5999465811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.0172806954451776, Train acc: 0.6010505698005698\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.0124988303370819, Train acc: 0.6030601343101343\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0102974857784743, Train acc: 0.6038995726495726\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0050844206542824, Train acc: 0.6052943969610636\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.0030161399107713, Train acc: 0.6058760683760683\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.0023569416055034, Train acc: 0.6055506993006993\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.0000199285412785, Train acc: 0.6067708333333334\n",
      "Val loss: 0.8253709673881531, Val acc: 0.678\n",
      "Epoch 4/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.9832427129786239, Train acc: 0.6017628205128205\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.9747444432005923, Train acc: 0.6123130341880342\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.97016295442554, Train acc: 0.6159188034188035\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9659126678593138, Train acc: 0.6189236111111112\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9656052173712315, Train acc: 0.619017094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9629511593753456, Train acc: 0.6212606837606838\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9566911726789742, Train acc: 0.6229777167277167\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9588161114698801, Train acc: 0.6221287393162394\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9567440878405303, Train acc: 0.623338081671415\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9551707019153823, Train acc: 0.6238782051282051\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9528537475979412, Train acc: 0.6239801864801865\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9529526463770799, Train acc: 0.6238871082621082\n",
      "Val loss: 0.8073558807373047, Val acc: 0.698\n",
      "Epoch 5/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.9258327794890119, Train acc: 0.6378205128205128\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.924889599156176, Train acc: 0.6379540598290598\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9155660286927835, Train acc: 0.6397792022792023\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9253980699512694, Train acc: 0.6380208333333334\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9275780630926801, Train acc: 0.6361111111111111\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9255174361063204, Train acc: 0.6388443732193733\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9264358295916929, Train acc: 0.6406822344322345\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9269175433959717, Train acc: 0.6407251602564102\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9250416234913941, Train acc: 0.6410553181386515\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.922932975159751, Train acc: 0.6421741452991453\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9232688280344936, Train acc: 0.6416812354312355\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.922028993099843, Train acc: 0.6413149928774928\n",
      "Val loss: 0.7853050827980042, Val acc: 0.7\n",
      "Epoch 6/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8927340792794513, Train acc: 0.6506410256410257\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8825284924007889, Train acc: 0.6566506410256411\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8893734018856984, Train acc: 0.6523326210826211\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8929706569283437, Train acc: 0.6502403846153846\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.890633501812943, Train acc: 0.6498397435897436\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8928866430989697, Train acc: 0.6498842592592593\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8935455937889178, Train acc: 0.6503739316239316\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8924784770187659, Train acc: 0.6510416666666666\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8921847175612182, Train acc: 0.6511455365622032\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8910959242500811, Train acc: 0.6510149572649573\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8891584405208061, Train acc: 0.6515879953379954\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8875374338268555, Train acc: 0.6519987535612536\n",
      "Val loss: 0.7697871923446655, Val acc: 0.698\n",
      "Epoch 7/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8521868357291589, Train acc: 0.6661324786324786\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8599074457445716, Train acc: 0.6635950854700855\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8613526301506238, Train acc: 0.6621260683760684\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8615261974752458, Train acc: 0.6595219017094017\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8590261679429274, Train acc: 0.6611645299145299\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8596189433013611, Train acc: 0.6600783475783476\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8572263954905509, Train acc: 0.661553724053724\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8611674027947279, Train acc: 0.6599559294871795\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8618304177000765, Train acc: 0.6604938271604939\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8615088278921241, Train acc: 0.6600694444444445\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8607267672550614, Train acc: 0.6602564102564102\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8620911610771788, Train acc: 0.6600560897435898\n",
      "Val loss: 0.7583688497543335, Val acc: 0.708\n",
      "Epoch 8/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8355590757141765, Train acc: 0.6773504273504274\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8437103362929108, Train acc: 0.6701388888888888\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8496674878305179, Train acc: 0.6688034188034188\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8393700884448158, Train acc: 0.6729433760683761\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8444187130683508, Train acc: 0.6688568376068376\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8423530969269934, Train acc: 0.6683137464387464\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8433227592917735, Train acc: 0.6681929181929182\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8426753589485445, Train acc: 0.6667334401709402\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8457091206159347, Train acc: 0.6655389363722697\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8447793668151921, Train acc: 0.6662126068376069\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8460244541031127, Train acc: 0.6661567599067599\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8448735450068091, Train acc: 0.6659099002849003\n",
      "Val loss: 0.7559645175933838, Val acc: 0.702\n",
      "Epoch 9/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.833533469428364, Train acc: 0.6728098290598291\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8145572262951452, Train acc: 0.6765491452991453\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.814104042671345, Train acc: 0.6751246438746439\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8232732810016371, Train acc: 0.6725427350427351\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8243158681779845, Train acc: 0.6701388888888888\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.826278259017189, Train acc: 0.6708956552706553\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8243648732247079, Train acc: 0.6722374847374848\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8224414370826676, Train acc: 0.672409188034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8230735387331174, Train acc: 0.6730769230769231\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8239989180086006, Train acc: 0.6735042735042736\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8244652755286402, Train acc: 0.6735625485625486\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8241716789086999, Train acc: 0.6733440170940171\n",
      "Val loss: 0.7263535857200623, Val acc: 0.72\n",
      "Epoch 10/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8298179711032118, Train acc: 0.6725427350427351\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8303254745964311, Train acc: 0.6717414529914529\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8213361053385286, Train acc: 0.6773504273504274\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8135998742575319, Train acc: 0.6820245726495726\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8145510661805797, Train acc: 0.6790064102564103\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8140647605358705, Train acc: 0.6804665242165242\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8099808258815272, Train acc: 0.681280525030525\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8083478844063914, Train acc: 0.6815237713675214\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.806648632286847, Train acc: 0.6817426400759734\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8038693759176466, Train acc: 0.6821314102564102\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8006293097020307, Train acc: 0.6826923076923077\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.801002991908466, Train acc: 0.6832932692307693\n",
      "Val loss: 0.7252511382102966, Val acc: 0.722\n",
      "Tiempo total de entrenamiento: 132.4550 [s]\n",
      "Epoch 1/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.6097697111276479, Train acc: 0.19791666666666666\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.598694642384847, Train acc: 0.22689636752136752\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.5781304044940871, Train acc: 0.2604166666666667\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.5542232327991061, Train acc: 0.28685897435897434\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.5282286857947325, Train acc: 0.3111111111111111\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.4955095657256254, Train acc: 0.3352920227920228\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.4640211716239706, Train acc: 0.35740995115995117\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.4338648619814816, Train acc: 0.3775040064102564\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.4072548854611424, Train acc: 0.3949133428300095\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.3873680982324812, Train acc: 0.40697115384615384\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.369069580709462, Train acc: 0.4190947940947941\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.3516410330676625, Train acc: 0.4302439458689459\n",
      "Val loss: 0.9870673418045044, Val acc: 0.626\n",
      "Epoch 2/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.1229615491679592, Train acc: 0.5646367521367521\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.1056844857003953, Train acc: 0.5715811965811965\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.1056007639295355, Train acc: 0.5736289173789174\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.097288111336211, Train acc: 0.5767227564102564\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.0902736733102392, Train acc: 0.5792735042735043\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.084126900913369, Train acc: 0.5807069088319088\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.0790143998405726, Train acc: 0.5835241147741148\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0748863604206305, Train acc: 0.5866386217948718\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0706696900320642, Train acc: 0.586835232668566\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.066554444620752, Train acc: 0.5880608974358974\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.0641991377062083, Train acc: 0.58879662004662\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.0603310116420783, Train acc: 0.5892761752136753\n",
      "Val loss: 0.8695507645606995, Val acc: 0.658\n",
      "Epoch 3/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.0106826745546782, Train acc: 0.6132478632478633\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.0106704966125326, Train acc: 0.6076388888888888\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9973665402825401, Train acc: 0.61369301994302\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9942207272745606, Train acc: 0.6135149572649573\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9927285318700676, Train acc: 0.6165598290598291\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9909667444874418, Train acc: 0.6168981481481481\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9890332828947913, Train acc: 0.6148504273504274\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9856318350658457, Train acc: 0.6167868589743589\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.984956089983865, Train acc: 0.6171652421652422\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9797206270389068, Train acc: 0.6191773504273504\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9790276987384064, Train acc: 0.6191967754467754\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9767733295673658, Train acc: 0.6201700498575499\n",
      "Val loss: 0.8106873035430908, Val acc: 0.688\n",
      "Epoch 4/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.9374127551021739, Train acc: 0.6426282051282052\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.9449204466281793, Train acc: 0.6343482905982906\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9441988690626247, Train acc: 0.635505698005698\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9376661229846824, Train acc: 0.6384214743589743\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9380296116201287, Train acc: 0.6393162393162393\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9335446113195175, Train acc: 0.6413372507122507\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9252068648553738, Train acc: 0.6455280830280831\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9217864849373826, Train acc: 0.6462339743589743\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9187365548449931, Train acc: 0.6468720322886989\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9177762497694064, Train acc: 0.6469017094017094\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.917697021084973, Train acc: 0.646950271950272\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9150218710175946, Train acc: 0.6474358974358975\n",
      "Val loss: 0.7902363538742065, Val acc: 0.682\n",
      "Epoch 5/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8742594301191151, Train acc: 0.6578525641025641\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8716249692643809, Train acc: 0.6555822649572649\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8797260298688188, Train acc: 0.6544693732193733\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8774097363154093, Train acc: 0.6581864316239316\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8764578504440111, Train acc: 0.6576923076923077\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8796666920015276, Train acc: 0.656517094017094\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8774217832947243, Train acc: 0.6576617826617827\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8770963007695655, Train acc: 0.6578859508547008\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8727274875582001, Train acc: 0.6596331908831908\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8709814021984736, Train acc: 0.6596153846153846\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8696249035456983, Train acc: 0.6603535353535354\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8695526825439217, Train acc: 0.6609241452991453\n",
      "Val loss: 0.7718875408172607, Val acc: 0.706\n",
      "Epoch 6/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8638166122966342, Train acc: 0.6688034188034188\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8515527673766145, Train acc: 0.6740117521367521\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8560596980430462, Train acc: 0.6738782051282052\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.847381072357679, Train acc: 0.6772168803418803\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8466755549622397, Train acc: 0.6772970085470086\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8446086971593378, Train acc: 0.676460113960114\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.839893030196028, Train acc: 0.6783043345543346\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8392188558275373, Train acc: 0.6775507478632479\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8377897454406235, Train acc: 0.6771426875593543\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8356910429194442, Train acc: 0.6774839743589743\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8344915905198851, Train acc: 0.6781031468531469\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.834939566836228, Train acc: 0.6785523504273504\n",
      "Val loss: 0.7134027481079102, Val acc: 0.712\n",
      "Epoch 7/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8449838828836751, Train acc: 0.6762820512820513\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8413581369269607, Train acc: 0.6769497863247863\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8283990045897981, Train acc: 0.6826923076923077\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8220447996615344, Train acc: 0.6857638888888888\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8181969217765026, Train acc: 0.6859508547008547\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8211155600017972, Train acc: 0.6860754985754985\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.82431443037422, Train acc: 0.6839514652014652\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8222161284369282, Train acc: 0.6831597222222222\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8202615945767133, Train acc: 0.6847400284900285\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8154425751194995, Train acc: 0.6865918803418803\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8140895238150527, Train acc: 0.6856546231546231\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8128133337636958, Train acc: 0.6852519586894587\n",
      "Val loss: 0.7211746573448181, Val acc: 0.722\n",
      "Epoch 8/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7855149532994653, Train acc: 0.6936431623931624\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7908512259650434, Train acc: 0.6908386752136753\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7939711755666977, Train acc: 0.6913283475783476\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7955723169904488, Train acc: 0.6897035256410257\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7893053748668768, Train acc: 0.693215811965812\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7922265566142536, Train acc: 0.6915509259259259\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7905027834341494, Train acc: 0.6924603174603174\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7910561040680633, Train acc: 0.6946113782051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7892135140279986, Train acc: 0.6954237891737892\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7875160420552278, Train acc: 0.6958867521367521\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7852214378447336, Train acc: 0.696168414918415\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7833153960796503, Train acc: 0.6966924857549858\n",
      "Val loss: 0.6826082468032837, Val acc: 0.728\n",
      "Epoch 9/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7560505765116113, Train acc: 0.7091346153846154\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7649629183559337, Train acc: 0.7096688034188035\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7667884800339017, Train acc: 0.7080662393162394\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7658515149074742, Train acc: 0.7065972222222222\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7693086996037736, Train acc: 0.7049679487179488\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7665116307657329, Train acc: 0.7051282051282052\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7620633383883079, Train acc: 0.7080280830280831\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7632199416940029, Train acc: 0.706764155982906\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7604981313290646, Train acc: 0.707977207977208\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7587190519286017, Train acc: 0.7100427350427351\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7585352810205372, Train acc: 0.709717365967366\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7579402520700738, Train acc: 0.7102920227920227\n",
      "Val loss: 0.6620954275131226, Val acc: 0.74\n",
      "Epoch 10/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7411617203655406, Train acc: 0.7112713675213675\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7424899223777983, Train acc: 0.7104700854700855\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7421749140801932, Train acc: 0.7110933048433048\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.741753733693025, Train acc: 0.7136084401709402\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7439315531498346, Train acc: 0.7134615384615385\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7454347158768917, Train acc: 0.7131410256410257\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7414684454308907, Train acc: 0.7152777777777778\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7409338423838983, Train acc: 0.7163795405982906\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7382142429168408, Train acc: 0.7172958214624882\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7374673905280921, Train acc: 0.7177884615384615\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7373791760544306, Train acc: 0.7181672494172494\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7353121297609093, Train acc: 0.7192619301994302\n",
      "Val loss: 0.6339940428733826, Val acc: 0.756\n",
      "Tiempo total de entrenamiento: 131.5313 [s]\n",
      "Epoch 1/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.610839148871919, Train acc: 0.19764957264957264\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.5971387028694153, Train acc: 0.23971688034188035\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.5898276129339495, Train acc: 0.260505698005698\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.5798553162150912, Train acc: 0.2787793803418803\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.5689797802868053, Train acc: 0.2918269230769231\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.5578950524669766, Train acc: 0.30644586894586895\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.5407185301239237, Train acc: 0.32238247863247865\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.5240295668188324, Train acc: 0.3379407051282051\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.506019880968621, Train acc: 0.3518815289648623\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.4858078915848691, Train acc: 0.3659989316239316\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.4666165311668118, Train acc: 0.37810800310800313\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.4488251727191133, Train acc: 0.38969017094017094\n",
      "Val loss: 1.1425970792770386, Val acc: 0.554\n",
      "Epoch 2/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.2154813408851624, Train acc: 0.5269764957264957\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.2094013683306866, Train acc: 0.5351228632478633\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.1927727211234915, Train acc: 0.5424679487179487\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.178427972854712, Train acc: 0.5483440170940171\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.1683620036157787, Train acc: 0.5522435897435898\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.1603571823519518, Train acc: 0.5539529914529915\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.1524541113111708, Train acc: 0.555746336996337\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.144028193739235, Train acc: 0.5585269764957265\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.134750156773914, Train acc: 0.5625296771130105\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.1306428290839887, Train acc: 0.564556623931624\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 1.1235046536772402, Train acc: 0.5660207847707848\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 1.1160647427169685, Train acc: 0.5689547720797721\n",
      "Val loss: 0.8992478251457214, Val acc: 0.628\n",
      "Epoch 3/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.013990563714606, Train acc: 0.6113782051282052\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.0170739254890344, Train acc: 0.6081730769230769\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.0112408216183002, Train acc: 0.6097756410256411\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.0098127824628456, Train acc: 0.6069711538461539\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.006635959739359, Train acc: 0.6086004273504273\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.0044368103868262, Train acc: 0.6108885327635327\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.0019670460687016, Train acc: 0.6116452991452992\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9975162549660757, Train acc: 0.6142160790598291\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9961223811165899, Train acc: 0.6138117283950617\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9914103109612424, Train acc: 0.615090811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9897974544749671, Train acc: 0.6150446775446775\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9880678281824813, Train acc: 0.6158075142450142\n",
      "Val loss: 0.8495775461196899, Val acc: 0.654\n",
      "Epoch 4/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.9563981187649262, Train acc: 0.6236645299145299\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.9447011226772243, Train acc: 0.6298076923076923\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9401905648728721, Train acc: 0.6321225071225072\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9356889925960802, Train acc: 0.6345486111111112\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9364515329018618, Train acc: 0.6332264957264957\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.9395974536566992, Train acc: 0.6310096153846154\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.9360295123669691, Train acc: 0.6332417582417582\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9330277171654578, Train acc: 0.6343482905982906\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9328149851457572, Train acc: 0.6351792497625831\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9313512509704656, Train acc: 0.6352029914529914\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9302827722357518, Train acc: 0.635975135975136\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9283506993384782, Train acc: 0.6368856837606838\n",
      "Val loss: 0.8236420750617981, Val acc: 0.658\n",
      "Epoch 5/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8908921388479379, Train acc: 0.6466346153846154\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.9102132457953233, Train acc: 0.6438301282051282\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.9118426603469414, Train acc: 0.6458333333333334\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.9028103086046684, Train acc: 0.6491052350427351\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.9016334215290526, Train acc: 0.6492521367521368\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.898863414541269, Train acc: 0.6489049145299145\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8964367960471664, Train acc: 0.6505647130647131\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.894585477713591, Train acc: 0.6509081196581197\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8930143438286704, Train acc: 0.6523622981956315\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8919277152444562, Train acc: 0.6534188034188034\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.890159033626758, Train acc: 0.6532634032634033\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8894278242802008, Train acc: 0.6544471153846154\n",
      "Val loss: 0.7897894978523254, Val acc: 0.684\n",
      "Epoch 6/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8620872110383123, Train acc: 0.6701388888888888\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8590377734767066, Train acc: 0.6746794871794872\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8546012049047356, Train acc: 0.6733440170940171\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.852983771608426, Train acc: 0.6722088675213675\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8524394425571474, Train acc: 0.6713675213675213\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8469246025608476, Train acc: 0.6741898148148148\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8503176099797017, Train acc: 0.6726953601953602\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8503002326330568, Train acc: 0.672142094017094\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.851214854635744, Train acc: 0.6713853276353277\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8520916645343487, Train acc: 0.6708333333333333\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.85348343603724, Train acc: 0.6706002331002331\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.851944260362886, Train acc: 0.6711182336182336\n",
      "Val loss: 0.7613965272903442, Val acc: 0.71\n",
      "Epoch 7/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.846570275278173, Train acc: 0.6773504273504274\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.8450592257018782, Train acc: 0.6773504273504274\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8409793748814836, Train acc: 0.6800213675213675\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8318924058196892, Train acc: 0.6815571581196581\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8330646849086142, Train acc: 0.6807692307692308\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8314284946758863, Train acc: 0.6809116809116809\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8299680292533577, Train acc: 0.6816239316239316\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.8283199693402673, Train acc: 0.6820245726495726\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8264017000384027, Train acc: 0.683374881291548\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.824279333001528, Train acc: 0.6835737179487179\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.8235617741282447, Train acc: 0.6844162781662781\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8231609446710331, Train acc: 0.6845619658119658\n",
      "Val loss: 0.7297025918960571, Val acc: 0.728\n",
      "Epoch 8/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.8155592376859779, Train acc: 0.6971153846153846\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7967781331549343, Train acc: 0.7041933760683761\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.8010773579789023, Train acc: 0.7005876068376068\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.8001721954116454, Train acc: 0.7003872863247863\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.8012932045337481, Train acc: 0.7000534188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.8035873006127159, Train acc: 0.6986734330484331\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.8030367668658968, Train acc: 0.6974969474969475\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7997715266851279, Train acc: 0.6990852029914529\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.8012777697100372, Train acc: 0.698005698005698\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.8025062408712175, Train acc: 0.6967681623931624\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7985482731574991, Train acc: 0.6984994172494172\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7980983016794903, Train acc: 0.6988737535612536\n",
      "Val loss: 0.7102723121643066, Val acc: 0.732\n",
      "Epoch 9/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7580734697672037, Train acc: 0.7040598290598291\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7579189803865221, Train acc: 0.7095352564102564\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7618836189097489, Train acc: 0.70744301994302\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7616093508962892, Train acc: 0.7083333333333334\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7663971705823882, Train acc: 0.7050213675213676\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7654709427954465, Train acc: 0.7060185185185185\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.7667046954198052, Train acc: 0.7062347374847375\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7664125009288645, Train acc: 0.706497061965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7653686693477721, Train acc: 0.7076804368471035\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7666886443765755, Train acc: 0.7071581196581197\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7658492119486423, Train acc: 0.7079691142191142\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7649780029797146, Train acc: 0.7073094729344729\n",
      "Val loss: 0.6981073617935181, Val acc: 0.738\n",
      "Epoch 10/10\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7638054423862033, Train acc: 0.7053952991452992\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.751602744954264, Train acc: 0.7144764957264957\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7527367422553549, Train acc: 0.7141203703703703\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7466383803731356, Train acc: 0.7142761752136753\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7435954835679796, Train acc: 0.7169871794871795\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7489406651581115, Train acc: 0.7151887464387464\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.748160203625431, Train acc: 0.7155448717948718\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7462715286857042, Train acc: 0.7160790598290598\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.7458812414694149, Train acc: 0.7164945394112061\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.7441381446571431, Train acc: 0.7173076923076923\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.7436458442038867, Train acc: 0.7178515928515928\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.7424669534901948, Train acc: 0.7176816239316239\n",
      "Val loss: 0.6693999767303467, Val acc: 0.734\n",
      "Tiempo total de entrenamiento: 132.1088 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.5542482229379506, Train acc: 0.2451061483319548\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.4690501348585145, Train acc: 0.2914254204576785\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.3910056580165853, Train acc: 0.32129399871335357\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.333566947370513, Train acc: 0.3506341328921974\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.2931167457857702, Train acc: 0.3764543700027571\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.261101168360126, Train acc: 0.39536807278742764\n",
      "Val loss: 1.0720947980880737, Val acc: 0.408\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.0514001285927927, Train acc: 0.5380479735318445\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.039406487574944, Train acc: 0.5517645437000276\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.0288188875230968, Train acc: 0.5590478816285268\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.0217165226100857, Train acc: 0.5625172318720706\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.0101341251634126, Train acc: 0.5706368899917287\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.0031750936623651, Train acc: 0.5742119290506388\n",
      "Val loss: 1.2249205112457275, Val acc: 0.38\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9143272908324869, Train acc: 0.6210366694237662\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8788266874786116, Train acc: 0.6337882547559966\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8801126857089181, Train acc: 0.6321569708666482\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8729653704879631, Train acc: 0.6377171215880894\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8652520089067964, Train acc: 0.6412737799834574\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.860309299580392, Train acc: 0.6443341604631927\n",
      "Val loss: 1.3939827680587769, Val acc: 0.444\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.8112478994915628, Train acc: 0.6668045216432313\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8022837649043809, Train acc: 0.6734215605183347\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8002872636854819, Train acc: 0.676086756731918\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7948220755554672, Train acc: 0.6775916735594155\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7929221116579496, Train acc: 0.6785221946512269\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7955677133170288, Train acc: 0.6784532671629446\n",
      "Val loss: 1.400803804397583, Val acc: 0.536\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7551668092735813, Train acc: 0.7043010752688172\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7427092674705718, Train acc: 0.7085745795423215\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7345317761782567, Train acc: 0.7141806819226174\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7258153304967105, Train acc: 0.7172939068100358\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7213469763596853, Train acc: 0.7211745244003308\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7188374415846632, Train acc: 0.723095303740465\n",
      "Val loss: 1.5104260444641113, Val acc: 0.536\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6925982417713883, Train acc: 0.7397298042459333\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6878151351060623, Train acc: 0.7440722360077199\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6781264664950194, Train acc: 0.7478632478632479\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6699366035242366, Train acc: 0.7518265784394816\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6676397930862558, Train acc: 0.7529363110008271\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6676157662416795, Train acc: 0.7521597279661796\n",
      "Val loss: 1.6962575912475586, Val acc: 0.528\n",
      "Early stopping at epoch 6 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 54.9668 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.5758354113652155, Train acc: 0.2550317066446099\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.4952546780944889, Train acc: 0.3154121863799283\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.3931050956079423, Train acc: 0.3690837239224336\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.3231641851429246, Train acc: 0.4030879514750482\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.2664586019312214, Train acc: 0.43507030603804797\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.2209453261815584, Train acc: 0.4596084918665564\n",
      "Val loss: 0.8243275880813599, Val acc: 0.712\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9424006536475613, Train acc: 0.6148331954783568\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.9216834268508813, Train acc: 0.6208298869589192\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.9151105410353071, Train acc: 0.6234261556842202\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.9054798047002565, Train acc: 0.6298249241797629\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8926138543675088, Train acc: 0.637082988695892\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8788937776007204, Train acc: 0.6439435713629262\n",
      "Val loss: 0.8252418041229248, Val acc: 0.624\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7892337599371233, Train acc: 0.688861317893576\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.781193619609898, Train acc: 0.6959608491866557\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7800458866646487, Train acc: 0.6984652145942468\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7758029356726215, Train acc: 0.6995450785773366\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7712934888835646, Train acc: 0.7013785497656465\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7669668783489455, Train acc: 0.7036347762154214\n",
      "Val loss: 0.9038686156272888, Val acc: 0.502\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7354848079192333, Train acc: 0.7196029776674938\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7285483526623148, Train acc: 0.7204301075268817\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7197061534281131, Train acc: 0.7235548203290139\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7165633659714308, Train acc: 0.7256341328921974\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7127454931919391, Train acc: 0.7268265784394816\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7103325725875349, Train acc: 0.727874276261373\n",
      "Val loss: 0.865032970905304, Val acc: 0.618\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6930873315049033, Train acc: 0.7372484146677695\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6694116110985096, Train acc: 0.7433829611248967\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6759063173563052, Train acc: 0.7420733388475323\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6684075327765229, Train acc: 0.7470016542597188\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.666730296306121, Train acc: 0.7472015439757376\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6618622873838131, Train acc: 0.7505514199062586\n",
      "Val loss: 0.9732603430747986, Val acc: 0.59\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6381266053415772, Train acc: 0.7606837606837606\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6251941374224476, Train acc: 0.7671629445822994\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.623406552351438, Train acc: 0.7697821891370279\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6189511709360995, Train acc: 0.7718155500413565\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6155916306707594, Train acc: 0.7732285635511442\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6125364834257001, Train acc: 0.7745841374873633\n",
      "Val loss: 0.9888817071914673, Val acc: 0.618\n",
      "Early stopping at epoch 6 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 54.8820 [s]\n",
      "Epoch 1/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 1.5523128998585236, Train acc: 0.29942100909842845\n",
      "Iteration 234 - Batch 234/782 - Train loss: 1.4705703981921203, Train acc: 0.3480838158257513\n",
      "Iteration 351 - Batch 351/782 - Train loss: 1.3674795515516884, Train acc: 0.39137027846705263\n",
      "Iteration 468 - Batch 468/782 - Train loss: 1.2859511165282664, Train acc: 0.4282464846980976\n",
      "Iteration 585 - Batch 585/782 - Train loss: 1.2284827071377356, Train acc: 0.4574303832368348\n",
      "Iteration 702 - Batch 702/782 - Train loss: 1.1795355280240376, Train acc: 0.482860031247128\n",
      "Val loss: 0.7733727097511292, Val acc: 0.724\n",
      "Epoch 2/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.9013606559517037, Train acc: 0.615936035290874\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.8785300937473265, Train acc: 0.6255858836503998\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.8604498295023231, Train acc: 0.6371197500229758\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.8487581456573601, Train acc: 0.6404052936311001\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.8363890426790612, Train acc: 0.647835676867935\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.8319675592105953, Train acc: 0.6520310633213859\n",
      "Val loss: 0.7947669625282288, Val acc: 0.672\n",
      "Epoch 3/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.7816338890638107, Train acc: 0.6786600496277916\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.7704956787518966, Train acc: 0.685690653432589\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.7608561465713034, Train acc: 0.6881720430107527\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.7503349841532544, Train acc: 0.6933416046319272\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.7439109545997066, Train acc: 0.6982905982905983\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.7387862497585111, Train acc: 0.702784670526606\n",
      "Val loss: 0.8585437536239624, Val acc: 0.524\n",
      "Epoch 4/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6886739539794433, Train acc: 0.7368348497380756\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6899728843799005, Train acc: 0.7386958919216984\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6866998140968149, Train acc: 0.7403731274699017\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6815663006825324, Train acc: 0.740729252826027\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6809094333750569, Train acc: 0.7420733388475323\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6776092494400139, Train acc: 0.7430842753423399\n",
      "Val loss: 0.8992927670478821, Val acc: 0.486\n",
      "Epoch 5/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6348249224516062, Train acc: 0.7591673559415495\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.6502433861168022, Train acc: 0.757375241246209\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.6527034414459837, Train acc: 0.7552155132800294\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.6486261123393335, Train acc: 0.7565481113868211\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.6437841888676342, Train acc: 0.7587813620071685\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.6370902168445098, Train acc: 0.7612811322488742\n",
      "Val loss: 0.929185688495636, Val acc: 0.484\n",
      "Epoch 6/30\n",
      "Iteration 117 - Batch 117/782 - Train loss: 0.6023761762513055, Train acc: 0.7725392886683209\n",
      "Iteration 234 - Batch 234/782 - Train loss: 0.5976255836649838, Train acc: 0.7769506479183899\n",
      "Iteration 351 - Batch 351/782 - Train loss: 0.5909304675723073, Train acc: 0.7811782005330392\n",
      "Iteration 468 - Batch 468/782 - Train loss: 0.5945472312279236, Train acc: 0.7805348773090709\n",
      "Iteration 585 - Batch 585/782 - Train loss: 0.5946810463045398, Train acc: 0.7808381582575131\n",
      "Iteration 702 - Batch 702/782 - Train loss: 0.5968438824612191, Train acc: 0.7804199981619336\n",
      "Val loss: 0.8867421746253967, Val acc: 0.438\n",
      "Early stopping at epoch 6 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 55.8736 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.5675884230524046, Train acc: 0.2705662393162393\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.4728826600262241, Train acc: 0.3348023504273504\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.3576289194601554, Train acc: 0.39538817663817666\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.267640672560431, Train acc: 0.4428418803418803\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.1973126970804655, Train acc: 0.47884615384615387\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.145794410651226, Train acc: 0.5066773504273504\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.1001801442022872, Train acc: 0.5305250305250305\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0624974773416662, Train acc: 0.5504139957264957\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0334997505713732, Train acc: 0.5658535137701804\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.0063355336841355, Train acc: 0.5801014957264957\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.983140304626748, Train acc: 0.5920260295260296\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9620097149428819, Train acc: 0.6023860398860399\n",
      "Val loss: 0.6717599034309387, Val acc: 0.732\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6865613019873953, Train acc: 0.7435897435897436\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.692307912513741, Train acc: 0.735176282051282\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6920091939447952, Train acc: 0.7378917378917379\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6853147002621593, Train acc: 0.7396501068376068\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.6762530308503371, Train acc: 0.7428952991452992\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6714822889393212, Train acc: 0.744613603988604\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6692774329284463, Train acc: 0.7456883394383395\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.666334801313714, Train acc: 0.7471287393162394\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6668963156930628, Train acc: 0.7475664767331434\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6632531104943691, Train acc: 0.7491452991452991\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6588758048431393, Train acc: 0.7511655011655012\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6561338651681218, Train acc: 0.7523593304843305\n",
      "Val loss: 0.6195573806762695, Val acc: 0.772\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6058136729093698, Train acc: 0.7729700854700855\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5855176111317089, Train acc: 0.7796474358974359\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5900476291308715, Train acc: 0.7766203703703703\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5887817422039489, Train acc: 0.7761752136752137\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5894987292778797, Train acc: 0.7776175213675214\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5860240007737423, Train acc: 0.7797364672364673\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5824395363947993, Train acc: 0.7833485958485958\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5819571695776067, Train acc: 0.7834535256410257\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5813244377587482, Train acc: 0.7839506172839507\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5810074394457361, Train acc: 0.7836538461538461\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5767373824615115, Train acc: 0.7856691919191919\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5727390390409534, Train acc: 0.7878160612535613\n",
      "Val loss: 0.5731986165046692, Val acc: 0.826\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5300823497211832, Train acc: 0.8071581196581197\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5314896661374304, Train acc: 0.8064903846153846\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5254747448993204, Train acc: 0.8061787749287749\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.52364816230077, Train acc: 0.8070913461538461\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5239421470042986, Train acc: 0.8075320512820513\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5225649257840594, Train acc: 0.8077813390313391\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5183114598849754, Train acc: 0.8100579975579976\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.518684433391079, Train acc: 0.8097288995726496\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5181571445432257, Train acc: 0.8094729344729344\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5174692326376581, Train acc: 0.809909188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5170237395706896, Train acc: 0.8097562160062161\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5155512778316638, Train acc: 0.810608084045584\n",
      "Val loss: 0.5421267747879028, Val acc: 0.822\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4850934605058442, Train acc: 0.8266559829059829\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4969817612033624, Train acc: 0.8185096153846154\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.49236546860121594, Train acc: 0.8200676638176638\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.49261705787518084, Train acc: 0.8202457264957265\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4937736223396073, Train acc: 0.819818376068376\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.49656419596101486, Train acc: 0.8176638176638177\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.49542788644486146, Train acc: 0.8186813186813187\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.49396617030804485, Train acc: 0.8193108974358975\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.49037729096152277, Train acc: 0.8206612060778727\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4890213819077382, Train acc: 0.8214476495726496\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4895993267658522, Train acc: 0.82126554001554\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4881881848879774, Train acc: 0.8224269943019943\n",
      "Val loss: 0.5162397027015686, Val acc: 0.832\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4550057053565979, Train acc: 0.8325320512820513\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4408291850207198, Train acc: 0.8401442307692307\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4442560855734382, Train acc: 0.8396545584045584\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4436510061988464, Train acc: 0.8394097222222222\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4486740218777942, Train acc: 0.8376068376068376\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4524877867595083, Train acc: 0.8354255698005698\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4524996195156787, Train acc: 0.8360805860805861\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.45565546143195057, Train acc: 0.835403311965812\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4579008116483235, Train acc: 0.8347578347578347\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.45834076672028273, Train acc: 0.8353098290598291\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.45734045817209196, Train acc: 0.8353729603729604\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.45550331621406936, Train acc: 0.8363158831908832\n",
      "Val loss: 0.5664398074150085, Val acc: 0.854\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4549419229101931, Train acc: 0.8330662393162394\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4569363361622533, Train acc: 0.8372061965811965\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.46173748335777187, Train acc: 0.834045584045584\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.45758906713663006, Train acc: 0.8359375\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.44819304359774303, Train acc: 0.8393696581196581\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.44468825637360243, Train acc: 0.8405448717948718\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.44365645583004887, Train acc: 0.8406211843711844\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44037073720087355, Train acc: 0.8419471153846154\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.44182005482181863, Train acc: 0.8419693732193733\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4468396008778841, Train acc: 0.8399038461538462\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4457134220752034, Train acc: 0.8408119658119658\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4445197892473464, Train acc: 0.8414351851851852\n",
      "Val loss: 0.5752933621406555, Val acc: 0.854\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.43225186439151436, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.43457649170588225, Train acc: 0.844017094017094\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4281702400357635, Train acc: 0.8451745014245015\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4284758484070627, Train acc: 0.8456864316239316\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4284498794975444, Train acc: 0.8465277777777778\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4290241944654035, Train acc: 0.8458422364672364\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42829957957378384, Train acc: 0.8465735653235653\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4265975016766252, Train acc: 0.8465544871794872\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.42293120340460955, Train acc: 0.8476970560303894\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.42252947540364716, Train acc: 0.8481837606837607\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.41919717083814634, Train acc: 0.8497231934731935\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.41918663118045213, Train acc: 0.849670584045584\n",
      "Val loss: 0.49499276280403137, Val acc: 0.846\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.39950843461048907, Train acc: 0.8568376068376068\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4005312015995001, Train acc: 0.8560363247863247\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.39918077471758906, Train acc: 0.8563924501424501\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.39773816637631154, Train acc: 0.8572382478632479\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.39789392311348876, Train acc: 0.8579059829059829\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3981739902428412, Train acc: 0.8578614672364673\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.39688136065617585, Train acc: 0.8587072649572649\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.39965016892354965, Train acc: 0.8573050213675214\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4011638023645563, Train acc: 0.8568672839506173\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4039682286863144, Train acc: 0.8560897435897435\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.40303641909671184, Train acc: 0.8562791375291375\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4011035903948664, Train acc: 0.8569711538461539\n",
      "Val loss: 0.5576388835906982, Val acc: 0.85\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.39115451467342865, Train acc: 0.8616452991452992\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3738066697349915, Train acc: 0.8687232905982906\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.38086963572308546, Train acc: 0.8663639601139601\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.38422549600338834, Train acc: 0.8646501068376068\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.37806063102096577, Train acc: 0.8671474358974359\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.38027601075647904, Train acc: 0.8659188034188035\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.37896474874754676, Train acc: 0.8663385225885226\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3804140530096797, Train acc: 0.8658854166666666\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.37937022452680474, Train acc: 0.8664826685660019\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.37838109583299384, Train acc: 0.8672275641025641\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3785790134432618, Train acc: 0.8666715229215229\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3798944909189247, Train acc: 0.8660746082621082\n",
      "Val loss: 0.5257526636123657, Val acc: 0.844\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3480201604274603, Train acc: 0.8806089743589743\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3506451017008378, Train acc: 0.8816773504273504\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.35749287154005466, Train acc: 0.8783831908831908\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.36213147656148315, Train acc: 0.8757345085470085\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.36047680135975535, Train acc: 0.8753205128205128\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3630670995645204, Train acc: 0.874465811965812\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3635621622605056, Train acc: 0.8745802808302808\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3651323889294623, Train acc: 0.8730301816239316\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.36564749382358447, Train acc: 0.8728632478632479\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3650907350656314, Train acc: 0.8733974358974359\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.36364592798198947, Train acc: 0.8733731546231546\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3635294177964797, Train acc: 0.8732861467236467\n",
      "Val loss: 0.5088050961494446, Val acc: 0.852\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.34688344871641225, Train acc: 0.8816773504273504\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3491224632877061, Train acc: 0.8770032051282052\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3478116017588523, Train acc: 0.8768696581196581\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3477219711097642, Train acc: 0.8781383547008547\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.34900296512577267, Train acc: 0.8765491452991453\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3501392539664891, Train acc: 0.8768251424501424\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.35125351805462796, Train acc: 0.8766407203907204\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.34856990724802017, Train acc: 0.8781717414529915\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.346603174643469, Train acc: 0.8793328584995251\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.34688293820517696, Train acc: 0.879059829059829\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.34796773251536844, Train acc: 0.8784236596736597\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.34796842671081724, Train acc: 0.8786280270655271\n",
      "Val loss: 0.4755786955356598, Val acc: 0.854\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.34824591174594355, Train acc: 0.8795405982905983\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.34040402210293674, Train acc: 0.8812767094017094\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.34266393497968334, Train acc: 0.8798076923076923\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3445303489764531, Train acc: 0.8784722222222222\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.34834932329562995, Train acc: 0.8777777777777778\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3497088990277714, Train acc: 0.8775819088319088\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3491683195052857, Train acc: 0.877251221001221\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.34806365246733284, Train acc: 0.8778712606837606\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.34838862187191066, Train acc: 0.8782348053181387\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.348270518772113, Train acc: 0.8786057692307693\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3467491817166385, Train acc: 0.8790306915306916\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3473113266077222, Train acc: 0.8782941595441596\n",
      "Val loss: 0.4719576835632324, Val acc: 0.858\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3370444270917493, Train acc: 0.8880876068376068\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.336934253318697, Train acc: 0.8867521367521367\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.330411851512231, Train acc: 0.8874643874643875\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.32937657863347447, Train acc: 0.8871527777777778\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3305040855565642, Train acc: 0.8862713675213675\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.32994127936894735, Train acc: 0.8856837606837606\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.33084581038199357, Train acc: 0.8855311355311355\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.32941665280705845, Train acc: 0.8854834401709402\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3306661463304576, Train acc: 0.8847934472934473\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.331465770998317, Train acc: 0.8840010683760684\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.331815129005594, Train acc: 0.8839112276612276\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.33127464367304943, Train acc: 0.8841034544159544\n",
      "Val loss: 0.48667851090431213, Val acc: 0.872\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.30190554560504407, Train acc: 0.8944978632478633\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3012935041298724, Train acc: 0.8958333333333334\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.31174473920863577, Train acc: 0.8898682336182336\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.30575124491165334, Train acc: 0.8924946581196581\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3114570422177641, Train acc: 0.8900106837606837\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3120271565652641, Train acc: 0.8892004985754985\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3102278469109928, Train acc: 0.8898809523809523\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.31113753847093284, Train acc: 0.8902577457264957\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.31149195293486404, Train acc: 0.8898682336182336\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3117842548073102, Train acc: 0.8899305555555556\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3132720966537562, Train acc: 0.8892773892773893\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3141319070626952, Train acc: 0.8892227564102564\n",
      "Val loss: 0.49294283986091614, Val acc: 0.858\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.30782288790513307, Train acc: 0.8915598290598291\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.32342216646314687, Train acc: 0.8874198717948718\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3129824848893361, Train acc: 0.8899572649572649\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3124774154275656, Train acc: 0.8913595085470085\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3130543508590796, Train acc: 0.8905448717948717\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3137486099220409, Train acc: 0.8905359686609686\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.31572681063188013, Train acc: 0.8900717338217338\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3162288907994954, Train acc: 0.8896567841880342\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3127765055367297, Train acc: 0.8904914529914529\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3103035770547696, Train acc: 0.8912927350427351\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3101589407803666, Train acc: 0.8911470473970474\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3094485844525262, Train acc: 0.8913149928774928\n",
      "Val loss: 0.4795759320259094, Val acc: 0.856\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2875715573119302, Train acc: 0.9027777777777778\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2976335843658855, Train acc: 0.8971688034188035\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.30110232330370496, Train acc: 0.89494301994302\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2992109097699579, Train acc: 0.8944978632478633\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2975047970875206, Train acc: 0.8955662393162394\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.29977150364882416, Train acc: 0.8951655982905983\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2970902237436201, Train acc: 0.8965201465201466\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.29703272214063847, Train acc: 0.8958333333333334\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2970267457167591, Train acc: 0.8959817188983855\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2972466717896044, Train acc: 0.8959935897435898\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.29819258930278825, Train acc: 0.8952263014763014\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2978025002012353, Train acc: 0.8952546296296297\n",
      "Val loss: 0.5052544474601746, Val acc: 0.836\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2817648755561592, Train acc: 0.9017094017094017\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.28269888552972394, Train acc: 0.9037126068376068\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.28801121314366657, Train acc: 0.9017094017094017\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2842231449535769, Train acc: 0.9021768162393162\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.28068620945271266, Train acc: 0.9026709401709402\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.28035623684228317, Train acc: 0.9026442307692307\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2805030165383449, Train acc: 0.902281746031746\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2832887752228377, Train acc: 0.9014423076923077\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.28492408362902694, Train acc: 0.9014126305792972\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.28390952440726963, Train acc: 0.9019497863247863\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2858036573666561, Train acc: 0.9013209013209014\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.28616572518771954, Train acc: 0.9011752136752137\n",
      "Val loss: 0.4775547683238983, Val acc: 0.856\n",
      "Early stopping at epoch 18 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 244.2930 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.49684296713935, Train acc: 0.31650641025641024\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.3628688688971038, Train acc: 0.40411324786324787\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.271357062875036, Train acc: 0.4519230769230769\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.2051293990041456, Train acc: 0.48257211538461536\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.161827267337049, Train acc: 0.5034722222222222\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.1196803903138195, Train acc: 0.5274661680911681\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.0817734329109518, Train acc: 0.5475427350427351\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 1.0520541181421688, Train acc: 0.5626001602564102\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 1.0253227910651328, Train acc: 0.5754985754985755\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 1.0027683451134934, Train acc: 0.5873397435897436\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9838169306261927, Train acc: 0.5981449106449106\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.9677191856197822, Train acc: 0.6062589031339032\n",
      "Val loss: 0.6881306171417236, Val acc: 0.748\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.7523996672059736, Train acc: 0.7131410256410257\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.7472004856054599, Train acc: 0.7171474358974359\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.7330965065208935, Train acc: 0.7244480056980057\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.7274489012411517, Train acc: 0.7254941239316239\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.7167082207325177, Train acc: 0.7301282051282051\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.7105072155976907, Train acc: 0.7337072649572649\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.70523230944361, Train acc: 0.7355769230769231\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.7008310292775815, Train acc: 0.7375801282051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6957971438383445, Train acc: 0.738960113960114\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6908116704633094, Train acc: 0.7411057692307692\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6845492670354495, Train acc: 0.7431283993783994\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6800113171339035, Train acc: 0.7445023148148148\n",
      "Val loss: 0.6609342694282532, Val acc: 0.786\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6103377461942852, Train acc: 0.7745726495726496\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6141399506829742, Train acc: 0.7759081196581197\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.6060335987653488, Train acc: 0.7802706552706553\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5958723405805918, Train acc: 0.7831196581196581\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.592162397198188, Train acc: 0.785309829059829\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5947953308706949, Train acc: 0.7843215811965812\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5937869245861913, Train acc: 0.7834630647130647\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5908830346555537, Train acc: 0.7844551282051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5885552528888298, Train acc: 0.7847222222222222\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5865409831460725, Train acc: 0.7853899572649573\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5816292511277543, Train acc: 0.7873203185703186\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5813834962894094, Train acc: 0.7871038105413105\n",
      "Val loss: 0.6532655358314514, Val acc: 0.778\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5343478176838312, Train acc: 0.8154380341880342\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5269802252196858, Train acc: 0.8126335470085471\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.5275296066646222, Train acc: 0.8129451566951567\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.5305829734947437, Train acc: 0.8118322649572649\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5277118224618781, Train acc: 0.8125534188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5260344118601576, Train acc: 0.8112090455840456\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5240895320389588, Train acc: 0.8115460927960928\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5190053429836646, Train acc: 0.8138354700854701\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5172036271347732, Train acc: 0.8142806267806267\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5138893919368076, Train acc: 0.815090811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5111793610205831, Train acc: 0.8167977855477856\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5103614113885283, Train acc: 0.8168847934472935\n",
      "Val loss: 0.5969259142875671, Val acc: 0.836\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4662054337752171, Train acc: 0.8357371794871795\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.488102759879369, Train acc: 0.8294604700854701\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.48903109672402384, Train acc: 0.8285256410256411\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4867286722565818, Train acc: 0.828392094017094\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.48383536021678875, Train acc: 0.8282051282051283\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.47922840203951905, Train acc: 0.8299501424501424\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4769333687330049, Train acc: 0.8298229548229549\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.47471225609541196, Train acc: 0.8302283653846154\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.47271157340531333, Train acc: 0.8314636752136753\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4724071942651883, Train acc: 0.8313301282051282\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4735662546573263, Train acc: 0.831002331002331\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4729031237516223, Train acc: 0.8311075498575499\n",
      "Val loss: 0.5538926124572754, Val acc: 0.838\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4522112265993387, Train acc: 0.8410790598290598\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4614810023106571, Train acc: 0.8396100427350427\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4600576211371992, Train acc: 0.8372507122507122\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4578419836381307, Train acc: 0.8382745726495726\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.45713906108568875, Train acc: 0.8382478632478633\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.45133944676175414, Train acc: 0.8406784188034188\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.44996729752410464, Train acc: 0.8405830280830281\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44986432834536344, Train acc: 0.8404780982905983\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4496890965704112, Train acc: 0.8406635802469136\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.44660576256549256, Train acc: 0.841693376068376\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.44773175732956993, Train acc: 0.8411519036519036\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4478692329220707, Train acc: 0.8415242165242165\n",
      "Val loss: 0.5469294190406799, Val acc: 0.844\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.41034768521785736, Train acc: 0.8571047008547008\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4199953880638648, Train acc: 0.8549679487179487\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4266317012345689, Train acc: 0.8507834757834758\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.440255822486475, Train acc: 0.8448851495726496\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4375701068940326, Train acc: 0.8460470085470085\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4367019738205987, Train acc: 0.8454861111111112\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.43309138333186126, Train acc: 0.8463064713064713\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4302764949030601, Train acc: 0.8476896367521367\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4259490870144519, Train acc: 0.8493886514719848\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4261860686999101, Train acc: 0.8486111111111111\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4232150396070188, Train acc: 0.8502573815073815\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4223174560029707, Train acc: 0.8504496082621082\n",
      "Val loss: 0.5639691352844238, Val acc: 0.836\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4030160366469978, Train acc: 0.8517628205128205\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.40574137108702946, Train acc: 0.8544337606837606\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.40928557209479505, Train acc: 0.8505163817663818\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4058809113394246, Train acc: 0.8514957264957265\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4080867073602147, Train acc: 0.8524572649572649\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4071094589228304, Train acc: 0.8531873219373219\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.4094470533586684, Train acc: 0.8529838217338217\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4098060351008406, Train acc: 0.8535990918803419\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4080151924536212, Train acc: 0.8542260208926875\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.40821707964452925, Train acc: 0.8549679487179487\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4070249595610597, Train acc: 0.8554535742035742\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.40623360116597257, Train acc: 0.8558582621082621\n",
      "Val loss: 0.5312849879264832, Val acc: 0.844\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3946545031080898, Train acc: 0.8592414529914529\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.39837778174979055, Train acc: 0.8545673076923077\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4055203961001502, Train acc: 0.854255698005698\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.40626777692610383, Train acc: 0.8550347222222222\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.40061686599356494, Train acc: 0.8560897435897435\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4031349453024375, Train acc: 0.8550124643874644\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.40365758163442833, Train acc: 0.8554258241758241\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3991067884489894, Train acc: 0.8573050213675214\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3977414638242717, Train acc: 0.8580840455840456\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3939497207792906, Train acc: 0.8595352564102564\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.39135650280796475, Train acc: 0.8603098290598291\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.391027534449542, Train acc: 0.8609998219373219\n",
      "Val loss: 0.5231260657310486, Val acc: 0.814\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3839435616874287, Train acc: 0.8653846153846154\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3756445980606935, Train acc: 0.8660523504273504\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3782834443781111, Train acc: 0.865295584045584\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3789750650111172, Train acc: 0.8658520299145299\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.37969756323812354, Train acc: 0.8659188034188035\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.37911183778674173, Train acc: 0.8664529914529915\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3759716449621542, Train acc: 0.8680555555555556\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.37487506150053096, Train acc: 0.8688234508547008\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.37219695558688354, Train acc: 0.8701626305792972\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.37572779267007467, Train acc: 0.8689636752136752\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.37376593444973993, Train acc: 0.8698523698523698\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3727658572566942, Train acc: 0.8707042378917379\n",
      "Val loss: 0.5061209201812744, Val acc: 0.848\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3787160492223552, Train acc: 0.8704594017094017\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3615768376387592, Train acc: 0.8766025641025641\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.36114437451474685, Train acc: 0.8754451566951567\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.35662501006044894, Train acc: 0.8774706196581197\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3571242441733678, Train acc: 0.875801282051282\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3579715623751155, Train acc: 0.8756232193732194\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.35852317391952754, Train acc: 0.8749618437118437\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3581211470767983, Train acc: 0.8753338675213675\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.35806877922411773, Train acc: 0.8754154795821463\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3570346792538961, Train acc: 0.8756143162393163\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3583135582917683, Train acc: 0.8753156565656566\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.35824753044762164, Train acc: 0.8753338675213675\n",
      "Val loss: 0.5329436659812927, Val acc: 0.814\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3397791098453041, Train acc: 0.8830128205128205\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.34562579889455414, Train acc: 0.8779380341880342\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.34883817682239066, Train acc: 0.8790954415954416\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3540214022510072, Train acc: 0.8774706196581197\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.34981870507327917, Train acc: 0.8775106837606838\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.34941718498101604, Train acc: 0.8774928774928775\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.34752213889425926, Train acc: 0.878968253968254\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.34654348015657854, Train acc: 0.8793736645299145\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.34548219120004353, Train acc: 0.8800154320987654\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3449003151275663, Train acc: 0.8800213675213675\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3455861791991502, Train acc: 0.8796134421134422\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3456798430181976, Train acc: 0.8796518874643875\n",
      "Val loss: 0.5135939717292786, Val acc: 0.854\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.33710784598802906, Train acc: 0.8843482905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.34112975825993425, Train acc: 0.8820779914529915\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3400175258559719, Train acc: 0.8831018518518519\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.34283901303688175, Train acc: 0.8816773504273504\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3505107328295708, Train acc: 0.8785790598290598\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.34537710142229017, Train acc: 0.8802528490028491\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3411059894659289, Train acc: 0.882020757020757\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3381082060961769, Train acc: 0.883346688034188\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.33740868108083494, Train acc: 0.8837547483380817\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.33426644838391206, Train acc: 0.8848290598290598\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3344431606923876, Train acc: 0.8846153846153846\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3342513052953614, Train acc: 0.8845040954415955\n",
      "Val loss: 0.5177702307701111, Val acc: 0.838\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.29099814956768966, Train acc: 0.8950320512820513\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2964938572393014, Train acc: 0.8930288461538461\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3002128230455594, Train acc: 0.8933404558404558\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.30349623224037325, Train acc: 0.8927617521367521\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3051422741295945, Train acc: 0.8912393162393163\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.30679974383395964, Train acc: 0.8905804843304843\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3047884508730873, Train acc: 0.8918650793650794\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.30689364969411975, Train acc: 0.8912927350427351\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3099641702885245, Train acc: 0.8907882241215574\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3101746636100559, Train acc: 0.890892094017094\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.31164986528774613, Train acc: 0.8901272338772339\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3127827361953097, Train acc: 0.8897124287749287\n",
      "Val loss: 0.5103928446769714, Val acc: 0.848\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.30041911631313145, Train acc: 0.8878205128205128\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.30828689239346063, Train acc: 0.8859508547008547\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.30512270494828536, Train acc: 0.8901353276353277\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3065891369190226, Train acc: 0.8906917735042735\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.30865571751044346, Train acc: 0.8896901709401709\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3093409623282078, Train acc: 0.8900462962962963\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.30854809213252293, Train acc: 0.8904914529914529\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.30966559834141505, Train acc: 0.8907585470085471\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3090295457163541, Train acc: 0.8912333808167141\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.30855224600587133, Train acc: 0.8919604700854701\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3078787077157509, Train acc: 0.8920454545454546\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3103642185428246, Train acc: 0.8910256410256411\n",
      "Val loss: 0.4994133710861206, Val acc: 0.822\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.317392387196549, Train acc: 0.8867521367521367\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.30782428374275184, Train acc: 0.890625\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3089158480450978, Train acc: 0.8906695156695157\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3055886003292269, Train acc: 0.8922275641025641\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.30601324225083376, Train acc: 0.8917735042735043\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.30598272922371866, Train acc: 0.8912482193732194\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3028272072692494, Train acc: 0.8928189865689866\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.30170751456967276, Train acc: 0.8939302884615384\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.30131617711338676, Train acc: 0.8939636752136753\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3016197139062943, Train acc: 0.8936965811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2994580263533885, Train acc: 0.8949106449106449\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2981169332784635, Train acc: 0.8956330128205128\n",
      "Val loss: 0.47197434306144714, Val acc: 0.854\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2695115776334563, Train acc: 0.9073183760683761\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2729034938204747, Train acc: 0.9035790598290598\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.27325761333065496, Train acc: 0.9025106837606838\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2793940419697354, Train acc: 0.8996394230769231\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.28488596508390884, Train acc: 0.8983974358974359\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2826868471108441, Train acc: 0.8999732905982906\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.28527614731746426, Train acc: 0.8993437118437119\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2855719511643944, Train acc: 0.8990050747863247\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2841585659884546, Train acc: 0.8998100664767331\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.28499680598958943, Train acc: 0.8998397435897436\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2858405485922918, Train acc: 0.8999125874125874\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.28393380434127613, Train acc: 0.9008413461538461\n",
      "Val loss: 0.4564937353134155, Val acc: 0.848\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2862593346936071, Train acc: 0.9017094017094017\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.28813581948733735, Train acc: 0.8978365384615384\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.2811195578139562, Train acc: 0.8989494301994302\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2813885639079361, Train acc: 0.8998397435897436\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.27903770910114306, Train acc: 0.9021367521367522\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2782771391555285, Train acc: 0.9022435897435898\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2785617790033675, Train acc: 0.9017857142857143\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.27874162760523397, Train acc: 0.9016092414529915\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2786609488419997, Train acc: 0.9017687559354226\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.28048021263546413, Train acc: 0.9012820512820513\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.28177063906943045, Train acc: 0.901029526029526\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.28125000585061616, Train acc: 0.9011974715099715\n",
      "Val loss: 0.508945882320404, Val acc: 0.842\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.25832254847145486, Train acc: 0.9081196581196581\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2666264635980384, Train acc: 0.9077190170940171\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.2690227252784108, Train acc: 0.905982905982906\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.26215469179690903, Train acc: 0.9083867521367521\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2632810085120364, Train acc: 0.9080662393162393\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.26246167584043795, Train acc: 0.9078525641025641\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.26094538842183473, Train acc: 0.9088064713064713\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.26240912795815075, Train acc: 0.9081864316239316\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2615351570382757, Train acc: 0.9091880341880342\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.26123033817379904, Train acc: 0.9091346153846154\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2644215701806425, Train acc: 0.9081196581196581\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2643145606707134, Train acc: 0.9081864316239316\n",
      "Val loss: 0.41905367374420166, Val acc: 0.862\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.24816096995949236, Train acc: 0.9131944444444444\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.25228399901977205, Train acc: 0.9118589743589743\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.2571903541312683, Train acc: 0.9107905982905983\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.252700997544373, Train acc: 0.9124599358974359\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.25063174069246164, Train acc: 0.913034188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2526212009625175, Train acc: 0.911369301994302\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.25312379151537234, Train acc: 0.911553724053724\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2522785581318009, Train acc: 0.9118923611111112\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.25227093941939804, Train acc: 0.9116809116809117\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2562528324735343, Train acc: 0.9100961538461538\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.25545961255093325, Train acc: 0.9103778166278166\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.25567416837332807, Train acc: 0.9101451210826211\n",
      "Val loss: 0.4580599069595337, Val acc: 0.846\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2405692836922458, Train acc: 0.9134615384615384\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.24572018414544752, Train acc: 0.9119925213675214\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.2527776284838504, Train acc: 0.9115918803418803\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2509209707570382, Train acc: 0.913261217948718\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.25436035519481726, Train acc: 0.9117521367521367\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2537356010157923, Train acc: 0.9123486467236467\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.25551913020465983, Train acc: 0.9115918803418803\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2536658830701923, Train acc: 0.9121928418803419\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.25374494038401957, Train acc: 0.9125712250712251\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2533053825012384, Train acc: 0.9123931623931624\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.24888804290377084, Train acc: 0.9136072261072261\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.24937816903371884, Train acc: 0.9134170227920227\n",
      "Val loss: 0.4292364716529846, Val acc: 0.852\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2320515773872025, Train acc: 0.9182692307692307\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.23195055345248464, Train acc: 0.9197382478632479\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.24172680306001607, Train acc: 0.9150641025641025\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.24180924671932927, Train acc: 0.9157986111111112\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.24210365899225586, Train acc: 0.9159188034188034\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.24380021482345215, Train acc: 0.9153311965811965\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.24545557339822416, Train acc: 0.9151404151404151\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.24791637655650067, Train acc: 0.9140291132478633\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.24521786985602248, Train acc: 0.9150641025641025\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.24619397261203863, Train acc: 0.9141559829059829\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.24452868857722737, Train acc: 0.9141171328671329\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.24490442438226226, Train acc: 0.9142628205128205\n",
      "Val loss: 0.4658139944076538, Val acc: 0.844\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.21946414951712656, Train acc: 0.9268162393162394\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2317278025369359, Train acc: 0.9206730769230769\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.23022706987617533, Train acc: 0.9201388888888888\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.22975501232247195, Train acc: 0.9193376068376068\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2323005375221499, Train acc: 0.9189102564102564\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.232846051430664, Train acc: 0.9189814814814815\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2361615592174878, Train acc: 0.9172008547008547\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2375856678491124, Train acc: 0.9168336004273504\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.23800371617878321, Train acc: 0.9168744064577398\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.23726438295860322, Train acc: 0.9176014957264957\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2371815936603723, Train acc: 0.9170308857808858\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2368135589912555, Train acc: 0.9169337606837606\n",
      "Val loss: 0.4306529760360718, Val acc: 0.862\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.24072296624509698, Train acc: 0.9163995726495726\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.23248693229168907, Train acc: 0.9170673076923077\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.23206321051360196, Train acc: 0.9174679487179487\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2284984768041943, Train acc: 0.9184695512820513\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.23196137681858153, Train acc: 0.9179487179487179\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.23173878556568483, Train acc: 0.9177350427350427\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2311124822127113, Train acc: 0.9184218559218559\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.23197258547004154, Train acc: 0.9182692307692307\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2329624776992901, Train acc: 0.9185363247863247\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2327426088703239, Train acc: 0.9186965811965812\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2320091105236295, Train acc: 0.9190462315462316\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.23348219711678447, Train acc: 0.9184472934472935\n",
      "Val loss: 0.5113967061042786, Val acc: 0.834\n",
      "Early stopping at epoch 24 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 312.4859 [s]\n",
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 1.4685939147941067, Train acc: 0.312232905982906\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 1.3079294941873632, Train acc: 0.3997061965811966\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 1.2044347867666827, Train acc: 0.45521723646723644\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 1.1359496996698217, Train acc: 0.499866452991453\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 1.0814214647325695, Train acc: 0.5313034188034188\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 1.0439847105758482, Train acc: 0.5534188034188035\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 1.007810954136435, Train acc: 0.5721153846153846\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.9813701292006378, Train acc: 0.5868723290598291\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.9549877401317514, Train acc: 0.6012879867046533\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.9338926199154977, Train acc: 0.6126869658119658\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.9142646091616052, Train acc: 0.6228146853146853\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.8977564329605157, Train acc: 0.6305867165242165\n",
      "Val loss: 0.6533573865890503, Val acc: 0.748\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.6533862250482935, Train acc: 0.7513354700854701\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.6580423878935667, Train acc: 0.7514690170940171\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.657467571121675, Train acc: 0.749465811965812\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.6588978143647696, Train acc: 0.7498664529914529\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.651741041816198, Train acc: 0.7529380341880342\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.6434275725076342, Train acc: 0.7561876780626781\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.6387117316580226, Train acc: 0.7588141025641025\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.6363781486184169, Train acc: 0.7593816773504274\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.6290881323706843, Train acc: 0.7625534188034188\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.6247435339877748, Train acc: 0.76383547008547\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.6227706518739161, Train acc: 0.7643016705516705\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.6199426367942594, Train acc: 0.7659811253561254\n",
      "Val loss: 0.5621210932731628, Val acc: 0.812\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.5559996143620238, Train acc: 0.7879273504273504\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.5580199408607606, Train acc: 0.7904647435897436\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.553024211033457, Train acc: 0.7919337606837606\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.552219652180743, Train acc: 0.7950053418803419\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.5488808508866873, Train acc: 0.7968482905982905\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.5459656745747283, Train acc: 0.7981214387464387\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.5425877446292812, Train acc: 0.7995650183150184\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.5405288948717281, Train acc: 0.8008814102564102\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.5368993542557768, Train acc: 0.8019052706552706\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.5334520173251119, Train acc: 0.8030181623931624\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.5304253269117195, Train acc: 0.804438616938617\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.5285579929727944, Train acc: 0.8055332977207977\n",
      "Val loss: 0.5061435699462891, Val acc: 0.854\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.4938445224975928, Train acc: 0.8202457264957265\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.4957249969498724, Train acc: 0.8177083333333334\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.49252056077844397, Train acc: 0.8172186609686609\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4860916862375716, Train acc: 0.8213808760683761\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4843941435090497, Train acc: 0.8213141025641025\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.48455977592712796, Train acc: 0.8214031339031339\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.48664346435567835, Train acc: 0.8211614774114774\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4897417537231221, Train acc: 0.8198450854700855\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4874276222072096, Train acc: 0.820809591642925\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4857648600243096, Train acc: 0.8224626068376069\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.48246133958496784, Train acc: 0.8229895104895105\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4820585628690203, Train acc: 0.8230724715099715\n",
      "Val loss: 0.5639979839324951, Val acc: 0.83\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.45803226836216754, Train acc: 0.8336004273504274\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.45742964712727785, Train acc: 0.8346688034188035\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.4532129191859835, Train acc: 0.8370726495726496\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.44656057267362237, Train acc: 0.8405448717948718\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.45326448172076134, Train acc: 0.837767094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4514341186678987, Train acc: 0.8392984330484331\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.44881322019266123, Train acc: 0.8389041514041514\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.44950253412159336, Train acc: 0.8391426282051282\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4488248334665131, Train acc: 0.839357787274454\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4473474539561659, Train acc: 0.8403311965811966\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.446364320189386, Train acc: 0.8410304972804973\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4472233955177911, Train acc: 0.8403890669515669\n",
      "Val loss: 0.5251083970069885, Val acc: 0.85\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.41646536248616683, Train acc: 0.8509615384615384\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.42518636975915003, Train acc: 0.8464209401709402\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42206343704903565, Train acc: 0.8513176638176638\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.42316275218931526, Train acc: 0.8511618589743589\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.4239127222162027, Train acc: 0.8506944444444444\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4271950722483658, Train acc: 0.8490473646723646\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.42794022492618644, Train acc: 0.8496642246642246\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4265147037485726, Train acc: 0.8496260683760684\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.42580741761360186, Train acc: 0.8492402659069326\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4271493475024517, Train acc: 0.8492521367521367\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4261110784614744, Train acc: 0.8496746309246309\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.4246635274308869, Train acc: 0.8492031695156695\n",
      "Val loss: 0.489078551530838, Val acc: 0.856\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.40928663186028474, Train acc: 0.8525641025641025\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.42008463818675434, Train acc: 0.8510950854700855\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.42266343179506455, Train acc: 0.8495370370370371\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.4204334561418519, Train acc: 0.8501602564102564\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.41309340322374277, Train acc: 0.8531517094017094\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.4125195691452237, Train acc: 0.854255698005698\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.41295662885292983, Train acc: 0.8540140415140415\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.4109832051520546, Train acc: 0.8539997329059829\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.4067016886679875, Train acc: 0.8554724596391263\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.4077784530627422, Train acc: 0.855849358974359\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.4058820112567126, Train acc: 0.8568376068376068\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.40538484692318827, Train acc: 0.8570824430199431\n",
      "Val loss: 0.5024532079696655, Val acc: 0.864\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.37639498073830563, Train acc: 0.8653846153846154\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.37595562286611295, Train acc: 0.8649839743589743\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.38196802489523196, Train acc: 0.863960113960114\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.38090060103652823, Train acc: 0.8640491452991453\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3782211369938321, Train acc: 0.8645299145299146\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.37713048588006925, Train acc: 0.8653846153846154\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3763390693799917, Train acc: 0.865995115995116\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.37575024235038423, Train acc: 0.8658520299145299\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.37592544373397246, Train acc: 0.8659781576448243\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3775628130723778, Train acc: 0.8653846153846154\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.37840437859574677, Train acc: 0.865433177933178\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3819868172495029, Train acc: 0.8638488247863247\n",
      "Val loss: 0.4925660192966461, Val acc: 0.852\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.37593927571916175, Train acc: 0.8677884615384616\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3738001919327638, Train acc: 0.8699252136752137\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3772654544636395, Train acc: 0.8679665242165242\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3772833500948981, Train acc: 0.8672542735042735\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3756116119969604, Train acc: 0.86875\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.37684970461575057, Train acc: 0.8683671652421653\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3703637974443635, Train acc: 0.8706501831501832\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3699164845260322, Train acc: 0.8709268162393162\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.368485147409417, Train acc: 0.8719432573599241\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3698154780297325, Train acc: 0.8709134615384615\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3691852782297917, Train acc: 0.8711149961149961\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.36909105366711475, Train acc: 0.8709045584045584\n",
      "Val loss: 0.4889732301235199, Val acc: 0.858\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.33826083400183254, Train acc: 0.8774038461538461\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.34651690831360143, Train acc: 0.8775373931623932\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3414898525042581, Train acc: 0.8784722222222222\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.33580510418575543, Train acc: 0.8802751068376068\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3396573667979648, Train acc: 0.8796474358974359\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3404596125298416, Train acc: 0.8791844729344729\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.3427129578714114, Train acc: 0.8790064102564102\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.3451190795510625, Train acc: 0.8780381944444444\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.34826252378120043, Train acc: 0.8769883665716999\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.34795891682689006, Train acc: 0.8776442307692308\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.34808603280514083, Train acc: 0.8778166278166278\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.34690901590909207, Train acc: 0.8784054487179487\n",
      "Val loss: 0.5662875175476074, Val acc: 0.83\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3436779401495925, Train acc: 0.8795405982905983\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3370321919329656, Train acc: 0.8816773504273504\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3320159895977064, Train acc: 0.8823896011396012\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3364591468284782, Train acc: 0.8812099358974359\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.3330000073609189, Train acc: 0.8833867521367521\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.3320865077722786, Train acc: 0.8831908831908832\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.33314600498648644, Train acc: 0.8828601953601953\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.33502899720452917, Train acc: 0.8823450854700855\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.33632131039402874, Train acc: 0.8820928300094967\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3380608198002108, Train acc: 0.8815438034188035\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.33867792674306296, Train acc: 0.8813374125874126\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.3381851591366181, Train acc: 0.8815883190883191\n",
      "Val loss: 0.45535439252853394, Val acc: 0.874\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.3112201153213142, Train acc: 0.8942307692307693\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.3162546358111068, Train acc: 0.8910256410256411\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.31649381231166357, Train acc: 0.8913817663817664\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3182067355602725, Train acc: 0.8900240384615384\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.31929826833880864, Train acc: 0.8888888888888888\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.32122304876921354, Train acc: 0.8880430911680912\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.32208542126857465, Train acc: 0.8880494505494505\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.32325996173959637, Train acc: 0.8870526175213675\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.32218116346007286, Train acc: 0.8875830959164293\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.3244764866641699, Train acc: 0.8863247863247863\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.32498090466153, Train acc: 0.8861693861693861\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.32609566586341526, Train acc: 0.8854166666666666\n",
      "Val loss: 0.48361894488334656, Val acc: 0.88\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.32777271811396647, Train acc: 0.8846153846153846\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.31725244470832187, Train acc: 0.8872863247863247\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.3174673545033674, Train acc: 0.8879095441595442\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.3136612059086816, Train acc: 0.8892895299145299\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.31557655637590293, Train acc: 0.8882478632478632\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.31834275865571793, Train acc: 0.8873753561253561\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.31728339357181057, Train acc: 0.8882402319902319\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.31556791805813456, Train acc: 0.8885550213675214\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.3147221649205356, Train acc: 0.8889779202279202\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.31376943390211487, Train acc: 0.8891826923076923\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.3146406068830594, Train acc: 0.8885975135975136\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.31215218558610336, Train acc: 0.8895121082621082\n",
      "Val loss: 0.4579063653945923, Val acc: 0.868\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.27791615976737094, Train acc: 0.9083867521367521\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2759205592939487, Train acc: 0.906784188034188\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.28593336546692755, Train acc: 0.9009081196581197\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.28697520970470375, Train acc: 0.8991052350427351\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.28995442372611446, Train acc: 0.8986111111111111\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2899242442239214, Train acc: 0.8992165242165242\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.29127623984119394, Train acc: 0.8983134920634921\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2944495667122368, Train acc: 0.8973357371794872\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.29532531202405493, Train acc: 0.8964565527065527\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2948187021745576, Train acc: 0.8964209401709402\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.29305469320845307, Train acc: 0.897047397047397\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.293489133580946, Train acc: 0.8970352564102564\n",
      "Val loss: 0.44976046681404114, Val acc: 0.874\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2841225661592096, Train acc: 0.8971688034188035\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2889976749976731, Train acc: 0.8946314102564102\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.289562176679529, Train acc: 0.8971688034188035\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.29086463816432107, Train acc: 0.8968349358974359\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2909140632256993, Train acc: 0.8966346153846154\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.28796980225420066, Train acc: 0.8984597578347578\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.2884530055726695, Train acc: 0.8981990231990232\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2865240717004252, Train acc: 0.8987713675213675\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2837165412470939, Train acc: 0.8999881291547959\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2848127282168875, Train acc: 0.8997061965811965\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.284567832197088, Train acc: 0.8999368686868687\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.28335167508190257, Train acc: 0.9002849002849003\n",
      "Val loss: 0.46719875931739807, Val acc: 0.866\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2857633509442338, Train acc: 0.9006410256410257\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.2828606433975391, Train acc: 0.9021100427350427\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.2824011518489941, Train acc: 0.9018874643874644\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2812868434872128, Train acc: 0.9017094017094017\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2754603924277501, Train acc: 0.9034188034188034\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.27434962136708096, Train acc: 0.9037571225071225\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.27482514388140594, Train acc: 0.9033501221001221\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.27503994595195747, Train acc: 0.9037793803418803\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2774449162475291, Train acc: 0.9025106837606838\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2778312512251556, Train acc: 0.9025106837606838\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2785798508121047, Train acc: 0.9019036519036518\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2769289785714825, Train acc: 0.9023771367521367\n",
      "Val loss: 0.4990864396095276, Val acc: 0.842\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.26845061887278515, Train acc: 0.9038461538461539\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.27437108664367443, Train acc: 0.9003739316239316\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.26790625324998146, Train acc: 0.9047364672364673\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.27171195776034623, Train acc: 0.9027777777777778\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.2681145934467642, Train acc: 0.9047008547008547\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2711172412624556, Train acc: 0.9043803418803419\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.27087762608887656, Train acc: 0.9042277167277167\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2695129780600277, Train acc: 0.9044805021367521\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2694785964462096, Train acc: 0.9046771130104464\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2735953260849938, Train acc: 0.903659188034188\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2726988648241726, Train acc: 0.9043317793317793\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2715604245487569, Train acc: 0.9047587250712251\n",
      "Val loss: 0.4265355169773102, Val acc: 0.87\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2520464062690735, Train acc: 0.9094551282051282\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.24987247719978675, Train acc: 0.9107905982905983\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.24855122562402335, Train acc: 0.9110576923076923\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2521672715456822, Train acc: 0.9113915598290598\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.25507680832320806, Train acc: 0.9090811965811966\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.25856760294943454, Train acc: 0.9080751424501424\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.25772436794180137, Train acc: 0.9088064713064713\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2596181238340771, Train acc: 0.9078525641025641\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.26127053092517744, Train acc: 0.9076745014245015\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.26174136977324375, Train acc: 0.9075053418803419\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2621264948112356, Train acc: 0.9071969696969697\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.2618425897391582, Train acc: 0.9072516025641025\n",
      "Val loss: 0.46550923585891724, Val acc: 0.87\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2599019227533514, Train acc: 0.9089209401709402\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.24466263664424673, Train acc: 0.9143963675213675\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.24635032934277315, Train acc: 0.9149750712250713\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.24494626429170752, Train acc: 0.9143963675213675\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.24564041329436323, Train acc: 0.9142628205128205\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2490807845843611, Train acc: 0.9124821937321937\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.251137060594264, Train acc: 0.9120879120879121\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2521971331682438, Train acc: 0.9118589743589743\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.2523348952103176, Train acc: 0.9112357549857549\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.25229701818022715, Train acc: 0.9111645299145299\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2516767959318123, Train acc: 0.911470473970474\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.25339066778890723, Train acc: 0.910835113960114\n",
      "Val loss: 0.4868047535419464, Val acc: 0.862\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2444109124187221, Train acc: 0.9169337606837606\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.23894411134414184, Train acc: 0.9159989316239316\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.23880689611334746, Train acc: 0.9155982905982906\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.23903229226891556, Train acc: 0.9158653846153846\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.23731422138392416, Train acc: 0.9159722222222222\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.2396887167173828, Train acc: 0.9151531339031339\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.23935941103231775, Train acc: 0.9151022588522588\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.23872351843035883, Train acc: 0.9151308760683761\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.23852511196054965, Train acc: 0.9153311965811965\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.24207106086178723, Train acc: 0.9141025641025641\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2422478874709686, Train acc: 0.9142628205128205\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.24334829364056856, Train acc: 0.9135950854700855\n",
      "Val loss: 0.48310527205467224, Val acc: 0.878\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.2370337223650044, Train acc: 0.9182692307692307\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.23394450166413927, Train acc: 0.9189369658119658\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.23968060633056185, Train acc: 0.9156873219373219\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2335111020037379, Train acc: 0.9174679487179487\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.23406684453416074, Train acc: 0.9173076923076923\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.23273569273834044, Train acc: 0.9174679487179487\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.23550807719684055, Train acc: 0.9170100732600732\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.2365662701165256, Train acc: 0.9163661858974359\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.23471872015358264, Train acc: 0.9169337606837606\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2368577152172215, Train acc: 0.9158653846153846\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.23579629677946287, Train acc: 0.9162296037296037\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.23429356790028322, Train acc: 0.9169337606837606\n",
      "Val loss: 0.5181419849395752, Val acc: 0.852\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/1515 - Train loss: 0.245600150914019, Train acc: 0.9121260683760684\n",
      "Iteration 234 - Batch 234/1515 - Train loss: 0.24118438132234618, Train acc: 0.9114583333333334\n",
      "Iteration 351 - Batch 351/1515 - Train loss: 0.24334586735422115, Train acc: 0.9120370370370371\n",
      "Iteration 468 - Batch 468/1515 - Train loss: 0.2386358016385482, Train acc: 0.9133279914529915\n",
      "Iteration 585 - Batch 585/1515 - Train loss: 0.24156334460036366, Train acc: 0.9123931623931624\n",
      "Iteration 702 - Batch 702/1515 - Train loss: 0.23784179955009807, Train acc: 0.9137731481481481\n",
      "Iteration 819 - Batch 819/1515 - Train loss: 0.23432692583372888, Train acc: 0.9158272283272283\n",
      "Iteration 936 - Batch 936/1515 - Train loss: 0.234018941095465, Train acc: 0.9167334401709402\n",
      "Iteration 1053 - Batch 1053/1515 - Train loss: 0.23536361803823386, Train acc: 0.9167853751187085\n",
      "Iteration 1170 - Batch 1170/1515 - Train loss: 0.2354754831132471, Train acc: 0.9168002136752137\n",
      "Iteration 1287 - Batch 1287/1515 - Train loss: 0.2362723009170213, Train acc: 0.9165938228438228\n",
      "Iteration 1404 - Batch 1404/1515 - Train loss: 0.23690654678360434, Train acc: 0.9160657051282052\n",
      "Val loss: 0.4519503712654114, Val acc: 0.886\n",
      "Early stopping at epoch 22 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 297.5899 [s]\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\milan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 117 - Batch 117/4545 - Train loss: 1.612349682384067, Train acc: 0.2013888888888889\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 1.6105858861890614, Train acc: 0.19951923076923078\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 1.587591045942062, Train acc: 0.23326210826210828\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 1.5582270214700291, Train acc: 0.2609508547008547\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 1.531363909264915, Train acc: 0.28920940170940174\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 1.507793965332868, Train acc: 0.31285612535612534\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 1.4871652124128936, Train acc: 0.3340201465201465\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 1.4656940349656293, Train acc: 0.3504273504273504\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 1.4470795101476417, Train acc: 0.36651234567901236\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 1.4251125060594998, Train acc: 0.38087606837606836\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 1.406953504591277, Train acc: 0.39223970473970476\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 1.3920721044058133, Train acc: 0.4030448717948718\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 1.3753973450964803, Train acc: 0.4137491781722551\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 1.3621281271889096, Train acc: 0.4248321123321123\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 1.3481132090940775, Train acc: 0.4338319088319088\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 1.3362020614246528, Train acc: 0.44137286324786323\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 1.3258900472181654, Train acc: 0.4487807943690297\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 1.315209124304517, Train acc: 0.4551282051282051\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 1.3030237354408551, Train acc: 0.46165092217723797\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 1.290520631095283, Train acc: 0.46818910256410257\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 1.2787921499759627, Train acc: 0.47402828652828655\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 1.266697638796204, Train acc: 0.479458041958042\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 1.2565264756885084, Train acc: 0.48494983277591974\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 1.2464434596102292, Train acc: 0.4899394586894587\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 1.2365290623546665, Train acc: 0.4951068376068376\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 1.2274827421358117, Train acc: 0.49979454306377386\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 1.219908303260124, Train acc: 0.503858024691358\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 1.211431972354978, Train acc: 0.5084134615384616\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 1.2030428349269093, Train acc: 0.5123231653404067\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 1.1956795219342593, Train acc: 0.5157407407407407\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 1.1883761835617568, Train acc: 0.51895505927764\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 1.1813135000789523, Train acc: 0.5222522702991453\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 1.17393380349487, Train acc: 0.5254467754467754\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 1.16676360370227, Train acc: 0.5289089994972348\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 1.1605841467086504, Train acc: 0.532051282051282\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 1.1545056333342623, Train acc: 0.5350338319088319\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 1.1484928736650357, Train acc: 0.5379273504273504\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 1.142860968257466, Train acc: 0.5408794421952317\n",
      "Val loss: 0.8225433826446533, Val acc: 0.676\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.9289973663468646, Train acc: 0.6426282051282052\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.9390706735798436, Train acc: 0.6319444444444444\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.9199056973484506, Train acc: 0.6429843304843305\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.9115030280290506, Train acc: 0.6469017094017094\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.9095272263910017, Train acc: 0.6473290598290599\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.9130687961551199, Train acc: 0.6461004273504274\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.911221123294807, Train acc: 0.6472069597069597\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.9059578015699855, Train acc: 0.6478365384615384\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.9073251594222396, Train acc: 0.6485636277302944\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.9061976920081, Train acc: 0.6503739316239316\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.9037659255774705, Train acc: 0.6509324009324009\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.8998670763524509, Train acc: 0.6525997150997151\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.8971034196706918, Train acc: 0.6532708744247205\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.897387350708137, Train acc: 0.6534264346764347\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.8966255497728658, Train acc: 0.653988603988604\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.8971301999866453, Train acc: 0.6536124465811965\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.8959322570256099, Train acc: 0.6535005027652087\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.8926720773732221, Train acc: 0.6546771130104464\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.8924591879532565, Train acc: 0.6551675663517769\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.8910106152678148, Train acc: 0.655982905982906\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.8891394142702822, Train acc: 0.6561609686609686\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.8855953740814256, Train acc: 0.6577311577311578\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.8826664414316339, Train acc: 0.6593970642883686\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.8829604810650362, Train acc: 0.6601673789173789\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.882355504372181, Train acc: 0.6611752136752137\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.880863641738343, Train acc: 0.6625986193293886\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.879707058148129, Train acc: 0.663144982589427\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.8774810025652686, Train acc: 0.663804945054945\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.8763899130347623, Train acc: 0.6641246684350133\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.8757004645390388, Train acc: 0.6642450142450143\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.8737667551259545, Train acc: 0.6651674937965261\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.8724435371641293, Train acc: 0.6659488514957265\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.8714276666438694, Train acc: 0.6663752913752914\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.8698943616188369, Train acc: 0.667090874811463\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.8685860376873296, Train acc: 0.6678113553113553\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.8674943061423438, Train acc: 0.6685066476733144\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.8656625259305346, Train acc: 0.6693231693231694\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.8630120831395504, Train acc: 0.6703356950067476\n",
      "Val loss: 0.7103661298751831, Val acc: 0.718\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.7859511059573573, Train acc: 0.7045940170940171\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.80916851071211, Train acc: 0.6931089743589743\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.7999311255933212, Train acc: 0.6994301994301995\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.8043109721098191, Train acc: 0.6969818376068376\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.7981600789432852, Train acc: 0.6991452991452991\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.7970511695088824, Train acc: 0.7000534188034188\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.8013351464300656, Train acc: 0.6972680097680097\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.7967958844497672, Train acc: 0.6980502136752137\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.7962470103589445, Train acc: 0.6983618233618234\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.7945437931606912, Train acc: 0.6993589743589743\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.7926625272240302, Train acc: 0.6996891996891997\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.7930524810469388, Train acc: 0.7017004985754985\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.7916569007140567, Train acc: 0.7025394477317555\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.7907502288601483, Train acc: 0.7033730158730159\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.789341461981124, Train acc: 0.7041666666666667\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.7899577192739289, Train acc: 0.703926282051282\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.7880738407538248, Train acc: 0.7051910507792861\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.7884037997874326, Train acc: 0.7056327160493827\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.7873222570107211, Train acc: 0.7066183085919928\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.7860141846741366, Train acc: 0.7069978632478633\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.7848277328021166, Train acc: 0.7068325193325193\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.7827981858866662, Train acc: 0.7078719891219891\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.7814201365523691, Train acc: 0.7084494611668525\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.7814342544926198, Train acc: 0.7086449430199431\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.7800284493377067, Train acc: 0.7092094017094017\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.7790073534421118, Train acc: 0.709689349112426\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.7774209388829213, Train acc: 0.7101139601139601\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.7761343703838349, Train acc: 0.7108707264957265\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.7743548498703883, Train acc: 0.711262157382847\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.7741774035654856, Train acc: 0.7109508547008547\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.7731600612738851, Train acc: 0.7112110559691205\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.7721725864511015, Train acc: 0.711471688034188\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.7717012846000576, Train acc: 0.7122021497021497\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.7697958028034807, Train acc: 0.71276395173454\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.7689034408265418, Train acc: 0.7130799755799756\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.7674858695053534, Train acc: 0.7140313390313391\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.7668450840228863, Train acc: 0.7143537768537769\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.7656034332739036, Train acc: 0.7147014170040485\n",
      "Val loss: 0.6499014496803284, Val acc: 0.748\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.726027214119577, Train acc: 0.7313034188034188\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.7274726063777239, Train acc: 0.7329059829059829\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.7296771349390687, Train acc: 0.7268518518518519\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.7309075630882866, Train acc: 0.7266292735042735\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.7268878758463085, Train acc: 0.7287393162393162\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.7249188177563526, Train acc: 0.729522792022792\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.7239729051224624, Train acc: 0.7301587301587301\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.7226021505542036, Train acc: 0.7315037393162394\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.7218379834127335, Train acc: 0.732846628679962\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.7196501024640524, Train acc: 0.7337606837606837\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.7177915512334495, Train acc: 0.73494560994561\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.712214404945275, Train acc: 0.7377581908831908\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.7100327062046426, Train acc: 0.7382889546351085\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.709123628489191, Train acc: 0.7386294261294262\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.7072470086846936, Train acc: 0.7388888888888889\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.7074114549188659, Train acc: 0.7384147970085471\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.7074922873648404, Train acc: 0.7379336349924586\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.703670100346986, Train acc: 0.7392568850902185\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.7029248583748487, Train acc: 0.740019118308592\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.702414384465187, Train acc: 0.7400106837606838\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.7029737726272389, Train acc: 0.7395197395197395\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.7016850077628904, Train acc: 0.740263209013209\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.6998592456813109, Train acc: 0.7405007432181345\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.698617028762345, Train acc: 0.7410078347578347\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.698581346258139, Train acc: 0.7410042735042736\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.6994862031194037, Train acc: 0.7406106180144642\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.6992120225478969, Train acc: 0.7408990186767964\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.6981475987359063, Train acc: 0.7416819291819292\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.698242791535016, Train acc: 0.7420055997642204\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.6973922870055562, Train acc: 0.742289886039886\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.6973675974668601, Train acc: 0.7423490488006617\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.6972687363775814, Train acc: 0.7424045138888888\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.6965385716841709, Train acc: 0.7427803677803678\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.6970796816691553, Train acc: 0.7427256158873806\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.695612252671931, Train acc: 0.7432234432234432\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.6951497006995103, Train acc: 0.7433671652421653\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.6942374916998061, Train acc: 0.7437629937629938\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.6934427315762222, Train acc: 0.7440536437246964\n",
      "Val loss: 0.611861526966095, Val acc: 0.76\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.6799800897765363, Train acc: 0.7462606837606838\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.6558266403073938, Train acc: 0.7620192307692307\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.6581225846058283, Train acc: 0.7628205128205128\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.6608979434857511, Train acc: 0.7598824786324786\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.6583792488544415, Train acc: 0.7611111111111111\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.6575092605163908, Train acc: 0.7581018518518519\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.6527077612968591, Train acc: 0.7605311355311355\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.6510295266301459, Train acc: 0.7611511752136753\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.6546500333410609, Train acc: 0.7594966761633428\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.6537900598512756, Train acc: 0.7604166666666666\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.6527662184711751, Train acc: 0.760440947940948\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.6522386780385449, Train acc: 0.7612624643874644\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.6514013651697201, Train acc: 0.7621219592373438\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.6509938577647174, Train acc: 0.7617139804639804\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.6531446525089422, Train acc: 0.7609330484330484\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.653351500033377, Train acc: 0.7610176282051282\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.6543655237281305, Train acc: 0.7605580693815988\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.6543469534229915, Train acc: 0.7605650522317189\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.6544360941492969, Train acc: 0.7603744939271255\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.6530777150940182, Train acc: 0.7606837606837606\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.6527560872788055, Train acc: 0.7603785103785103\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.6525337309710444, Train acc: 0.7606351981351981\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.652471006032881, Train acc: 0.7606140839836492\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.6519369974233571, Train acc: 0.7608173076923077\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.6512992134206316, Train acc: 0.7608760683760684\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.6510220166396186, Train acc: 0.7609508547008547\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.6505877248619734, Train acc: 0.7612773029439696\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.6501908606834828, Train acc: 0.7618093711843712\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.6483165737743263, Train acc: 0.7626363100501031\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.6483672660682616, Train acc: 0.7627314814814815\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.6470948716588082, Train acc: 0.7634236283429832\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.6471081638398269, Train acc: 0.7631543803418803\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.647042401160054, Train acc: 0.7629985754985755\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.6468345568505527, Train acc: 0.7631347410759175\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.6457174250931094, Train acc: 0.7632936507936507\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.6455080293734529, Train acc: 0.7633843779677113\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.6460379351091374, Train acc: 0.763527951027951\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.645490408738052, Train acc: 0.7636920827710302\n",
      "Val loss: 0.5878720283508301, Val acc: 0.77\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.6156675098543494, Train acc: 0.7735042735042735\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.6152078580652547, Train acc: 0.7719017094017094\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.6223959728353723, Train acc: 0.7720797720797721\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.6212752463662217, Train acc: 0.7741720085470085\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.6189254134129255, Train acc: 0.7767094017094017\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.6215194182900282, Train acc: 0.7760861823361823\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.6174665947282125, Train acc: 0.7773199023199023\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.6183290340038191, Train acc: 0.7759081196581197\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.6169816436845692, Train acc: 0.7766500474833808\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.617850201876245, Train acc: 0.7768696581196581\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.6179177327357991, Train acc: 0.7752039627039627\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.6197864047312669, Train acc: 0.7727475071225072\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.6197106209066179, Train acc: 0.7726002629848784\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.6202397498601231, Train acc: 0.7721688034188035\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.6197034697417181, Train acc: 0.7717236467236467\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.617550920161745, Train acc: 0.7726696047008547\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.6174605984134013, Train acc: 0.7728758169934641\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.616026745958772, Train acc: 0.7738603988603988\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.6173714775654093, Train acc: 0.7732231219073324\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.6182222522476799, Train acc: 0.7729166666666667\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.6170887130278128, Train acc: 0.7732753357753358\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.6160342392005843, Train acc: 0.7742812742812742\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.6155332258834311, Train acc: 0.7743403939056113\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.6124829271081251, Train acc: 0.7757523148148148\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.612762839941897, Train acc: 0.7757905982905983\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.6117044699982661, Train acc: 0.7762368507560815\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.6130624327050352, Train acc: 0.7753640392529282\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.6121505418398915, Train acc: 0.7761179792429792\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.6131807227591269, Train acc: 0.7753463012083702\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.6135448792699905, Train acc: 0.7752492877492877\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.6127962726028392, Train acc: 0.7755893300248139\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.6128658178823594, Train acc: 0.7759081196581197\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.6124566306790148, Train acc: 0.7764504014504015\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.6112800796051281, Train acc: 0.7768193815987934\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.6109738059951417, Train acc: 0.7768009768009768\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.6100967471405566, Train acc: 0.7769171415004749\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.6092096391861338, Train acc: 0.7771136521136521\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.6091274205298639, Train acc: 0.7771733018443545\n",
      "Val loss: 0.5576044917106628, Val acc: 0.786\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.6077669504870716, Train acc: 0.7729700854700855\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5863824042244854, Train acc: 0.7836538461538461\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.5891855105630353, Train acc: 0.7829415954415955\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5805554932827114, Train acc: 0.7867254273504274\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5855003484803387, Train acc: 0.7855769230769231\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.587169615907377, Train acc: 0.7835648148148148\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5897658792789456, Train acc: 0.7817460317460317\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.587571517110635, Train acc: 0.7831864316239316\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.588836511867678, Train acc: 0.7832977207977208\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5864208580846461, Train acc: 0.7836004273504273\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5839165778195293, Train acc: 0.7858877233877234\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5811458799796991, Train acc: 0.7865028490028491\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5823525034630871, Train acc: 0.7869411571334648\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5816091674542616, Train acc: 0.7867445054945055\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5808757298140445, Train acc: 0.7865740740740741\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5804838409973706, Train acc: 0.7867254273504274\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5798106006210386, Train acc: 0.7868275515334339\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.5809904169556592, Train acc: 0.7867699430199431\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5808266847286737, Train acc: 0.786774628879892\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5796206334621733, Train acc: 0.7866452991452991\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5807974284342696, Train acc: 0.7864265364265364\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.5813543905750809, Train acc: 0.7861548174048174\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5796536807647783, Train acc: 0.7869751021924934\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5781095075551049, Train acc: 0.7877715455840456\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5779692782168715, Train acc: 0.7880128205128205\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5774863462706563, Train acc: 0.7883382642998028\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5772208438531135, Train acc: 0.7885011079455524\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.5772146657132003, Train acc: 0.7884233821733821\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5764569870176794, Train acc: 0.7885352195697023\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.5762162192191324, Train acc: 0.7888354700854701\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5760562733547038, Train acc: 0.7890301902398676\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5759467362470001, Train acc: 0.7890291132478633\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.5755923438650343, Train acc: 0.7892061642061642\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.5764630708418311, Train acc: 0.7893728004022121\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.5764658155840832, Train acc: 0.7893620268620268\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.5762448880418158, Train acc: 0.7892034662867996\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5758948645217365, Train acc: 0.7892122892122893\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.5759618989989208, Train acc: 0.7893331084120557\n",
      "Val loss: 0.5202532410621643, Val acc: 0.792\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.5771148649927897, Train acc: 0.7825854700854701\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5973196818978868, Train acc: 0.7841880341880342\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.5810406705380505, Train acc: 0.791488603988604\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5738214211872755, Train acc: 0.7927350427350427\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5704307346135123, Train acc: 0.7922008547008547\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5686854382504595, Train acc: 0.7921118233618234\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5669732361358746, Train acc: 0.7920482295482295\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.561228195611292, Train acc: 0.7941372863247863\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5604279215137163, Train acc: 0.7950498575498576\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5594963919594247, Train acc: 0.7958867521367521\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.558072492590271, Train acc: 0.7956973581973582\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5598148729206405, Train acc: 0.7948272792022792\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5597195743113501, Train acc: 0.7939677843523998\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5612515044490715, Train acc: 0.7933455433455433\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5619062177603401, Train acc: 0.7934472934472935\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5609633346231511, Train acc: 0.7944377670940171\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5591223882206904, Train acc: 0.79521744595274\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.5595687080532001, Train acc: 0.7953169515669516\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5593027744075225, Train acc: 0.7953778677462888\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5592941167103684, Train acc: 0.7952991452991452\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5588509976700112, Train acc: 0.7953296703296703\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.558718411264928, Train acc: 0.795770202020202\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5599735592326148, Train acc: 0.7954292084726867\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5592020784247338, Train acc: 0.7956285612535613\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5581927549074858, Train acc: 0.7961111111111111\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5578410879345727, Train acc: 0.7959812623274162\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5565041629158269, Train acc: 0.7963754352643242\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.5558083453721688, Train acc: 0.7963980463980463\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5549785086065487, Train acc: 0.7970822281167109\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.5553140589187288, Train acc: 0.7968660968660969\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5551130277971804, Train acc: 0.7969051557761235\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5538382720894729, Train acc: 0.7974425747863247\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.5540286957507491, Train acc: 0.7975265475265475\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.5531880476738174, Train acc: 0.7979040975364505\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.5534538494398015, Train acc: 0.7983211233211234\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.553063462751034, Train acc: 0.7985665954415955\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5521450429659128, Train acc: 0.799000924000924\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.5522604981052731, Train acc: 0.7989625506072875\n",
      "Val loss: 0.5260497331619263, Val acc: 0.804\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.531214870703526, Train acc: 0.8114316239316239\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5455779579078031, Train acc: 0.8098290598290598\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.5374992944278948, Train acc: 0.8110754985754985\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5398943769212208, Train acc: 0.8070245726495726\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5395161023761472, Train acc: 0.8064102564102564\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5375851986882014, Train acc: 0.8070690883190883\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5354149332031225, Train acc: 0.80746336996337\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.539801770574453, Train acc: 0.8058226495726496\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5457318397371518, Train acc: 0.8031220322886989\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5450374172475093, Train acc: 0.8044337606837607\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5409888276814395, Train acc: 0.8058469308469308\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5404566997062532, Train acc: 0.8060452279202279\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5373771801924015, Train acc: 0.8064184746877054\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.536114998639394, Train acc: 0.8068528693528694\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5332369329540478, Train acc: 0.8081908831908832\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5301626801737545, Train acc: 0.8095619658119658\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5314771763272415, Train acc: 0.8087921065862242\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.5318829349480066, Train acc: 0.8080187559354226\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5320460251823664, Train acc: 0.808816914080072\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5322795493639687, Train acc: 0.808440170940171\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5303884862088604, Train acc: 0.8089133089133089\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.5298165139346366, Train acc: 0.8090277777777778\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5295591668479507, Train acc: 0.8092251950947603\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5295508777393703, Train acc: 0.8094729344729344\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5307457109623485, Train acc: 0.8091666666666667\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5308137826993816, Train acc: 0.8095619658119658\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5312540759487256, Train acc: 0.8095125039569484\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.5307384948831811, Train acc: 0.8095619658119658\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5300648248379223, Train acc: 0.8095343353964044\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.5304660581052303, Train acc: 0.8090990028490028\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5299175436363787, Train acc: 0.8086745244003308\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5293719458899214, Train acc: 0.8089276175213675\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.5292345151031181, Train acc: 0.8090682465682466\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.5282423565470435, Train acc: 0.8092006033182504\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.5279341375853261, Train acc: 0.8093406593406594\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.5280907469600835, Train acc: 0.8091019705603039\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5262413845189066, Train acc: 0.8097279972279973\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.5267693654952879, Train acc: 0.8095619658119658\n",
      "Val loss: 0.506254255771637, Val acc: 0.806\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4892444590218047, Train acc: 0.8173076923076923\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4986353647759837, Train acc: 0.8210470085470085\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.5021422032447282, Train acc: 0.8201566951566952\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5056628370259562, Train acc: 0.8183760683760684\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5038616582369193, Train acc: 0.8157051282051282\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5002437681384236, Train acc: 0.8168625356125356\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5000883499854069, Train acc: 0.8176892551892552\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.5038213628447719, Train acc: 0.8153712606837606\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5038993517954012, Train acc: 0.8155864197530864\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5037085335733544, Train acc: 0.8160790598290598\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5011082098594264, Train acc: 0.8165306915306916\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5007924502476667, Train acc: 0.8169960826210826\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5010181229309296, Train acc: 0.8176775147928994\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5010125863437469, Train acc: 0.817803724053724\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.501733131477466, Train acc: 0.8177706552706553\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5000790067128519, Train acc: 0.8183092948717948\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4983718168981235, Train acc: 0.8186902966314731\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4981139743062053, Train acc: 0.8186431623931624\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5007576474226975, Train acc: 0.8183479532163743\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5008115955206573, Train acc: 0.8181356837606838\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5002043828592584, Train acc: 0.8181471306471306\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.5006368452916102, Train acc: 0.8177447552447552\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5005403667001731, Train acc: 0.8179115570419918\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5005998023151502, Train acc: 0.8183760683760684\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5014117407391214, Train acc: 0.8181410256410256\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5012610528022933, Train acc: 0.818129520052597\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5012124678183107, Train acc: 0.8182375751820197\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.5004387953829234, Train acc: 0.8181662087912088\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5007241226857896, Train acc: 0.8177313586796345\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.500762153552723, Train acc: 0.8178240740740741\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5019369825005926, Train acc: 0.8178418803418803\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5023395742516582, Train acc: 0.8180588942307693\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.503490083676041, Train acc: 0.8177447552447552\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.5035359001151427, Train acc: 0.8178418803418803\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.5024567292505131, Train acc: 0.8184065934065934\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.502595384943972, Train acc: 0.8183612298195632\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5018584204342438, Train acc: 0.8186215061215061\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.501953703407039, Train acc: 0.818516644174539\n",
      "Val loss: 0.49968305230140686, Val acc: 0.826\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4994226842481866, Train acc: 0.8274572649572649\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.502585563235558, Train acc: 0.8173076923076923\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.49411872097230025, Train acc: 0.8210470085470085\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4980029875778744, Train acc: 0.8215811965811965\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.49693803565624434, Train acc: 0.8236111111111111\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.4951448537451759, Train acc: 0.8215811965811965\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.49318381443492365, Train acc: 0.8234126984126984\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4928237705801924, Train acc: 0.8230502136752137\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.49390019006595537, Train acc: 0.8234211775878443\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.4920821030679931, Train acc: 0.8242521367521367\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.4886204102991993, Train acc: 0.8254662004662005\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.489700475410476, Train acc: 0.8249198717948718\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4841281355387439, Train acc: 0.8270463510848126\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.48770313338809834, Train acc: 0.8263507326007326\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4857572532829396, Train acc: 0.8273504273504273\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.48584543214314896, Train acc: 0.8275574252136753\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4844593605904078, Train acc: 0.8274886877828054\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4831439095061955, Train acc: 0.8271901709401709\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.484257342355025, Train acc: 0.8265856950067476\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4860249339770048, Train acc: 0.8259348290598291\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.48622343980009397, Train acc: 0.8256003256003256\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.4864728469763417, Train acc: 0.825781857031857\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.48546249481192844, Train acc: 0.8260172798216276\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.48606451604239365, Train acc: 0.8258101851851852\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4855519660619589, Train acc: 0.8262393162393162\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4860626841570146, Train acc: 0.8264094345825115\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.48562579990688026, Train acc: 0.8266658752769864\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.4858664203371502, Train acc: 0.8265987484737485\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.4856460274359381, Train acc: 0.826849395814913\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.48530196325264424, Train acc: 0.8270121082621082\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.4852711170570792, Train acc: 0.8271815550041357\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.4844205788352614, Train acc: 0.8273904914529915\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.48536387062372244, Train acc: 0.8271820771820771\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4838467821630552, Train acc: 0.8275201106083458\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.48428715533043587, Train acc: 0.8271367521367521\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4848334643245097, Train acc: 0.8268340455840456\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4852229645985617, Train acc: 0.8269230769230769\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.48471703741563715, Train acc: 0.827204228520018\n",
      "Val loss: 0.4968623220920563, Val acc: 0.824\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4818756227564608, Train acc: 0.8301282051282052\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4783176133074822, Train acc: 0.8285256410256411\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.47722334155754487, Train acc: 0.8263888888888888\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4828233122984823, Train acc: 0.8267895299145299\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4771286089068804, Train acc: 0.8306623931623932\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.47476168527689755, Train acc: 0.8310185185185185\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.46766391059017587, Train acc: 0.8324175824175825\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.466293799611302, Train acc: 0.8326655982905983\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.46814167051658334, Train acc: 0.8325023741690408\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.46824800049265225, Train acc: 0.8328525641025641\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.46739509957028436, Train acc: 0.831973581973582\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.46669114565216746, Train acc: 0.8319533475783476\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4674274468847124, Train acc: 0.830991124260355\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.46896582134568354, Train acc: 0.8313110500610501\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.46849786970350477, Train acc: 0.8315170940170941\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4693408201281459, Train acc: 0.8313635149572649\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4704719962352721, Train acc: 0.8311965811965812\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4701139170283775, Train acc: 0.8314339981006648\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.46960400123680385, Train acc: 0.8313652721547459\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4701223577094129, Train acc: 0.8310630341880342\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4691394487100983, Train acc: 0.831476393976394\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.468233212901, Train acc: 0.8316336441336442\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.46927956250277825, Train acc: 0.831498513563731\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.46927204960293833, Train acc: 0.8313301282051282\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4692775061955819, Train acc: 0.8314529914529915\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4690493691827732, Train acc: 0.8315253122945431\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4698283756508522, Train acc: 0.8316120607787274\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.4705347946398097, Train acc: 0.8313682844932845\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.47078156290312906, Train acc: 0.8314728853521957\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4711671266036156, Train acc: 0.8311787749287749\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.47150964753947244, Train acc: 0.830955334987593\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.47104193008122724, Train acc: 0.8311798878205128\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.47113780363741947, Train acc: 0.8311480186480187\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.47052003917096547, Train acc: 0.8314636752136753\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.4706060064472122, Train acc: 0.8314102564102565\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4707983042354937, Train acc: 0.8315230294396961\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.47022912522046884, Train acc: 0.831976206976207\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.47042571894365337, Train acc: 0.8317448268106162\n",
      "Val loss: 0.5201511383056641, Val acc: 0.83\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4667600204801967, Train acc: 0.8269230769230769\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4622251607605025, Train acc: 0.8282585470085471\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4513671688332177, Train acc: 0.8338675213675214\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.44804834286308187, Train acc: 0.8354700854700855\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4492085340186062, Train acc: 0.8368589743589744\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.44779105021743015, Train acc: 0.8387642450142451\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.4520650821964237, Train acc: 0.8368437118437119\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4550096328314553, Train acc: 0.836738782051282\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.4567454060980058, Train acc: 0.8364197530864198\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.46171532535018067, Train acc: 0.8348824786324787\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.45798905740297385, Train acc: 0.8363442113442113\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.45697113967094666, Train acc: 0.8369391025641025\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.456668705174321, Train acc: 0.8370315581854043\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.4564209968825036, Train acc: 0.8366529304029304\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4554633004670469, Train acc: 0.8372863247863248\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4571007147240333, Train acc: 0.8365050747863247\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4570979795237472, Train acc: 0.8367898441427853\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.45653656630618505, Train acc: 0.83710232668566\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.4558226498923911, Train acc: 0.8374100314889789\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.456853735469218, Train acc: 0.8372863247863248\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.45627001930129785, Train acc: 0.8373778998778999\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.45842281748665886, Train acc: 0.8366598679098679\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.45827871728280095, Train acc: 0.8368636194723151\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.45886513097821985, Train acc: 0.8366274928774928\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4590230749483801, Train acc: 0.836431623931624\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.458496409199549, Train acc: 0.8364151873767258\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4597076653287961, Train acc: 0.8361427666983222\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.4600099177828675, Train acc: 0.8361568986568987\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.46035465788927327, Train acc: 0.8362253168287651\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4595761765548137, Train acc: 0.8363782051282052\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.4590655841654347, Train acc: 0.8364867659222498\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.45961770948229563, Train acc: 0.8363715277777778\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.45953193819837784, Train acc: 0.8364575239575239\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.45881222860579546, Train acc: 0.8364756158873806\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.45795675152332793, Train acc: 0.8366758241758242\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.45720604710291507, Train acc: 0.8369539411206078\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4561445876064464, Train acc: 0.8373758373758374\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.4556835655104562, Train acc: 0.8373397435897436\n",
      "Val loss: 0.5063863396644592, Val acc: 0.838\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.44304770925360865, Train acc: 0.8493589743589743\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.45529906028228945, Train acc: 0.843215811965812\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4410320345280517, Train acc: 0.8454415954415955\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4444590271410779, Train acc: 0.842948717948718\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4466682479677037, Train acc: 0.8423076923076923\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.44587481467642337, Train acc: 0.8417022792022792\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.44755323934431335, Train acc: 0.8418040293040293\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4469135099321476, Train acc: 0.8428819444444444\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.44240582671373774, Train acc: 0.8444325735992403\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.4386710845857349, Train acc: 0.8457264957264957\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.43996524355477756, Train acc: 0.8455710955710956\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.43957036487131973, Train acc: 0.8455751424501424\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4407436387597496, Train acc: 0.8447978303747534\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.43968170021816416, Train acc: 0.8450473137973138\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.43940614086398033, Train acc: 0.8457621082621083\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4411943269411946, Train acc: 0.8454861111111112\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4439132130639735, Train acc: 0.8448026646556058\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4430632002754287, Train acc: 0.8449964387464387\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.4437118690616769, Train acc: 0.8449167791273055\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.44572506768421993, Train acc: 0.8439636752136752\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4451554205085162, Train acc: 0.8445258445258446\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.445678838496129, Train acc: 0.8437742812742812\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.44537061351946367, Train acc: 0.8436454849498328\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.44410515979245474, Train acc: 0.8440616096866097\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4441293351377687, Train acc: 0.8438461538461538\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.44396152571553976, Train acc: 0.8437294543063774\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4434571007691765, Train acc: 0.8435422602089269\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.44309123255817334, Train acc: 0.8435401404151404\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.4435645180004749, Train acc: 0.8432618626584144\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4426509825390713, Train acc: 0.8434650997150998\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.44274319757750813, Train acc: 0.8435518334711883\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.44333112828580773, Train acc: 0.8435830662393162\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.44326202379869106, Train acc: 0.8435314685314685\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.44343952496071115, Train acc: 0.8430744092508798\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.4439705723252998, Train acc: 0.8429029304029304\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4447736594629636, Train acc: 0.8425480769230769\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4451666430790269, Train acc: 0.8422124047124047\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.4461709480997455, Train acc: 0.8417116509221773\n",
      "Val loss: 0.4778643846511841, Val acc: 0.842\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4311670223489786, Train acc: 0.8413461538461539\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.420674584646765, Train acc: 0.8466880341880342\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4288820501024227, Train acc: 0.8413461538461539\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4343094958995397, Train acc: 0.8380074786324786\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4332502672050753, Train acc: 0.839957264957265\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.43289025643697154, Train acc: 0.8409009971509972\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.43748328747439297, Train acc: 0.8397435897435898\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4379595668795399, Train acc: 0.8398771367521367\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.4339447297573316, Train acc: 0.8418803418803419\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.4320334035807695, Train acc: 0.8422008547008547\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.43136091675050703, Train acc: 0.8425602175602176\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.43202290621896583, Train acc: 0.8421474358974359\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.43223449141081033, Train acc: 0.8422912557527942\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.432330885526789, Train acc: 0.8433684371184371\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4328772311748942, Train acc: 0.8437321937321938\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4322590878567634, Train acc: 0.8445178952991453\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.43360895617425355, Train acc: 0.8440485168426345\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4342637576750088, Train acc: 0.8443138651471985\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.4334663481753633, Train acc: 0.8446918578497525\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.43431116385847074, Train acc: 0.8445245726495727\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.43395375282385024, Train acc: 0.8447802197802198\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.43289267887428645, Train acc: 0.8451825951825952\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.43314591631575705, Train acc: 0.8449228911185432\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.4349610805272674, Train acc: 0.8440616096866097\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.43570968141667865, Train acc: 0.8439529914529914\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.43822472287313785, Train acc: 0.8432979947403024\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.43775781343097586, Train acc: 0.8436016144349477\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.43750884238589594, Train acc: 0.8438072344322345\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.43750072892062475, Train acc: 0.8438697318007663\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4371462677003142, Train acc: 0.8440527065527066\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.43743730409968024, Train acc: 0.8439309346567411\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.4376071384654213, Train acc: 0.8435830662393162\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.4378868373787734, Train acc: 0.8432562807562808\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4380851578418785, Train acc: 0.8429644293614882\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.43747929457634216, Train acc: 0.8432234432234432\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4368803964243231, Train acc: 0.843215811965812\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.43647179430573113, Train acc: 0.8436850311850311\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.43701424248251786, Train acc: 0.8436656545209177\n",
      "Val loss: 0.4873138964176178, Val acc: 0.83\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4166052352923613, Train acc: 0.8563034188034188\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.41737566413915056, Train acc: 0.8509615384615384\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.42303892191064324, Train acc: 0.8457977207977208\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4253084221775206, Train acc: 0.8461538461538461\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.42678651142324137, Train acc: 0.8448717948717949\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.4274703860813566, Train acc: 0.8438390313390314\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.4287507676033892, Train acc: 0.8443986568986569\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.43047729088391506, Train acc: 0.8443509615384616\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.42362440162376347, Train acc: 0.8464506172839507\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.4239534316727748, Train acc: 0.8467948717948718\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.4244857671748045, Train acc: 0.8473193473193473\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.42710632897191747, Train acc: 0.8459757834757835\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4300468482669837, Train acc: 0.8447978303747534\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.42956951422041445, Train acc: 0.8449710012210012\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4315940549782878, Train acc: 0.8441239316239316\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4300806468320835, Train acc: 0.8451188568376068\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.42968814654108145, Train acc: 0.845211161387632\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4295802136311787, Train acc: 0.8452338556505223\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.43072929921393666, Train acc: 0.8450011246063878\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4292262548660366, Train acc: 0.8456997863247864\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4292920234305712, Train acc: 0.8457468457468458\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.4299997168780531, Train acc: 0.8458624708624709\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.42834753393351654, Train acc: 0.8466880341880342\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.42888828696805104, Train acc: 0.8460648148148148\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4284593799888578, Train acc: 0.8464957264957265\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.42839497103209484, Train acc: 0.8465647600262984\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.428882614594164, Train acc: 0.8460351377018044\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.42844573082004345, Train acc: 0.8460775335775336\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.4281002306548185, Train acc: 0.846282788093133\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4279580986907339, Train acc: 0.8463497150997151\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.42799116219161526, Train acc: 0.8462227736421285\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.42913740476529694, Train acc: 0.8459535256410257\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.4292961328303187, Train acc: 0.8458948458948459\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4284432926756045, Train acc: 0.8464209401709402\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.42790921637026524, Train acc: 0.8466422466422466\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.42749551804302877, Train acc: 0.8467770655270656\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.42690103269079094, Train acc: 0.846962346962347\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.4270810920609395, Train acc: 0.8467864372469636\n",
      "Val loss: 0.465375691652298, Val acc: 0.842\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.42770334798046666, Train acc: 0.8301282051282052\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.438476171471879, Train acc: 0.8370726495726496\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.42673251115613514, Train acc: 0.8436609686609686\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4245247924461579, Train acc: 0.8454861111111112\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4274533010764509, Train acc: 0.8454059829059829\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.4246383820556932, Train acc: 0.8471331908831908\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.42159290017244, Train acc: 0.8489010989010989\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.41845557124266386, Train acc: 0.8497596153846154\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.41814606186402264, Train acc: 0.8509021842355176\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.41976015796072974, Train acc: 0.8504273504273504\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.4188739837356427, Train acc: 0.8507187257187258\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.42089764749169606, Train acc: 0.8498931623931624\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4205058655535642, Train acc: 0.8503862590401052\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.42284443474378125, Train acc: 0.8497023809523809\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4233057707654275, Train acc: 0.8492521367521367\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4239632367638027, Train acc: 0.8485243055555556\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4243522866560233, Train acc: 0.8483848667672197\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4233521585143105, Train acc: 0.8491215574548908\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.42304821322314706, Train acc: 0.8491621682411156\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.42298448112371384, Train acc: 0.8487446581196582\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.42379167164862763, Train acc: 0.8484177859177859\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.4234698738514937, Train acc: 0.8487276612276612\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.42281151635330744, Train acc: 0.8491034931252323\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.4226038580789547, Train acc: 0.849136396011396\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4228988597395583, Train acc: 0.8486965811965812\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4220520761108943, Train acc: 0.8493384286653517\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4211806327790899, Train acc: 0.8498338081671415\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.4208261178333371, Train acc: 0.8500267094017094\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.4201638952584928, Train acc: 0.8503536693191865\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.42118760798328114, Train acc: 0.849732905982906\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.42207918685573603, Train acc: 0.8495485249517507\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.42204856724402684, Train acc: 0.8496427617521367\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.42122854448988073, Train acc: 0.8498931623931624\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4211327856057265, Train acc: 0.8500345651080945\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.4204969414870308, Train acc: 0.8503052503052503\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.42065749938377767, Train acc: 0.8501602564102564\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4216417252585433, Train acc: 0.8497199122199122\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.42167734654115036, Train acc: 0.849527665317139\n",
      "Val loss: 0.45772191882133484, Val acc: 0.846\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4038095493347217, Train acc: 0.8536324786324786\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4003238036400742, Train acc: 0.8525641025641025\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4058328424805929, Train acc: 0.8516737891737892\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.40738348396988505, Train acc: 0.8512286324786325\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4100240459936297, Train acc: 0.85\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.4068679717552458, Train acc: 0.8514957264957265\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.4066685616733536, Train acc: 0.851572039072039\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4081210562099631, Train acc: 0.8510950854700855\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.40897755833574395, Train acc: 0.8513176638176638\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.40689155924269277, Train acc: 0.8516025641025641\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.4072268290420605, Train acc: 0.8503302253302253\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.4097345704663853, Train acc: 0.8488247863247863\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4111701139958574, Train acc: 0.8483727810650887\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.41183317585032936, Train acc: 0.848519536019536\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.41173415646339073, Train acc: 0.849465811965812\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4108005664191949, Train acc: 0.8505275106837606\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.41109668627164064, Train acc: 0.8510558069381599\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4105312106078393, Train acc: 0.8511692782526116\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.41062456120712176, Train acc: 0.8514113810166442\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.41048290750537164, Train acc: 0.852056623931624\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4099721369744075, Train acc: 0.8520807895807896\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.40970900724262066, Train acc: 0.8521756021756022\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.4097796465962051, Train acc: 0.8520763656633222\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.40902508129803544, Train acc: 0.8522302350427351\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4094727058033658, Train acc: 0.8523076923076923\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4078905846039415, Train acc: 0.8530982905982906\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4084824706961664, Train acc: 0.8526630262741374\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.40950619421617307, Train acc: 0.8524305555555556\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.4098837335676069, Train acc: 0.8522325375773652\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4110226906517632, Train acc: 0.8522970085470085\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.41054018758399646, Train acc: 0.8524607113316791\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.4103020241854187, Train acc: 0.8527644230769231\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.41017959852372693, Train acc: 0.8528392903392903\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4107406038834019, Train acc: 0.8526426596279537\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.41101848698688515, Train acc: 0.8526098901098901\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4106252227519822, Train acc: 0.8527124881291548\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4100254644323756, Train acc: 0.8530116655116655\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.40991256643778773, Train acc: 0.8530982905982906\n",
      "Val loss: 0.4862339496612549, Val acc: 0.858\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.3771507694807827, Train acc: 0.8579059829059829\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.38046251383856833, Train acc: 0.8605769230769231\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.38264706140739624, Train acc: 0.8591524216524217\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.3880992532731631, Train acc: 0.8569711538461539\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.39486177194322275, Train acc: 0.8554487179487179\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.39725730671120163, Train acc: 0.8549679487179487\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.3956612005229398, Train acc: 0.8548534798534798\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4042205159178274, Train acc: 0.8530315170940171\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.406249375431909, Train acc: 0.8525047483380817\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.40789079236933307, Train acc: 0.8522435897435897\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.4098784483407668, Train acc: 0.8532925407925408\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.40646478944481945, Train acc: 0.8544782763532763\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4076706669994438, Train acc: 0.8537968441814595\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.40566809075601373, Train acc: 0.8546245421245421\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4057768653176109, Train acc: 0.854594017094017\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.40591202253221065, Train acc: 0.8544671474358975\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.40637154290579025, Train acc: 0.854323780794369\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4075690552217272, Train acc: 0.8541666666666666\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.4072207970124909, Train acc: 0.8541104363472785\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.40808863003507384, Train acc: 0.8539529914529914\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4078871115972392, Train acc: 0.8540140415140415\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.407357982647111, Train acc: 0.8544337606837606\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.407110955704603, Train acc: 0.8545615013006317\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.4071115897902078, Train acc: 0.8546118233618234\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.40819780421690044, Train acc: 0.8541880341880342\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.40640949887112715, Train acc: 0.8548857659434582\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4047096230610899, Train acc: 0.855630737575182\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.40510219516134816, Train acc: 0.8552541208791209\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.405689952583781, Train acc: 0.8552534630120837\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4047506018337446, Train acc: 0.8556623931623931\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.4052915108881363, Train acc: 0.855510752688172\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.4051347943291498, Train acc: 0.85546875\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.4044874217009041, Train acc: 0.8556882931882932\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.40353756242788963, Train acc: 0.8561148818501759\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.40308201595485865, Train acc: 0.8564255189255189\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.40293416892977013, Train acc: 0.8565259971509972\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.40356640538880995, Train acc: 0.8563034188034188\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.40378898041497946, Train acc: 0.8561487854251012\n",
      "Val loss: 0.4956294000148773, Val acc: 0.846\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4014551821045386, Train acc: 0.8514957264957265\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4183992767372193, Train acc: 0.8485576923076923\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.41350412542833564, Train acc: 0.8491809116809117\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.41029874451904214, Train acc: 0.8513621794871795\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4033789951195065, Train acc: 0.8542735042735042\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.3998149578801842, Train acc: 0.854522792022792\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.40025982944356214, Train acc: 0.8539377289377289\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.40524373687087345, Train acc: 0.8530982905982906\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.4065255284493352, Train acc: 0.8529202279202279\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.40829493209210216, Train acc: 0.8525106837606837\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.40554383520989246, Train acc: 0.8532439782439782\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.40586844165716585, Train acc: 0.8541221509971509\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.40261003499719594, Train acc: 0.8553172255095332\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.40069636071876363, Train acc: 0.8562271062271062\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4005359574324555, Train acc: 0.8567663817663818\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.40012801887515265, Train acc: 0.8565037393162394\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.3992792041443172, Train acc: 0.8568690296631473\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.3991848640320761, Train acc: 0.8572827635327636\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.3982338998810268, Train acc: 0.8576248313090419\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.39739427452222403, Train acc: 0.8579059829059829\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.39760913615716537, Train acc: 0.8581603581603582\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.3986608150958575, Train acc: 0.857736013986014\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.39787825748922434, Train acc: 0.8583472686733556\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.39867727363885425, Train acc: 0.8581285612535613\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.39824096371093365, Train acc: 0.8584188034188034\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.39787614389514353, Train acc: 0.8583168967784353\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.3970047482756657, Train acc: 0.8585390946502057\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.39547267589963725, Train acc: 0.8588217338217338\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.39581088540263665, Train acc: 0.8588638373121131\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.3968358073999145, Train acc: 0.8583867521367521\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.3970854546908656, Train acc: 0.8581816928591122\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.3971940265942572, Train acc: 0.8580896100427351\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.39717857146090996, Train acc: 0.8582135457135457\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.3971759891310728, Train acc: 0.8581102312719959\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.3961594131366965, Train acc: 0.8584859584859584\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.39544595604990623, Train acc: 0.8587072649572649\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.39615275980171083, Train acc: 0.858526796026796\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.39638684109503547, Train acc: 0.8585526315789473\n",
      "Val loss: 0.4780389368534088, Val acc: 0.856\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.3778260540631082, Train acc: 0.8627136752136753\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.3938391663006738, Train acc: 0.8595085470085471\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.38478086665909517, Train acc: 0.8636039886039886\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.3913122322410345, Train acc: 0.8607104700854701\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.39325966458672135, Train acc: 0.8598290598290599\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.3945605667875788, Train acc: 0.8603098290598291\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.39976329860869925, Train acc: 0.8577533577533577\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4022828458696922, Train acc: 0.8547676282051282\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.39760740135300193, Train acc: 0.856718898385565\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.39834686746327286, Train acc: 0.857852564102564\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.39461552993725135, Train acc: 0.8590229215229215\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.39478473600127506, Train acc: 0.8584401709401709\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.3948068485168506, Train acc: 0.8584812623274162\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.39382352715446833, Train acc: 0.8586691086691086\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.3920103851546589, Train acc: 0.8594017094017095\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.39227783971696967, Train acc: 0.8590745192307693\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.3919254834550658, Train acc: 0.8593828557063852\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.3917836099650785, Train acc: 0.8596569325735992\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.39092072251944515, Train acc: 0.8600427350427351\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.39126628616617787, Train acc: 0.8603632478632479\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.3910891977167246, Train acc: 0.8603225478225478\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.3910029817751747, Train acc: 0.8601641414141414\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.38926013906298407, Train acc: 0.8608788554440728\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.3883688306473089, Train acc: 0.8614449786324786\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.38912115514023693, Train acc: 0.8611111111111112\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.3897434647521402, Train acc: 0.8612343852728468\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.38956437620295115, Train acc: 0.8612298195631529\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.38981988757703184, Train acc: 0.861359126984127\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.38825650243299953, Train acc: 0.8619400235779546\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.3879034780602679, Train acc: 0.8620904558404558\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.3883088241357727, Train acc: 0.8621794871794872\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.38810210990218014, Train acc: 0.8623130341880342\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.3888332798220246, Train acc: 0.8620985495985496\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.3883302976597848, Train acc: 0.8621794871794872\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.3882980323791213, Train acc: 0.8619810744810745\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.3889085435116591, Train acc: 0.8619123931623932\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.38971548061237016, Train acc: 0.8616308616308617\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.3895100453978273, Train acc: 0.8616593567251462\n",
      "Val loss: 0.4748833179473877, Val acc: 0.854\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.3935587598472579, Train acc: 0.8632478632478633\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4015375402174954, Train acc: 0.8595085470085471\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.39598701849707174, Train acc: 0.8593304843304843\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.3886462169675491, Train acc: 0.8613782051282052\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.3785407715118848, Train acc: 0.8645299145299146\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.3739052844682225, Train acc: 0.8679665242165242\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.37840585786762837, Train acc: 0.8667582417582418\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.37914115155083883, Train acc: 0.8657184829059829\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.3802559728940271, Train acc: 0.8655033238366572\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.3778516141936565, Train acc: 0.8658653846153846\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.37771773639061973, Train acc: 0.8656759906759907\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.37631928313130836, Train acc: 0.8659188034188035\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.38063178575407275, Train acc: 0.8646860618014465\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.3811463356331919, Train acc: 0.8636675824175825\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.3809618124998363, Train acc: 0.8634259259259259\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.3804141437738306, Train acc: 0.8634147970085471\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.3825333005200384, Train acc: 0.862965057817999\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.38460163489436944, Train acc: 0.8621794871794872\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.38462546419853355, Train acc: 0.8627136752136753\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.3847486553602239, Train acc: 0.8625801282051282\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.3849510095337323, Train acc: 0.8626882376882377\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.3852806813499681, Train acc: 0.862519425019425\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.3862861147636997, Train acc: 0.862202712746191\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.3855257591463498, Train acc: 0.8625133547008547\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.38545672982039614, Train acc: 0.8622649572649572\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.385512745022568, Train acc: 0.8623438527284681\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.3860588460988488, Train acc: 0.8623179803735359\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.38484943621838524, Train acc: 0.8629807692307693\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.3852680140605041, Train acc: 0.86280577659888\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.3845126778057158, Train acc: 0.8633368945868946\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.3858955876333246, Train acc: 0.8626447477253929\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.3857673422461296, Train acc: 0.862479967948718\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.3848929066356678, Train acc: 0.8626974876974877\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.3847398899660421, Train acc: 0.8625722724987431\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.3855081251907698, Train acc: 0.8620726495726496\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.38529541091029407, Train acc: 0.862031101614435\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.3843306940118637, Train acc: 0.862540425040425\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.38523816357874086, Train acc: 0.8622497750787225\n",
      "Val loss: 0.49383991956710815, Val acc: 0.854\n",
      "Early stopping at epoch 22 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 780.4381 [s]\n",
      "Mejores parámetros encontrados:  {'lr': 5e-05, 'epochs': 50, 'dropout_p': 0.6, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "# Wrapper del modelo para Scikit-learn\n",
    "class TorchModelWrapper(object):\n",
    "    def __init__(self, dropout_p=0.5, batch_size=64, lr=0.001, epochs=30, use_gpu=True):\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.use_gpu = use_gpu\n",
    "        self._model = None\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            \"dropout_p\": self.dropout_p,\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"lr\": self.lr,\n",
    "            \"epochs\": self.epochs,\n",
    "            \"use_gpu\": self.use_gpu\n",
    "        }\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "    def fit(self, X, y):\n",
    "        model = CNNModel(dropout_p=self.dropout_p)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self._model = train_model(model, X, y, Val_images, Val_labels, self.epochs, criterion, self.batch_size, self.lr, self.use_gpu)\n",
    "\n",
    "    def predict(self, X):\n",
    "        dataset = torch.utils.data.TensorDataset(X)\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(X), shuffle=False)\n",
    "        \n",
    "        self._model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in data_loader:\n",
    "                if self.use_gpu:\n",
    "                    inputs = inputs.cuda()\n",
    "                outputs = self._model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "        return preds.cpu().numpy()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n",
    "\n",
    "\n",
    "# Parámetros a considerar en la búsqueda\n",
    "param_dist = {\n",
    "    'dropout_p': [0.5, 0.6, 0.8],\n",
    "    'batch_size': [16, 32, 62],\n",
    "    'lr': [5e-3, 1e-3, 5e-4, 1e-4, 5e-5],\n",
    "    'epochs': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "# Crear el modelo de búsqueda aleatoria\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=TorchModelWrapper(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    scoring=make_scorer(accuracy_score),\n",
    "    verbose=1,\n",
    "    cv=3,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "# Lanzar la búsqueda\n",
    "random_search.fit(Train_images, Train_labels)\n",
    "\n",
    "# Mostrar los mejores parámetros encontrados\n",
    "print(\"Mejores parámetros encontrados: \", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mejores parámetros encontrados:  {'lr': 5e-05, 'epochs': 50, 'dropout_p': 0.6, 'batch_size': 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 1.6074765981772008, Train acc: 0.23397435897435898\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 1.6077633031413086, Train acc: 0.21875\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 1.5928913123926884, Train acc: 0.24697293447293447\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 1.554276876215242, Train acc: 0.28378739316239315\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 1.519529754280025, Train acc: 0.3141025641025641\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 1.4883436166662776, Train acc: 0.33582621082621084\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 1.464015001124078, Train acc: 0.3570665445665446\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 1.4416002417222047, Train acc: 0.37246260683760685\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 1.4217236058438039, Train acc: 0.38829534662868\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 1.4010073415744, Train acc: 0.4014957264957265\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 1.3855950085340587, Train acc: 0.4122960372960373\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 1.3699033014539979, Train acc: 0.4236556267806268\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 1.352951866635204, Train acc: 0.4335963182117028\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 1.3387884781110273, Train acc: 0.4432234432234432\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 1.3262926203233225, Train acc: 0.4513532763532764\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 1.3148092750109668, Train acc: 0.4582331730769231\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 1.3033552815292035, Train acc: 0.465026395173454\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 1.291608503373147, Train acc: 0.4714506172839506\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 1.2811297916690347, Train acc: 0.4780420602789024\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 1.2704960342401113, Train acc: 0.4843482905982906\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 1.2619505220183425, Train acc: 0.48908730158730157\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 1.2519034568477991, Train acc: 0.49446386946386944\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 1.2416505694212145, Train acc: 0.5000696767001115\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 1.2325652036232146, Train acc: 0.5048076923076923\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 1.2241734123229981, Train acc: 0.5089957264957264\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 1.2151804219571327, Train acc: 0.5135807034845496\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 1.2072105119408743, Train acc: 0.5177864830642609\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 1.1997838047434268, Train acc: 0.5211195054945055\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 1.1925988787576016, Train acc: 0.5247200117889773\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 1.1857901842675658, Train acc: 0.5274394586894587\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 1.1795799993173721, Train acc: 0.5306038047973531\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 1.1735031820961044, Train acc: 0.5333867521367521\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 1.1671963756533985, Train acc: 0.5364866614866615\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 1.1618094134264823, Train acc: 0.5391057063851181\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 1.155481285963012, Train acc: 0.5418803418803418\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 1.1492374434869062, Train acc: 0.5444711538461539\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 1.1445323898200763, Train acc: 0.5469074844074844\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 1.1393116926602613, Train acc: 0.5491734143049932\n",
      "Val loss: 0.805252730846405, Val acc: 0.694\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.9052070397087651, Train acc: 0.6629273504273504\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.9215937745876801, Train acc: 0.6631944444444444\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.9192700833506734, Train acc: 0.6586538461538461\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.9157293293083835, Train acc: 0.6574519230769231\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.9146553654956002, Train acc: 0.6584401709401709\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.9111290801199753, Train acc: 0.6597222222222222\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.9104090635738675, Train acc: 0.657967032967033\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.9090481599171957, Train acc: 0.6577190170940171\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.9079116843251648, Train acc: 0.6569325735992403\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.9103573171756206, Train acc: 0.6572649572649573\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.9055811421947139, Train acc: 0.6587995337995338\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.9048864330321635, Train acc: 0.6590099715099715\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.9024643768298634, Train acc: 0.659845496383958\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.9026312984717197, Train acc: 0.6602945665445665\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.9008078412962096, Train acc: 0.6607193732193732\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.897930645566975, Train acc: 0.6619257478632479\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.898132255474127, Train acc: 0.6616704374057315\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.8966789564289372, Train acc: 0.6617105887939221\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.8945513632026171, Train acc: 0.6624212775528565\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.8935326596355846, Train acc: 0.6627136752136752\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.8919739630657044, Train acc: 0.6632071632071632\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.8911985792364366, Train acc: 0.6633644133644133\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.8887234527087486, Train acc: 0.6640886287625418\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.8884139060337319, Train acc: 0.6641960470085471\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.8881712059078054, Train acc: 0.6642948717948718\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.8875354518826113, Train acc: 0.6643860946745562\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.8861874979426869, Train acc: 0.664866255144033\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.8832570883799095, Train acc: 0.6663423382173382\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.8814313016105042, Train acc: 0.6669613910993222\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.8794962886052254, Train acc: 0.6674323361823362\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.8777232786118244, Train acc: 0.6681141439205955\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.8750797042575402, Train acc: 0.6689369658119658\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.8739257196986715, Train acc: 0.6695966070966071\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.8718920334267101, Train acc: 0.6703274258421317\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.8712578868254637, Train acc: 0.670589133089133\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.8701992861268527, Train acc: 0.6711033950617284\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.8689559510093441, Train acc: 0.6717197967197968\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.868562291528371, Train acc: 0.6717555105713\n",
      "Val loss: 0.709259033203125, Val acc: 0.712\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.8252628636665833, Train acc: 0.6789529914529915\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.8220265916524789, Train acc: 0.6848290598290598\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.8270697225193013, Train acc: 0.6862535612535613\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.8190612555441693, Train acc: 0.6908386752136753\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.8136691044538449, Train acc: 0.6919871794871795\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.8065112805858976, Train acc: 0.6960470085470085\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.8065682053565979, Train acc: 0.695054945054945\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.802229706930299, Train acc: 0.6973824786324786\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.8000435888597429, Train acc: 0.6984805318138652\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.801894071443468, Train acc: 0.6981837606837606\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.8046748769913149, Train acc: 0.6968725718725719\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.8028800206051933, Train acc: 0.6965366809116809\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.8018384304664231, Train acc: 0.6970742932281394\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.7993281355925969, Train acc: 0.6985271672771672\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.7996067423935969, Train acc: 0.6985042735042735\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.7967649425387892, Train acc: 0.6991853632478633\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.7955620175423246, Train acc: 0.700006284565108\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.7944837207892681, Train acc: 0.7003501899335233\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.7946475993739663, Train acc: 0.7003486279802069\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.7914430144250902, Train acc: 0.7022702991452991\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.7917201921471164, Train acc: 0.702024827024827\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.7911929946795118, Train acc: 0.702020202020202\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.7895446003967392, Train acc: 0.7022946859903382\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.7884949191216051, Train acc: 0.7025017806267806\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.7872531694836087, Train acc: 0.7032264957264958\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.7860297420693416, Train acc: 0.7036489151873767\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.7860829729507829, Train acc: 0.7036443494776828\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.7858936280667127, Train acc: 0.7039644383394383\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.7843353880688465, Train acc: 0.7045571765399352\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.7832126458740641, Train acc: 0.7051994301994302\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.7825890399714887, Train acc: 0.7055590019299697\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.7815281667659043, Train acc: 0.7055789262820513\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.7813996915116702, Train acc: 0.705937580937581\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.7803191740221268, Train acc: 0.7063379839115134\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.7797838251560162, Train acc: 0.7065628815628816\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.7788088431251071, Train acc: 0.7069978632478633\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.777571061376089, Train acc: 0.7075537075537075\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.7774147973657727, Train acc: 0.7074336482231219\n",
      "Val loss: 0.6491864919662476, Val acc: 0.76\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.7282147040733924, Train acc: 0.7280982905982906\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.7158879426300017, Train acc: 0.7302350427350427\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.7357636546647107, Train acc: 0.7229344729344729\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.7306620616816047, Train acc: 0.7270299145299145\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.7313639535863176, Train acc: 0.7251068376068376\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.7361531419377042, Train acc: 0.7224893162393162\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.7358296858522045, Train acc: 0.7231379731379731\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.7381268052909619, Train acc: 0.7222889957264957\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.7339460809572697, Train acc: 0.7241215574548908\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.7326955269544553, Train acc: 0.7251068376068376\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.7339896226957405, Train acc: 0.7254273504273504\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.7351376779504821, Train acc: 0.7261396011396012\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.7321237391013993, Train acc: 0.7277284681130834\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.7300123207462139, Train acc: 0.7283272283272283\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.7301676822269064, Train acc: 0.7278490028490029\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.7256818071851491, Train acc: 0.7296006944444444\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.7252137688774748, Train acc: 0.7296380090497737\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.7255309055234745, Train acc: 0.7297305318138652\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.7256086031113237, Train acc: 0.7293634727845254\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.7271755177623186, Train acc: 0.7291399572649573\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.7252290419412426, Train acc: 0.729421041921042\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.7231936535299427, Train acc: 0.7300165112665112\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.7225680074553116, Train acc: 0.7299098848011891\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.7220447581537344, Train acc: 0.7300124643874644\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.7219312656486137, Train acc: 0.7304059829059829\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.7215548837355373, Train acc: 0.7309335963182118\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.7201918820606149, Train acc: 0.7314023425134536\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.7187593758142621, Train acc: 0.7319329975579976\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.7190006615716449, Train acc: 0.7323533745947539\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.7194873520281919, Train acc: 0.7323717948717948\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.7186896310123426, Train acc: 0.73281982354563\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.7190787937794613, Train acc: 0.7328559027777778\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.7181179928010809, Train acc: 0.7336020461020462\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.7175599083064668, Train acc: 0.7339272247360482\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.7166425070010116, Train acc: 0.7341117216117216\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.7162341389695407, Train acc: 0.7344937084520418\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.7154476233506538, Train acc: 0.7350282975282976\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.714850708423329, Train acc: 0.735295771479982\n",
      "Val loss: 0.6053673624992371, Val acc: 0.772\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.6899495955206391, Train acc: 0.7398504273504274\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.7005087151868731, Train acc: 0.7382478632478633\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.6989337090837989, Train acc: 0.7362891737891738\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.6846736535493635, Train acc: 0.7437232905982906\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.6821167367391098, Train acc: 0.7450854700854701\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.6757071993162489, Train acc: 0.7459935897435898\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.6740471323513141, Train acc: 0.7464133089133089\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.6765613725456672, Train acc: 0.7461939102564102\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.678457137314003, Train acc: 0.7469135802469136\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.6779842896466581, Train acc: 0.7468482905982906\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.6806895915004942, Train acc: 0.7465034965034965\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.68003766085857, Train acc: 0.7463497150997151\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.6805691087347829, Train acc: 0.746835963182117\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.6799898958868451, Train acc: 0.7476724664224664\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.6796598412457354, Train acc: 0.7476139601139601\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.6801799410062596, Train acc: 0.7474626068376068\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.6796452819611811, Train acc: 0.7481460532931121\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.678744743253544, Train acc: 0.7481006647673314\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.6776314107244529, Train acc: 0.7482568600989654\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.6793562804913928, Train acc: 0.7472489316239316\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.6785628292125854, Train acc: 0.7473036223036224\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.676409925585258, Train acc: 0.7479846542346542\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.6742894978150697, Train acc: 0.7488387216648086\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.6747128565097891, Train acc: 0.7486422720797721\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.6736181418253825, Train acc: 0.7490811965811965\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.6740419377893072, Train acc: 0.7488083497698882\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.6728628123295235, Train acc: 0.749465811965812\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.6737965495440477, Train acc: 0.7489888583638583\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.6719751306956334, Train acc: 0.7493737105806071\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.6723231223276538, Train acc: 0.7493589743589744\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.6720416516698291, Train acc: 0.7494313482216708\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.6718578100618389, Train acc: 0.7496661324786325\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.6716513131310735, Train acc: 0.7496438746438746\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.6711687641101245, Train acc: 0.7497486173956762\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.670284728776841, Train acc: 0.7502594627594628\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.6705527832591862, Train acc: 0.7500445156695157\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.669909722612537, Train acc: 0.7505919380919381\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.6694554559185973, Train acc: 0.7507169365721997\n",
      "Val loss: 0.5681187510490417, Val acc: 0.782\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.6739296115871168, Train acc: 0.7542735042735043\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.6404790941976074, Train acc: 0.7628205128205128\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.6394720041429215, Train acc: 0.7644230769230769\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.6409542537334129, Train acc: 0.7642895299145299\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.6439166947053029, Train acc: 0.7626068376068376\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.6383434784548235, Train acc: 0.7630876068376068\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.6387697422242427, Train acc: 0.7657203907203908\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.6375867417161791, Train acc: 0.7674946581196581\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.6379048033402517, Train acc: 0.767153371320038\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.6432270422577858, Train acc: 0.7647970085470085\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.6435928489106889, Train acc: 0.7643259518259519\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.6408400714524791, Train acc: 0.765357905982906\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.6432800282975861, Train acc: 0.7638067061143984\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.6467328394179815, Train acc: 0.7632020757020757\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.6452950789208426, Train acc: 0.763568376068376\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.644579667200008, Train acc: 0.7627871260683761\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.6441630322379405, Train acc: 0.7622549019607843\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.6435128202819304, Train acc: 0.7625534188034188\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.641624184239034, Train acc: 0.7632703553756185\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.6387649378906458, Train acc: 0.7650641025641025\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.6382624829657252, Train acc: 0.7653388278388278\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.6370974381127928, Train acc: 0.7660742035742035\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.6358076035145799, Train acc: 0.7671404682274248\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.6356931772677392, Train acc: 0.7667156339031339\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.6359327891290697, Train acc: 0.7665384615384615\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.6361388335497971, Train acc: 0.7666420118343196\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.6364158248035316, Train acc: 0.7668170307059196\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.6363495133171297, Train acc: 0.7669986263736264\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.6365637053589255, Train acc: 0.7665414087827881\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.6369646417576703, Train acc: 0.7668447293447294\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.6369587365616969, Train acc: 0.7670078577336642\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.6357974101765416, Train acc: 0.7673444177350427\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.6354590404901255, Train acc: 0.7675472675472675\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.635789599878336, Train acc: 0.7671882855706386\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.6351742070236486, Train acc: 0.7672924297924298\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.6345840675291117, Train acc: 0.7676578822412156\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.6344832760180962, Train acc: 0.7674260799260799\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.6338972115819647, Train acc: 0.7676282051282052\n",
      "Val loss: 0.5617907643318176, Val acc: 0.784\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.6033974450368148, Train acc: 0.7884615384615384\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.6089616865556464, Train acc: 0.780448717948718\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.6203154970353145, Train acc: 0.7772435897435898\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.6221515938448601, Train acc: 0.7739049145299145\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.6233743456184354, Train acc: 0.773931623931624\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.6234690453167315, Train acc: 0.7718126780626781\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.6263839678340779, Train acc: 0.7720543345543346\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.622091138512533, Train acc: 0.7734375\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.6201219128520853, Train acc: 0.7739791073124407\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.6209724844138846, Train acc: 0.7730235042735043\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.6222861740903143, Train acc: 0.7723387723387724\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.6239956850743192, Train acc: 0.7706997863247863\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.6261688223173554, Train acc: 0.7700525969756739\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.6242813041140308, Train acc: 0.7707570207570208\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.6235227093652442, Train acc: 0.7704415954415954\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.621557869685766, Train acc: 0.7713341346153846\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.6192308537635808, Train acc: 0.7727501256913022\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.6167321003626781, Train acc: 0.7735933048433048\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.6156239407694238, Train acc: 0.7742914979757085\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.6147929016150471, Train acc: 0.774866452991453\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.6139809767866116, Train acc: 0.7755901505901506\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.6142138950671442, Train acc: 0.7753253690753691\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.6136682094062673, Train acc: 0.7753855444072836\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.6119709070028997, Train acc: 0.7757523148148148\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.6111524794346247, Train acc: 0.775982905982906\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.6104791393842766, Train acc: 0.776154667981591\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.6095807490827313, Train acc: 0.7765709085153529\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.6097556190549308, Train acc: 0.77621336996337\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.6083928683699671, Train acc: 0.7766909814323607\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.6073622367278463, Train acc: 0.7771367521367522\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.6072762993936996, Train acc: 0.7769161841742487\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.6079971703867881, Train acc: 0.7766593215811965\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.6070964386181583, Train acc: 0.7767417767417767\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.6065568120606045, Train acc: 0.7766779788838613\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.6059080539423614, Train acc: 0.7770299145299145\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.6057149882323098, Train acc: 0.7771100427350427\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.6056265141629931, Train acc: 0.7772435897435898\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.6053024810067287, Train acc: 0.7774403958614485\n",
      "Val loss: 0.5297470092773438, Val acc: 0.794\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.5785635853679771, Train acc: 0.7964743589743589\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5853909178931489, Train acc: 0.7884615384615384\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.5890695950244567, Train acc: 0.7838319088319088\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5868845585829172, Train acc: 0.7855235042735043\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5872576943574808, Train acc: 0.7863247863247863\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5861536189701483, Train acc: 0.7857015669515669\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5868094830213157, Train acc: 0.7843406593406593\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.5862317983156595, Train acc: 0.7851228632478633\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5835863973764952, Train acc: 0.7866215574548908\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5844568570582276, Train acc: 0.7864850427350427\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5813703042018664, Train acc: 0.7883158508158508\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5828699324269411, Train acc: 0.7876157407407407\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5804842809934039, Train acc: 0.7884615384615384\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5810761496024399, Train acc: 0.7883852258852259\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5830078074735114, Train acc: 0.7880698005698006\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5813728535356812, Train acc: 0.7890625\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5822626841748522, Train acc: 0.7883986928104575\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.581918338644482, Train acc: 0.7886099240265907\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5838798243037978, Train acc: 0.7876180836707153\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5851713719339962, Train acc: 0.786832264957265\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5852978288411705, Train acc: 0.7865791615791616\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.5844111712685647, Train acc: 0.7869560994560995\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5854166388755304, Train acc: 0.7866034931252323\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5853612514536435, Train acc: 0.7863470441595442\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5842876889053573, Train acc: 0.7867948717948718\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5860967409260017, Train acc: 0.7862426035502958\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.58689802985057, Train acc: 0.7862258626147515\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.586122086983497, Train acc: 0.7866300366300366\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5857783233323632, Train acc: 0.7871168582375478\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.5840759500755034, Train acc: 0.7878383190883191\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5831350183537061, Train acc: 0.7883236834849738\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5827440038008982, Train acc: 0.788528311965812\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.5819457358502409, Train acc: 0.7887691012691013\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.5824138530557661, Train acc: 0.7886029411764706\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.5816367972082708, Train acc: 0.7888278388278388\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.580866461863842, Train acc: 0.7890105650522318\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5805374022963059, Train acc: 0.7892989142989143\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.5796717129131587, Train acc: 0.7898391812865497\n",
      "Val loss: 0.511672854423523, Val acc: 0.806\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.5553226956190207, Train acc: 0.7932692307692307\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5606363133614899, Train acc: 0.7922008547008547\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.550359267221387, Train acc: 0.7959401709401709\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5545509405529652, Train acc: 0.796073717948718\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5530574989497152, Train acc: 0.7975427350427351\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5519284313794045, Train acc: 0.7973646723646723\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5548299370977177, Train acc: 0.7980006105006106\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.5567914324008629, Train acc: 0.7958066239316239\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5569988549364145, Train acc: 0.7952872744539411\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5556444754394201, Train acc: 0.7965277777777777\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5576419927521371, Train acc: 0.7961829836829837\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5565641360925219, Train acc: 0.7973201566951567\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5556568786168788, Train acc: 0.7980769230769231\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5563280464301252, Train acc: 0.79872557997558\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5559080282210285, Train acc: 0.7984330484330484\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5569797681851519, Train acc: 0.7980769230769231\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5582124900284456, Train acc: 0.7982968828557064\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.5580165711747163, Train acc: 0.7981659544159544\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5575188518214, Train acc: 0.7982737291947818\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5576789327029489, Train acc: 0.7979433760683761\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5570170620822499, Train acc: 0.7984076109076109\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.5568424577664848, Train acc: 0.7986353923853924\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5569832534406407, Train acc: 0.7985182088442958\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5585173193227362, Train acc: 0.7984107905982906\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5593973409569162, Train acc: 0.7984188034188034\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5588012939462295, Train acc: 0.798672748191979\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5590828019027734, Train acc: 0.7984924026590693\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.557195041760784, Train acc: 0.7992025335775336\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5572544997670535, Train acc: 0.7990900383141762\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.5584521133590628, Train acc: 0.7981481481481482\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5576837191798321, Train acc: 0.7983354011579818\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5576039952281703, Train acc: 0.798293936965812\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.5570314498885991, Train acc: 0.7985301735301735\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.556593181997118, Train acc: 0.7986896681749623\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.5566864057611196, Train acc: 0.7985958485958486\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.5565884797679082, Train acc: 0.798804012345679\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5559002562572538, Train acc: 0.7991741741741741\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.5555743158745085, Train acc: 0.7991452991452992\n",
      "Val loss: 0.5012684464454651, Val acc: 0.814\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.5322271703909605, Train acc: 0.8082264957264957\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5298188093126329, Train acc: 0.8111645299145299\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.5410568088802516, Train acc: 0.8069800569800569\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5435891455819464, Train acc: 0.8034188034188035\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5449398156924126, Train acc: 0.802991452991453\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5430729940660999, Train acc: 0.8040420227920227\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5453124817583587, Train acc: 0.8038003663003663\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.5451308835106783, Train acc: 0.8036191239316239\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5454915544868987, Train acc: 0.8040716999050332\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5436577616721137, Train acc: 0.8041132478632479\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5419699351455967, Train acc: 0.804438616938617\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5433090671238906, Train acc: 0.8047542735042735\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5415848152250307, Train acc: 0.8050213675213675\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5400599502208978, Train acc: 0.8058226495726496\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5403510757873201, Train acc: 0.8058404558404558\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5403405908121067, Train acc: 0.8059228098290598\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5385989320786111, Train acc: 0.8069695827048768\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.5395244596612703, Train acc: 0.8069800569800569\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.538050980297526, Train acc: 0.8071862348178138\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5368757798822008, Train acc: 0.8075587606837606\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5350347641259614, Train acc: 0.8078449328449329\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.5343551576866971, Train acc: 0.8086149961149961\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.533994935117925, Train acc: 0.8085748792270532\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5331181253219007, Train acc: 0.8089832621082621\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5340454402031043, Train acc: 0.809017094017094\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5333737971642, Train acc: 0.8091715976331361\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5327065883180166, Train acc: 0.8093937955049066\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.5329795760194679, Train acc: 0.8087988400488401\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5337782974900871, Train acc: 0.8086685823754789\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.5330494977682404, Train acc: 0.8088319088319088\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5324718193111256, Train acc: 0.8090536255858837\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5320544525837669, Train acc: 0.8091112446581197\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.5318539368036466, Train acc: 0.8091815591815592\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.5325722778155133, Train acc: 0.809043489190548\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.532770636157384, Train acc: 0.8091575091575092\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.5328958278481211, Train acc: 0.8091168091168092\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5339204581085654, Train acc: 0.8089483714483714\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.5340226367618712, Train acc: 0.8087887989203779\n",
      "Val loss: 0.4819691479206085, Val acc: 0.812\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.5478106572841986, Train acc: 0.8034188034188035\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5374011676280926, Train acc: 0.8092948717948718\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.5307289253485169, Train acc: 0.8087606837606838\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5167955921915097, Train acc: 0.8142361111111112\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.5073991757300165, Train acc: 0.818482905982906\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5100943056118284, Train acc: 0.8173967236467237\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5115974829219491, Train acc: 0.8165445665445665\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.5099837202339982, Train acc: 0.8176415598290598\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5105063993154768, Train acc: 0.8173670465337132\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.513191566361576, Train acc: 0.816025641025641\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5114960771838454, Train acc: 0.815510878010878\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5117048007513043, Train acc: 0.8159722222222222\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.513447075416037, Train acc: 0.8160749506903353\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5160073015352163, Train acc: 0.8149801587301587\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5183322990721787, Train acc: 0.8138888888888889\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5169901866386206, Train acc: 0.8146033653846154\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5179337763633484, Train acc: 0.8139140271493213\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.5200049312748121, Train acc: 0.8135090218423552\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5209051437987651, Train acc: 0.8132028789923527\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5193907600883235, Train acc: 0.8131143162393163\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5185983651565933, Train acc: 0.812932437932438\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.517815981471293, Train acc: 0.8132770007770008\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5169162452220917, Train acc: 0.813312894834634\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5173531321336401, Train acc: 0.8133235398860399\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5164346423464963, Train acc: 0.8133547008547009\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5163948579809526, Train acc: 0.8135889217619987\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5167541106730633, Train acc: 0.8137662234884457\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.5165348622142832, Train acc: 0.8142361111111112\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.5166206303165966, Train acc: 0.8142499263188918\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.5173675508601883, Train acc: 0.8139245014245015\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.5171574289669897, Train acc: 0.8139474772539289\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.5167485872108457, Train acc: 0.8144865117521367\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.5166767718406947, Train acc: 0.814474876974877\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.516464486152548, Train acc: 0.8146053293112117\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.5159869825301444, Train acc: 0.8146672771672772\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.5144543397786044, Train acc: 0.8153193257359924\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.5148475506160297, Train acc: 0.8154308154308154\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.5145611737628966, Train acc: 0.8158175888439047\n",
      "Val loss: 0.5183323621749878, Val acc: 0.806\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.5183776429830453, Train acc: 0.8076923076923077\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.5073667988181114, Train acc: 0.8092948717948718\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4971658051905469, Train acc: 0.8121438746438746\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.5033550179507743, Train acc: 0.812232905982906\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.506654938023824, Train acc: 0.8141025641025641\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.5035432642617627, Train acc: 0.8143696581196581\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.5047008486621546, Train acc: 0.8143315018315018\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.5111863937181158, Train acc: 0.8111645299145299\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.5103174043927211, Train acc: 0.811965811965812\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.5072152523785575, Train acc: 0.812767094017094\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.5072330133851962, Train acc: 0.814005439005439\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.5079883602567208, Train acc: 0.8142806267806267\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.5046638931630866, Train acc: 0.8164036817882971\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.5027309691622144, Train acc: 0.8174221611721612\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.5041683934607737, Train acc: 0.8172008547008547\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.5027506364843785, Train acc: 0.8182091346153846\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.5021349899564453, Train acc: 0.8190045248868778\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.5029166140447655, Train acc: 0.8187321937321937\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.5033568517345595, Train acc: 0.8188259109311741\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.5034934451501084, Train acc: 0.8192307692307692\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.5025219552015744, Train acc: 0.8198260073260073\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.5011802513815213, Train acc: 0.8199786324786325\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.5011285562019772, Train acc: 0.8202573392790784\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.5015622491223944, Train acc: 0.8205128205128205\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.5008554901883133, Train acc: 0.8210897435897436\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.5000964643790055, Train acc: 0.8208004602235371\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.5003196462875644, Train acc: 0.820849161126939\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.4996713567343205, Train acc: 0.8210470085470085\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.4995218909356821, Train acc: 0.8215259357500737\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.49984448182574365, Train acc: 0.8213675213675213\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.49982526064584626, Train acc: 0.8216845878136201\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.49983414268304205, Train acc: 0.821731436965812\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.4991156613288751, Train acc: 0.8218401968401968\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4984354965179005, Train acc: 0.8218640020110608\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.4976835969276044, Train acc: 0.8224053724053724\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.49827618156148495, Train acc: 0.8222934472934473\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.49703385260280547, Train acc: 0.8228083853083853\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.49715614363758937, Train acc: 0.8226495726495726\n",
      "Val loss: 0.4999014139175415, Val acc: 0.82\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4920702413616017, Train acc: 0.8322649572649573\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4756977665437083, Train acc: 0.8295940170940171\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4861923321614578, Train acc: 0.8247863247863247\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4931715766015725, Train acc: 0.8266559829059829\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4918571723704664, Train acc: 0.8277777777777777\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.4875063756931881, Train acc: 0.8301282051282052\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.4932124871619601, Train acc: 0.8285256410256411\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.49171263125971854, Train acc: 0.8285924145299145\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.4897122778500813, Train acc: 0.8279914529914529\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.4893281681288002, Train acc: 0.8285256410256411\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.4892091721921534, Train acc: 0.828428515928516\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.4908173720105591, Train acc: 0.8276353276353277\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.49018452538913365, Train acc: 0.8275805391190006\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.48994212790923386, Train acc: 0.8269612332112332\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.4897603890452629, Train acc: 0.8266737891737892\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.48851353814029413, Train acc: 0.8270232371794872\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.48974196493340955, Train acc: 0.826263197586727\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4894523104654531, Train acc: 0.8269824311490979\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.48880786741310767, Train acc: 0.8269230769230769\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4894852235180955, Train acc: 0.8268963675213675\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4888235522865473, Train acc: 0.8272028897028897\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.488384301819793, Train acc: 0.827287296037296\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.4886775309187424, Train acc: 0.8269695280564846\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.48731706771426475, Train acc: 0.8277021011396012\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4863007778362331, Train acc: 0.8280555555555555\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.485029903553392, Train acc: 0.8284023668639053\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4845607434483448, Train acc: 0.8285652105096549\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.485942504583643, Train acc: 0.8279532967032966\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.48597063490670234, Train acc: 0.828028293545535\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4859506967209513, Train acc: 0.8278490028490029\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.48595951296243256, Train acc: 0.8278363661428177\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.4851021810044718, Train acc: 0.8279914529914529\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.4836644991051345, Train acc: 0.8286713286713286\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4826699476005475, Train acc: 0.8289812719959779\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.4824495494365692, Train acc: 0.8291208791208792\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.48205186217202534, Train acc: 0.8293566001899335\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.48172403405770936, Train acc: 0.8294640794640795\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.4811511983658433, Train acc: 0.8295518443544759\n",
      "Val loss: 0.4610428512096405, Val acc: 0.838\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.44887031143547124, Train acc: 0.8349358974358975\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.46271726638715494, Train acc: 0.8344017094017094\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.46107301841943693, Train acc: 0.8397435897435898\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4597273767153677, Train acc: 0.8388087606837606\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4669370985820762, Train acc: 0.8351495726495727\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.46715832528183265, Train acc: 0.8343126780626781\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.4662462192086073, Train acc: 0.833409645909646\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4663575434993602, Train acc: 0.8319978632478633\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.4657969421796423, Train acc: 0.8312559354226021\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.46346936364077096, Train acc: 0.8320512820512821\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.46310563664521553, Train acc: 0.83250777000777\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.46322935834484563, Train acc: 0.8323539886039886\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.46301138563660865, Train acc: 0.8331278763971072\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.46544693294862544, Train acc: 0.832226800976801\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.46607019775613423, Train acc: 0.8320156695156695\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4661358781158924, Train acc: 0.8327657585470085\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4657444783030832, Train acc: 0.8327991452991453\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.46534264327539, Train acc: 0.8329475308641975\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.46421110159304835, Train acc: 0.8337831758884391\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4628971954489238, Train acc: 0.8345352564102564\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.46361606664679667, Train acc: 0.8340710215710215\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.4647842115435312, Train acc: 0.8336732711732712\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.4651437055060674, Train acc: 0.8334726867335563\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.4646190675890917, Train acc: 0.8336672008547008\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.46332838588672826, Train acc: 0.834017094017094\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4634901574463201, Train acc: 0.8343606180144642\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4655719359345491, Train acc: 0.8340257993035771\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.464734811421088, Train acc: 0.8342872405372406\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.465029807520315, Train acc: 0.8345859121721191\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.46583343994859444, Train acc: 0.8340277777777778\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.46607286597365577, Train acc: 0.8339364488558036\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.46656872853776044, Train acc: 0.8341012286324786\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.46716158675274466, Train acc: 0.833980833980834\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4670960294694278, Train acc: 0.8337418300653595\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.4670606804099639, Train acc: 0.8337912087912088\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.46727452506129924, Train acc: 0.8337636514719848\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4671327846146712, Train acc: 0.8337664587664587\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.46842359546764684, Train acc: 0.8330521817363923\n",
      "Val loss: 0.4729762673377991, Val acc: 0.844\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4912910534021182, Train acc: 0.8360042735042735\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.4953385306856571, Train acc: 0.8269230769230769\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4823630260691004, Train acc: 0.8304843304843305\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.47969808112670725, Train acc: 0.8309294871794872\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.4749484934485876, Train acc: 0.8315170940170941\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.470805870830758, Train acc: 0.8328881766381766\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.4649115321800677, Train acc: 0.8346306471306472\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4636187579435034, Train acc: 0.8350026709401709\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.4656862517763973, Train acc: 0.8341642924976258\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.46493111410711563, Train acc: 0.8344017094017094\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.46598613989867255, Train acc: 0.8342560217560218\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.46553065535477084, Train acc: 0.8341791310541311\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.46368408385348275, Train acc: 0.8356344510190664\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.46190112230159863, Train acc: 0.8360424297924298\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.45977884950885745, Train acc: 0.8362891737891738\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.46017077622903335, Train acc: 0.8358707264957265\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4598480127171214, Train acc: 0.8358157365510307\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.46031611675974987, Train acc: 0.835440408357075\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.4612773783267, Train acc: 0.835076473234368\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4604155357608683, Train acc: 0.8355502136752136\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4613767529788132, Train acc: 0.8352157102157102\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.4611669770834885, Train acc: 0.8351301476301476\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.4608464777131853, Train acc: 0.8356326644370122\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.46037583574493474, Train acc: 0.8360710470085471\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.46060203928213855, Train acc: 0.8360897435897436\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4599346852004724, Train acc: 0.8362302761341223\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.45905947919679085, Train acc: 0.8366769547325102\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.45928886834809485, Train acc: 0.837129884004884\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.45880437283750625, Train acc: 0.8374410551134689\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4584508826611219, Train acc: 0.8377136752136752\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.45743386706875017, Train acc: 0.8382788806175903\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.45640195805666983, Train acc: 0.8384081196581197\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.45516124290525833, Train acc: 0.8387561512561512\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4555553504986461, Train acc: 0.8389894419306184\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.454752493550562, Train acc: 0.8393009768009768\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.45550626393050264, Train acc: 0.8389423076923077\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.45416112405394926, Train acc: 0.8394548394548395\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.4546349417138384, Train acc: 0.8392094017094017\n",
      "Val loss: 0.46404144167900085, Val acc: 0.842\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4644113832559341, Train acc: 0.8226495726495726\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.45882394390865266, Train acc: 0.8279914529914529\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.4581955912658292, Train acc: 0.8288817663817664\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.45337722475966835, Train acc: 0.8331997863247863\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.45778933586473136, Train acc: 0.8358974358974359\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.4559093604548874, Train acc: 0.8374287749287749\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.45502971663106784, Train acc: 0.8378357753357754\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.45891722469216484, Train acc: 0.8368055555555556\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.45814900587738294, Train acc: 0.837369420702754\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.45385210764331696, Train acc: 0.8390491452991453\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.45290109001696344, Train acc: 0.8392094017094017\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.45645036618954943, Train acc: 0.8384971509971509\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.45405271126693364, Train acc: 0.8389628533859304\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.4557679068747458, Train acc: 0.8384081196581197\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.45385629035468794, Train acc: 0.8391737891737892\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4529747981339311, Train acc: 0.839576655982906\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.4518693300566223, Train acc: 0.8399949723479135\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.451426305149214, Train acc: 0.8401293922127255\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.451321153408415, Train acc: 0.8399685110211426\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4506788045072403, Train acc: 0.8395566239316239\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4503410147866147, Train acc: 0.8395909645909646\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.4494361786494537, Train acc: 0.8403263403263403\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.4478312496639946, Train acc: 0.8409977703455964\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.4482561665165246, Train acc: 0.8406339031339032\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.44744033680257633, Train acc: 0.8408760683760684\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4461536577937636, Train acc: 0.841120151216305\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.44713842414553007, Train acc: 0.8407921810699589\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.44695353656279446, Train acc: 0.8411553724053724\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.4463113477882333, Train acc: 0.8414566755083996\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4458144194040543, Train acc: 0.841755698005698\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.4461234857926265, Train acc: 0.8418803418803419\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.44573903505184936, Train acc: 0.8416967147435898\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.44544651947589436, Train acc: 0.8417346542346542\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.4446033051124151, Train acc: 0.8418646304675717\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.44493371401168635, Train acc: 0.841468253968254\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4450950951334036, Train acc: 0.8413758309591642\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4451768426054632, Train acc: 0.841014091014091\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.4444532702847464, Train acc: 0.8412477507872245\n",
      "Val loss: 0.4705488383769989, Val acc: 0.84\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.4326033289106483, Train acc: 0.8418803418803419\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.43244683637450904, Train acc: 0.8472222222222222\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.43787465064104464, Train acc: 0.843482905982906\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.4387276509983672, Train acc: 0.8448183760683761\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.440314165980388, Train acc: 0.8427350427350427\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.445406140764894, Train acc: 0.8407229344729344\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.44058301699176083, Train acc: 0.8424145299145299\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.43498323931016475, Train acc: 0.8456864316239316\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.43363997218502437, Train acc: 0.8453822412155746\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.4359472567135962, Train acc: 0.844284188034188\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.4377514132426613, Train acc: 0.8437742812742812\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.43785712450339415, Train acc: 0.8426816239316239\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.4388525094506303, Train acc: 0.8423734385272846\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.4374257361013738, Train acc: 0.8426816239316239\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.43798815881849357, Train acc: 0.8428418803418803\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.43507854016816133, Train acc: 0.8436832264957265\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.43467485349552304, Train acc: 0.8441742081447964\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.4341781979916725, Train acc: 0.8443138651471985\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.43579543885002786, Train acc: 0.8439327485380117\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4355021177162217, Train acc: 0.8444177350427351\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.436306659761463, Train acc: 0.844449531949532\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.43719024033153253, Train acc: 0.8440899378399378\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.4374761898146713, Train acc: 0.8439938684503902\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.43769154949591327, Train acc: 0.8442174145299145\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4374205654286421, Train acc: 0.8443803418803418\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4366364519710038, Train acc: 0.8442225509533202\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.43439602730757415, Train acc: 0.8450656853434632\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.4339715234027841, Train acc: 0.845066391941392\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.43446970679526864, Train acc: 0.8452144120247569\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.4337350695147219, Train acc: 0.8457621082621083\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.4336302451715428, Train acc: 0.8457402812241522\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.43315020179610825, Train acc: 0.8458366720085471\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.4322957281140011, Train acc: 0.8462833462833463\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.43274086193767336, Train acc: 0.8462166918049271\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.43290641285542747, Train acc: 0.8462454212454212\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4330291709706102, Train acc: 0.8463319088319088\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4328150862548651, Train acc: 0.8465725340725341\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.4332884853941213, Train acc: 0.8463787674313991\n",
      "Val loss: 0.4653319716453552, Val acc: 0.846\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/4545 - Train loss: 0.41495215077685493, Train acc: 0.8504273504273504\n",
      "Iteration 234 - Batch 234/4545 - Train loss: 0.431511104457144, Train acc: 0.8461538461538461\n",
      "Iteration 351 - Batch 351/4545 - Train loss: 0.42641878966507407, Train acc: 0.8479344729344729\n",
      "Iteration 468 - Batch 468/4545 - Train loss: 0.42095475257016146, Train acc: 0.8486912393162394\n",
      "Iteration 585 - Batch 585/4545 - Train loss: 0.41825428324886876, Train acc: 0.8478632478632478\n",
      "Iteration 702 - Batch 702/4545 - Train loss: 0.4203570141666635, Train acc: 0.8457977207977208\n",
      "Iteration 819 - Batch 819/4545 - Train loss: 0.41877179694736105, Train acc: 0.8472222222222222\n",
      "Iteration 936 - Batch 936/4545 - Train loss: 0.4172878540797621, Train acc: 0.8482238247863247\n",
      "Iteration 1053 - Batch 1053/4545 - Train loss: 0.42129197584399697, Train acc: 0.8475783475783476\n",
      "Iteration 1170 - Batch 1170/4545 - Train loss: 0.4211435745605546, Train acc: 0.8473290598290598\n",
      "Iteration 1287 - Batch 1287/4545 - Train loss: 0.42144379789688463, Train acc: 0.8472222222222222\n",
      "Iteration 1404 - Batch 1404/4545 - Train loss: 0.42053354796329967, Train acc: 0.8475783475783476\n",
      "Iteration 1521 - Batch 1521/4545 - Train loss: 0.41916524135874417, Train acc: 0.8483316896778436\n",
      "Iteration 1638 - Batch 1638/4545 - Train loss: 0.41968027063814856, Train acc: 0.8480616605616605\n",
      "Iteration 1755 - Batch 1755/4545 - Train loss: 0.42132993065393887, Train acc: 0.8480769230769231\n",
      "Iteration 1872 - Batch 1872/4545 - Train loss: 0.4199896191533368, Train acc: 0.8483907585470085\n",
      "Iteration 1989 - Batch 1989/4545 - Train loss: 0.42105160545709447, Train acc: 0.8485734037204625\n",
      "Iteration 2106 - Batch 2106/4545 - Train loss: 0.42077998577105014, Train acc: 0.8490325261158594\n",
      "Iteration 2223 - Batch 2223/4545 - Train loss: 0.4201739831679692, Train acc: 0.8490778227620333\n",
      "Iteration 2340 - Batch 2340/4545 - Train loss: 0.4207878357197484, Train acc: 0.8489583333333334\n",
      "Iteration 2457 - Batch 2457/4545 - Train loss: 0.4221082132083762, Train acc: 0.8485958485958486\n",
      "Iteration 2574 - Batch 2574/4545 - Train loss: 0.42126335707734097, Train acc: 0.8493104118104118\n",
      "Iteration 2691 - Batch 2691/4545 - Train loss: 0.4206776495874814, Train acc: 0.8493357487922706\n",
      "Iteration 2808 - Batch 2808/4545 - Train loss: 0.42211261889008417, Train acc: 0.8487580128205128\n",
      "Iteration 2925 - Batch 2925/4545 - Train loss: 0.4208052039770489, Train acc: 0.8492307692307692\n",
      "Iteration 3042 - Batch 3042/4545 - Train loss: 0.4203909972840571, Train acc: 0.8494617028270874\n",
      "Iteration 3159 - Batch 3159/4545 - Train loss: 0.4214275588504628, Train acc: 0.8493391896169674\n",
      "Iteration 3276 - Batch 3276/4545 - Train loss: 0.4209907195713912, Train acc: 0.8498168498168498\n",
      "Iteration 3393 - Batch 3393/4545 - Train loss: 0.42273311193010255, Train acc: 0.8493037135278515\n",
      "Iteration 3510 - Batch 3510/4545 - Train loss: 0.42304165012009465, Train acc: 0.8490562678062679\n",
      "Iteration 3627 - Batch 3627/4545 - Train loss: 0.42291976237653667, Train acc: 0.8490315687896333\n",
      "Iteration 3744 - Batch 3744/4545 - Train loss: 0.4239453883208812, Train acc: 0.8486745459401709\n",
      "Iteration 3861 - Batch 3861/4545 - Train loss: 0.4240206668587106, Train acc: 0.8488409738409738\n",
      "Iteration 3978 - Batch 3978/4545 - Train loss: 0.42462593493931644, Train acc: 0.8485576923076923\n",
      "Iteration 4095 - Batch 4095/4545 - Train loss: 0.4246572266759235, Train acc: 0.8484584859584859\n",
      "Iteration 4212 - Batch 4212/4545 - Train loss: 0.4245227569190596, Train acc: 0.8483796296296297\n",
      "Iteration 4329 - Batch 4329/4545 - Train loss: 0.4238016074783644, Train acc: 0.8486082236082236\n",
      "Iteration 4446 - Batch 4446/4545 - Train loss: 0.42358815623091195, Train acc: 0.8485717498875394\n",
      "Val loss: 0.47022053599357605, Val acc: 0.848\n",
      "Early stopping at epoch 18 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 624.2220 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-5\n",
    "dropout_p = 0.6\n",
    "batch_size = 16\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 50\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.88      0.79      0.83       100\n",
      "          SN       0.84      0.78      0.81       100\n",
      "          VS       0.82      0.89      0.86       100\n",
      "    asteroid       0.81      0.94      0.87       100\n",
      "       bogus       0.92      0.86      0.89       100\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.86      0.85      0.85       500\n",
      "weighted avg       0.86      0.85      0.85       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(model, data_loader, use_gpu):\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    if use_gpu:\n",
    "        model = model.cuda()  # Transfiere el modelo a la GPU\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(labels.numpy())\n",
    "\n",
    "    return all_true, all_preds\n",
    "\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Asume que ya has obtenido true_labels y predicted_labels, como en el código anterior\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.611445421846504, Train acc: 0.19417735042735043\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.6037703344964573, Train acc: 0.21674679487179488\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.5815222745607382, Train acc: 0.25258190883190884\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.5540312161812415, Train acc: 0.28552350427350426\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.527955819806482, Train acc: 0.3138888888888889\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.5004308138817464, Train acc: 0.33738425925925924\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.472836331642882, Train acc: 0.3582493894993895\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4474922081089427, Train acc: 0.3764356303418803\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4224743537979814, Train acc: 0.392687559354226\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4007956092174236, Train acc: 0.4067040598290598\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3810928927527533, Train acc: 0.42006604506604506\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3621104496326881, Train acc: 0.4311565170940171\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.346661284085875, Train acc: 0.44138313609467456\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3303926400794797, Train acc: 0.4501297313797314\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3153674004084703, Train acc: 0.45865384615384613\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.302630992845083, Train acc: 0.4651442307692308\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.2892627244980985, Train acc: 0.47162518853695323\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.277105110125211, Train acc: 0.4774008784425451\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.2667324526643517, Train acc: 0.48276540710751237\n",
      "Val loss: 0.9282698035240173, Val acc: 0.634\n",
      "Epoch 2/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.057708440682827, Train acc: 0.5844017094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.0599306837106361, Train acc: 0.5825320512820513\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.048418711732935, Train acc: 0.5844017094017094\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.0374147930206397, Train acc: 0.5874732905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.0351806455188328, Train acc: 0.5892094017094017\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.028638954940345, Train acc: 0.5905003561253561\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.0280453318932408, Train acc: 0.5913843101343101\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.0251643949339533, Train acc: 0.5935496794871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.0218556262941896, Train acc: 0.5957383665716999\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.0196099388803173, Train acc: 0.5965010683760684\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.0178724679728304, Train acc: 0.5971008158508159\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.0158074260861785, Train acc: 0.5980012464387464\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.0145915340703229, Train acc: 0.5989686061801447\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.0088154806031122, Train acc: 0.6014766483516484\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.0070890974115443, Train acc: 0.6023504273504273\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.0037699829245734, Train acc: 0.6036157852564102\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.0018650330209804, Train acc: 0.6043709150326797\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.9996352416214667, Train acc: 0.6049679487179487\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.9968902047989983, Train acc: 0.6059238641475484\n",
      "Val loss: 0.8844938278198242, Val acc: 0.66\n",
      "Epoch 3/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.9618360084346217, Train acc: 0.6038995726495726\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.955841664575104, Train acc: 0.6148504273504274\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.9533752150345393, Train acc: 0.6166310541310541\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.9476149856534779, Train acc: 0.6193910256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.9476416229182838, Train acc: 0.6174679487179487\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.9466473296327129, Train acc: 0.6180110398860399\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.9458585512507093, Train acc: 0.6181318681318682\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.9439756869632974, Train acc: 0.6180889423076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.9419036405718225, Train acc: 0.6198361823361823\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.9402352017724616, Train acc: 0.6205662393162393\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.9389330449730459, Train acc: 0.6216249028749029\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.9349217783776443, Train acc: 0.6236200142450142\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.9367406641394586, Train acc: 0.6236850756081526\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.9345677430230911, Train acc: 0.6246375152625152\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.9314169714253853, Train acc: 0.6264601139601139\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.9292598791802541, Train acc: 0.6277877938034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.9274640031170162, Train acc: 0.6279851684263449\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.9265277424202799, Train acc: 0.6287096391263058\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.9243876169889401, Train acc: 0.6295546558704453\n",
      "Val loss: 0.8230376839637756, Val acc: 0.662\n",
      "Epoch 4/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.868655676770414, Train acc: 0.6493055555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8782772097067956, Train acc: 0.6426282051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8813229180299319, Train acc: 0.6428062678062678\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8758901484374307, Train acc: 0.6451655982905983\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8741839142436655, Train acc: 0.6465811965811966\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8722944768660429, Train acc: 0.6483262108262108\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8728871489983048, Train acc: 0.649229242979243\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8723533864523101, Train acc: 0.6487379807692307\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8720376479376641, Train acc: 0.6492165242165242\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8708273267898804, Train acc: 0.6487713675213675\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8696656050891401, Train acc: 0.6496212121212122\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8690103694923923, Train acc: 0.6495949074074074\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8695832577917938, Train acc: 0.6497164694280079\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8702230102704412, Train acc: 0.6493437118437119\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8686813471833525, Train acc: 0.6498219373219373\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8679617813541594, Train acc: 0.650657719017094\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8684280363832544, Train acc: 0.6500911261940674\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8699231563385396, Train acc: 0.6496171652421653\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.870234121999599, Train acc: 0.649699167791273\n",
      "Val loss: 0.7765830159187317, Val acc: 0.702\n",
      "Epoch 5/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8685782775919662, Train acc: 0.6482371794871795\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.849600825809006, Train acc: 0.6545138888888888\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8452661120653832, Train acc: 0.6566061253561254\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8429835348302482, Train acc: 0.6579861111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8378057978601537, Train acc: 0.6622863247863248\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8364817644880708, Train acc: 0.6622150997150997\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8335818295732086, Train acc: 0.6629273504273504\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8348222877034265, Train acc: 0.6641626602564102\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8325352975559144, Train acc: 0.665954415954416\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8328397802817515, Train acc: 0.6664797008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8309496784960473, Train acc: 0.6674436674436675\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8313338120396321, Train acc: 0.6669115028490028\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8310731635879013, Train acc: 0.666214661406969\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8300402665895129, Train acc: 0.6673534798534798\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8300359087112622, Train acc: 0.6671474358974359\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8290480594668124, Train acc: 0.6681356837606838\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8281171351536367, Train acc: 0.6688348416289592\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.825516177972837, Train acc: 0.6700053418803419\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8236146314665374, Train acc: 0.6708417678812416\n",
      "Val loss: 0.7001733183860779, Val acc: 0.73\n",
      "Epoch 6/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8147836903221587, Train acc: 0.6738782051282052\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8066470070272429, Train acc: 0.6794871794871795\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8045789743760372, Train acc: 0.6801103988603988\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8045946958228054, Train acc: 0.6785523504273504\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8002298082041944, Train acc: 0.6805021367521368\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7960400590275087, Train acc: 0.6831374643874644\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7945480514431466, Train acc: 0.6837988400488401\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.7919664713434684, Train acc: 0.6850961538461539\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.790508280735523, Train acc: 0.6853929249762583\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7882024373763646, Train acc: 0.6861912393162393\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7888016366365813, Train acc: 0.6868201243201243\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.7886871729905789, Train acc: 0.687232905982906\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7892048901864214, Train acc: 0.6869452662721893\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7889317253534028, Train acc: 0.6867559523809523\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.7882791029252219, Train acc: 0.6872150997150998\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.7879903468852624, Train acc: 0.6872829861111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.7856477328794814, Train acc: 0.6887569130216189\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.7854301651828309, Train acc: 0.6890135327635327\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7839604145602176, Train acc: 0.6899600764732343\n",
      "Val loss: 0.6911625862121582, Val acc: 0.732\n",
      "Epoch 7/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7688311608428628, Train acc: 0.7083333333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.765854306710072, Train acc: 0.7005876068376068\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7673674297128987, Train acc: 0.7017450142450142\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7660597536044244, Train acc: 0.703125\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.763586790388466, Train acc: 0.7025641025641025\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7614686102354289, Train acc: 0.7045049857549858\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7598556560328883, Train acc: 0.7060439560439561\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.7615260458272747, Train acc: 0.7055622329059829\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.7598889796991973, Train acc: 0.7042378917378918\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7596633840575178, Train acc: 0.703659188034188\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7568158425836482, Train acc: 0.7041812354312355\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.755679967417846, Train acc: 0.7042824074074074\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7534769555795043, Train acc: 0.7049022024983563\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7526064434330979, Train acc: 0.7054334554334555\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.7530863340933438, Train acc: 0.7059650997150997\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.7547274988590398, Train acc: 0.7057291666666666\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.755840315298557, Train acc: 0.7058823529411765\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.7539252956227133, Train acc: 0.706849477682811\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7528599263095556, Train acc: 0.7079537786774629\n",
      "Val loss: 0.6444390416145325, Val acc: 0.746\n",
      "Epoch 8/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.738132687460663, Train acc: 0.7080662393162394\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.7398912193428757, Train acc: 0.7163461538461539\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7372341185723275, Train acc: 0.7163461538461539\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7417280035268548, Train acc: 0.7149439102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.7385638298132481, Train acc: 0.7156517094017094\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7387041458037504, Train acc: 0.7146990740740741\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.74091325167189, Train acc: 0.7154304029304029\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.7399228739305439, Train acc: 0.7152110042735043\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.7365489476709737, Train acc: 0.7148622981956315\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7365906089799017, Train acc: 0.7141559829059829\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7334107030660678, Train acc: 0.7160547785547785\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.733453714100384, Train acc: 0.71525551994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7325037619849712, Train acc: 0.7160379684418146\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7307331407572324, Train acc: 0.7169757326007326\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.7296324384178531, Train acc: 0.7179309116809117\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.7278640895413283, Train acc: 0.7185997596153846\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.7267524530655058, Train acc: 0.7194727249874309\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.7260570625181099, Train acc: 0.719877730294397\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7247746116772891, Train acc: 0.720844579397211\n",
      "Val loss: 0.6310014724731445, Val acc: 0.75\n",
      "Epoch 9/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6828290356530083, Train acc: 0.7430555555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.694114331760977, Train acc: 0.7383814102564102\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7037881003995227, Train acc: 0.7344195156695157\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.702397996161738, Train acc: 0.7335069444444444\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6976168703829121, Train acc: 0.7357905982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.701841737257789, Train acc: 0.7313479344729344\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7040178466847528, Train acc: 0.7296245421245421\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.7031122223816366, Train acc: 0.7292334401709402\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.704029836234657, Train acc: 0.7293447293447294\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7057793371188334, Train acc: 0.728392094017094\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7042048332898674, Train acc: 0.72885101010101\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.7034593188227751, Train acc: 0.7293224715099715\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7029392265865946, Train acc: 0.7300295857988166\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7026913599617169, Train acc: 0.7296436202686203\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.7023641005031064, Train acc: 0.729789886039886\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.7009433976605407, Train acc: 0.7301849626068376\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.7012953952876331, Train acc: 0.7297479889391654\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.7003109564921568, Train acc: 0.7298344017094017\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7004305411312851, Train acc: 0.7302209851551957\n",
      "Val loss: 0.614513635635376, Val acc: 0.762\n",
      "Epoch 10/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7229816587562234, Train acc: 0.7203525641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.71182841508307, Train acc: 0.7313034188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6994205502702979, Train acc: 0.7317485754985755\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6931781882149541, Train acc: 0.7334401709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6941940703962604, Train acc: 0.7336538461538461\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6932014340264165, Train acc: 0.7340633903133903\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6902809715460217, Train acc: 0.735424297924298\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6884293107268138, Train acc: 0.7363782051282052\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6847977411543202, Train acc: 0.7382478632478633\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6842409720023473, Train acc: 0.7388888888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6849669722178785, Train acc: 0.7379564879564879\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6844787411922403, Train acc: 0.7381365740740741\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6830031984460582, Train acc: 0.7385765943458251\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6841155698884538, Train acc: 0.7388011294261294\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6838026556194338, Train acc: 0.7386217948717949\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6833358968838922, Train acc: 0.7381643963675214\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6830250859080756, Train acc: 0.7387506284565108\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6831238045227154, Train acc: 0.7387968898385565\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6815778415480874, Train acc: 0.7400612910481331\n",
      "Val loss: 0.6057260036468506, Val acc: 0.764\n",
      "Epoch 11/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6935130808597956, Train acc: 0.7390491452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6929788965190578, Train acc: 0.7394497863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6777745307850362, Train acc: 0.7445690883190883\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.67974133668547, Train acc: 0.7425213675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6740705027539506, Train acc: 0.7439102564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.672600233147287, Train acc: 0.7442129629629629\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6707684444638836, Train acc: 0.7443147130647131\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6703305165163982, Train acc: 0.7450587606837606\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6704021912750922, Train acc: 0.7442723171889839\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6683705527048844, Train acc: 0.7454594017094017\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6684828668068616, Train acc: 0.7443181818181818\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6663750343428039, Train acc: 0.74568198005698\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6649230560293956, Train acc: 0.7458086785009862\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6655693852566683, Train acc: 0.7452876984126984\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6661938740826739, Train acc: 0.7448717948717949\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6656564436693732, Train acc: 0.7453258547008547\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6634641125940569, Train acc: 0.7465749120160885\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.663579972593533, Train acc: 0.74667616334283\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6640354435533968, Train acc: 0.7466824111560953\n",
      "Val loss: 0.5958189368247986, Val acc: 0.766\n",
      "Epoch 12/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6628101004494561, Train acc: 0.7473290598290598\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.656773525934953, Train acc: 0.749198717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6496467293157876, Train acc: 0.7499109686609686\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6488777868386008, Train acc: 0.7517361111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6500924293302063, Train acc: 0.7518162393162393\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6552503386199304, Train acc: 0.7507567663817664\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6559179893197885, Train acc: 0.7502289377289377\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6569293696337786, Train acc: 0.7495659722222222\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6571762136697543, Train acc: 0.7495845204178537\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6551594614982605, Train acc: 0.7498931623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6540531101806644, Train acc: 0.7496600621600622\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6524563284214066, Train acc: 0.7493100071225072\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6523736058333295, Train acc: 0.7496918145956607\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6509683340678721, Train acc: 0.7502289377289377\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6497544271144432, Train acc: 0.7504451566951567\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.648572891473006, Train acc: 0.7505508814102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6471916244160656, Train acc: 0.7512254901960784\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6472252806027731, Train acc: 0.7511870845204178\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.647549048059436, Train acc: 0.7514619883040936\n",
      "Val loss: 0.5874678492546082, Val acc: 0.758\n",
      "Epoch 13/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.658076845937305, Train acc: 0.7425213675213675\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6542511886765814, Train acc: 0.7493322649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6508228845066495, Train acc: 0.750801282051282\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6466527830204393, Train acc: 0.7534722222222222\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6421600503289802, Train acc: 0.7551816239316239\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6394723426242839, Train acc: 0.7573005698005698\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6403858417483622, Train acc: 0.7567155067155067\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6400538042633452, Train acc: 0.7576455662393162\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6393841577504549, Train acc: 0.7575676638176638\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6399475548257175, Train acc: 0.7568643162393163\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.639917513869396, Train acc: 0.7567987567987567\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6406564920546323, Train acc: 0.7563434829059829\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.638830387106387, Train acc: 0.7571088099934253\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6365013009025937, Train acc: 0.757802960927961\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6351558018274117, Train acc: 0.7583155270655271\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6348671775120192, Train acc: 0.7589977297008547\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6357211513606791, Train acc: 0.7583741830065359\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6350619692889601, Train acc: 0.7588141025641025\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6341846342216565, Train acc: 0.7589406207827261\n",
      "Val loss: 0.5748518109321594, Val acc: 0.774\n",
      "Epoch 14/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.628157816381536, Train acc: 0.7569444444444444\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6413193915644263, Train acc: 0.7578792735042735\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6389673821776681, Train acc: 0.7554309116809117\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6283154299753344, Train acc: 0.7594150641025641\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6258498410893302, Train acc: 0.7601495726495726\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6255071121573108, Train acc: 0.7613514957264957\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6255127051083305, Train acc: 0.7612179487179487\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6200804953328055, Train acc: 0.7635550213675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6227668438881551, Train acc: 0.7630282526115859\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6220264472767838, Train acc: 0.7632745726495727\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6221485102741683, Train acc: 0.7630876068376068\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6207821507357124, Train acc: 0.7641559829059829\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6226792903537424, Train acc: 0.7629232412886259\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6235670336000212, Train acc: 0.7621146214896215\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6214600116948457, Train acc: 0.7623753561253561\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6217091664767418, Train acc: 0.7621694711538461\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6207494022349966, Train acc: 0.7627262443438914\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6209206074363033, Train acc: 0.7626869658119658\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6199565984030537, Train acc: 0.7630735492577598\n",
      "Val loss: 0.5548215508460999, Val acc: 0.782\n",
      "Epoch 15/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6131129881255647, Train acc: 0.7697649572649573\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6146648691760169, Train acc: 0.766159188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6169262017279948, Train acc: 0.7670940170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.615948922932148, Train acc: 0.7681623931623932\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6160054005109347, Train acc: 0.76741452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6111224333615045, Train acc: 0.7703881766381766\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6134906183377873, Train acc: 0.7688492063492064\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6132006233191898, Train acc: 0.7681623931623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6133531508339323, Train acc: 0.7678952991452992\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6147386043244957, Train acc: 0.7678952991452992\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6144034899800159, Train acc: 0.7678952991452992\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6145607439583523, Train acc: 0.7672498219373219\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.61293092107475, Train acc: 0.7679158448389217\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6124758459855058, Train acc: 0.767876221001221\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6111947840605026, Train acc: 0.7686253561253561\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6104219022533323, Train acc: 0.7692808493589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6103370124937723, Train acc: 0.7691522121669181\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6104343347145281, Train acc: 0.7688004510921178\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6100139528097358, Train acc: 0.76865440845704\n",
      "Val loss: 0.5682511925697327, Val acc: 0.774\n",
      "Epoch 16/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6078563128781115, Train acc: 0.7641559829059829\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.601306049232809, Train acc: 0.7692307692307693\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5988033423226783, Train acc: 0.7702991452991453\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5999290454718802, Train acc: 0.7700320512820513\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5945001442717691, Train acc: 0.7730769230769231\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5943925478349724, Train acc: 0.7736823361823362\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5957638187577291, Train acc: 0.7731227106227107\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5951423195119088, Train acc: 0.773704594017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5969185460383623, Train acc: 0.7739197530864198\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5953026998501557, Train acc: 0.7745726495726496\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.596019454699852, Train acc: 0.7735285547785548\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5962685546771413, Train acc: 0.7738158831908832\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5954658372142768, Train acc: 0.7743055555555556\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5947181843994447, Train acc: 0.7743437118437119\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5954916138679553, Train acc: 0.7739494301994302\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.595667347128893, Train acc: 0.7736211271367521\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5957196012266681, Train acc: 0.7739756158873806\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.595248922344796, Train acc: 0.774335232668566\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5935370129169883, Train acc: 0.7749943769680612\n",
      "Val loss: 0.5837096571922302, Val acc: 0.782\n",
      "Epoch 17/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.628107739182619, Train acc: 0.7598824786324786\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6070114039203041, Train acc: 0.7685630341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5962263014581468, Train acc: 0.7765313390313391\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5879298701372921, Train acc: 0.7791800213675214\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5890919291056119, Train acc: 0.7794337606837607\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5894219672408199, Train acc: 0.7791132478632479\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5886483935136644, Train acc: 0.7799145299145299\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5883427433605887, Train acc: 0.7800480769230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.591511659076524, Train acc: 0.7788164767331434\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5920204351600419, Train acc: 0.7786858974358974\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5921172951670145, Train acc: 0.7783605283605284\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5916627047323434, Train acc: 0.7781561609686609\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.590315370017725, Train acc: 0.7782914201183432\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5899860550250325, Train acc: 0.7782547313797313\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5885470747692972, Train acc: 0.778187321937322\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5867871906465063, Train acc: 0.7793469551282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5864145446432002, Train acc: 0.7797417043740573\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5858049997166578, Train acc: 0.779988722697056\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5852888492155977, Train acc: 0.7800972784525416\n",
      "Val loss: 0.5478954911231995, Val acc: 0.788\n",
      "Epoch 18/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5964362951170685, Train acc: 0.7740384615384616\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5742462419419208, Train acc: 0.7864583333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.575994936701579, Train acc: 0.7844551282051282\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.575871240793385, Train acc: 0.7844551282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5755170971155167, Train acc: 0.7847222222222222\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5715150510271391, Train acc: 0.7861467236467237\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5724357937682097, Train acc: 0.7866681929181929\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5722525223420981, Train acc: 0.7863247863247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5719623381001318, Train acc: 0.7856125356125356\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5716200795056473, Train acc: 0.7866185897435898\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.572380820643837, Train acc: 0.7856449106449106\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.572077897921247, Train acc: 0.7856793091168092\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5709298672564793, Train acc: 0.7864069690992768\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5708318326317493, Train acc: 0.7862103174603174\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5687822328758375, Train acc: 0.7868233618233619\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.569144907932824, Train acc: 0.7862913995726496\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.570694075008623, Train acc: 0.7858220211161387\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5705753718869865, Train acc: 0.7858796296296297\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5700844264732949, Train acc: 0.7858890013495277\n",
      "Val loss: 0.556779146194458, Val acc: 0.792\n",
      "Epoch 19/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5840468072993124, Train acc: 0.7713675213675214\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5835026556100601, Train acc: 0.7765758547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5784562697947195, Train acc: 0.7824964387464387\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.579889598947305, Train acc: 0.78125\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5730178232376392, Train acc: 0.7841880341880342\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5724450398629207, Train acc: 0.7846331908831908\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5719744061302935, Train acc: 0.785752442002442\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5703633431440744, Train acc: 0.785690438034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5689431857406131, Train acc: 0.7866215574548908\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5677101181867795, Train acc: 0.7870192307692307\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5663033189843956, Train acc: 0.7873931623931624\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5655445556058164, Train acc: 0.7886173433048433\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.565062029435398, Train acc: 0.7883999013806706\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5645862384921029, Train acc: 0.78813721001221\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5653932014082232, Train acc: 0.7879095441595442\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5651328946885645, Train acc: 0.7883780715811965\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5646554162419579, Train acc: 0.7886814982403217\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5637481281532069, Train acc: 0.7895002374169041\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5632100165480168, Train acc: 0.7900781601439496\n",
      "Val loss: 0.5434216260910034, Val acc: 0.798\n",
      "Epoch 20/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5996631681919098, Train acc: 0.7769764957264957\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5820228489290955, Train acc: 0.7823183760683761\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5754973828962385, Train acc: 0.7862357549857549\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5731009659476769, Train acc: 0.7877270299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5725549989785903, Train acc: 0.7874465811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5710207603001527, Train acc: 0.7872150997150997\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5680296983983781, Train acc: 0.788537851037851\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5662623556792481, Train acc: 0.7894631410256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5621468623903741, Train acc: 0.7918447293447294\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5615782392712739, Train acc: 0.7912660256410257\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5605706897069987, Train acc: 0.7912296037296037\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5582270889922425, Train acc: 0.7921118233618234\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5568451092656234, Train acc: 0.7929199539776463\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5568136870861053, Train acc: 0.7930212148962149\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5561383580295448, Train acc: 0.7934294871794871\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5547998535693583, Train acc: 0.7938701923076923\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5539901206066645, Train acc: 0.7942747611865258\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5529419345761749, Train acc: 0.794678893637227\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5545473409380078, Train acc: 0.79384559154296\n",
      "Val loss: 0.5358850359916687, Val acc: 0.804\n",
      "Epoch 21/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5334777187587868, Train acc: 0.7988782051282052\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5329894562944387, Train acc: 0.8044871794871795\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5348399426268037, Train acc: 0.8027065527065527\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.540230350712171, Train acc: 0.8008814102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5395884163614012, Train acc: 0.8014957264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5360433245104262, Train acc: 0.8020388176638177\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5332610535279589, Train acc: 0.8018543956043956\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.534304222298993, Train acc: 0.8009147970085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5328669982333468, Train acc: 0.8013710826210826\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5338049195006362, Train acc: 0.8010416666666667\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5347042250698077, Train acc: 0.7998980186480187\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.535633483715886, Train acc: 0.7990117521367521\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.537954870413041, Train acc: 0.7979947403024326\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5380032199988726, Train acc: 0.7982677045177046\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5390498934308349, Train acc: 0.7983618233618234\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5382280419381638, Train acc: 0.798561030982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5386942218091992, Train acc: 0.798799648064354\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5386693339798412, Train acc: 0.7990562678062678\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5379066163631264, Train acc: 0.7995529689608637\n",
      "Val loss: 0.5207984447479248, Val acc: 0.81\n",
      "Epoch 22/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5049886130369626, Train acc: 0.8146367521367521\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5208612935155885, Train acc: 0.8103632478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5254754461454191, Train acc: 0.8072471509971509\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5317975925520445, Train acc: 0.8046875\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5301619512148392, Train acc: 0.8050213675213675\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5271651720007261, Train acc: 0.8051549145299145\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.530136713851357, Train acc: 0.8031898656898657\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5318098852299472, Train acc: 0.8031517094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5307854696301653, Train acc: 0.8030923551756886\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5339034423996241, Train acc: 0.8020566239316239\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5336888085503678, Train acc: 0.8026175213675214\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5330090822699742, Train acc: 0.8030181623931624\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5319619994270104, Train acc: 0.803439349112426\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5336928388993091, Train acc: 0.8025030525030525\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5333811536388859, Train acc: 0.802457264957265\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5312506568848959, Train acc: 0.8034521901709402\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5314633895204318, Train acc: 0.8033402463549523\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5311185963954228, Train acc: 0.803389126305793\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5310821574092501, Train acc: 0.8030251911830859\n",
      "Val loss: 0.5260993242263794, Val acc: 0.814\n",
      "Epoch 23/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5344842910511881, Train acc: 0.8015491452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5263489598137701, Train acc: 0.8042200854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5299161686601802, Train acc: 0.8039529914529915\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.525870380556991, Train acc: 0.8062232905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5221880396716615, Train acc: 0.8075320512820513\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5197354918956077, Train acc: 0.8104077635327636\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5220604152469844, Train acc: 0.809714590964591\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5203391133656359, Train acc: 0.8104300213675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5217827250588427, Train acc: 0.8100664767331434\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5216636312440929, Train acc: 0.8099358974358974\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5196390834744808, Train acc: 0.8102661227661228\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5200028270483017, Train acc: 0.8100516381766382\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5193950950433203, Train acc: 0.8098085141354372\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5178419312368965, Train acc: 0.8102678571428571\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5191773194957663, Train acc: 0.809704415954416\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5202360118094546, Train acc: 0.8091112446581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5209510127628431, Train acc: 0.8086507038712921\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5205929915138573, Train acc: 0.8086271367521367\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5207690507361227, Train acc: 0.808563877642825\n",
      "Val loss: 0.5185205936431885, Val acc: 0.816\n",
      "Epoch 24/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4955928049281112, Train acc: 0.8165064102564102\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5077672183641002, Train acc: 0.8170405982905983\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5156625525818931, Train acc: 0.8115206552706553\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5100749706546975, Train acc: 0.8118990384615384\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.513768520951271, Train acc: 0.8088141025641026\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.512262966473218, Train acc: 0.8111200142450142\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5124341419764927, Train acc: 0.8105921855921856\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5136102517891643, Train acc: 0.8093616452991453\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5120087088371388, Train acc: 0.8102445394112061\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5145917937923701, Train acc: 0.8094551282051282\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5156281363644523, Train acc: 0.8093677156177156\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.516305002954356, Train acc: 0.8096732549857549\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5161830418247835, Train acc: 0.8098085141354372\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5154544592485906, Train acc: 0.8104014041514042\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5154885289506016, Train acc: 0.8104166666666667\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5134389087334912, Train acc: 0.8105802617521367\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5133644388122376, Train acc: 0.8107560331825038\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5134843882219291, Train acc: 0.8109122744539411\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5136710935839114, Train acc: 0.8111223571749887\n",
      "Val loss: 0.5247822403907776, Val acc: 0.808\n",
      "Epoch 25/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4951165906893901, Train acc: 0.8191773504273504\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5031584875705915, Train acc: 0.8181089743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5000654356686818, Train acc: 0.8173967236467237\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4992052108749875, Train acc: 0.8161725427350427\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5000082414374393, Train acc: 0.8170940170940171\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5023433114176462, Train acc: 0.8156606125356125\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.49802629096531315, Train acc: 0.8171169108669109\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5002441180981377, Train acc: 0.8167067307692307\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5023707499531259, Train acc: 0.816625118708452\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5013607347877617, Train acc: 0.8164797008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5021891269743118, Train acc: 0.8166763791763791\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5040927195757033, Train acc: 0.8165509259259259\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5060934144991311, Train acc: 0.8145545693622617\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5045659765510186, Train acc: 0.8149229242979243\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5038603093719211, Train acc: 0.8152065527065527\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5049180284333534, Train acc: 0.8144197382478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5052025368715304, Train acc: 0.8144639265962795\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5055759894075217, Train acc: 0.8141174026590693\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5044516082408993, Train acc: 0.8146648672964463\n",
      "Val loss: 0.5057930946350098, Val acc: 0.826\n",
      "Epoch 26/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.49075111746788025, Train acc: 0.8269230769230769\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.48938635042589956, Train acc: 0.8219818376068376\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5004566367874798, Train acc: 0.8173076923076923\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4992981019119422, Train acc: 0.8159722222222222\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4946422996174576, Train acc: 0.8178952991452991\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4956718482650243, Train acc: 0.8177528490028491\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4924303735168571, Train acc: 0.8198260073260073\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.49192736826391303, Train acc: 0.820045405982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4949514139496703, Train acc: 0.8190883190883191\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.494769640878225, Train acc: 0.8190972222222223\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.494872228538425, Train acc: 0.8188374125874126\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.4944219181468005, Train acc: 0.8189325142450142\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.49283267877981274, Train acc: 0.8191568047337278\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.493241754734618, Train acc: 0.8187957875457875\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4933761072549385, Train acc: 0.8186075498575499\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.49364424395000833, Train acc: 0.8185763888888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4937334452339487, Train acc: 0.8187688536953243\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4962261934452474, Train acc: 0.8176934947768281\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.49561194202088044, Train acc: 0.8177434772829509\n",
      "Val loss: 0.5254352688789368, Val acc: 0.812\n",
      "Epoch 27/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.49357387932956726, Train acc: 0.8191773504273504\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4856703496004781, Train acc: 0.8186431623931624\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.48484469011977865, Train acc: 0.8181980056980057\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.48122864757847583, Train acc: 0.8202457264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4844877889013698, Train acc: 0.8202457264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.48932656146946796, Train acc: 0.8194444444444444\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4906748979192077, Train acc: 0.8191773504273504\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4880266721941467, Train acc: 0.820045405982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4870110279672619, Train acc: 0.821076685660019\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4850429573374936, Train acc: 0.8224358974358974\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.48446761020737833, Train acc: 0.8226252913752914\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.485703649121387, Train acc: 0.8223602207977208\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4859814100521007, Train acc: 0.8218482905982906\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.48826023959924303, Train acc: 0.8207608363858364\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.48625993554578545, Train acc: 0.8214743589743589\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.48737989647839314, Train acc: 0.8208967681623932\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4860392945564351, Train acc: 0.821408371040724\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4862343103386964, Train acc: 0.8212695868945868\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.4860067783217681, Train acc: 0.8214406207827261\n",
      "Val loss: 0.49177485704421997, Val acc: 0.82\n",
      "Epoch 28/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4642122874402592, Train acc: 0.8336004273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.466828168839471, Train acc: 0.8297275641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.467962834952224, Train acc: 0.8279024216524217\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4717517461404841, Train acc: 0.8266559829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4721009129642421, Train acc: 0.8270833333333333\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.47352563020935085, Train acc: 0.8262553418803419\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.47785620133009293, Train acc: 0.8241758241758241\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.48017737401537913, Train acc: 0.8235176282051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.47606305462455933, Train acc: 0.8251424501424501\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4748549743467926, Train acc: 0.8254807692307692\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4758031920999543, Train acc: 0.8244706682206682\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.47710566219483685, Train acc: 0.823539886039886\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4783068679611123, Train acc: 0.8232659434582511\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.4784362821297331, Train acc: 0.8233936202686203\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4802736280310867, Train acc: 0.822275641025641\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.47918713592693335, Train acc: 0.8228498931623932\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4795590715530591, Train acc: 0.8226809954751131\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4802821974313384, Train acc: 0.8225160256410257\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.4806095257944424, Train acc: 0.8225933423301844\n",
      "Val loss: 0.5145658850669861, Val acc: 0.82\n",
      "Epoch 29/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.48878963775614387, Train acc: 0.8285256410256411\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4746851325671897, Train acc: 0.8269230769230769\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4772170810108511, Train acc: 0.8250534188034188\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.47753094907245064, Train acc: 0.8247195512820513\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4809834588287223, Train acc: 0.8238782051282051\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4841004926764388, Train acc: 0.8237179487179487\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4791649482298247, Train acc: 0.8270375457875457\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4781942661119323, Train acc: 0.8268229166666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.47655842295456025, Train acc: 0.8266559829059829\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4760260953098281, Train acc: 0.8275106837606837\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4739012748535604, Train acc: 0.8281128593628594\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.4724162379624667, Train acc: 0.8286369301994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4725058802760486, Train acc: 0.8278681788297173\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.47243466651265004, Train acc: 0.8276480463980463\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.47307362583627727, Train acc: 0.8271367521367521\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.47353340186274206, Train acc: 0.8265558226495726\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4740679959905633, Train acc: 0.8265145801910507\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4731607701265008, Train acc: 0.8267450142450142\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.47261281595056354, Train acc: 0.8268527890238416\n",
      "Val loss: 0.485908180475235, Val acc: 0.818\n",
      "Epoch 30/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.46341634038676566, Train acc: 0.8269230769230769\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.47164893990907913, Train acc: 0.8247863247863247\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.47288004209173373, Train acc: 0.8250534188034188\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.47699737220875216, Train acc: 0.8237847222222222\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.47512283633407365, Train acc: 0.8239850427350427\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4723361725905682, Train acc: 0.8253650284900285\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.47156406844419146, Train acc: 0.8259691697191697\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4694014232064414, Train acc: 0.8263555021367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.46828462558480977, Train acc: 0.8260030864197531\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.46486299886153293, Train acc: 0.8269764957264957\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4644863774193843, Train acc: 0.8271416083916084\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.46396444211488436, Train acc: 0.8274350071225072\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4644588484115933, Train acc: 0.8277038132807364\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.46438462600130154, Train acc: 0.8271710927960928\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4662427948411034, Train acc: 0.8260861823361824\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.46513913243881655, Train acc: 0.8263221153846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.46523368099396284, Train acc: 0.8262160633484162\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.46529943677831126, Train acc: 0.8266114672364673\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.4654920678711452, Train acc: 0.826417004048583\n",
      "Val loss: 0.5049327611923218, Val acc: 0.824\n",
      "Epoch 31/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.48581488252195537, Train acc: 0.8170405982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4749688827074491, Train acc: 0.8195779914529915\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.47201552554073495, Train acc: 0.823005698005698\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4646984589341869, Train acc: 0.8259214743589743\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.46086566002450435, Train acc: 0.8286324786324787\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4588145835489629, Train acc: 0.8296385327635327\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.46306410168044004, Train acc: 0.8283348595848596\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4631073019729975, Train acc: 0.8282585470085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4614516439346167, Train acc: 0.8291488603988604\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4608137601079085, Train acc: 0.8296741452991453\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4605090497025428, Train acc: 0.8296668609168609\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.46037652902835796, Train acc: 0.8303952991452992\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.46169953893706017, Train acc: 0.8299638395792241\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.4614537102380661, Train acc: 0.8302808302808303\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4606260949687401, Train acc: 0.8305555555555556\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.46157740789234764, Train acc: 0.8304286858974359\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4605756822192471, Train acc: 0.8305052790346908\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4593521076708268, Train acc: 0.8310630341880342\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.4597728213934656, Train acc: 0.8309435447593342\n",
      "Val loss: 0.5040973424911499, Val acc: 0.826\n",
      "Epoch 32/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.48632550990989065, Train acc: 0.8285256410256411\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4847430690613567, Train acc: 0.8267895299145299\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4740980166910041, Train acc: 0.8295940170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4684636789955135, Train acc: 0.8307291666666666\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4674261159621752, Train acc: 0.8308760683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4633521506631816, Train acc: 0.8330217236467237\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4632766130715791, Train acc: 0.8324175824175825\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.45998147254188854, Train acc: 0.8334334935897436\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4584265940973901, Train acc: 0.8342830009496676\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4585275552593745, Train acc: 0.833974358974359\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4591234818899826, Train acc: 0.83364898989899\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.45791907290108186, Train acc: 0.8334668803418803\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.45728987223180323, Train acc: 0.8333127876397107\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.4551723121741607, Train acc: 0.8338675213675214\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.45572889243435655, Train acc: 0.8336894586894587\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.45560290592794234, Train acc: 0.8343516292735043\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.45569056047753154, Train acc: 0.8342760180995475\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4556567354051702, Train acc: 0.8340900997150997\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.45515553508451595, Train acc: 0.8345985155195681\n",
      "Val loss: 0.49892374873161316, Val acc: 0.822\n",
      "Epoch 33/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4548076481645943, Train acc: 0.8317307692307693\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4478632644073576, Train acc: 0.8372061965811965\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4516133450374984, Train acc: 0.8341346153846154\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.45528817925060916, Train acc: 0.8313969017094017\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4527196516338577, Train acc: 0.8326923076923077\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.45138916661936335, Train acc: 0.833511396011396\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.44946632817187443, Train acc: 0.8348214285714286\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4493873640656089, Train acc: 0.8355702457264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4491739192622, Train acc: 0.8355294396961064\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4460527839887346, Train acc: 0.8363514957264957\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.44729566494785916, Train acc: 0.8356643356643356\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.44679107845487587, Train acc: 0.8360487891737892\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4471286090383555, Train acc: 0.8360042735042735\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.4473648034172617, Train acc: 0.8362141330891331\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.44766051388873673, Train acc: 0.8364672364672364\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.44701689806504125, Train acc: 0.8362713675213675\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4487905226921304, Train acc: 0.8352972599296129\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.44822643760234426, Train acc: 0.8355591168091168\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.44768274407557307, Train acc: 0.8357231219073324\n",
      "Val loss: 0.5179298520088196, Val acc: 0.82\n",
      "Epoch 34/50\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4364391536029995, Train acc: 0.8424145299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.43484553248963803, Train acc: 0.844551282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4390489522293422, Train acc: 0.8418803418803419\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4374822347592085, Train acc: 0.8420806623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4414247865605558, Train acc: 0.8392094017094017\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.43962708799078254, Train acc: 0.8398771367521367\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4417588365638358, Train acc: 0.8393620268620269\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.441911157204682, Train acc: 0.8391760149572649\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4416248963938819, Train acc: 0.8386158594491928\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4435862100786633, Train acc: 0.8379807692307693\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.44376755453119376, Train acc: 0.838262432012432\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.44470559055266895, Train acc: 0.8383190883190883\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.444788897609648, Train acc: 0.8379972057856673\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.44541321986870014, Train acc: 0.8373588217338217\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.44523300459751713, Train acc: 0.8372150997150997\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.4458892934788496, Train acc: 0.8371060363247863\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4449168126402697, Train acc: 0.8375282805429864\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4450462486485244, Train acc: 0.8375474833808167\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.44485441444159307, Train acc: 0.8374803193882141\n",
      "Val loss: 0.5287754535675049, Val acc: 0.824\n",
      "Early stopping at epoch 34 due to no improvement after 5 epochs.\n",
      "Tiempo total de entrenamiento: 637.0645 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-5\n",
    "dropout_p = 0.6\n",
    "batch_size = 32\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 50\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.82      0.82      0.82       100\n",
      "          SN       0.81      0.68      0.74       100\n",
      "          VS       0.85      0.85      0.85       100\n",
      "    asteroid       0.87      0.87      0.87       100\n",
      "       bogus       0.79      0.92      0.85       100\n",
      "\n",
      "    accuracy                           0.83       500\n",
      "   macro avg       0.83      0.83      0.83       500\n",
      "weighted avg       0.83      0.83      0.83       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict(model, data_loader, use_gpu):\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    if use_gpu:\n",
    "        model = model.cuda()  # Transfiere el modelo a la GPU\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(labels.numpy())\n",
    "\n",
    "    return all_true, all_preds\n",
    "\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "# Asume que ya has obtenido true_labels y predicted_labels, como en el código anterior\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

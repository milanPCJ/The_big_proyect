{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo de red convolucional para la clasificación detección temprana de supernovas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red convolucional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se abre la data pre-procesada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys de pkl dict_keys(['Train', 'Validation', 'Test'])\n",
      "keys de Test dict_keys(['images', 'labels', 'features'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], dtype=int64)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para cargar el contenido del archivo .pkl\n",
    "def load_pickle_data(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pkl.load(f)\n",
    "    return data\n",
    "\n",
    "# Cargar el archivo\n",
    "filename = 'processed_data.pkl' \n",
    "data_procesada = load_pickle_data(filename)\n",
    "\n",
    "# Visualizar el contenido\n",
    "print('keys de pkl', data_procesada.keys())\n",
    "print('keys de Test',data_procesada['Validation'].keys())\n",
    "data_procesada['Validation']['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se generan las variables para Train, Val y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tipo de dato de Train: <built-in method type of Tensor object at 0x000001EB4BA757B0>\n",
      "forma de Train: torch.Size([72710, 3, 21, 21])\n",
      "forma de Validation: torch.Size([500, 3, 21, 21])\n",
      "forma de Test: torch.Size([500, 3, 21, 21])\n"
     ]
    }
   ],
   "source": [
    "def preparar_imagenes_para_modelo(data_dict, key_principal='Train', key_imagenes='images'):\n",
    "    # Extraer imágenes del diccionario\n",
    "    imagenes = data_dict[key_principal][key_imagenes]\n",
    "    \n",
    "    # Cambiar el orden de las dimensiones a [canales, altura, ancho]\n",
    "    imagenes = np.transpose(imagenes, (0, 3, 1, 2))\n",
    "    \n",
    "    # Convertir a tensor de PyTorch\n",
    "    imagenes_tensor = torch.tensor(imagenes, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    return imagenes_tensor\n",
    "\n",
    "# Preparar las imágenes para el modelo\n",
    "Train = preparar_imagenes_para_modelo(data_procesada)\n",
    "Val = preparar_imagenes_para_modelo(data_procesada, key_principal='Validation')\n",
    "Test = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "\n",
    "print('tipo de dato de Train:', Train.type)\n",
    "print('forma de Train:', Train.shape)\n",
    "print('forma de Validation:', Val.shape)\n",
    "print('forma de Test:', Test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función que extrae las etiquetas, es una mouskierramienta que nos servirá más adelante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_etiquetas(data_dict, key_principal='Train', key_etiquetas='labels'):\n",
    "    # Extraer etiquetas del diccionario\n",
    "    etiquetas = data_dict[key_principal][key_etiquetas]\n",
    "    # Convertir a tensor de PyTorch\n",
    "    etiquetas_tensor = torch.tensor(etiquetas, dtype=torch.long)  # Usamos dtype long porque son índices de clase\n",
    "    return etiquetas_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forma de metadata_train: torch.Size([72710, 26])\n",
      "forma de metadata_val: torch.Size([500, 26])\n",
      "forma de metadata_test: torch.Size([500, 26])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7878e-01, -6.2952e-01,  8.8655e-02,  ..., -1.4237e-01,\n",
       "         -2.2997e-01, -1.3517e-01],\n",
       "        [ 7.6781e-01, -6.2212e-01,  1.2532e+00,  ..., -2.5771e-01,\n",
       "         -3.8337e-01,  9.2198e-02],\n",
       "        [ 7.5052e-01, -6.1387e-01, -1.0466e+00,  ..., -3.4228e-01,\n",
       "         -3.9726e-01,  3.2286e-03],\n",
       "        ...,\n",
       "        [-1.8353e+00,  2.9948e+00,  8.8655e-02,  ...,  1.6006e-01,\n",
       "         -2.2666e-01, -5.8001e-01],\n",
       "        [-1.8213e+00,  1.4213e+00,  8.8655e-02,  ...,  5.7526e-01,\n",
       "         -3.6618e-01,  6.2541e-02],\n",
       "        [ 7.7110e-01,  1.4931e+00,  1.2525e+00,  ...,  3.3484e+00,\n",
       "         -1.3608e-01, -4.7127e-01]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extraer_metadata(data_dict, key_principal='Train', key_metadata='features'):\n",
    "    # Extraer características de metadatos del diccionario\n",
    "    metadata = data_dict[key_principal][key_metadata]\n",
    "    \n",
    "    # Convertir explícitamente a float32\n",
    "    metadata = metadata.astype(np.float32)\n",
    "    \n",
    "    # Convertir a tensor de PyTorch\n",
    "    metadata_tensor = torch.tensor(metadata, dtype=torch.float32)\n",
    "    \n",
    "    return metadata_tensor\n",
    "\n",
    "\n",
    "# Extraer características de metadatos para los conjuntos de datos\n",
    "metadata_train = extraer_metadata(data_procesada)\n",
    "metadata_val = extraer_metadata(data_procesada, key_principal='Validation')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "print('forma de metadata_train:', metadata_train.shape)\n",
    "print('forma de metadata_val:', metadata_val.shape)\n",
    "print('forma de metadata_test:', metadata_test.shape)\n",
    "\n",
    "metadata_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo convolucional, incorporando invariancia rotacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, dropout_p):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Zero padding para evitar perder datos de las imágenes después de las convoluciones\n",
    "        self.padding = nn.ZeroPad2d(4)  # Ajustado a 5 para compensar el mayor tamaño del kernel\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Bloques de convolución\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=0),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # Tamaño del kernel ajustado a 5, padding ajustado\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        # MLP 1\n",
    "        self.mlp1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2304, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Normalización de Batch para Metadata\n",
    "        self.metadata_bn = nn.BatchNorm1d(26)\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Dropout(p=self.dropout_p),\n",
    "            nn.Linear(90, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mlp3 = nn.Sequential(\n",
    "            nn.Linear(64, 5),\n",
    "        )\n",
    "\n",
    "\n",
    "    def rotate_input(self, x):\n",
    "        # Rotaciones para la invariancia \n",
    "        x_90 = torch.rot90(x, 1, [2, 3])\n",
    "        x_180 = torch.rot90(x, 2, [2, 3])\n",
    "        x_270 = torch.rot90(x, 3, [2, 3])\n",
    "        return torch.cat([x, x_90, x_180, x_270], dim=0)\n",
    "    \n",
    "    def cyclic_pooling(self, x):\n",
    "        B = x.size(0) // 4\n",
    "        return (x[:B] + x[B:2*B] + x[2*B:3*B] + x[3*B:]) / 4.0\n",
    "\n",
    "    def forward(self, x, metadata):\n",
    "        # Verificación de dimensiones de metadatos\n",
    "        if metadata.dim() != 2:\n",
    "            raise ValueError(\"La metadata debe ser un tensor de 2 dimensiones\")\n",
    "\n",
    "        # Zero padding\n",
    "        x = self.padding(x)\n",
    "\n",
    "        # Rotación de la entrada y bloques de convolución\n",
    "        x_rotated = self.rotate_input(x)\n",
    "        x_rotated = self.conv_blocks(x_rotated)\n",
    "\n",
    "        # Primera capa fully connected\n",
    "        x_rotated_dense = self.mlp1(x_rotated)\n",
    "\n",
    "        # Pooling Cíclico\n",
    "        x_pooled = self.cyclic_pooling(x_rotated_dense)\n",
    "\n",
    "        # Normalización de metadatos\n",
    "        normalized_metadata = self.metadata_bn(metadata)\n",
    "\n",
    "        # Concatenación de características de imagen y metadatos\n",
    "        combined_features = torch.cat([x_pooled, normalized_metadata], dim=1)\n",
    "\n",
    "        # Pasar por la segunda parte del MLP\n",
    "        out = self.mlp2(combined_features)\n",
    "        out = self.mlp3(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de entrenamiento y visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_curves(curves):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
    "    fig.set_facecolor('white')\n",
    "\n",
    "    epochs = np.arange(len(curves[\"val_loss\"])) + 1\n",
    "\n",
    "    ax[0].plot(epochs, curves['val_loss'], label='validation')\n",
    "    ax[0].plot(epochs, curves['train_loss'], label='training')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[0].set_title('Loss evolution during training')\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(epochs, curves['val_acc'], label='validation')\n",
    "    ax[1].plot(epochs, curves['train_acc'], label='training')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    ax[1].set_title('Accuracy evolution during training')\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perdida_regularizada_entropia(salidas, objetivos, beta=0.1):\n",
    "    # Pérdida de entropía cruzada\n",
    "    perdida_ec = F.cross_entropy(salidas, objetivos, reduction='mean')\n",
    "    \n",
    "    # Calcula la entropía de las probabilidades\n",
    "    probabilidades = F.softmax(salidas, dim=1)\n",
    "    entropia = -torch.sum(probabilidades * torch.log(probabilidades + 1e-9), dim=1)\n",
    "    entropia_reg = torch.mean(entropia)\n",
    "    \n",
    "    # Pérdida combinada\n",
    "    perdida_total = perdida_ec - beta * entropia_reg\n",
    "    \n",
    "    return perdida_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch, metadata_batch, model, optimizer, criterion, beta):\n",
    "    # Predicción\n",
    "    y_predicted = model(x_batch, metadata_batch)\n",
    "\n",
    "    # Cálculo de la pérdida\n",
    "    loss = criterion(y_predicted, y_batch, beta=beta)\n",
    "\n",
    "    # Actualización de parámetros\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return y_predicted, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(data_loader, model, criterion, use_gpu):\n",
    "    cumulative_loss = 0\n",
    "    cumulative_predictions = 0\n",
    "    data_count = 0\n",
    "\n",
    "    for x_val, y_val, metadata_val in data_loader:\n",
    "        if use_gpu:\n",
    "            x_val = x_val.cuda()\n",
    "            y_val = y_val.cuda()\n",
    "            metadata_val = metadata_val.cuda()\n",
    "\n",
    "        y_predicted = model(x_val, metadata_val)\n",
    "        \n",
    "        loss = criterion(y_predicted, y_val)\n",
    "\n",
    "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "\n",
    "        cumulative_predictions += (y_val == class_prediction).sum().item()\n",
    "        cumulative_loss += loss.item()\n",
    "        data_count += y_val.shape[0]\n",
    "\n",
    "    val_acc = cumulative_predictions / data_count\n",
    "    val_loss = cumulative_loss / len(data_loader)\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_data_images,\n",
    "    train_data_labels,\n",
    "    train_data_metadata,\n",
    "    val_data_images,\n",
    "    val_data_labels,\n",
    "    val_data_metadata,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu,\n",
    "    beta=0.1,\n",
    "    patience=12,\n",
    "    val_batch_size=None\n",
    "):\n",
    "    if use_gpu:\n",
    "        model.cuda()\n",
    "\n",
    "    # Definición de dataloader\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data_images, train_data_labels, train_data_metadata)\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_data_images, val_data_labels, val_data_metadata)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=val_batch_size or len(val_data_images), shuffle=False, pin_memory=use_gpu)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    curves = {\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
    "        cumulative_train_loss = 0\n",
    "        cumulative_train_corrects = 0\n",
    "        train_loss_count = 0\n",
    "        train_acc_count = 0\n",
    "\n",
    "        # Entrenamiento del modelo\n",
    "        model.train()\n",
    "\n",
    "        for i, (x_batch, y_batch, metadata_batch) in enumerate(train_loader):\n",
    "            if use_gpu:\n",
    "                x_batch = x_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                metadata_batch = metadata_batch.cuda()\n",
    "\n",
    "            y_predicted, loss = train_step(x_batch, y_batch, metadata_batch, model, optimizer, criterion, beta)\n",
    "\n",
    "\n",
    "\n",
    "            cumulative_train_loss += loss.item()\n",
    "            train_loss_count += 1\n",
    "            train_acc_count += y_batch.shape[0]\n",
    "\n",
    "            # Calculamos número de aciertos\n",
    "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
    "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
    "\n",
    "            if (i + 1) % 117 == 0:\n",
    "                print(f\"Iteration {i + 1} - Batch {i + 1}/{len(train_loader)} - Train loss: {cumulative_train_loss / train_loss_count}, Train acc: {cumulative_train_corrects / train_acc_count}\")\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc, val_loss = evaluate(val_loader, model, criterion, use_gpu)\n",
    "\n",
    "        print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
    "\n",
    "        train_loss = cumulative_train_loss / train_loss_count\n",
    "        train_acc = cumulative_train_corrects / train_acc_count\n",
    "\n",
    "        curves[\"train_acc\"].append(train_acc)\n",
    "        curves[\"val_acc\"].append(val_acc)\n",
    "        curves[\"train_loss\"].append(train_loss)\n",
    "        curves[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        # Lógica de early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement == patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement after {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Tiempo total de entrenamiento: {time.perf_counter() - t0:.4f} [s]\")\n",
    "    model.cpu()\n",
    "\n",
    "    return curves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de hiperparámetros con funciones de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir las imágenes y metadatos en conjuntos de entrenamiento y validación\n",
    "Train_images = preparar_imagenes_para_modelo(data_procesada)\n",
    "Val_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Validation')\n",
    "Train_labels = extraer_etiquetas(data_procesada)\n",
    "Val_labels = extraer_etiquetas(data_procesada, key_principal='Validation')\n",
    "# Extraer características de metadatos para los conjuntos de datos\n",
    "metadata_train = extraer_metadata(data_procesada)\n",
    "metadata_val = extraer_metadata(data_procesada, key_principal='Validation')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mejores parámetros encontrados:  {'lr': 5e-05, 'epochs': 50, 'dropout_p': 0.6, 'batch_size': 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciamos el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera combinación de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.9686836365960602, Train acc: 0.2264957264957265\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.9421360653180343, Train acc: 0.2821180555555556\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.914702194708365, Train acc: 0.3298165954415954\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.8836670701320355, Train acc: 0.3733974358974359\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.8523313874872321, Train acc: 0.4059561965811966\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.824579087842224, Train acc: 0.43093393874643876\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.7971667870788202, Train acc: 0.45289606227106227\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.7721962642211181, Train acc: 0.4716212606837607\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.7497001260886957, Train acc: 0.48710529439696104\n",
      "Val loss: 0.8687974810600281, Val acc: 0.668\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.5208793430246859, Train acc: 0.6474358974358975\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.510861717737638, Train acc: 0.6533119658119658\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.500479931844945, Train acc: 0.6588319088319088\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4925018286603129, Train acc: 0.6646300747863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48349284045716634, Train acc: 0.6691773504273504\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.47521448653308074, Train acc: 0.6742343304843305\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.46879621969037877, Train acc: 0.6771978021978022\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4622140172072965, Train acc: 0.681106436965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.45560802990554744, Train acc: 0.6855413105413105\n",
      "Val loss: 0.6774467825889587, Val acc: 0.772\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.39182757516192573, Train acc: 0.7239583333333334\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3851712088809054, Train acc: 0.7309027777777778\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.377161853079103, Train acc: 0.7349537037037037\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3735882165467637, Train acc: 0.7370459401709402\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.37192130165222365, Train acc: 0.7386485042735043\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.368837368454349, Train acc: 0.7415420227920227\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.365776783599085, Train acc: 0.7433608058608059\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.36141288560679835, Train acc: 0.7452090010683761\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.35713425406023985, Train acc: 0.7485161443494777\n",
      "Val loss: 0.5873544216156006, Val acc: 0.796\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3206062548690372, Train acc: 0.7736378205128205\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3145813088641207, Train acc: 0.7759081196581197\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.31070993496821475, Train acc: 0.7767094017094017\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3095533808327129, Train acc: 0.7772769764957265\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.30950190679639833, Train acc: 0.7768963675213675\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3076688864207336, Train acc: 0.7777332621082621\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.30543375295451564, Train acc: 0.779532967032967\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.30384276896460444, Train acc: 0.7805822649572649\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.30251988035547994, Train acc: 0.7810719373219374\n",
      "Val loss: 0.5299497842788696, Val acc: 0.802\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.29037527294240445, Train acc: 0.7905982905982906\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.2791946537983723, Train acc: 0.7986111111111112\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.2761814501720276, Train acc: 0.8014155982905983\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2764190187056859, Train acc: 0.8003806089743589\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.27353398855934796, Train acc: 0.8018963675213675\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.27213425297512966, Train acc: 0.8018384971509972\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.27104046576916807, Train acc: 0.8018353174603174\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.26931677303380436, Train acc: 0.8021834935897436\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.2678807594327845, Train acc: 0.8028252611585945\n",
      "Val loss: 0.49172520637512207, Val acc: 0.828\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.2583317889107598, Train acc: 0.8078258547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.2499163848722083, Train acc: 0.8129006410256411\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.25094750268846494, Train acc: 0.8124554843304843\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2496337088254782, Train acc: 0.8128004807692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.24933444918730321, Train acc: 0.8135950854700855\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.24667043041469705, Train acc: 0.8147925569800569\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.24480028517808086, Train acc: 0.815533424908425\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.24411245464132383, Train acc: 0.8156717414529915\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.24266593819103802, Train acc: 0.8163431861348528\n",
      "Val loss: 0.4605872631072998, Val acc: 0.836\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.227448146057944, Train acc: 0.8258547008547008\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.22377218076815972, Train acc: 0.8261217948717948\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.22577751998887782, Train acc: 0.8251869658119658\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.22750127653026173, Train acc: 0.8246861645299145\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.22812536995635074, Train acc: 0.8241452991452991\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.22747955099809544, Train acc: 0.8252092236467237\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.22554716536995836, Train acc: 0.8262362637362637\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.22426391415234304, Train acc: 0.8271567841880342\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.223300566700449, Train acc: 0.8277688746438746\n",
      "Val loss: 0.4308840036392212, Val acc: 0.84\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.19470152055096424, Train acc: 0.8469551282051282\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.2072698308361901, Train acc: 0.8378739316239316\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.2087374844951847, Train acc: 0.8368500712250713\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.20926743045321897, Train acc: 0.8369724893162394\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.2088319624081636, Train acc: 0.8369391025641025\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.20953421205536932, Train acc: 0.8365607193732194\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.2099338395109398, Train acc: 0.8362522893772893\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.20810297147458434, Train acc: 0.8372729700854701\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.20640305777107776, Train acc: 0.8388087606837606\n",
      "Val loss: 0.4073459208011627, Val acc: 0.854\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.19527442562274444, Train acc: 0.8508279914529915\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.19309191061900213, Train acc: 0.8497596153846154\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.19587447568561955, Train acc: 0.848335113960114\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.19721562606401932, Train acc: 0.8458867521367521\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1972617234939184, Train acc: 0.8453525641025641\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.19552736952264085, Train acc: 0.8468438390313391\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.19571456316772107, Train acc: 0.8469932844932845\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.19491356229170775, Train acc: 0.8475393963675214\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1950405429118266, Train acc: 0.8476377018043685\n",
      "Val loss: 0.3929708003997803, Val acc: 0.86\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.19288020638319162, Train acc: 0.8485576923076923\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.19237171393683833, Train acc: 0.8502270299145299\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.18600661802156018, Train acc: 0.8532318376068376\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.18598644887534982, Train acc: 0.8526308760683761\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.18597701161335675, Train acc: 0.8518429487179487\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1853251085746662, Train acc: 0.8516515313390314\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.18447476185139575, Train acc: 0.8522397741147741\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1847339058533693, Train acc: 0.8521300747863247\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.18466409828588154, Train acc: 0.852193138651472\n",
      "Val loss: 0.3867609202861786, Val acc: 0.848\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.17933557430903116, Train acc: 0.8514957264957265\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.17946881998298514, Train acc: 0.8530982905982906\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.17950409778162965, Train acc: 0.8532318376068376\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.17658318044283453, Train acc: 0.8559027777777778\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.17700281841123205, Train acc: 0.8556623931623931\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1774250590308779, Train acc: 0.8553240740740741\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.17692136633527147, Train acc: 0.8562843406593407\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1778218918121778, Train acc: 0.8556690705128205\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.17718698767854957, Train acc: 0.8566150284900285\n",
      "Val loss: 0.3712858259677887, Val acc: 0.864\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.17467290443232936, Train acc: 0.8617788461538461\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.17142327410033625, Train acc: 0.8629139957264957\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.17120440597208136, Train acc: 0.8618233618233618\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.17111542961026868, Train acc: 0.8620793269230769\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.17049441480229044, Train acc: 0.862633547008547\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.16954398507576043, Train acc: 0.8629362535612536\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1686685716276204, Train acc: 0.8638202075702076\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.16903119429181784, Train acc: 0.8637820512820513\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.16880016016145039, Train acc: 0.8640491452991453\n",
      "Val loss: 0.36676478385925293, Val acc: 0.878\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1688804030418396, Train acc: 0.8668536324786325\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.16531781724884978, Train acc: 0.8680555555555556\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.16358888505870461, Train acc: 0.8676549145299145\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.16584158045613867, Train acc: 0.8658854166666666\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.16452715936888995, Train acc: 0.8664797008547008\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.16350531904955534, Train acc: 0.8671207264957265\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1636918116183508, Train acc: 0.8670253357753358\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.16327352229601297, Train acc: 0.8672208867521367\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.16319152376024584, Train acc: 0.867417497625831\n",
      "Val loss: 0.35268908739089966, Val acc: 0.878\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.15559459267518458, Train acc: 0.8723290598290598\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1549162672370927, Train acc: 0.8718616452991453\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.1608084422400874, Train acc: 0.869880698005698\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.16082819583069566, Train acc: 0.8689569978632479\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1603123362757202, Train acc: 0.8691239316239316\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.16005108070339572, Train acc: 0.8689013532763533\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.15921436655943502, Train acc: 0.8698679792429792\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.15881678901421717, Train acc: 0.8701589209401709\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.15943364534396165, Train acc: 0.8696284425451092\n",
      "Val loss: 0.3675725758075714, Val acc: 0.854\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.16667996334214497, Train acc: 0.8637820512820513\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.15914324091540444, Train acc: 0.8696581196581197\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.15790288409276565, Train acc: 0.8697916666666666\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.15760697660028425, Train acc: 0.8697582799145299\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.15710089889347043, Train acc: 0.8711004273504274\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1559818979681727, Train acc: 0.8717503561253561\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.15518357753025888, Train acc: 0.871947496947497\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.15531438407607567, Train acc: 0.8716947115384616\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.15433361209355867, Train acc: 0.8719877730294397\n",
      "Val loss: 0.3420257568359375, Val acc: 0.886\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.15088585910634097, Train acc: 0.8748664529914529\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.15151056494468298, Train acc: 0.8733974358974359\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.1506137550726236, Train acc: 0.8729522792022792\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1511226242933518, Train acc: 0.8723958333333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.15023542994107955, Train acc: 0.8734508547008547\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1501419504313727, Train acc: 0.8741764601139601\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1512372413688818, Train acc: 0.8732448107448108\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.15084634265965885, Train acc: 0.8734308226495726\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1503083859866614, Train acc: 0.8742877492877493\n",
      "Val loss: 0.3361411988735199, Val acc: 0.884\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.14884299574754176, Train acc: 0.8748664529914529\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1527679898799994, Train acc: 0.8730635683760684\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.15225876430500607, Train acc: 0.8741096866096866\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1525207755402622, Train acc: 0.8734308226495726\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1501807039619511, Train acc: 0.8751602564102564\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.14897296507643837, Train acc: 0.8760683760683761\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.14939055974812443, Train acc: 0.8758966727716728\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.14913580783157268, Train acc: 0.8760349893162394\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.14790199457863232, Train acc: 0.8763799857549858\n",
      "Val loss: 0.34037160873413086, Val acc: 0.878\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1486988360555763, Train acc: 0.8743322649572649\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.14817390584538126, Train acc: 0.8754674145299145\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.14605589911469027, Train acc: 0.8772702991452992\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.14712910322297332, Train acc: 0.8768362713675214\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.14716333471811735, Train acc: 0.8768696581196581\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.14626569120462804, Train acc: 0.8775596509971509\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.14575262369544806, Train acc: 0.8777281746031746\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1454349447391991, Train acc: 0.8775540865384616\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.14434013195187279, Train acc: 0.8782644824311491\n",
      "Val loss: 0.33222731947898865, Val acc: 0.88\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.14576796563262615, Train acc: 0.8776709401709402\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.14004276947587982, Train acc: 0.8820779914529915\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13982229287128503, Train acc: 0.8812321937321937\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.14040813747888956, Train acc: 0.8808760683760684\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.14053069572163443, Train acc: 0.8798076923076923\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.140622148698891, Train acc: 0.8799189814814815\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.14035721052260625, Train acc: 0.8799030830280831\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.14047153208118218, Train acc: 0.8795572916666666\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1401426582990775, Train acc: 0.8801489791073125\n",
      "Val loss: 0.3278735280036926, Val acc: 0.886\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1383472910294166, Train acc: 0.8779380341880342\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13669312573396242, Train acc: 0.8812767094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13786736122223725, Train acc: 0.8820779914529915\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13711527333809778, Train acc: 0.8825787927350427\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13786354900425316, Train acc: 0.8822115384615384\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13685168614584498, Train acc: 0.8828125\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1371315700230581, Train acc: 0.8827648046398047\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1372903728714356, Train acc: 0.8827791132478633\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.13678481846566667, Train acc: 0.8830870132953467\n",
      "Val loss: 0.3259798288345337, Val acc: 0.89\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.13394072422614464, Train acc: 0.8843482905982906\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13117913010283413, Train acc: 0.8856837606837606\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13549025488375258, Train acc: 0.8827902421652422\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13726561936812523, Train acc: 0.8817441239316239\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13849840434188518, Train acc: 0.8805288461538462\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13829861980387967, Train acc: 0.8808538105413105\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.13651545234797785, Train acc: 0.8825167887667887\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.13611832098701063, Train acc: 0.8828959668803419\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.13530016822805885, Train acc: 0.8837547483380817\n",
      "Val loss: 0.32937878370285034, Val acc: 0.88\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1402500317646907, Train acc: 0.8836805555555556\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13443452985877666, Train acc: 0.8874866452991453\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13383524081645867, Train acc: 0.886039886039886\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13292312800374806, Train acc: 0.8863514957264957\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1318201710525741, Train acc: 0.8868856837606838\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13225531106830662, Train acc: 0.8857060185185185\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.13224844254271306, Train acc: 0.8857791514041514\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.13279335856692404, Train acc: 0.8846988514957265\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.13315669081376375, Train acc: 0.88497150997151\n",
      "Val loss: 0.31859731674194336, Val acc: 0.89\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12849751879007387, Train acc: 0.8888888888888888\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13176880955186665, Train acc: 0.8862847222222222\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.1320640178996953, Train acc: 0.885772792022792\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13134433660242292, Train acc: 0.8862513354700855\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13014423210396725, Train acc: 0.8872329059829059\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12971490069671912, Train acc: 0.8878205128205128\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1295527447201539, Train acc: 0.8879349816849816\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12910158773008576, Train acc: 0.8879707532051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12980686970132935, Train acc: 0.8875089031339032\n",
      "Val loss: 0.3144078552722931, Val acc: 0.898\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.13838556740019056, Train acc: 0.8870192307692307\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1333438249734732, Train acc: 0.8869524572649573\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.12929495973804397, Train acc: 0.8890669515669516\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13115934441741714, Train acc: 0.8872195512820513\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13090215906118735, Train acc: 0.8867521367521367\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13021856809613372, Train acc: 0.8861734330484331\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.12989333595839467, Train acc: 0.8867330586080586\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1290577630010935, Train acc: 0.8871694711538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1278610926294825, Train acc: 0.8876572886989553\n",
      "Val loss: 0.3142569363117218, Val acc: 0.898\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12976677015296414, Train acc: 0.8860844017094017\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13087286449905133, Train acc: 0.8855502136752137\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13078211787079813, Train acc: 0.8870192307692307\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1303459852297082, Train acc: 0.8867855235042735\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.12784229007541623, Train acc: 0.8879540598290598\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1271382614842847, Train acc: 0.8883769586894587\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1264952401174585, Train acc: 0.8891178266178266\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12595832946463528, Train acc: 0.8897903311965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1255789430279564, Train acc: 0.8896901709401709\n",
      "Val loss: 0.3138492703437805, Val acc: 0.896\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12881716958477965, Train acc: 0.8874198717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.12411712186458783, Train acc: 0.8904246794871795\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.12487806112338336, Train acc: 0.8895566239316239\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.12524635147335184, Train acc: 0.8905916132478633\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.12448292104606955, Train acc: 0.8907051282051283\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12489476010330722, Train acc: 0.889823717948718\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.12492748272579086, Train acc: 0.8900526556776557\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12364023895217822, Train acc: 0.8908754006410257\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12334450918045478, Train acc: 0.8909959639126306\n",
      "Val loss: 0.3133927583694458, Val acc: 0.898\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1192754178475111, Train acc: 0.891426282051282\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.12182177985325837, Train acc: 0.891426282051282\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.1200034767813832, Train acc: 0.8929843304843305\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.12069925907840076, Train acc: 0.8926615918803419\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1205986152856778, Train acc: 0.8931089743589744\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12021924610491153, Train acc: 0.8931401353276354\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.12087546953060398, Train acc: 0.8926854395604396\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12064594250076856, Train acc: 0.8924946581196581\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1212873417257923, Train acc: 0.8918120845204178\n",
      "Val loss: 0.3058519959449768, Val acc: 0.888\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12512241087408146, Train acc: 0.8891559829059829\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.12259093984069987, Train acc: 0.8912927350427351\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.12088035110734467, Train acc: 0.8924501424501424\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.11668244844827896, Train acc: 0.8954660790598291\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.11700111006060217, Train acc: 0.8956463675213675\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1183077224299439, Train acc: 0.8944978632478633\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1185897884569762, Train acc: 0.8943833943833944\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11829782872755304, Train acc: 0.8944644764957265\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1183980893545341, Train acc: 0.8941268993352327\n",
      "Val loss: 0.3086671233177185, Val acc: 0.892\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.11878349307255867, Train acc: 0.8932959401709402\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.11933284450290549, Train acc: 0.8931623931623932\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11745782829078175, Train acc: 0.8937410968660968\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.11749527520603603, Train acc: 0.89453125\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.11568545485154176, Train acc: 0.8956463675213675\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.11595432018792187, Train acc: 0.8955662393162394\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1157292573734372, Train acc: 0.8954708485958486\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11679417111425318, Train acc: 0.8949652777777778\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.11678487662462994, Train acc: 0.8951804368471035\n",
      "Val loss: 0.30774474143981934, Val acc: 0.9\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.11542528256391868, Train acc: 0.8947649572649573\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.11449771444512229, Train acc: 0.8936298076923077\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11525247698156242, Train acc: 0.8937856125356125\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1157820857106111, Train acc: 0.8945980235042735\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.11384554235344259, Train acc: 0.895940170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1142511181427203, Train acc: 0.8959891381766382\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.11372701767308954, Train acc: 0.8963484432234432\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11408260869037391, Train acc: 0.8962339743589743\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.11429188229985612, Train acc: 0.8961449430199431\n",
      "Val loss: 0.3116873502731323, Val acc: 0.894\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.10703477314394763, Train acc: 0.9015758547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.11247748569545583, Train acc: 0.8973023504273504\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11322012467262073, Train acc: 0.8976584757834758\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.11370703800875917, Train acc: 0.8971354166666666\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.11359182127520569, Train acc: 0.8974358974358975\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.11325745957444536, Train acc: 0.8978810541310541\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.11369561383066305, Train acc: 0.897645757020757\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11403989866694324, Train acc: 0.8974192040598291\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.11366334714611032, Train acc: 0.8977326685660019\n",
      "Val loss: 0.292377233505249, Val acc: 0.902\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1090121809233967, Train acc: 0.9011752136752137\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1144625224873551, Train acc: 0.8968349358974359\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11368815842856708, Train acc: 0.8974358974358975\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.11335824646501459, Train acc: 0.8977697649572649\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.11207233973038502, Train acc: 0.8974358974358975\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1109919400721194, Train acc: 0.8984152421652422\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.11158845778349992, Train acc: 0.8979891636141636\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11175216186759818, Train acc: 0.8978532318376068\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.11270985811071405, Train acc: 0.8968423551756886\n",
      "Val loss: 0.3009336590766907, Val acc: 0.896\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.11385541874119359, Train acc: 0.8966346153846154\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.11404384704481842, Train acc: 0.8961672008547008\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11510497403790129, Train acc: 0.8967236467236467\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.11189790264281452, Train acc: 0.8988715277777778\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1119894698389575, Train acc: 0.8982371794871795\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.11209417798919895, Train acc: 0.8970352564102564\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.11110400047348823, Train acc: 0.8974168192918193\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11127613857388496, Train acc: 0.8978031517094017\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.11208664369039725, Train acc: 0.8975249287749287\n",
      "Val loss: 0.31370189785957336, Val acc: 0.888\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1150371771082919, Train acc: 0.8948985042735043\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.11293618788576534, Train acc: 0.8965678418803419\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10963798970238775, Train acc: 0.8991274928774928\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10849170896232638, Train acc: 0.8988047542735043\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.10796410400643308, Train acc: 0.8986111111111111\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.10758381373012847, Train acc: 0.8993055555555556\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10841977796420656, Train acc: 0.8988476800976801\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10744050794687027, Train acc: 0.8998731303418803\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.10757910806229312, Train acc: 0.9000029677113011\n",
      "Val loss: 0.3027755618095398, Val acc: 0.902\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.10681444610285963, Train acc: 0.9037126068376068\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10385103027025859, Train acc: 0.9037793803418803\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10669838329326053, Train acc: 0.9017094017094017\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10679798770664084, Train acc: 0.9006744123931624\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1062680592903724, Train acc: 0.901469017094017\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.10608281787389363, Train acc: 0.9017761752136753\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10651395119662978, Train acc: 0.9011942918192918\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10704462037573004, Train acc: 0.9008580395299145\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1067827385120582, Train acc: 0.9008190883190883\n",
      "Val loss: 0.30720680952072144, Val acc: 0.89\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.10579286515712738, Train acc: 0.9025106837606838\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10175101567282636, Train acc: 0.9045138888888888\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10212330244205615, Train acc: 0.9036235754985755\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10435019210617766, Train acc: 0.9018763354700855\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.10574849441520169, Train acc: 0.9006143162393162\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1063767335778288, Train acc: 0.900329415954416\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10472179958380767, Train acc: 0.9015186202686203\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10458361252378194, Train acc: 0.9020599626068376\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.10519602074016307, Train acc: 0.9018429487179487\n",
      "Val loss: 0.29486700892448425, Val acc: 0.904\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09708799867548494, Train acc: 0.9046474358974359\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10058490970195869, Train acc: 0.9041800213675214\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10306185399025594, Train acc: 0.9022435897435898\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10221459608302157, Train acc: 0.9028111645299145\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.10122867185845334, Train acc: 0.9036057692307692\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.10157660953169874, Train acc: 0.903556801994302\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1027735506658589, Train acc: 0.9024343711843712\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10309350748474781, Train acc: 0.9026442307692307\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.10433242916494806, Train acc: 0.902258428300095\n",
      "Val loss: 0.3034888803958893, Val acc: 0.892\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09995719968763173, Train acc: 0.9011752136752137\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09992371703314985, Train acc: 0.9036458333333334\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.09947153713628437, Train acc: 0.9042913105413105\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10020874093613054, Train acc: 0.9044471153846154\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.10166956356957428, Train acc: 0.9040598290598291\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.10250002614877841, Train acc: 0.9034900284900285\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10241984031066499, Train acc: 0.9033310439560439\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10224642434244992, Train acc: 0.903762686965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.10150266515846379, Train acc: 0.904053893637227\n",
      "Val loss: 0.2899482250213623, Val acc: 0.904\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1013732160258497, Train acc: 0.9066506410256411\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10167764738584176, Train acc: 0.9048477564102564\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10163612600065704, Train acc: 0.9038461538461539\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10105159350185312, Train acc: 0.9037793803418803\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.10182584692270327, Train acc: 0.9042735042735043\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1021983864215704, Train acc: 0.9042690527065527\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10250052169274643, Train acc: 0.9041132478632479\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10099752973287533, Train acc: 0.9048477564102564\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.10063814112150658, Train acc: 0.9050629154795822\n",
      "Val loss: 0.29204273223876953, Val acc: 0.9\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.10503718868280068, Train acc: 0.9006410256410257\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10336327909404396, Train acc: 0.9021768162393162\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10252750437823456, Train acc: 0.9028668091168092\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10129168757006653, Train acc: 0.9032118055555556\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09978408314223983, Train acc: 0.9048344017094017\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09943310160752375, Train acc: 0.9052261396011396\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0995520674265348, Train acc: 0.9050289987789988\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1003645562972778, Train acc: 0.9046307425213675\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09994249205756844, Train acc: 0.9050777540360874\n",
      "Val loss: 0.2904878854751587, Val acc: 0.9\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08488937384552425, Train acc: 0.9123931623931624\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09165472677375516, Train acc: 0.9099225427350427\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.09573369330999858, Train acc: 0.9073183760683761\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0956853506847834, Train acc: 0.9080528846153846\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09629444003614605, Train acc: 0.907772435897436\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09663862938809599, Train acc: 0.9071403133903134\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.09648369609290718, Train acc: 0.9064980158730159\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.09619308441367924, Train acc: 0.9065337873931624\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09674020847420634, Train acc: 0.9061164529914529\n",
      "Val loss: 0.2878565192222595, Val acc: 0.898\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0972164642607045, Train acc: 0.9049145299145299\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09596123438105623, Train acc: 0.9073183760683761\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0965302160322836, Train acc: 0.90625\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.09718538801639508, Train acc: 0.905982905982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0968967010322799, Train acc: 0.9059027777777777\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09656580726475457, Train acc: 0.9066506410256411\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0958155462375054, Train acc: 0.9071466727716728\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.09532630577301368, Train acc: 0.9073016826923077\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0951140934472297, Train acc: 0.9074964387464387\n",
      "Val loss: 0.2869470715522766, Val acc: 0.902\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09162937168382172, Train acc: 0.9111912393162394\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09248369753870189, Train acc: 0.9107905982905983\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0918796952293809, Train acc: 0.9115473646723646\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.09211814747406886, Train acc: 0.9104901175213675\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09251743707901393, Train acc: 0.9102831196581197\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09362744610024314, Train acc: 0.9097667378917379\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.09396832317150265, Train acc: 0.9088064713064713\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.094742946820254, Train acc: 0.9085536858974359\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09470630432182794, Train acc: 0.9082680436847104\n",
      "Val loss: 0.28315219283103943, Val acc: 0.896\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09523626499705845, Train acc: 0.905715811965812\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09404296128668337, Train acc: 0.9077857905982906\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.09533392253764335, Train acc: 0.9083867521367521\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.09430012049583289, Train acc: 0.9091212606837606\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09489542668701237, Train acc: 0.908306623931624\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09478455297967307, Train acc: 0.9086983618233618\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.09477450037250007, Train acc: 0.9084630647130647\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.09448778570398815, Train acc: 0.9088374732905983\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09443435665832059, Train acc: 0.9093809354226021\n",
      "Val loss: 0.2907946705818176, Val acc: 0.896\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09384662778968485, Train acc: 0.9091880341880342\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09095843938680795, Train acc: 0.9107238247863247\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.09048403931139541, Train acc: 0.9116809116809117\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0901183914552387, Train acc: 0.9115251068376068\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09227289570193005, Train acc: 0.9094284188034188\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09252510262265844, Train acc: 0.9091657763532763\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.09161890217163333, Train acc: 0.9095123626373627\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.09155015231898198, Train acc: 0.9096053685897436\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09079451962300741, Train acc: 0.9098706077872745\n",
      "Val loss: 0.28745365142822266, Val acc: 0.902\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08811811606089275, Train acc: 0.9118589743589743\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08613427906719029, Train acc: 0.9128605769230769\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08591536927087355, Train acc: 0.9129718660968661\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08845025772212917, Train acc: 0.9111578525641025\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08898526926835378, Train acc: 0.9113247863247863\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08928728589729706, Train acc: 0.9116141381766382\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08936493258845966, Train acc: 0.9118398962148963\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08915795202757049, Train acc: 0.9121761485042735\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08954566506295689, Train acc: 0.9119776828110161\n",
      "Val loss: 0.29231032729148865, Val acc: 0.898\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09136981192307594, Train acc: 0.9094551282051282\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09481218005092736, Train acc: 0.9091212606837606\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.092705253182653, Train acc: 0.9093660968660968\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.09031596929471716, Train acc: 0.9107238247863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08959957442731939, Train acc: 0.9112179487179487\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08811998044663345, Train acc: 0.9119257478632479\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08828505988304432, Train acc: 0.9118780525030525\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.088064060267857, Train acc: 0.9122930021367521\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08846190769901076, Train acc: 0.9115918803418803\n",
      "Val loss: 0.28641998767852783, Val acc: 0.898\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07991221661751087, Train acc: 0.9135950854700855\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08558917166585596, Train acc: 0.9097889957264957\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08620847235208223, Train acc: 0.9098557692307693\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08738526125621592, Train acc: 0.9094885149572649\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08731662159801548, Train acc: 0.9103632478632478\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08722413783399467, Train acc: 0.9108573717948718\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08704722986553177, Train acc: 0.9110958485958486\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08773971380841018, Train acc: 0.9110576923076923\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08725565168139827, Train acc: 0.911636396011396\n",
      "Val loss: 0.28426340222358704, Val acc: 0.904\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08479481731724535, Train acc: 0.9111912393162394\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08405726237429513, Train acc: 0.9141960470085471\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08472537726928027, Train acc: 0.9136841168091168\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08385344002491389, Train acc: 0.9139623397435898\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08362891215544481, Train acc: 0.9140491452991453\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08506775912735877, Train acc: 0.9133725071225072\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08506283196774157, Train acc: 0.9133279914529915\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08671398820658015, Train acc: 0.9121260683760684\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08569299325644121, Train acc: 0.9128234805318138\n",
      "Val loss: 0.2838357388973236, Val acc: 0.902\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08580264563743885, Train acc: 0.9163995726495726\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08753566386607978, Train acc: 0.9128605769230769\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.085404319768278, Train acc: 0.9128383190883191\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08271713472074932, Train acc: 0.9146300747863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08464179874485374, Train acc: 0.9138087606837607\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08446982023213324, Train acc: 0.913528311965812\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08417034043642772, Train acc: 0.9136332417582418\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08346558822334832, Train acc: 0.9141292735042735\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08428642698559213, Train acc: 0.9137286324786325\n",
      "Val loss: 0.30004364252090454, Val acc: 0.896\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08750240657574092, Train acc: 0.9153311965811965\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08743699359842855, Train acc: 0.9149973290598291\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08656100958500833, Train acc: 0.9157318376068376\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08572761765402606, Train acc: 0.9157318376068376\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0848958553157301, Train acc: 0.9155181623931624\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08404153521753784, Train acc: 0.9159766737891738\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08413401637249086, Train acc: 0.9160943223443223\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08425128701915088, Train acc: 0.916015625\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08437747745319196, Train acc: 0.9157169990503324\n",
      "Val loss: 0.28098803758621216, Val acc: 0.906\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08405957606613126, Train acc: 0.9127938034188035\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08330597390985897, Train acc: 0.9144631410256411\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08141061200885012, Train acc: 0.9152421652421653\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08187258686138014, Train acc: 0.9152310363247863\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0830070314499048, Train acc: 0.9146901709401709\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08328200063729219, Train acc: 0.9146634615384616\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08287427699027335, Train acc: 0.9154647435897436\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08288480196562079, Train acc: 0.9151976495726496\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08196754166316896, Train acc: 0.9156724833808167\n",
      "Val loss: 0.2763184607028961, Val acc: 0.9\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08143673722560589, Train acc: 0.9151976495726496\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07882317136495541, Train acc: 0.9176014957264957\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07856596012910207, Train acc: 0.9182692307692307\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07827833126116003, Train acc: 0.9191038995726496\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07801596043456314, Train acc: 0.9189369658119658\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0777464287191035, Train acc: 0.9189592236467237\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07904955138871958, Train acc: 0.9183646214896215\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07949281202103847, Train acc: 0.9179520566239316\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08010985758173274, Train acc: 0.9175273029439696\n",
      "Val loss: 0.280983567237854, Val acc: 0.898\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08519491120281382, Train acc: 0.9127938034188035\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08023236742895892, Train acc: 0.9163995726495726\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07744014050885822, Train acc: 0.9178240740740741\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07876989369591077, Train acc: 0.9169003739316239\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07853888622206501, Train acc: 0.9172542735042735\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0791327862063704, Train acc: 0.9168224715099715\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07916462865359035, Train acc: 0.9169528388278388\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07944223932667166, Train acc: 0.9170840010683761\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08009730667187165, Train acc: 0.9165331196581197\n",
      "Val loss: 0.2758978307247162, Val acc: 0.9\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08606842134752844, Train acc: 0.9109241452991453\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08410469805582976, Train acc: 0.9121928418803419\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08100809343689867, Train acc: 0.9159989316239316\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08032760527144131, Train acc: 0.9164329594017094\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08171088450994247, Train acc: 0.915892094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08136687582714265, Train acc: 0.9155982905982906\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08000593975422875, Train acc: 0.9158081501831502\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07906282231466383, Train acc: 0.915748530982906\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07939305094911842, Train acc: 0.9156428062678063\n",
      "Val loss: 0.28134414553642273, Val acc: 0.896\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07449909828157507, Train acc: 0.9201388888888888\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0767897864819592, Train acc: 0.9174679487179487\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07919857895102596, Train acc: 0.9169337606837606\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07617518264386389, Train acc: 0.9192374465811965\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07710204417379493, Train acc: 0.9184561965811966\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07553708362273681, Train acc: 0.919181801994302\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07550398135723795, Train acc: 0.9194139194139194\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07653406423190211, Train acc: 0.9187366452991453\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07656196627238764, Train acc: 0.9187589031339032\n",
      "Val loss: 0.28503522276878357, Val acc: 0.9\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07478926031507997, Train acc: 0.9190705128205128\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0733243821777849, Train acc: 0.9207398504273504\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.075870219119254, Train acc: 0.9185808404558404\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07564738475614124, Train acc: 0.9184695512820513\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07517691690188189, Train acc: 0.9185897435897435\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07580848861793507, Train acc: 0.9186921296296297\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07686807901140243, Train acc: 0.9179830586080586\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07645002973831108, Train acc: 0.9183526976495726\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07688607180673286, Train acc: 0.9182692307692307\n",
      "Val loss: 0.28406453132629395, Val acc: 0.902\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08757288843138605, Train acc: 0.9102564102564102\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08322381623025633, Train acc: 0.9147970085470085\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08182976341824925, Train acc: 0.9160879629629629\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08018187003640029, Train acc: 0.9169671474358975\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07786508788410415, Train acc: 0.9183226495726495\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07712140906485397, Train acc: 0.9189369658119658\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07641430190049103, Train acc: 0.9194902319902319\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07550522220185679, Train acc: 0.9200721153846154\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07448946194261567, Train acc: 0.9205840455840456\n",
      "Val loss: 0.28177034854888916, Val acc: 0.894\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06980535438937, Train acc: 0.9254807692307693\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07462312790573153, Train acc: 0.9210069444444444\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07696719718115282, Train acc: 0.9191150284900285\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07620325117793858, Train acc: 0.9186030982905983\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07519750014329568, Train acc: 0.919417735042735\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07534045042560311, Train acc: 0.9190482549857549\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07548872344150356, Train acc: 0.9187080280830281\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07492053927455702, Train acc: 0.9194043803418803\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07469930241533607, Train acc: 0.9190705128205128\n",
      "Val loss: 0.2891518771648407, Val acc: 0.9\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07436719931598403, Train acc: 0.9204059829059829\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0748732767553411, Train acc: 0.9202724358974359\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07258252290069547, Train acc: 0.9200943732193733\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07231839064858918, Train acc: 0.9207732371794872\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07346982545832284, Train acc: 0.9199252136752136\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07224492760550262, Train acc: 0.9210514601139601\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07235935652430678, Train acc: 0.9209401709401709\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07201537491483057, Train acc: 0.9214743589743589\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07303116891572052, Train acc: 0.9209550094966762\n",
      "Val loss: 0.27947255969047546, Val acc: 0.9\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0648180736690505, Train acc: 0.9265491452991453\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07012729817985469, Train acc: 0.9217414529914529\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07029056434447949, Train acc: 0.921875\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06967488520293154, Train acc: 0.9220753205128205\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07028962169956957, Train acc: 0.9219818376068376\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06965010135601728, Train acc: 0.9220308048433048\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07029219157311506, Train acc: 0.9219322344322345\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.0711148941777965, Train acc: 0.9215578258547008\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07225975999918192, Train acc: 0.9209698480531814\n",
      "Val loss: 0.28165292739868164, Val acc: 0.892\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07607443261350322, Train acc: 0.921073717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07047500696956602, Train acc: 0.9232104700854701\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06718637698735946, Train acc: 0.9250801282051282\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06738400790426466, Train acc: 0.9244457799145299\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06929556788542332, Train acc: 0.9225160256410256\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07019027135735563, Train acc: 0.922142094017094\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0701809143954581, Train acc: 0.9223901098901099\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07037221466819955, Train acc: 0.9225093482905983\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07067626329795135, Train acc: 0.9222162867996201\n",
      "Val loss: 0.2877238392829895, Val acc: 0.892\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06947451791702172, Train acc: 0.921073717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06826823968917896, Train acc: 0.921340811965812\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06768200412774697, Train acc: 0.9214298433048433\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0700610878630581, Train acc: 0.9190037393162394\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06915815182221241, Train acc: 0.9194711538461539\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06955101795261062, Train acc: 0.9199385683760684\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06870874627897008, Train acc: 0.9206539987789988\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.0692866041810594, Train acc: 0.9207064636752137\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06932628712309957, Train acc: 0.921073717948718\n",
      "Val loss: 0.28648239374160767, Val acc: 0.894\n",
      "Epoch 64/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06971558661032946, Train acc: 0.922409188034188\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06972489919927385, Train acc: 0.9230101495726496\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07010442434552729, Train acc: 0.9234330484330484\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07024725252746516, Train acc: 0.9227430555555556\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06945123601163554, Train acc: 0.9232104700854701\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06944934497361849, Train acc: 0.9230991809116809\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06898933183288108, Train acc: 0.9228861416361417\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06884010936905685, Train acc: 0.9231103098290598\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06807512517555034, Train acc: 0.9235962725546059\n",
      "Val loss: 0.28214412927627563, Val acc: 0.892\n",
      "Epoch 65/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07196941220352793, Train acc: 0.9200053418803419\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07256999962095521, Train acc: 0.9204059829059829\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0718284030670454, Train acc: 0.9214743589743589\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07067919321931325, Train acc: 0.9219751602564102\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06846416908451634, Train acc: 0.9235844017094017\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06830653558854025, Train acc: 0.9237001424501424\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0686297384483037, Train acc: 0.923324938949939\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06978858267076504, Train acc: 0.9226095085470085\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06826148423430813, Train acc: 0.9235814339981007\n",
      "Val loss: 0.290937602519989, Val acc: 0.898\n",
      "Epoch 66/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07132620918445098, Train acc: 0.9220085470085471\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07033796442879571, Train acc: 0.9214075854700855\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0683717004254333, Train acc: 0.9235665954415955\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06756805698586325, Train acc: 0.9237112713675214\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06661272479428185, Train acc: 0.9249198717948718\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06621890718865599, Train acc: 0.9246349715099715\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06631983674707867, Train acc: 0.9246413308913309\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06610179600170535, Train acc: 0.9249131944444444\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06666726097535317, Train acc: 0.9245904558404558\n",
      "Val loss: 0.2918688654899597, Val acc: 0.892\n",
      "Epoch 67/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06452435064010131, Train acc: 0.9246794871794872\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06816198944281308, Train acc: 0.9234775641025641\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06808961155237975, Train acc: 0.9237001424501424\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06575868998327826, Train acc: 0.925247061965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06538970195330106, Train acc: 0.9252670940170941\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0665631065426389, Train acc: 0.9240785256410257\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06602125455681075, Train acc: 0.9242979242979243\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06622037869424392, Train acc: 0.9242788461538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06483398065154936, Train acc: 0.9252581908831908\n",
      "Val loss: 0.27446386218070984, Val acc: 0.898\n",
      "Epoch 68/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06809326636995006, Train acc: 0.9232104700854701\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06478325080158365, Train acc: 0.9248798076923077\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06565737724304199, Train acc: 0.9250801282051282\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06430750031374459, Train acc: 0.9264823717948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06371193352418068, Train acc: 0.9262553418803419\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06425763312143477, Train acc: 0.9261485042735043\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06359168497779576, Train acc: 0.9266826923076923\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06324897811580928, Train acc: 0.9264990651709402\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.063794629512123, Train acc: 0.9262078584995251\n",
      "Val loss: 0.28188037872314453, Val acc: 0.9\n",
      "Epoch 69/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05988803442217346, Train acc: 0.9277510683760684\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.061520954546255946, Train acc: 0.9258814102564102\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06224927289193852, Train acc: 0.9257478632478633\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06102382531787595, Train acc: 0.9273504273504274\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06179896144785433, Train acc: 0.9274305555555555\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06347492827564223, Train acc: 0.9265046296296297\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06314603094652896, Train acc: 0.9264919108669109\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06252239152598076, Train acc: 0.9265658386752137\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0625720472700349, Train acc: 0.9265936609686609\n",
      "Val loss: 0.28951895236968994, Val acc: 0.894\n",
      "Epoch 70/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06374004432278821, Train acc: 0.9253472222222222\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06308032514957282, Train acc: 0.9258146367521367\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06037591894467672, Train acc: 0.9274394586894587\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06098707940461289, Train acc: 0.9270833333333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06188884224647131, Train acc: 0.9268162393162394\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06198873963111486, Train acc: 0.9269497863247863\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.061335986241316184, Train acc: 0.927159645909646\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.061300990561771594, Train acc: 0.9272168803418803\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06167866623582777, Train acc: 0.9274097815764483\n",
      "Val loss: 0.29016268253326416, Val acc: 0.898\n",
      "Epoch 71/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05619901991807497, Train acc: 0.9304220085470085\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05976386170866143, Train acc: 0.9280849358974359\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06211300826819874, Train acc: 0.9261039886039886\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.061521949198765635, Train acc: 0.9274172008547008\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06117727142623347, Train acc: 0.9281517094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06159725889266386, Train acc: 0.9280404202279202\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06097077002455464, Train acc: 0.9283806471306472\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06114000034255859, Train acc: 0.9283186431623932\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06099556670908914, Train acc: 0.9286265432098766\n",
      "Val loss: 0.27444756031036377, Val acc: 0.906\n",
      "Epoch 72/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05919501287305457, Train acc: 0.9249465811965812\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05758144610967392, Train acc: 0.9275507478632479\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0605343173541914, Train acc: 0.926460113960114\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.059524521104290955, Train acc: 0.9271167200854701\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05877872736026079, Train acc: 0.9274839743589743\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05989391280290408, Train acc: 0.9270833333333334\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06005763057824019, Train acc: 0.9270642551892552\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06061628134523192, Train acc: 0.9268162393162394\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0603365734120493, Train acc: 0.927261396011396\n",
      "Val loss: 0.2800573706626892, Val acc: 0.9\n",
      "Epoch 73/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05651201982783456, Train acc: 0.9296207264957265\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05684028419419231, Train acc: 0.9298210470085471\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.058095202699006455, Train acc: 0.9287304131054132\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05862792237446858, Train acc: 0.9289863782051282\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05889868382205311, Train acc: 0.928392094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05967646274726275, Train acc: 0.9275952635327636\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.058827295408144104, Train acc: 0.9285332722832723\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05941691187520822, Train acc: 0.928001469017094\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.059374539756480556, Train acc: 0.9282704178537512\n",
      "Val loss: 0.29473820328712463, Val acc: 0.898\n",
      "Epoch 74/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05736260421765156, Train acc: 0.9248130341880342\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05736242718676217, Train acc: 0.9278178418803419\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05737855962538651, Train acc: 0.9277065527065527\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.056700598193794235, Train acc: 0.9279180021367521\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05791440588286799, Train acc: 0.9274038461538462\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05761327783436517, Train acc: 0.9276842948717948\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05914817623093597, Train acc: 0.92691163003663\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05905928580552085, Train acc: 0.927200186965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05799125954296514, Train acc: 0.9281813865147198\n",
      "Val loss: 0.2866921126842499, Val acc: 0.898\n",
      "Epoch 75/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.058129940659571916, Train acc: 0.9293536324786325\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.061432276335027486, Train acc: 0.9275507478632479\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05981217800212382, Train acc: 0.9281071937321937\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.059279721994430594, Train acc: 0.9283186431623932\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.057548809637371294, Train acc: 0.9297542735042735\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.058195858620680295, Train acc: 0.9290865384615384\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05650937580873096, Train acc: 0.9303647741147741\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05619983475368757, Train acc: 0.9302550747863247\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05589496245977432, Train acc: 0.9305555555555556\n",
      "Val loss: 0.2767977714538574, Val acc: 0.9\n",
      "Epoch 76/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06289014844303457, Train acc: 0.9269497863247863\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.062301332369828835, Train acc: 0.9266159188034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0596091374967173, Train acc: 0.9286413817663818\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05807326062240152, Train acc: 0.9297876602564102\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05698083338574467, Train acc: 0.9301282051282052\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.056588852868290705, Train acc: 0.9303329772079773\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05595622310853849, Train acc: 0.9306318681318682\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05570579923562005, Train acc: 0.9309395032051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.054877670430503116, Train acc: 0.9312826448243114\n",
      "Val loss: 0.2843753397464752, Val acc: 0.898\n",
      "Epoch 77/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04839254240704398, Train acc: 0.9345619658119658\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05406752642658022, Train acc: 0.9318910256410257\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05484267143782048, Train acc: 0.931579415954416\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05540842018448389, Train acc: 0.9309895833333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0553145172758999, Train acc: 0.9317040598290598\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05554571594947424, Train acc: 0.9313790954415955\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05579492276914245, Train acc: 0.9312232905982906\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05582009335486298, Train acc: 0.9314903846153846\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05617227789513406, Train acc: 0.9310452279202279\n",
      "Val loss: 0.28111764788627625, Val acc: 0.9\n",
      "Epoch 78/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.051251696852537304, Train acc: 0.9334935897435898\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.055684836183348276, Train acc: 0.9318910256410257\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.055106054938416874, Train acc: 0.9313568376068376\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05612985382222722, Train acc: 0.9300213675213675\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05622486250522809, Train acc: 0.9299679487179487\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05587360256502771, Train acc: 0.9300213675213675\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05599227820560609, Train acc: 0.9299259768009768\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05551306413024919, Train acc: 0.9305054754273504\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0549060087456436, Train acc: 0.9307484567901234\n",
      "Val loss: 0.27993786334991455, Val acc: 0.896\n",
      "Epoch 79/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06048758786458235, Train acc: 0.9258814102564102\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.057159101670114405, Train acc: 0.9292868589743589\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05602711048560944, Train acc: 0.9287304131054132\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.054857374511213385, Train acc: 0.9297542735042735\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05272655937916193, Train acc: 0.9310096153846154\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05214986894969587, Train acc: 0.93184650997151\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05285089529404856, Train acc: 0.9318719474969475\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05350292446967374, Train acc: 0.9314403044871795\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.053210533737909876, Train acc: 0.9318168328584995\n",
      "Val loss: 0.27913185954093933, Val acc: 0.904\n",
      "Epoch 80/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04647349954670311, Train acc: 0.9349626068376068\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0468944169771977, Train acc: 0.9347622863247863\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.049713240177543076, Train acc: 0.93318198005698\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05023625929258827, Train acc: 0.932792467948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0492902466119864, Train acc: 0.9332532051282051\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05051938565368326, Train acc: 0.9328703703703703\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05115343548439361, Train acc: 0.9323679792429792\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05132565064690052, Train acc: 0.9322916666666666\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05164906644357009, Train acc: 0.9320245726495726\n",
      "Val loss: 0.2805911898612976, Val acc: 0.9\n",
      "Epoch 81/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04401341424538539, Train acc: 0.936698717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04669630005318894, Train acc: 0.9356971153846154\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.048988620100537596, Train acc: 0.9351406695156695\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04894271187293224, Train acc: 0.9337940705128205\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0495939572652181, Train acc: 0.9335202991452991\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.050687212804783444, Train acc: 0.9334713319088319\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04968091330612681, Train acc: 0.9339896214896215\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05059470662958602, Train acc: 0.9332264957264957\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.050913173022546436, Train acc: 0.9330187559354226\n",
      "Val loss: 0.29587963223457336, Val acc: 0.894\n",
      "Epoch 82/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0495097963219015, Train acc: 0.9334935897435898\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04842846675051583, Train acc: 0.9344284188034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.049347194808500784, Train acc: 0.9351406695156695\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.050236428522656106, Train acc: 0.934395032051282\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04939644853783469, Train acc: 0.9353899572649572\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.049481796573030305, Train acc: 0.9351629273504274\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04927393057189145, Train acc: 0.9350579975579976\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.049892121177707985, Train acc: 0.934395032051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04992872019495606, Train acc: 0.9341613247863247\n",
      "Val loss: 0.2902551293373108, Val acc: 0.896\n",
      "Epoch 83/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.055343894622264765, Train acc: 0.9301549145299145\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05167106774627653, Train acc: 0.9330261752136753\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05254551870191199, Train acc: 0.9326923076923077\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05132236166132821, Train acc: 0.932792467948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.051666930852792205, Train acc: 0.9330929487179487\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05133247821249513, Train acc: 0.9332042378917379\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05099372183665251, Train acc: 0.9330929487179487\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05063875349095234, Train acc: 0.9334268162393162\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.050646729010236, Train acc: 0.9335529439696106\n",
      "Val loss: 0.29365214705467224, Val acc: 0.898\n",
      "Epoch 84/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04884101679691902, Train acc: 0.9357638888888888\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.047209772352988906, Train acc: 0.9360309829059829\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.046267602730680396, Train acc: 0.9359864672364673\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.046439627137703776, Train acc: 0.9357305021367521\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04603844784263872, Train acc: 0.9365117521367521\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04735303434551272, Train acc: 0.9349848646723646\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.047667557994524636, Train acc: 0.9347718253968254\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.047298884719737574, Train acc: 0.9351295405982906\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04725939986372945, Train acc: 0.9355116334283001\n",
      "Val loss: 0.2857441008090973, Val acc: 0.9\n",
      "Epoch 85/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.040292776802666165, Train acc: 0.9432425213675214\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.042871516548160814, Train acc: 0.9408386752136753\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.046185878861663685, Train acc: 0.9381677350427351\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04842443031887723, Train acc: 0.9366319444444444\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04692844646608728, Train acc: 0.9372863247863248\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.046827483996536655, Train acc: 0.9371216168091168\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04686880406442579, Train acc: 0.9370802808302808\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04724584631303437, Train acc: 0.9364817040598291\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04721039264743258, Train acc: 0.9361496913580247\n",
      "Val loss: 0.2892821133136749, Val acc: 0.892\n",
      "Epoch 86/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04482772500596495, Train acc: 0.937232905982906\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04632153094579012, Train acc: 0.9353632478632479\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04619915698498402, Train acc: 0.9354967948717948\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04685427035149346, Train acc: 0.9357972756410257\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04629729884302514, Train acc: 0.9357371794871795\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.046551514427546424, Train acc: 0.9360087250712251\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04669322910399082, Train acc: 0.9361454517704517\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04651852512461507, Train acc: 0.9358640491452992\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.046310005944791795, Train acc: 0.9358974358974359\n",
      "Val loss: 0.2726367115974426, Val acc: 0.912\n",
      "Epoch 87/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04077631566259596, Train acc: 0.9356303418803419\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04501393437385559, Train acc: 0.9356303418803419\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.046468668598734754, Train acc: 0.9347400284900285\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.048474530633698165, Train acc: 0.9341947115384616\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.049138500853481454, Train acc: 0.9332532051282051\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04871968135365054, Train acc: 0.9338497150997151\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04841432421125917, Train acc: 0.9341613247863247\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04800971151671858, Train acc: 0.9341947115384616\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04756337516045185, Train acc: 0.9346955128205128\n",
      "Val loss: 0.27248266339302063, Val acc: 0.894\n",
      "Epoch 88/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05519683264259599, Train acc: 0.9316239316239316\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.049984829866478585, Train acc: 0.9336939102564102\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04721671979651492, Train acc: 0.9356748575498576\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04684024144951095, Train acc: 0.9353966346153846\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04675961707392309, Train acc: 0.9357905982905983\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04637075529733614, Train acc: 0.9356748575498576\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04606349862684406, Train acc: 0.9355731074481074\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04570257073093174, Train acc: 0.9360309829059829\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.045490261623662424, Train acc: 0.9357490503323836\n",
      "Val loss: 0.28451892733573914, Val acc: 0.9\n",
      "Epoch 89/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04210361494467808, Train acc: 0.9399038461538461\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04571046132562507, Train acc: 0.9365651709401709\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.045683914142796114, Train acc: 0.9355413105413105\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04405999339671216, Train acc: 0.9362646901709402\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0440411934740523, Train acc: 0.9361378205128205\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04446729242886573, Train acc: 0.9360754985754985\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.044927584273474555, Train acc: 0.9355731074481074\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04541591037478712, Train acc: 0.9353298611111112\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.045280458807152785, Train acc: 0.9355413105413105\n",
      "Val loss: 0.282977432012558, Val acc: 0.896\n",
      "Epoch 90/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04910699067971645, Train acc: 0.9337606837606838\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04260966582940175, Train acc: 0.9386351495726496\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04074108681278012, Train acc: 0.9392806267806267\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04243289564664547, Train acc: 0.9385016025641025\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0432831021455618, Train acc: 0.9381143162393163\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04347552180799664, Train acc: 0.9379006410256411\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0431992914141025, Train acc: 0.9380914224664225\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.042908100029214837, Train acc: 0.9380008012820513\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04308463817025408, Train acc: 0.9376038698955366\n",
      "Val loss: 0.29035162925720215, Val acc: 0.886\n",
      "Epoch 91/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.03470717459662348, Train acc: 0.9421741452991453\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0392863707155244, Train acc: 0.9393696581196581\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04132112605958922, Train acc: 0.9389245014245015\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.041976131530653715, Train acc: 0.9382345085470085\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04296063737482087, Train acc: 0.9378205128205128\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0429454049535966, Train acc: 0.9381454772079773\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04350830492664752, Train acc: 0.9381677350427351\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.042893837373226114, Train acc: 0.9386685363247863\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04248375761882532, Train acc: 0.9387464387464387\n",
      "Val loss: 0.2970772683620453, Val acc: 0.892\n",
      "Epoch 92/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.041158769502599016, Train acc: 0.9399038461538461\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04028650194915951, Train acc: 0.9401041666666666\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.040745054215107886, Train acc: 0.9390580484330484\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04071864555788855, Train acc: 0.9390357905982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.041349113929984915, Train acc: 0.9386752136752137\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.041395142568312476, Train acc: 0.9386351495726496\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0416647595909489, Train acc: 0.9381868131868132\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.0419786526925034, Train acc: 0.9382345085470085\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.042041124823765874, Train acc: 0.9381825735992403\n",
      "Val loss: 0.2846635580062866, Val acc: 0.888\n",
      "Epoch 93/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.047389536713942505, Train acc: 0.9349626068376068\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.043956111893694624, Train acc: 0.9362313034188035\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04301137199089398, Train acc: 0.9375\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04115748564656983, Train acc: 0.9390691773504274\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.042645158395807965, Train acc: 0.9384882478632479\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.041699376349945014, Train acc: 0.9392583689458689\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04231231087438935, Train acc: 0.9389308608058609\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04204681251420934, Train acc: 0.9391025641025641\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0421513078445949, Train acc: 0.9388057929724596\n",
      "Val loss: 0.2909148633480072, Val acc: 0.9\n",
      "Epoch 94/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.044740325110590354, Train acc: 0.9354967948717948\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04288453373134646, Train acc: 0.9368990384615384\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.043495671744020574, Train acc: 0.9365651709401709\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04232692759897974, Train acc: 0.9367654914529915\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04202478716516087, Train acc: 0.9365651709401709\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04166767920269246, Train acc: 0.9371438746438746\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04097630922319834, Train acc: 0.9374618437118437\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04110349359739031, Train acc: 0.9375500801282052\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04109406826446312, Train acc: 0.9377374169040835\n",
      "Val loss: 0.28611838817596436, Val acc: 0.888\n",
      "Epoch 95/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04094228364972987, Train acc: 0.9392361111111112\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.03816889468421284, Train acc: 0.9417735042735043\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.03760466480526829, Train acc: 0.9416844729344729\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.038545239239166945, Train acc: 0.9405381944444444\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.03894346076199132, Train acc: 0.9400641025641026\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.03912947210151586, Train acc: 0.9399928774928775\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.03946444952589834, Train acc: 0.9394078144078144\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.038833262462519176, Train acc: 0.9396868322649573\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.03883277513758975, Train acc: 0.939696106362773\n",
      "Val loss: 0.2748585641384125, Val acc: 0.898\n",
      "Epoch 96/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.045728960607805826, Train acc: 0.9362980769230769\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04448574227400315, Train acc: 0.9352964743589743\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04266433587610891, Train acc: 0.9373219373219374\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04216627381805681, Train acc: 0.9372996794871795\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.041023153754381037, Train acc: 0.937767094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.040373550839403756, Train acc: 0.9381677350427351\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.03948291572167906, Train acc: 0.9387019230769231\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.03988343318048705, Train acc: 0.9386351495726496\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.03997617924598321, Train acc: 0.9387316001899335\n",
      "Val loss: 0.28451645374298096, Val acc: 0.898\n",
      "Epoch 97/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.03726630231254121, Train acc: 0.9407051282051282\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.03807647825561018, Train acc: 0.9395699786324786\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.040194074610020024, Train acc: 0.9390135327635327\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.039192442965303734, Train acc: 0.9396367521367521\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.03919505482046013, Train acc: 0.9404380341880342\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.03902672791582906, Train acc: 0.940460292022792\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.03801318002416683, Train acc: 0.9412965506715507\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.03873178627119105, Train acc: 0.9410223023504274\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.038882158618820134, Train acc: 0.9409722222222222\n",
      "Val loss: 0.28051993250846863, Val acc: 0.904\n",
      "Epoch 98/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.03887078777337686, Train acc: 0.938034188034188\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.037314390715880275, Train acc: 0.9398370726495726\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0376191783664573, Train acc: 0.9396812678062678\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.03673553743805641, Train acc: 0.9397702991452992\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.03702560744733892, Train acc: 0.9404380341880342\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.03736022310039596, Train acc: 0.9401709401709402\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.037552240876633315, Train acc: 0.9403617216117216\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.037107937706586644, Train acc: 0.9406383547008547\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.03695156510852114, Train acc: 0.9407496438746439\n",
      "Val loss: 0.29946616291999817, Val acc: 0.9\n",
      "Epoch 99/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.03579588641977718, Train acc: 0.9417735042735043\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.03608034200902678, Train acc: 0.9402377136752137\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.03583983932635044, Train acc: 0.9407941595441596\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.03491466180381612, Train acc: 0.9411057692307693\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.035384615899151206, Train acc: 0.9409455128205129\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.035601025463169456, Train acc: 0.9406828703703703\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.03497277112889202, Train acc: 0.9410485347985348\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.03511778174493557, Train acc: 0.9413227831196581\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.03616515559065602, Train acc: 0.9407496438746439\n",
      "Val loss: 0.2999667227268219, Val acc: 0.894\n",
      "Epoch 100/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.03453108056997641, Train acc: 0.9433760683760684\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.03644842641730594, Train acc: 0.9421073717948718\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.036953170627270666, Train acc: 0.9416844729344729\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.03847954845708659, Train acc: 0.9405715811965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.03801182620545738, Train acc: 0.9402777777777778\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.03704963499239707, Train acc: 0.9404380341880342\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.037056866018908945, Train acc: 0.9404952686202687\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.03646485477240167, Train acc: 0.9410223023504274\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.03678162680392252, Train acc: 0.9407941595441596\n",
      "Val loss: 0.30506613850593567, Val acc: 0.898\n",
      "Tiempo total de entrenamiento: 1430.3191 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 1e-5\n",
    "dropout_p = 0.5\n",
    "batch_size = 64\n",
    "criterion = perdida_regularizada_entropia\n",
    "epochs = 100\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    metadata_train,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    metadata_val,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    "    beta=0.4,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.88      0.90      0.89       100\n",
      "          SN       0.90      0.79      0.84       100\n",
      "          VS       0.92      0.92      0.92       100\n",
      "    asteroid       0.88      0.98      0.93       100\n",
      "       bogus       0.96      0.95      0.95       100\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.91      0.91      0.91       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data_loader, use_gpu):\n",
    "    all_preds = []\n",
    "    all_true = []\n",
    "    \n",
    "    if use_gpu:\n",
    "        model = model.cuda()  # Transfiere el modelo a la GPU\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, metadata in data_loader:\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                metadata = metadata.cuda()\n",
    "\n",
    "            outputs = model(inputs, metadata)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(labels.numpy())\n",
    "\n",
    "    return all_true, all_preds\n",
    "\n",
    "# Asegúrate de tener tus datos de test preparados\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "fig, ax = plt.subplots(figsize=(7, 7))  # Ajusta el tamaño del gráfico\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax, colorbar=False)  # Elimina la barra de color\n",
    "\n",
    "# Ajustes adicionales para mejorar la visualización\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('True', fontsize=12)\n",
    "ax.grid(False)  # Elimina las líneas de la cuadrícula\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda combinación de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8711187391199617, Train acc: 0.10737179487179487\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.806493556397593, Train acc: 0.15397970085470086\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.787891534658579, Train acc: 0.166488603988604\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.77772869513585, Train acc: 0.17394497863247863\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.768204245608077, Train acc: 0.18178418803418803\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.7643122819074537, Train acc: 0.18425035612535612\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.7602916337340453, Train acc: 0.1874618437118437\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.7556697288130083, Train acc: 0.19270833333333334\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.75143807996259, Train acc: 0.19738247863247863\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.7470616151125005, Train acc: 0.20264423076923077\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.7414890106278236, Train acc: 0.20937742812742813\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.735806255768507, Train acc: 0.21587873931623933\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.7300317662080444, Train acc: 0.22236604207758054\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.7237945429716937, Train acc: 0.22941468253968253\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.718387055193257, Train acc: 0.23507834757834759\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.7111621571656985, Train acc: 0.24290531517094016\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.7017146568912067, Train acc: 0.2529066113624937\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.6915285068812644, Train acc: 0.2637701804368471\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.6815607696838653, Train acc: 0.27446018893387314\n",
      "Val loss: 3.2367734909057617, Val acc: 0.596\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4853617696680574, Train acc: 0.4890491452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4700932584257207, Train acc: 0.5050747863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.46268189191139, Train acc: 0.5122863247863247\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.450032302457043, Train acc: 0.5290464743589743\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4373519787421594, Train acc: 0.5432692307692307\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4298763410997526, Train acc: 0.5513265669515669\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4197688006656075, Train acc: 0.5623855311355311\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4093396615268836, Train acc: 0.5729834401709402\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4006724373454267, Train acc: 0.5820868945868946\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.3942814070954284, Train acc: 0.5883279914529914\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.3891246348457367, Train acc: 0.5933857808857809\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.3837794467934175, Train acc: 0.5984686609686609\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.3782649101355138, Train acc: 0.6036530243261012\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.374277753853245, Train acc: 0.6075816544566545\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.3693135965244045, Train acc: 0.6127136752136753\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.3658261109366374, Train acc: 0.6157518696581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.3618350444935627, Train acc: 0.6194067370537959\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.3590313941098797, Train acc: 0.6218393874643875\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.355679054384665, Train acc: 0.6248453666216824\n",
      "Val loss: 3.057784080505371, Val acc: 0.74\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.300038062609159, Train acc: 0.6736111111111112\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.295688523186578, Train acc: 0.6737446581196581\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.290706780561355, Train acc: 0.6803774928774928\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.2938505344920688, Train acc: 0.6775507478632479\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.290293592876858, Train acc: 0.6817307692307693\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.2878976194267597, Train acc: 0.6847845441595442\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.2869878350887833, Train acc: 0.6851343101343101\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.2846639841540246, Train acc: 0.6882011217948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.283459732901563, Train acc: 0.6899335232668566\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.2813561207208877, Train acc: 0.6920940170940171\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.2804115248356296, Train acc: 0.6929632867132867\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.280349515102528, Train acc: 0.6927305911680912\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.2783567549600794, Train acc: 0.6942800788954635\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.2766888697650987, Train acc: 0.6958943833943834\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.2758936757036086, Train acc: 0.6962250712250713\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.274437357982, Train acc: 0.6975494123931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.2734391804374363, Train acc: 0.698875062845651\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.2720588656232565, Train acc: 0.7002314814814815\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.2709618219640735, Train acc: 0.7013888888888888\n",
      "Val loss: 3.0092782974243164, Val acc: 0.786\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.2501405283936067, Train acc: 0.7203525641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.2525792447929707, Train acc: 0.7155448717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.2491256511449134, Train acc: 0.7185719373219374\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.247643265968714, Train acc: 0.7212873931623932\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.2468845387809298, Train acc: 0.7222222222222222\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.244703157335265, Train acc: 0.7245815527065527\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.2435623135176623, Train acc: 0.7253510378510378\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.2423832612669368, Train acc: 0.7261952457264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.239941341132067, Train acc: 0.7288105413105413\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.2396857530642778, Train acc: 0.7290331196581197\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.238564933096612, Train acc: 0.7302836052836053\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.237806747611771, Train acc: 0.7307024572649573\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.237912711934147, Train acc: 0.7303172255095332\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.237342889928992, Train acc: 0.7307883089133089\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.236960390965823, Train acc: 0.7309829059829059\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.23642790546784, Train acc: 0.7311197916666666\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.2357617285216858, Train acc: 0.7315705128205128\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.2346768177699046, Train acc: 0.7326685660018993\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.233640107387795, Train acc: 0.733721322537112\n",
      "Val loss: 2.976047992706299, Val acc: 0.808\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.2127085078475823, Train acc: 0.7516025641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.2130798437656503, Train acc: 0.7528044871794872\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.2132493057142297, Train acc: 0.7527599715099715\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.215353262730134, Train acc: 0.7492654914529915\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.2149219386597983, Train acc: 0.749732905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.215513269106547, Train acc: 0.7486645299145299\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.213435772137764, Train acc: 0.7507249694749695\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.2134821200982118, Train acc: 0.7503338675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.214190004897593, Train acc: 0.7494361348528015\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.2145089224872425, Train acc: 0.7483707264957264\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.2139873404602906, Train acc: 0.748761655011655\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.2134608617874973, Train acc: 0.7493545227920227\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.21174325400634, Train acc: 0.7509861932938856\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.2109011279212103, Train acc: 0.7515644078144078\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.210162687437487, Train acc: 0.7523326210826211\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.2104579414057937, Train acc: 0.7518028846153846\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.2097899653673054, Train acc: 0.7524824032176973\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.2090487387332027, Train acc: 0.7532496438746439\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.2083182265186867, Train acc: 0.7540766981556455\n",
      "Val loss: 2.9505808353424072, Val acc: 0.834\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.199578641826271, Train acc: 0.7612179487179487\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.201840753229255, Train acc: 0.7582799145299145\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.2007907856563556, Train acc: 0.7589921652421653\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1985839309855404, Train acc: 0.7602831196581197\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.198216635141617, Train acc: 0.7610576923076923\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.197101145388394, Train acc: 0.7622863247863247\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.196584864850446, Train acc: 0.7626678876678876\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1964793887912717, Train acc: 0.7629206730769231\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1976706841279525, Train acc: 0.7613366571699905\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1968735478882095, Train acc: 0.7618589743589743\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1962711635076824, Train acc: 0.7625534188034188\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.196151101691091, Train acc: 0.7623530982905983\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1960513582361285, Train acc: 0.7623685075608152\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1960207801744325, Train acc: 0.7624771062271062\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1954885929738013, Train acc: 0.7632122507122507\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1952396194394836, Train acc: 0.7633380074786325\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1940816800878418, Train acc: 0.7641716943187531\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1934004757705465, Train acc: 0.7645714624881291\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.193094207797563, Train acc: 0.7646761133603239\n",
      "Val loss: 2.942044734954834, Val acc: 0.838\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.193442623839419, Train acc: 0.7665598290598291\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.185058660996266, Train acc: 0.7724358974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1847385993370643, Train acc: 0.7720797720797721\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.187493723172408, Train acc: 0.7692307692307693\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1836806708930903, Train acc: 0.7733974358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1826412531725023, Train acc: 0.7740829772079773\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1829665810488375, Train acc: 0.7736568986568987\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1825778891897607, Train acc: 0.7736712072649573\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1838212946088453, Train acc: 0.7723171889838556\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.184710051055647, Train acc: 0.7715544871794872\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1835568506956657, Train acc: 0.7729700854700855\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1827003360643684, Train acc: 0.7738158831908832\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.182444190994679, Train acc: 0.7740590072320842\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.182305539265657, Train acc: 0.7740384615384616\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.182456065922381, Train acc: 0.774002849002849\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.182116655394053, Train acc: 0.7743723290598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1818568751463165, Train acc: 0.7745569381598794\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.181976009468068, Train acc: 0.7743203941120608\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1817886072721664, Train acc: 0.7745164192532613\n",
      "Val loss: 2.941880226135254, Val acc: 0.842\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1796496749943137, Train acc: 0.7748397435897436\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.180059190489288, Train acc: 0.7743055555555556\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1816371811760797, Train acc: 0.7731481481481481\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.182646930217743, Train acc: 0.7717013888888888\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1790515027494513, Train acc: 0.7754273504273504\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1777565282294553, Train acc: 0.776664886039886\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.176318415791997, Train acc: 0.7780067155067155\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.175063468197472, Train acc: 0.7796140491452992\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.17506621343571, Train acc: 0.7797364672364673\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1747908249879493, Train acc: 0.7795940170940171\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1749408782315793, Train acc: 0.7794531857031857\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.174580348183287, Train acc: 0.7798700142450142\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.174863189870319, Train acc: 0.7796885272846811\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.174683785846091, Train acc: 0.7796855921855922\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1742040510530827, Train acc: 0.7800747863247863\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1740376768458605, Train acc: 0.7801315438034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1737625538973187, Train acc: 0.7804172951231775\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.173738078174428, Train acc: 0.780522910731244\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.173999559648714, Train acc: 0.7801394511920827\n",
      "Val loss: 2.9453017711639404, Val acc: 0.828\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.169009208679199, Train acc: 0.781784188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1685078969368567, Train acc: 0.7827190170940171\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.169030681974188, Train acc: 0.7819622507122507\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.167781481376061, Train acc: 0.7836538461538461\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1697487488771094, Train acc: 0.7817307692307692\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1707782826871953, Train acc: 0.780715811965812\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.171039527298039, Train acc: 0.780715811965812\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1699944547353645, Train acc: 0.7819177350427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.168431608425586, Train acc: 0.7834164292497626\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.168524560867212, Train acc: 0.7833333333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1678700886128386, Train acc: 0.7842365967365967\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1674881202203258, Train acc: 0.7846777065527065\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1671066276342126, Train acc: 0.7851331360946746\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1663928555918264, Train acc: 0.7856952075702076\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1663855314934017, Train acc: 0.7858262108262108\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.165733385671917, Train acc: 0.7865418002136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1654365333616585, Train acc: 0.7868589743589743\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1652037920775236, Train acc: 0.7871260683760684\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1655413620760458, Train acc: 0.7865918803418803\n",
      "Val loss: 2.9314184188842773, Val acc: 0.844\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1572048990135517, Train acc: 0.7922008547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1588383994550786, Train acc: 0.7907318376068376\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1557198593079874, Train acc: 0.7947827635327636\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.155893593771845, Train acc: 0.7952724358974359\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1567244975994795, Train acc: 0.7947115384615384\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1565700628478983, Train acc: 0.7948272792022792\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.156198747137673, Train acc: 0.7951770451770451\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1575003692991714, Train acc: 0.7943042200854701\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1577912127530134, Train acc: 0.7941001899335233\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.158029786236266, Train acc: 0.7939636752136752\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.15757781021708, Train acc: 0.7944832944832945\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.157846591156772, Train acc: 0.7938701923076923\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.157897300974152, Train acc: 0.7940705128205128\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.157746868910807, Train acc: 0.7941468253968254\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.157498220500783, Train acc: 0.7944622507122507\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.15836479069076, Train acc: 0.7934695512820513\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1584588169392296, Train acc: 0.7931749622926093\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1585966639029674, Train acc: 0.7929427825261158\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.158730311432348, Train acc: 0.7927491003148898\n",
      "Val loss: 2.928264617919922, Val acc: 0.846\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.158814654391036, Train acc: 0.7940705128205128\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.155561382953937, Train acc: 0.7958066239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.153830474258488, Train acc: 0.7976317663817664\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.154061147545138, Train acc: 0.7972756410256411\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.152586323990781, Train acc: 0.7990918803418804\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.151853056375118, Train acc: 0.7995904558404558\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.151653739558908, Train acc: 0.7993360805860806\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.151101781262292, Train acc: 0.7997462606837606\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.151256089536553, Train acc: 0.7995014245014245\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.152098652542147, Train acc: 0.79866452991453\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.152331074078878, Train acc: 0.7982226107226107\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1520516382493184, Train acc: 0.7985220797720798\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1520987388571964, Train acc: 0.7981385601577909\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.152383586644253, Train acc: 0.7977144383394383\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1526092645449517, Train acc: 0.7974893162393163\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.152454655585635, Train acc: 0.7976262019230769\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.152751682091143, Train acc: 0.7973856209150327\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.151717693490973, Train acc: 0.7983588556505223\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1522251102123184, Train acc: 0.7978098290598291\n",
      "Val loss: 2.9378230571746826, Val acc: 0.832\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.159357489683689, Train acc: 0.7922008547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1591701400585666, Train acc: 0.7909989316239316\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.156723060838857, Train acc: 0.7942485754985755\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.15109033895354, Train acc: 0.7996127136752137\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1505097321974924, Train acc: 0.7998397435897436\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1482889822066, Train acc: 0.8016381766381766\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.148738783357781, Train acc: 0.8011675824175825\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1490730059961987, Train acc: 0.8006810897435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.149630614268927, Train acc: 0.8004510921177588\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.14959946860615, Train acc: 0.8004807692307693\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.149462325682266, Train acc: 0.8006993006993007\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.14906669427187, Train acc: 0.8009481837606838\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1488914249916755, Train acc: 0.8008711374095989\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1494848782035167, Train acc: 0.7998702686202687\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1489364849536168, Train acc: 0.8002670940170941\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1480627600581217, Train acc: 0.8011651976495726\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.148219137170196, Train acc: 0.8009521116138764\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.147902704881467, Train acc: 0.801267212725546\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.14791701122066, Train acc: 0.801211763382816\n",
      "Val loss: 2.9285125732421875, Val acc: 0.836\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1481297586718178, Train acc: 0.8023504273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.141502573958829, Train acc: 0.8082264957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1441513646362176, Train acc: 0.8053774928774928\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.144150482538419, Train acc: 0.8058894230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.143827234577929, Train acc: 0.8056089743589744\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.144873631815625, Train acc: 0.8044426638176638\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1452288375814903, Train acc: 0.8039529914529915\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.145429370367629, Train acc: 0.8035523504273504\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1444668115487238, Train acc: 0.8044871794871795\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1435866215290167, Train acc: 0.8053151709401709\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1430663584551333, Train acc: 0.8056526806526807\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.142566873392149, Train acc: 0.8062010327635327\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1429946589516935, Train acc: 0.805802103879027\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1436952674927436, Train acc: 0.804945054945055\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.142829520546133, Train acc: 0.8058760683760684\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1429329415799208, Train acc: 0.8057391826923077\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1424693934817958, Train acc: 0.8061997234791353\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1425167720655205, Train acc: 0.805985873694207\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1425570484973053, Train acc: 0.805949167791273\n",
      "Val loss: 2.919027090072632, Val acc: 0.856\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.141320355937012, Train acc: 0.8063568376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1434815475064464, Train acc: 0.8059561965811965\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1432541928060376, Train acc: 0.8056445868945868\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1427888949202676, Train acc: 0.8056891025641025\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1422868738826524, Train acc: 0.805982905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1419796663471775, Train acc: 0.8062678062678063\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.141523152917296, Train acc: 0.8066620879120879\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1409238206270413, Train acc: 0.8075253739316239\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1408840360125247, Train acc: 0.8074252136752137\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1409530584628764, Train acc: 0.8073450854700854\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1402580482420666, Train acc: 0.8081779331779332\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.139501590164978, Train acc: 0.8088497150997151\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1392330257301655, Train acc: 0.8089866863905325\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.139336320288452, Train acc: 0.8090277777777778\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1398799123247803, Train acc: 0.8084935897435898\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1396431963667912, Train acc: 0.8087439903846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.139536924731378, Train acc: 0.8087449723479135\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1388651236736083, Train acc: 0.8095322886989553\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1388672210587822, Train acc: 0.8095057354925776\n",
      "Val loss: 2.9201338291168213, Val acc: 0.854\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.129148124629616, Train acc: 0.8199786324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1339665569810786, Train acc: 0.8141025641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1363467069772573, Train acc: 0.8113425925925926\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.136562344355461, Train acc: 0.8108306623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.138084253898034, Train acc: 0.8091346153846154\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.136401115149854, Train acc: 0.8111645299145299\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1381393056794984, Train acc: 0.8095238095238095\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.138475164006918, Train acc: 0.8092280982905983\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.137894458240933, Train acc: 0.8099180911680912\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.137961670781812, Train acc: 0.8098824786324786\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1373146816844986, Train acc: 0.8106303418803419\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1369465840847743, Train acc: 0.8109196937321937\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.136498255842536, Train acc: 0.8113699868507561\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1372602726076986, Train acc: 0.8105731074481074\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.13773043461335, Train acc: 0.8100961538461539\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.137388503385915, Train acc: 0.8105301816239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1366350137749888, Train acc: 0.8112430869783811\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1366641942026043, Train acc: 0.8111793684710351\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1368006873227325, Train acc: 0.810939608636977\n",
      "Val loss: 2.91847825050354, Val acc: 0.854\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.132112066969912, Train acc: 0.8151709401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.13480141122117, Train acc: 0.8131677350427351\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1331070668337353, Train acc: 0.8147257834757835\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1333126086964564, Train acc: 0.8143028846153846\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.133621061969007, Train acc: 0.8142628205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.133333486539346, Train acc: 0.8149928774928775\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.133939746680859, Train acc: 0.814484126984127\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.134903487359357, Train acc: 0.8131677350427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.134301227382106, Train acc: 0.8137761158594492\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1346586524930773, Train acc: 0.8135149572649573\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1335992548200817, Train acc: 0.814539627039627\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.133426772817927, Train acc: 0.8147035256410257\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1331693148001647, Train acc: 0.8151914858645628\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1334032817346853, Train acc: 0.8148656898656899\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1326701291945587, Train acc: 0.8153311965811966\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1326250987939344, Train acc: 0.8153044871794872\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1325790380460643, Train acc: 0.8152337858220211\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1327513830501017, Train acc: 0.8149780389363722\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1323554890143996, Train acc: 0.8154520917678812\n",
      "Val loss: 2.921753168106079, Val acc: 0.852\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.128220122084658, Train acc: 0.8202457264957265\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1264708388564935, Train acc: 0.8205128205128205\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1292050693109843, Train acc: 0.8176638176638177\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.126120178617983, Train acc: 0.8215811965811965\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1267976744562134, Train acc: 0.8209401709401709\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.127782890939305, Train acc: 0.8201121794871795\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.128239520157941, Train acc: 0.8198260073260073\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1289626534423256, Train acc: 0.8193442841880342\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1302558694243543, Train acc: 0.8180496201329535\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.131161045824361, Train acc: 0.8170940170940171\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1301593991704197, Train acc: 0.8176961926961926\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1305464830493657, Train acc: 0.8174412393162394\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.130861612780168, Train acc: 0.81689677843524\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1309443396380825, Train acc: 0.8168307387057387\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1308047852964482, Train acc: 0.8169871794871795\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1301964428435025, Train acc: 0.8175080128205128\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.129806719814011, Train acc: 0.8176376319758673\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.129929983830973, Train acc: 0.8176044634377968\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1293916342980252, Train acc: 0.8181370895186685\n",
      "Val loss: 2.9142000675201416, Val acc: 0.856\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1359455524346767, Train acc: 0.8111645299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.127693491613763, Train acc: 0.8193108974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.12622433849889, Train acc: 0.8206018518518519\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1254058946401644, Train acc: 0.8217147435897436\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1259558194722885, Train acc: 0.8212072649572649\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.12697441659422, Train acc: 0.8201566951566952\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.127823027353438, Train acc: 0.819520757020757\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1271264798875547, Train acc: 0.8198784722222222\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.127343037302791, Train acc: 0.8198302469135802\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.127097450871753, Train acc: 0.8200320512820513\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.127712797822189, Train acc: 0.8193473193473193\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1268879367075755, Train acc: 0.8203125\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1273769096353194, Train acc: 0.8196499013806706\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1279760039478575, Train acc: 0.818986568986569\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.128316807067632, Train acc: 0.8187856125356126\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.128149937488075, Train acc: 0.8188267895299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1284676151937307, Train acc: 0.8185803167420814\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.128523821561312, Train acc: 0.8185392924976258\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1284100528348957, Train acc: 0.8187134502923976\n",
      "Val loss: 2.9155921936035156, Val acc: 0.86\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1322930144448566, Train acc: 0.8149038461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1329056391349206, Train acc: 0.8145032051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1288115394081486, Train acc: 0.8185541310541311\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.128752378571747, Train acc: 0.8183092948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1295714421150014, Train acc: 0.8174679487179487\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.128737781122539, Train acc: 0.81815349002849\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1267969542807275, Train acc: 0.8200931013431013\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.126358373424946, Train acc: 0.8205462072649573\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1260139215366114, Train acc: 0.820542497625831\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.126477750855633, Train acc: 0.8201121794871795\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.126907575751055, Train acc: 0.8195658508158508\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1263021264660393, Train acc: 0.8205128205128205\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.125374835858919, Train acc: 0.8213551939513478\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.125640849490742, Train acc: 0.8211996336996337\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.125647104839314, Train acc: 0.8212072649572649\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.125245718428722, Train acc: 0.8218315972222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1253421758514603, Train acc: 0.8217540221216691\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.125152201507619, Train acc: 0.8219669990503324\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1251122862489384, Train acc: 0.8220872694556906\n",
      "Val loss: 2.9072892665863037, Val acc: 0.866\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.127618353591006, Train acc: 0.8186431623931624\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.125485916932424, Train acc: 0.8213141025641025\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.123256060812208, Train acc: 0.8234508547008547\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.124608875085146, Train acc: 0.8215811965811965\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1271468926698733, Train acc: 0.8191239316239316\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1270211340695027, Train acc: 0.8189992877492878\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.125874704057044, Train acc: 0.8203983516483516\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1261385196549263, Train acc: 0.8198116987179487\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1254598093621526, Train acc: 0.8203050807217473\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.125271158442538, Train acc: 0.8207264957264957\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1252379173813813, Train acc: 0.8208770396270396\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1244567842395217, Train acc: 0.8219818376068376\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1233175970236773, Train acc: 0.8229988494411571\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1233506749255606, Train acc: 0.8230311355311355\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1237726039017026, Train acc: 0.8230235042735042\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1235857611028557, Train acc: 0.8231169871794872\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1234606832280836, Train acc: 0.8230894922071392\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1229445395985898, Train acc: 0.8236734330484331\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1231748460918944, Train acc: 0.8234227395411606\n",
      "Val loss: 2.911529302597046, Val acc: 0.864\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.12574972352411, Train acc: 0.8223824786324786\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1204300492237778, Train acc: 0.8271901709401709\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.119160539064652, Train acc: 0.8283475783475783\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1229209082248883, Train acc: 0.8243189102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.123530024341029, Train acc: 0.8235042735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1232499505719566, Train acc: 0.823272792022792\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.122866499991644, Train acc: 0.8236034798534798\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.122221025264161, Train acc: 0.8243522970085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1220428136452423, Train acc: 0.8244598765432098\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1220628777120867, Train acc: 0.8244925213675214\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1219468292749104, Train acc: 0.8245435120435121\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.121827348854467, Train acc: 0.8247195512820513\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1216211218648957, Train acc: 0.8250328731097962\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.121294450701666, Train acc: 0.8252823565323565\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1210108478524408, Train acc: 0.8254985754985755\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.121840704710056, Train acc: 0.8247028579059829\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1215058655041195, Train acc: 0.8251476872800402\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.121889795616255, Train acc: 0.824667616334283\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1217328946010197, Train acc: 0.8248706702654071\n",
      "Val loss: 2.901292085647583, Val acc: 0.874\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1183062139739337, Train acc: 0.8287927350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1164349465288668, Train acc: 0.8291933760683761\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.119441741891736, Train acc: 0.8257656695156695\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1190986294522243, Train acc: 0.8266559829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.120068458817963, Train acc: 0.8258547008547008\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1189704226632404, Train acc: 0.827323717948718\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1189169040966385, Train acc: 0.8269993894993894\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1187341838565645, Train acc: 0.8272569444444444\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1190777539301235, Train acc: 0.8268933998100665\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.118575575514736, Train acc: 0.8274839743589744\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1191963261194533, Train acc: 0.8269230769230769\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.118959983784249, Train acc: 0.8270343660968661\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1189611089770377, Train acc: 0.8270463510848126\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1185264378531365, Train acc: 0.8274191086691086\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1188464234697175, Train acc: 0.8271011396011396\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1190634650042934, Train acc: 0.8268396100427351\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1193293021395316, Train acc: 0.8264517345399698\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.119826347003748, Train acc: 0.8259437321937322\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.119652327577434, Train acc: 0.8261217948717948\n",
      "Val loss: 2.899632215499878, Val acc: 0.876\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1175630551118116, Train acc: 0.8287927350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1151693982955737, Train acc: 0.8306623931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1174566297449617, Train acc: 0.8276353276353277\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1172594155511284, Train acc: 0.827590811965812\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.115128924500229, Train acc: 0.8302350427350428\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1163798888184746, Train acc: 0.8292824074074074\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1167458896963005, Train acc: 0.8288690476190477\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.116182367516379, Train acc: 0.8295606303418803\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1170326216608033, Train acc: 0.8290598290598291\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1174035959773594, Train acc: 0.8285256410256411\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1177885376767955, Train acc: 0.8282828282828283\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1172784701881247, Train acc: 0.8288595085470085\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1172510123738797, Train acc: 0.8289981919789612\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1170081903791833, Train acc: 0.8292506105006106\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1170354020561586, Train acc: 0.8292378917378918\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.116889208746262, Train acc: 0.8293436164529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.117432574820674, Train acc: 0.8286670437405732\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1173837855670525, Train acc: 0.8287185422602089\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.117308795049927, Train acc: 0.828764619883041\n",
      "Val loss: 2.90352725982666, Val acc: 0.868\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1096185028043566, Train acc: 0.8360042735042735\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1122597782020893, Train acc: 0.8331997863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.11203285397967, Train acc: 0.8339565527065527\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1160708584337153, Train acc: 0.8299946581196581\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.116861963068318, Train acc: 0.8291132478632479\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1173657703603435, Train acc: 0.828926282051282\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1175433736435516, Train acc: 0.8286019536019537\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1192310521235833, Train acc: 0.8269564636752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1182237879389936, Train acc: 0.8277540360873694\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1184167644916436, Train acc: 0.8275106837606837\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.118269718147195, Train acc: 0.8275786713286714\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1178402215497107, Train acc: 0.8280582264957265\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1169574692561235, Train acc: 0.8290187376725838\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.116197474346347, Train acc: 0.8296130952380952\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.116924700438127, Train acc: 0.8290420227920228\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.116151095328168, Train acc: 0.8298277243589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1165005725613186, Train acc: 0.8294683257918553\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.116507417342828, Train acc: 0.8294604700854701\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.116161707227315, Train acc: 0.8298751686909581\n",
      "Val loss: 2.8982081413269043, Val acc: 0.874\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.109365839224595, Train acc: 0.8354700854700855\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.110131272393414, Train acc: 0.8346688034188035\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1121934224397707, Train acc: 0.832977207977208\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.111828345009404, Train acc: 0.8333333333333334\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.111205937923529, Train acc: 0.8340277777777778\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.112182151388239, Train acc: 0.8329326923076923\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.112324425006815, Train acc: 0.8332570207570208\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.113288120708914, Train acc: 0.832298344017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1134174035372104, Train acc: 0.832116571699905\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1144030562832823, Train acc: 0.8311431623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1132578195854963, Train acc: 0.8320464257964258\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1132634842497673, Train acc: 0.8319978632478633\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.114349310528205, Train acc: 0.8308678500986193\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1153126249092113, Train acc: 0.8298992673992674\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.116144083775686, Train acc: 0.8288817663817664\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1165099375777774, Train acc: 0.8285256410256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.11606800544196, Train acc: 0.8290126948215184\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1156912099601874, Train acc: 0.8294307929724596\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1155841391310735, Train acc: 0.8295940170940171\n",
      "Val loss: 2.9040474891662598, Val acc: 0.87\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1113303359757123, Train acc: 0.8373397435897436\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.110726479281727, Train acc: 0.8372061965811965\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1107311408404272, Train acc: 0.8360933048433048\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1123847018959174, Train acc: 0.8340678418803419\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.112039194147811, Train acc: 0.8345619658119658\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.112836713634665, Train acc: 0.8334668803418803\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1128075608985912, Train acc: 0.833409645909646\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1118990775102224, Train acc: 0.8342347756410257\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1115894479516113, Train acc: 0.8344610636277303\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1121598438319995, Train acc: 0.834107905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1121992950950745, Train acc: 0.8337703962703963\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1113384300657145, Train acc: 0.8346242877492878\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1111915623334423, Train acc: 0.8348331689677844\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1110736422049694, Train acc: 0.8348977411477412\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1111858242936963, Train acc: 0.8347756410256411\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1113795951390877, Train acc: 0.8344517895299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.111234890809306, Train acc: 0.8346688034188035\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1119040163041065, Train acc: 0.8340900997150997\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1120836563706664, Train acc: 0.8339518668466037\n",
      "Val loss: 2.9041826725006104, Val acc: 0.866\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.110162299922389, Train acc: 0.8344017094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1126003158398166, Train acc: 0.8330662393162394\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1113333542462427, Train acc: 0.8344017094017094\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.11320914595555, Train acc: 0.8316639957264957\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.112040762208466, Train acc: 0.833707264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1143129532493417, Train acc: 0.8315972222222222\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.113817507676298, Train acc: 0.8323412698412699\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.115641295655161, Train acc: 0.8302951388888888\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.114357688714523, Train acc: 0.8314043209876543\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1141620432209765, Train acc: 0.831650641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.112506079043912, Train acc: 0.8334304584304584\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1123036400035575, Train acc: 0.8335781695156695\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.112166784532285, Train acc: 0.8336826101249178\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1114691160391827, Train acc: 0.8343826312576312\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.111544379481563, Train acc: 0.8342770655270655\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1114537473290396, Train acc: 0.8341846955128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1112242165733788, Train acc: 0.8344017094017094\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.11163816521084, Train acc: 0.8338971984805318\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1110930089418596, Train acc: 0.8343735942420153\n",
      "Val loss: 2.900207281112671, Val acc: 0.874\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1037563116122513, Train acc: 0.84375\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.108954772989974, Train acc: 0.8360042735042735\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1098831878428443, Train acc: 0.8346688034188035\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.110914063249898, Train acc: 0.8336004273504274\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1127160596032426, Train acc: 0.8319978632478633\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1115268343194598, Train acc: 0.833244301994302\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.111604443982116, Train acc: 0.8334478021978022\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1111908589418116, Train acc: 0.8337339743589743\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1103251434346095, Train acc: 0.8345204178537512\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1092946436670093, Train acc: 0.8355769230769231\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.109055408694157, Train acc: 0.8359071484071484\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1093896208835123, Train acc: 0.8354923433048433\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.109993065759432, Train acc: 0.8348331689677844\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1107503072391647, Train acc: 0.8340964590964591\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1110709033460697, Train acc: 0.833707264957265\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1111786626597757, Train acc: 0.8335336538461539\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1108802712820713, Train acc: 0.8339303670186023\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1111387207976775, Train acc: 0.8336152659069326\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.111447755731659, Train acc: 0.833234930274404\n",
      "Val loss: 2.9343137741088867, Val acc: 0.836\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1089059185777974, Train acc: 0.8352029914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.103775366758689, Train acc: 0.8420138888888888\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1050671794136027, Train acc: 0.8407229344729344\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.107289388139024, Train acc: 0.8384748931623932\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1087737244418543, Train acc: 0.8366452991452992\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1093026336781318, Train acc: 0.8359152421652422\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.110683425878867, Train acc: 0.8346688034188035\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.11006020136878, Train acc: 0.8354700854700855\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.109805375648926, Train acc: 0.8358558879392213\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.110679151665451, Train acc: 0.8351228632478632\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1107214570693853, Train acc: 0.8349601787101787\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.110959894721664, Train acc: 0.8346910612535613\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.111321999790009, Train acc: 0.8341757067718606\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1109257935749888, Train acc: 0.834745115995116\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1107239306822123, Train acc: 0.8349180911680911\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1102721134566855, Train acc: 0.8355368589743589\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1096918650042658, Train acc: 0.8358628707893414\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.109441377045649, Train acc: 0.8360487891737892\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1091392886461677, Train acc: 0.8365384615384616\n",
      "Val loss: 2.9021711349487305, Val acc: 0.872\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.103889417444539, Train acc: 0.8426816239316239\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.104112275645264, Train acc: 0.843215811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.105287264894556, Train acc: 0.8409009971509972\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.105056202564484, Train acc: 0.8408787393162394\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.10575107590765, Train acc: 0.8405448717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1067506704914605, Train acc: 0.8389423076923077\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1077058866929366, Train acc: 0.8379120879120879\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1084544341533613, Train acc: 0.8365050747863247\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.108629938204404, Train acc: 0.8364197530864198\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1089518264827567, Train acc: 0.8361111111111111\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.108731848098737, Train acc: 0.8364170551670551\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.108831078663171, Train acc: 0.8362936253561254\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1086941147853793, Train acc: 0.8365179158448389\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1086975644650767, Train acc: 0.8366720085470085\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.108333640900093, Train acc: 0.837037037037037\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1082521671285996, Train acc: 0.8370225694444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.108277040069699, Train acc: 0.8369940925087984\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1086590241389622, Train acc: 0.8364939458689459\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1090750797771944, Train acc: 0.8359902159244265\n",
      "Val loss: 2.9111709594726562, Val acc: 0.856\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1009448980673766, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1047472953796387, Train acc: 0.8409455128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.103453203483864, Train acc: 0.8424145299145299\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1083891776382413, Train acc: 0.8371394230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.10918841035957, Train acc: 0.8364850427350428\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.109448650963286, Train acc: 0.8360933048433048\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1098293688998844, Train acc: 0.8358898046398047\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1092096257668276, Train acc: 0.8366052350427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1096317123936563, Train acc: 0.8360339506172839\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.109206805269942, Train acc: 0.8364583333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.109730499422448, Train acc: 0.8358828671328671\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1094243230133656, Train acc: 0.8364049145299145\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1095211860931364, Train acc: 0.8362302761341223\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1091920581347194, Train acc: 0.8363286019536019\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.108516201945791, Train acc: 0.8370192307692308\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.109264102756468, Train acc: 0.8361545138888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.108996780150017, Train acc: 0.836302790346908\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1085237123687723, Train acc: 0.836835232668566\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1083400926233895, Train acc: 0.8371007647323436\n",
      "Val loss: 2.8955790996551514, Val acc: 0.876\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1033624247608023, Train acc: 0.8381410256410257\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.104495229374649, Train acc: 0.8392094017094017\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.10535925949401, Train acc: 0.8384971509971509\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.107821829043902, Train acc: 0.8361378205128205\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.107631721659603, Train acc: 0.8365384615384616\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1080078069980326, Train acc: 0.8364939458689459\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.107187817967127, Train acc: 0.8374923687423688\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.107505555463652, Train acc: 0.8368389423076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.105364618591207, Train acc: 0.8392094017094017\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1059706133654994, Train acc: 0.8384882478632478\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.106197677106939, Train acc: 0.8382138694638694\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1064124458875413, Train acc: 0.837829415954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.107337870149531, Train acc: 0.8369699211045365\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1074678284344657, Train acc: 0.8368818681318682\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1080778585200295, Train acc: 0.83630698005698\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.107730914321211, Train acc: 0.8367220886752137\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1072531853446175, Train acc: 0.8372454751131222\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1072014844202474, Train acc: 0.8372655508072174\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.106860549290117, Train acc: 0.8376349527665317\n",
      "Val loss: 2.8975112438201904, Val acc: 0.874\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1064691400935507, Train acc: 0.8389423076923077\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1096714729936714, Train acc: 0.8349358974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.108386856538278, Train acc: 0.8360042735042735\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1091028747395573, Train acc: 0.8353365384615384\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.107406048081879, Train acc: 0.8370192307692308\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.10729316938297, Train acc: 0.8368945868945868\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1057970011481726, Train acc: 0.8388278388278388\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1054993618247857, Train acc: 0.8391426282051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.105327827185534, Train acc: 0.8393281101614435\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1043628726250088, Train acc: 0.8403044871794871\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1050828041359724, Train acc: 0.8395250582750583\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1054001848752004, Train acc: 0.8392984330484331\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.104577402861631, Train acc: 0.84021614069691\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.104621711668077, Train acc: 0.840182387057387\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.105067660597994, Train acc: 0.8395121082621083\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1054798828230963, Train acc: 0.8391092414529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1056121790930518, Train acc: 0.8390051533433887\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1055463802101264, Train acc: 0.8392242402659069\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.105955530703738, Train acc: 0.8388017318938371\n",
      "Val loss: 2.9044406414031982, Val acc: 0.866\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.100444338260553, Train acc: 0.8461538461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.098068952560425, Train acc: 0.8489583333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0997361274186703, Train acc: 0.8467770655270656\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1004470230167747, Train acc: 0.8462206196581197\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.101379790265336, Train acc: 0.8455128205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1013815306190753, Train acc: 0.8451299857549858\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1019093362985224, Train acc: 0.8444368131868132\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.102465814886949, Train acc: 0.8437833867521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1029811449992803, Train acc: 0.8428300094966762\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1030621044656153, Train acc: 0.8424412393162393\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1035878950065667, Train acc: 0.841977466977467\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.104197623128565, Train acc: 0.8412571225071225\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1039148757992256, Train acc: 0.8415310650887574\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.103837819399269, Train acc: 0.8417467948717948\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.104050310452779, Train acc: 0.8414351851851852\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1039761380634756, Train acc: 0.8415965544871795\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.103787717955864, Train acc: 0.8417860734037205\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1037704261393064, Train acc: 0.8417022792022792\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1037471986170884, Train acc: 0.8417819388214125\n",
      "Val loss: 2.8944764137268066, Val acc: 0.878\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.11465121130658, Train acc: 0.8306623931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1131473356841974, Train acc: 0.8309294871794872\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1097815281305556, Train acc: 0.8342236467236467\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1056543828075767, Train acc: 0.8386752136752137\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.10442416443784, Train acc: 0.8403311965811966\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1046385021291227, Train acc: 0.8398771367521367\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1049691298942426, Train acc: 0.839514652014652\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1041037218692975, Train acc: 0.8404113247863247\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.102811203496522, Train acc: 0.8417319563152896\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1036162606671325, Train acc: 0.8410523504273504\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1037094414280473, Train acc: 0.8410062160062161\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.103662272464176, Train acc: 0.8410790598290598\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.102872737060637, Train acc: 0.841715976331361\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.103258925989288, Train acc: 0.8412698412698413\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1030946885079063, Train acc: 0.8412749287749288\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1026667553899636, Train acc: 0.8416800213675214\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1029751467429074, Train acc: 0.8413932880844646\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.1025942285515984, Train acc: 0.841820987654321\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.102976192936831, Train acc: 0.8414445569050832\n",
      "Val loss: 2.8975698947906494, Val acc: 0.874\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0955844822092953, Train acc: 0.8490918803418803\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1002587176795697, Train acc: 0.844551282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.103120958703196, Train acc: 0.8413461538461539\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.1002318642587743, Train acc: 0.8450854700854701\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.100726949251615, Train acc: 0.8441239316239316\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1016211316116853, Train acc: 0.8434383903133903\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.102247551538423, Train acc: 0.8428342490842491\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.102850284968686, Train acc: 0.8424145299145299\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.1024773692586596, Train acc: 0.8428596866096866\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1022518929253278, Train acc: 0.8430288461538461\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1026556559468577, Train acc: 0.842511655011655\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.102223355206329, Train acc: 0.8428374287749287\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1021981025353456, Train acc: 0.8429692636423406\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.102185407692114, Train acc: 0.8430250305250305\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.102332015866228, Train acc: 0.8428418803418803\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.102789784534874, Train acc: 0.8423143696581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.102771992357368, Train acc: 0.8423359728506787\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.102517173783845, Train acc: 0.8424738841405508\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.1026708028148113, Train acc: 0.8422598965362124\n",
      "Val loss: 2.8944284915924072, Val acc: 0.878\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.11078394885756, Train acc: 0.8336004273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.10465209861087, Train acc: 0.8400106837606838\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.104804588858558, Train acc: 0.8394764957264957\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.101801115987647, Train acc: 0.8431490384615384\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.101947578609499, Train acc: 0.8430021367521368\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.100834853968389, Train acc: 0.8439280626780626\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1007903293230012, Train acc: 0.844017094017094\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1014520993344803, Train acc: 0.8432825854700855\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.10089064663292, Train acc: 0.8437796771130105\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.100935533005967, Train acc: 0.8437232905982905\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1007577136402085, Train acc: 0.844017094017094\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1006968406021085, Train acc: 0.8440616096866097\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1016309748034505, Train acc: 0.8431130834976989\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1011281733984477, Train acc: 0.8436164529914529\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.100443525803395, Train acc: 0.8444088319088319\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.100215368482292, Train acc: 0.8446180555555556\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.1003394865881804, Train acc: 0.8445827048768225\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0999161438611385, Train acc: 0.8449370845204178\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.100367999538022, Train acc: 0.8444950517318939\n",
      "Val loss: 2.8939199447631836, Val acc: 0.874\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.097888609282991, Train acc: 0.8456196581196581\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.1025738848580255, Train acc: 0.8409455128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.1033765118346257, Train acc: 0.8403668091168092\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.100658952425688, Train acc: 0.8433493589743589\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.1015584216158616, Train acc: 0.8425747863247863\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1011463931822707, Train acc: 0.8432603276353277\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.101556832682664, Train acc: 0.8427960927960928\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1021401956040635, Train acc: 0.8421808226495726\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.102754238437497, Train acc: 0.8416132478632479\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.1023419151958236, Train acc: 0.84196047008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.101956896570735, Train acc: 0.8422688422688422\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.1018365653831395, Train acc: 0.8424367877492878\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.1013719265120816, Train acc: 0.843051446416831\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1010133856964344, Train acc: 0.8433112026862026\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.1011329204608233, Train acc: 0.8432336182336182\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.1010341585064545, Train acc: 0.8434161324786325\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.10041473707762, Train acc: 0.844017094017094\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.100604465657603, Train acc: 0.8438538698955366\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.100156702439741, Train acc: 0.8442560728744939\n",
      "Val loss: 2.903996229171753, Val acc: 0.868\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.096443994432433, Train acc: 0.8485576923076923\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0991358700980487, Train acc: 0.8453525641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0992569213579184, Train acc: 0.8454415954415955\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0979011254942317, Train acc: 0.8470886752136753\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0978292673062056, Train acc: 0.8470619658119658\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.098530433286629, Train acc: 0.8461093304843305\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0993731020425557, Train acc: 0.8452380952380952\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.099822390410635, Train acc: 0.8449519230769231\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0992845909321525, Train acc: 0.8453228869895536\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.098811966639299, Train acc: 0.8459401709401709\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.100018665877626, Train acc: 0.8447455322455323\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.10030202673711, Train acc: 0.8443287037037037\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.099988913394996, Train acc: 0.8447772846811308\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.1001595878047965, Train acc: 0.8445131257631258\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.099641056821557, Train acc: 0.8448717948717949\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0996975067079577, Train acc: 0.8447516025641025\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0995948603549994, Train acc: 0.8449912016088487\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.099620507143841, Train acc: 0.8449964387464387\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0998917870622362, Train acc: 0.8446637426900585\n",
      "Val loss: 2.8954925537109375, Val acc: 0.876\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0976398194956984, Train acc: 0.8477564102564102\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0987955596711902, Train acc: 0.8457532051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0993821685470406, Train acc: 0.8451745014245015\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0990975692740874, Train acc: 0.8452857905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.100951171328879, Train acc: 0.8433226495726496\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.1016812580942767, Train acc: 0.8423700142450142\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.1009540876626094, Train acc: 0.8431013431013431\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.1011045592972355, Train acc: 0.8428151709401709\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0999692693168734, Train acc: 0.844017094017094\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0993112051588856, Train acc: 0.8448183760683761\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.1002582624705153, Train acc: 0.8439928127428128\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0997540434541184, Train acc: 0.8446180555555556\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0990179694851006, Train acc: 0.8453525641025641\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0990627897819176, Train acc: 0.845333485958486\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.098665803789753, Train acc: 0.8458511396011396\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0984470271147213, Train acc: 0.8459869123931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0984236857230605, Train acc: 0.8459653092006033\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0982110723357823, Train acc: 0.8462873931623932\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.098238234953252, Train acc: 0.8461960188933874\n",
      "Val loss: 2.895747661590576, Val acc: 0.876\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0940632697863455, Train acc: 0.8512286324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0937730328649535, Train acc: 0.8518963675213675\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.094619819241711, Train acc: 0.8506944444444444\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.093447894622118, Train acc: 0.8518963675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.093048616963574, Train acc: 0.8521901709401709\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.092971150521879, Train acc: 0.8523415242165242\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0934814440316187, Train acc: 0.8519917582417582\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0945057537820606, Train acc: 0.850761217948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0940414469013415, Train acc: 0.8513770180436847\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0952543350366444, Train acc: 0.8498130341880342\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.096580950252382, Train acc: 0.8482177544677545\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0969388816091747, Train acc: 0.8479789886039886\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0962795243304, Train acc: 0.8485782380013149\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0969966282483568, Train acc: 0.8477945665445665\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0972157205271924, Train acc: 0.8476317663817664\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0978139967490463, Train acc: 0.8470886752136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0977541002093276, Train acc: 0.8470179738562091\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.097920797715726, Train acc: 0.8468660968660968\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.098044763799728, Train acc: 0.8466177462887989\n",
      "Val loss: 2.9173521995544434, Val acc: 0.848\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1011343348739495, Train acc: 0.8413461538461539\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.099395224171826, Train acc: 0.843215811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0993578739655323, Train acc: 0.8449074074074074\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.096448448733387, Train acc: 0.8480235042735043\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.096847613856324, Train acc: 0.8474358974358974\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.096781118982538, Train acc: 0.8474448005698005\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.097065319217314, Train acc: 0.8473748473748474\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.097151376370691, Train acc: 0.847389155982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.096813896103123, Train acc: 0.8477267331433999\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.096663465866676, Train acc: 0.8477831196581197\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0964106111074474, Train acc: 0.8480235042735043\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0968206103710707, Train acc: 0.8476673789173789\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0969419810898913, Train acc: 0.847550953320184\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0967598705210237, Train acc: 0.8477754884004884\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0965924642024896, Train acc: 0.8480413105413105\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.096981557898032, Train acc: 0.8475894764957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.097123821456568, Train acc: 0.8473950477626948\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0973183917976628, Train acc: 0.8471925451092118\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0970432615687704, Train acc: 0.8473909131803868\n",
      "Val loss: 2.89705491065979, Val acc: 0.87\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0994928932597494, Train acc: 0.8461538461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0943465350020647, Train acc: 0.8502938034188035\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.092509210619152, Train acc: 0.8522079772079773\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0914453400505915, Train acc: 0.8538995726495726\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0934698349390275, Train acc: 0.8518696581196581\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.094263439674323, Train acc: 0.8511841168091168\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.094520249355116, Train acc: 0.8508852258852259\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0950719537133846, Train acc: 0.8499265491452992\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0942809167410914, Train acc: 0.8504273504273504\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0941703359285992, Train acc: 0.850534188034188\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.093885671934974, Train acc: 0.8506458818958819\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.094729016224543, Train acc: 0.8497596153846154\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0946383055380347, Train acc: 0.8496260683760684\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0949234375877985, Train acc: 0.8493971306471306\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.094521236419678, Train acc: 0.8497507122507123\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0950906076110325, Train acc: 0.8492922008547008\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0947575563758747, Train acc: 0.8495946455505279\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0952392275969642, Train acc: 0.8489731718898386\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.095304183953526, Train acc: 0.8488669590643275\n",
      "Val loss: 2.896108865737915, Val acc: 0.872\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.095358834307418, Train acc: 0.8498931623931624\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.09868061542511, Train acc: 0.8454861111111112\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0969306831685905, Train acc: 0.8476673789173789\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0945729391697125, Train acc: 0.8500934829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.095181162540729, Train acc: 0.8493589743589743\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.093205248018955, Train acc: 0.8511841168091168\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0930407035627354, Train acc: 0.851572039072039\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0934680882427426, Train acc: 0.8511618589743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.093573962747768, Train acc: 0.8511099240265907\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0935449885506916, Train acc: 0.8512820512820513\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.094230219663903, Train acc: 0.8505973193473193\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0946190182300035, Train acc: 0.8501157407407407\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.094051621447732, Train acc: 0.850612261669954\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0938278174662326, Train acc: 0.8508470695970696\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.094017403349917, Train acc: 0.850462962962963\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0941928402862997, Train acc: 0.8501769497863247\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.094813277039353, Train acc: 0.8495632227249874\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.094525766338718, Train acc: 0.8498486467236467\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0949637988532817, Train acc: 0.8493449167791273\n",
      "Val loss: 2.9007792472839355, Val acc: 0.87\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0894222605941644, Train acc: 0.8557692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.091512178253924, Train acc: 0.8529647435897436\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0921847793111774, Train acc: 0.8522079772079773\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0916569966536303, Train acc: 0.8526308760683761\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0915061353618265, Train acc: 0.8528311965811965\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0923774756936946, Train acc: 0.8518963675213675\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.091654296468611, Train acc: 0.8525641025641025\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0915764298958655, Train acc: 0.8526308760683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.091249455753555, Train acc: 0.8529499050332384\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0915763600259765, Train acc: 0.8527777777777777\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.091297670029445, Train acc: 0.8530740093240093\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0914084319545334, Train acc: 0.8530092592592593\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.090578994964158, Train acc: 0.8539406640368179\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.089998633651943, Train acc: 0.8545863858363858\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.090192946545419, Train acc: 0.8544871794871794\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0910229630704618, Train acc: 0.8536157852564102\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0912245797295856, Train acc: 0.8534125188536953\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.09116574315264, Train acc: 0.8535434472934473\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0911278920939416, Train acc: 0.853477845254161\n",
      "Val loss: 2.8988001346588135, Val acc: 0.872\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.1022217090313253, Train acc: 0.8424145299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.098187812882611, Train acc: 0.8464209401709402\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0986239237663074, Train acc: 0.8454415954415955\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0956302891429672, Train acc: 0.8480902777777778\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0953288960660625, Train acc: 0.8482905982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.094225062943592, Train acc: 0.8497596153846154\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.093797428267343, Train acc: 0.8501602564102564\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0938980371778846, Train acc: 0.8500267094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.093556574720037, Train acc: 0.8504570275403609\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.093362827993866, Train acc: 0.8506410256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0929358341755013, Train acc: 0.8512529137529138\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0929981929454367, Train acc: 0.8512286324786325\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.092358388217322, Train acc: 0.8518039119000658\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.092519528699882, Train acc: 0.8516292735042735\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0925595028108344, Train acc: 0.8515669515669516\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.092421942923823, Train acc: 0.8517127403846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.09279969375058, Train acc: 0.8513543237807943\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.093267588250884, Train acc: 0.8507241215574549\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0928726042384134, Train acc: 0.8511864597390914\n",
      "Val loss: 2.9023311138153076, Val acc: 0.862\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.096524475986122, Train acc: 0.8461538461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0957498560603867, Train acc: 0.8476228632478633\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0930559978186234, Train acc: 0.8510505698005698\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0949181606117477, Train acc: 0.8488247863247863\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0935069929840218, Train acc: 0.8504807692307692\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0935438295714874, Train acc: 0.8503828347578347\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0928588427321233, Train acc: 0.8512286324786325\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0933186582520475, Train acc: 0.8509281517094017\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0930751491928827, Train acc: 0.8511692782526116\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.092986079158946, Train acc: 0.8513087606837607\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0930057223119194, Train acc: 0.8511557886557887\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.093086736324506, Train acc: 0.8509170227920227\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0925330905676045, Train acc: 0.8514751808021038\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.092602927897294, Train acc: 0.8513431013431013\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.092258843125781, Train acc: 0.8516915954415955\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.092569897381159, Train acc: 0.8513454861111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.092666517615378, Train acc: 0.8512757667169432\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0924367053895936, Train acc: 0.851599596391263\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0921098213714893, Train acc: 0.8519315114709851\n",
      "Val loss: 2.900118589401245, Val acc: 0.874\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0989591321374617, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0985793619074373, Train acc: 0.8458867521367521\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.093289636818432, Train acc: 0.8514957264957265\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0921094392099953, Train acc: 0.852363782051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0918018371630938, Train acc: 0.8525641025641025\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0899620971448742, Train acc: 0.854255698005698\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0910481188323473, Train acc: 0.8534035409035409\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0904351535261188, Train acc: 0.8542668269230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.090169740198684, Train acc: 0.8542853751187085\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0905801029286835, Train acc: 0.8542200854700854\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0915021175734276, Train acc: 0.853219696969697\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0915864475092656, Train acc: 0.8529870014245015\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0916422104694434, Train acc: 0.852913379355687\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0908298260999687, Train acc: 0.853804181929182\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0912198970800113, Train acc: 0.8534366096866097\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0910469306967197, Train acc: 0.8534822382478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.091279991611636, Train acc: 0.8532868275515334\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.091017636421852, Train acc: 0.8534989316239316\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.091349146787615, Train acc: 0.8531404633378318\n",
      "Val loss: 2.8915460109710693, Val acc: 0.88\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.086339076360067, Train acc: 0.8573717948717948\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0873763505210223, Train acc: 0.8561698717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0920202168304356, Train acc: 0.8514066951566952\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.090411408334716, Train acc: 0.8526976495726496\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.088604833325769, Train acc: 0.8546474358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0881857011053295, Train acc: 0.8554576210826211\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0880229695812687, Train acc: 0.8556166056166056\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.088130595974433, Train acc: 0.8555021367521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.088892586324063, Train acc: 0.8548492402659069\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0896275990029687, Train acc: 0.8539262820512821\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0894942065033693, Train acc: 0.8541423853923854\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0896552410560454, Train acc: 0.8539663461538461\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0898890121448677, Train acc: 0.8536941157133465\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0899635661798226, Train acc: 0.8535370879120879\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.090118531148318, Train acc: 0.8534009971509972\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0898778571022882, Train acc: 0.8535490117521367\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.089974067989098, Train acc: 0.8534910759175465\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.089766132356095, Train acc: 0.8536769943019943\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0900152397756364, Train acc: 0.8533934997750787\n",
      "Val loss: 2.89190411567688, Val acc: 0.88\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.08654904161763, Train acc: 0.8579059829059829\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0844637512141824, Train acc: 0.859375\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0843303485473674, Train acc: 0.8601317663817664\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.086399646396311, Train acc: 0.8580395299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0866927760279075, Train acc: 0.8576388888888888\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0875114596467412, Train acc: 0.8567485754985755\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0878415301314788, Train acc: 0.8562652625152625\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0879865779071793, Train acc: 0.856270032051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.088541148621359, Train acc: 0.8555021367521367\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0882508066984324, Train acc: 0.8557158119658119\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0887279103871177, Train acc: 0.8553078865578866\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.088909731036917, Train acc: 0.8551905270655271\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0887997706887913, Train acc: 0.8554199539776463\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0888746009641515, Train acc: 0.8552922771672772\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.089328955041717, Train acc: 0.8548254985754986\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0894052349349375, Train acc: 0.8547342414529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.089343549379336, Train acc: 0.8548108345902463\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.089438472491497, Train acc: 0.8546860161443495\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0894287639623066, Train acc: 0.8546727395411606\n",
      "Val loss: 2.910785436630249, Val acc: 0.86\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0857975859927316, Train acc: 0.8587072649572649\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.092606980067033, Train acc: 0.8528311965811965\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0925469293213976, Train acc: 0.8519408831908832\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0917316402635002, Train acc: 0.8528979700854701\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.092042909728156, Train acc: 0.8524038461538461\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.090627604739958, Train acc: 0.8535879629629629\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0907530867573105, Train acc: 0.8534798534798534\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0910582976718235, Train acc: 0.8531650641025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0914401516728707, Train acc: 0.8526531339031339\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0918063564178273, Train acc: 0.8521634615384616\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.091207969531405, Train acc: 0.852782634032634\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0909355709016153, Train acc: 0.8531205484330484\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.091190935200096, Train acc: 0.8527695595003287\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0912532888634883, Train acc: 0.8527167277167277\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0904562656696024, Train acc: 0.8534722222222222\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0902452889798036, Train acc: 0.8537994123931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0892991381952544, Train acc: 0.8547322775263951\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.089205413122802, Train acc: 0.8548344017094017\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0897160783524025, Train acc: 0.8543775303643725\n",
      "Val loss: 2.9000132083892822, Val acc: 0.872\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0981042813032103, Train acc: 0.8450854700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.088309130098066, Train acc: 0.8555021367521367\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.089317214115393, Train acc: 0.8547008547008547\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0894842063769317, Train acc: 0.8545673076923077\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0882646047152007, Train acc: 0.8557158119658119\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0883121475195274, Train acc: 0.8559918091168092\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0898010540066765, Train acc: 0.8543192918192918\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.090343065241463, Train acc: 0.8537994123931624\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0900233305870413, Train acc: 0.8540776353276354\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0896283212889974, Train acc: 0.8545138888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.09070180133043, Train acc: 0.8533653846153846\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0904068872120307, Train acc: 0.8535434472934473\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0901144949877284, Train acc: 0.8538173898750822\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0899906050009025, Train acc: 0.8539377289377289\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0899705530231834, Train acc: 0.8539529914529914\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0893277134268713, Train acc: 0.8545840010683761\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0891460324724216, Train acc: 0.8547637003519356\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.088454972475003, Train acc: 0.8555021367521367\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.088529052843771, Train acc: 0.8554459064327485\n",
      "Val loss: 2.891453742980957, Val acc: 0.878\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0789029465781317, Train acc: 0.8643162393162394\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.08318099201235, Train acc: 0.859909188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0869611640941046, Train acc: 0.8565705128205128\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.085507805785562, Train acc: 0.8578392094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0860361671855308, Train acc: 0.85758547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0854339497721095, Train acc: 0.8583511396011396\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0853810431174162, Train acc: 0.8585164835164835\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0846733431785536, Train acc: 0.8592414529914529\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0852492251853545, Train acc: 0.8585292022792023\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0861326205424775, Train acc: 0.8574786324786324\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0860803105984904, Train acc: 0.8573960761460762\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.085863711147906, Train acc: 0.8575053418803419\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0865710306763257, Train acc: 0.856775969756739\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0868099526899058, Train acc: 0.8567231379731379\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.087387181618954, Train acc: 0.8560897435897435\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0872024663875246, Train acc: 0.8562366452991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.087420492330468, Train acc: 0.8562091503267973\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.087689499569754, Train acc: 0.8558434235517569\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0871235389619383, Train acc: 0.8565142825011246\n",
      "Val loss: 2.8962998390197754, Val acc: 0.876\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0934855581348777, Train acc: 0.8506944444444444\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.094563809215513, Train acc: 0.8501602564102564\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.093049642367241, Train acc: 0.8514066951566952\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.090637051142179, Train acc: 0.8539663461538461\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.091613909933302, Train acc: 0.8525641025641025\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.091542080936269, Train acc: 0.8527866809116809\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0908419295545024, Train acc: 0.8534416971916972\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.089266262884833, Train acc: 0.8549679487179487\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0881607237025204, Train acc: 0.8560660018993352\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0878019204506506, Train acc: 0.8562232905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0883152475631226, Train acc: 0.8557449494949495\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0879022735476154, Train acc: 0.8562366452991453\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0876556514988285, Train acc: 0.856508875739645\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0873265168896817, Train acc: 0.8566849816849816\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.086635889626636, Train acc: 0.8573539886039886\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.087336935612381, Train acc: 0.8566873664529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0872493269696433, Train acc: 0.8567747611865258\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0876690083079867, Train acc: 0.8563034188034188\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0872289606755885, Train acc: 0.8568235492577598\n",
      "Val loss: 2.896876573562622, Val acc: 0.87\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0823276083693547, Train acc: 0.8624465811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0829726457595825, Train acc: 0.8617788461538461\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0830343611899265, Train acc: 0.8608440170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0815683440265493, Train acc: 0.8633814102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.082605081745702, Train acc: 0.8626602564102565\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.082263509941916, Train acc: 0.8628027065527065\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0821301794459677, Train acc: 0.8627899877899878\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0829374176314754, Train acc: 0.8619791666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0836095163511983, Train acc: 0.8613485280151947\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0850332105261646, Train acc: 0.8598557692307692\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0851382930425126, Train acc: 0.8596299533799534\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0852358055080784, Train acc: 0.859375\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.085399709666426, Train acc: 0.8591592702169625\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0851668354501363, Train acc: 0.8594131562881563\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0851082695855037, Train acc: 0.8594373219373219\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.085519619540781, Train acc: 0.8589242788461539\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.086020769157141, Train acc: 0.8584244595274007\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.086086098794584, Train acc: 0.8582769468186134\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0862050538013714, Train acc: 0.8581309041835358\n",
      "Val loss: 2.89512300491333, Val acc: 0.876\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.079807284550789, Train acc: 0.8643162393162394\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.086505424263131, Train acc: 0.8569711538461539\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0904488709577467, Train acc: 0.8532763532763533\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.08646642550444, Train acc: 0.8573050213675214\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.085954627216372, Train acc: 0.8581730769230769\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.08568133418037, Train acc: 0.8583956552706553\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0852652152088242, Train acc: 0.8590888278388278\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.084848969283267, Train acc: 0.8595753205128205\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0847096093359836, Train acc: 0.8598053181386515\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.084385648242429, Train acc: 0.8600160256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.085307885716845, Train acc: 0.8591200466200466\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.085788957328878, Train acc: 0.8586850071225072\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0856393656395205, Train acc: 0.8589332675871137\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0856161158309026, Train acc: 0.8588217338217338\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.085529977847368, Train acc: 0.8590099715099715\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.085869803515255, Train acc: 0.8586237980769231\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.086036201276151, Train acc: 0.8584244595274007\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.086021477680261, Train acc: 0.8584104938271605\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.085786312161616, Train acc: 0.8586229194781826\n",
      "Val loss: 2.9049110412597656, Val acc: 0.866\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.080214510616074, Train acc: 0.8632478632478633\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0805519696993704, Train acc: 0.8637820512820513\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.083419380704222, Train acc: 0.8612001424501424\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.084006983754981, Train acc: 0.8603766025641025\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.083357584374583, Train acc: 0.8609508547008548\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.082345637330982, Train acc: 0.8620904558404558\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.083358387370686, Train acc: 0.860958485958486\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0828966466534853, Train acc: 0.8614449786324786\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0829414914929063, Train acc: 0.8613485280151947\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.082891999350654, Train acc: 0.8614583333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.083570423011365, Train acc: 0.8606497668997669\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.083536300903712, Train acc: 0.8606882122507122\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.083932882856962, Train acc: 0.8603303747534516\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.084128892902053, Train acc: 0.8601381257631258\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0839414202589595, Train acc: 0.8603276353276353\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0838690643381867, Train acc: 0.8602597489316239\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.084160747391426, Train acc: 0.8600270236299649\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0842703852558406, Train acc: 0.859909188034188\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0841905621864534, Train acc: 0.8598881016644174\n",
      "Val loss: 2.9016518592834473, Val acc: 0.87\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0787527112879305, Train acc: 0.8637820512820513\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0787793973572235, Train acc: 0.8645833333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0803217038809403, Train acc: 0.8630698005698005\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0808067428760038, Train acc: 0.8630475427350427\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.08105780996828, Train acc: 0.8628739316239317\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.081976278045578, Train acc: 0.8620014245014245\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0812292023310586, Train acc: 0.8628281440781441\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.081982536448373, Train acc: 0.8619791666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0818105513780094, Train acc: 0.8619717473884141\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0821891117299725, Train acc: 0.8615384615384616\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0831754149256887, Train acc: 0.8606740481740481\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0828246605871747, Train acc: 0.8609775641025641\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.083351066675255, Train acc: 0.8604947403024326\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0840724400257162, Train acc: 0.8598137973137974\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.084848114432093, Train acc: 0.8589743589743589\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0846322127259693, Train acc: 0.8590912126068376\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0842805700484326, Train acc: 0.8593042986425339\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0841737830514355, Train acc: 0.8593898385565052\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0844829372066234, Train acc: 0.8591430499325237\n",
      "Val loss: 2.888629674911499, Val acc: 0.882\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.077539085322975, Train acc: 0.8653846153846154\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0772701180898228, Train acc: 0.8667200854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.08131022636707, Train acc: 0.8628917378917379\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.082242974358746, Train acc: 0.8619791666666666\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.081991076673198, Train acc: 0.862232905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0804724438577638, Train acc: 0.86369301994302\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0812231313905727, Train acc: 0.8628663003663004\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0816704870289207, Train acc: 0.8623798076923077\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0812226052750664, Train acc: 0.8629214150047484\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.081286851463155, Train acc: 0.8629273504273505\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.081233982532267, Train acc: 0.8629807692307693\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0816446380567686, Train acc: 0.8624688390313391\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0813301931785015, Train acc: 0.8627547666009204\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0811184798987066, Train acc: 0.8627709096459096\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.081311619927061, Train acc: 0.8625712250712251\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0816711638218317, Train acc: 0.8621627938034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0820763274735095, Train acc: 0.8616610105580694\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.082308474825545, Train acc: 0.8613782051282052\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0820869279240255, Train acc: 0.8616452991452992\n",
      "Val loss: 2.893446683883667, Val acc: 0.876\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0978209106331196, Train acc: 0.8453525641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.091269359119937, Train acc: 0.8524305555555556\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0910502522759287, Train acc: 0.8525641025641025\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.088401386880467, Train acc: 0.8552350427350427\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.086950076135815, Train acc: 0.8563568376068376\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0878835546325076, Train acc: 0.8552795584045584\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0871440882502315, Train acc: 0.8561507936507936\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.085644106197561, Train acc: 0.8576388888888888\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0860064490455055, Train acc: 0.8574014719848053\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.08607681918348, Train acc: 0.8576121794871795\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.086212145050274, Train acc: 0.8575417637917638\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0860337170100958, Train acc: 0.8578169515669516\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0855459478793374, Train acc: 0.8582552596975674\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.084806724200173, Train acc: 0.8590506715506715\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.084298079781383, Train acc: 0.8594729344729345\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.084182963348352, Train acc: 0.8595586271367521\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0835036728596674, Train acc: 0.8602312719959779\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0834579047767297, Train acc: 0.8602207977207977\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0834086969850087, Train acc: 0.8602676563202879\n",
      "Val loss: 2.901042938232422, Val acc: 0.868\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.085861663533072, Train acc: 0.8568376068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0809644032747316, Train acc: 0.8623130341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0856202915523125, Train acc: 0.8579059829059829\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.08369024645569, Train acc: 0.859375\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0814956762851815, Train acc: 0.8621260683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.081022635314539, Train acc: 0.8627136752136753\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0825512356810516, Train acc: 0.8613400488400489\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0819222714401717, Train acc: 0.8623464209401709\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0831640684819743, Train acc: 0.861170465337132\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0826657127111385, Train acc: 0.8615918803418804\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0826766101908296, Train acc: 0.8615238927738927\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0822352901992636, Train acc: 0.8620014245014245\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.081672380113194, Train acc: 0.8626109467455622\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.081658128125909, Train acc: 0.8626373626373627\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0820889186315727, Train acc: 0.8623931623931624\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.082068756286405, Train acc: 0.8623631143162394\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.08248108184895, Train acc: 0.8618652589240825\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.082655870393697, Train acc: 0.861764007597341\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0827609566595444, Train acc: 0.8616874718848403\n",
      "Val loss: 2.893099546432495, Val acc: 0.874\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.076190314741216, Train acc: 0.8677884615384616\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.086735926122747, Train acc: 0.8565705128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0854017072253757, Train acc: 0.8582621082621082\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0825061976400194, Train acc: 0.8615785256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0828695831135806, Train acc: 0.860897435897436\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0825186663883026, Train acc: 0.8612001424501424\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0829347530709663, Train acc: 0.8609203296703297\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0821662198784003, Train acc: 0.8616786858974359\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.082996779017978, Train acc: 0.8609924026590693\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.083548847732381, Train acc: 0.8603899572649573\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0838184560466018, Train acc: 0.8600427350427351\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.083402630610344, Train acc: 0.8604433760683761\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0825589310566426, Train acc: 0.8612754766600921\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0824606127500243, Train acc: 0.8614545177045178\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0816541186764708, Train acc: 0.8623397435897436\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.082389575803382, Train acc: 0.8615618322649573\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0820097473771684, Train acc: 0.8619281045751634\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0816682851099446, Train acc: 0.8622833570750238\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0816836834919115, Train acc: 0.8622216599190283\n",
      "Val loss: 2.897634506225586, Val acc: 0.872\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.082594716650808, Train acc: 0.8627136752136753\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.081826085718269, Train acc: 0.8621794871794872\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0798971917894153, Train acc: 0.86369301994302\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0808890640226183, Train acc: 0.8628472222222222\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.08065293283544, Train acc: 0.8626602564102565\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0802334832330036, Train acc: 0.8631143162393162\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0808404486112395, Train acc: 0.862675518925519\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0813716402930074, Train acc: 0.8621127136752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0811426965599384, Train acc: 0.862298195631529\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0815816222093044, Train acc: 0.8618856837606838\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0815218344342368, Train acc: 0.8620095182595182\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0822550942245712, Train acc: 0.8613782051282052\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0819207071709367, Train acc: 0.8616658448389217\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.082104922141493, Train acc: 0.861359126984127\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.081983590193963, Train acc: 0.8613960113960114\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.081454086061726, Train acc: 0.8619290865384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.081041357815715, Train acc: 0.8622737556561086\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.08102101466821, Train acc: 0.8622388414055081\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.081200014617172, Train acc: 0.8620810841205578\n",
      "Val loss: 2.8957605361938477, Val acc: 0.876\n",
      "Epoch 64/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0824987867958527, Train acc: 0.8605769230769231\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0797559445739813, Train acc: 0.8640491452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0772259731238383, Train acc: 0.8667200854700855\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.080830431646771, Train acc: 0.8633146367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0818308035532636, Train acc: 0.8621260683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0820309178442016, Train acc: 0.8618233618233618\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0813781215157703, Train acc: 0.8625228937728938\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0813545243352904, Train acc: 0.862479967948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0802754488652475, Train acc: 0.8635149572649573\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.08014011362679, Train acc: 0.8636485042735043\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.080102665533645, Train acc: 0.8636606449106449\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.080326799823348, Train acc: 0.8633368945868946\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0809860522616934, Train acc: 0.8625287639710717\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0813143310092745, Train acc: 0.8622176434676435\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0815982984341788, Train acc: 0.8618055555555556\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0816533723448076, Train acc: 0.8617955395299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.081137510925876, Train acc: 0.8623523127199598\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0807374789391035, Train acc: 0.862832383665717\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.080418587267211, Train acc: 0.8631213450292398\n",
      "Val loss: 2.9004125595092773, Val acc: 0.87\n",
      "Epoch 65/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0759713486728506, Train acc: 0.8685897435897436\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0756149261425705, Train acc: 0.8685897435897436\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0778179433610706, Train acc: 0.8661858974358975\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0787165641275225, Train acc: 0.8649172008547008\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0788049734555756, Train acc: 0.8647435897435898\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.079070894976287, Train acc: 0.8644052706552706\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0800888059485674, Train acc: 0.863743894993895\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.080836196995189, Train acc: 0.8630809294871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.080819012885533, Train acc: 0.8630104463437797\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0800897901893682, Train acc: 0.8637019230769231\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0812230356552845, Train acc: 0.8625679875679876\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0815107581282613, Train acc: 0.8622685185185185\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0807217436350935, Train acc: 0.8630424063116371\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.079904855825962, Train acc: 0.863877442002442\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0794776102756165, Train acc: 0.8643162393162394\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.07926645836769, Train acc: 0.8645165598290598\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0791613045141526, Train acc: 0.8648032931121167\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0792074545949046, Train acc: 0.864761396011396\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0796082935727935, Train acc: 0.8644005847953217\n",
      "Val loss: 2.8954648971557617, Val acc: 0.878\n",
      "Epoch 66/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0750793677109938, Train acc: 0.8685897435897436\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.07846725700248, Train acc: 0.8648504273504274\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0800014863326677, Train acc: 0.86369301994302\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0809758570459156, Train acc: 0.8625133547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.078196772550925, Train acc: 0.8655982905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.077869798383142, Train acc: 0.8659188034188035\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0778532534728558, Train acc: 0.865995115995116\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0781348161717768, Train acc: 0.8656850961538461\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0775735514116422, Train acc: 0.8662452516619183\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0772930186018983, Train acc: 0.8665598290598291\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0769205902006243, Train acc: 0.8669871794871795\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0777736208547553, Train acc: 0.8660078347578347\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0783803720837906, Train acc: 0.865302432610125\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0781487930970894, Train acc: 0.8655753968253969\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0783530062759703, Train acc: 0.8654380341880342\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0786504245594015, Train acc: 0.8651509081196581\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0792374475449398, Train acc: 0.8645990447461036\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0795953640571008, Train acc: 0.8642123694207028\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.079676163448603, Train acc: 0.8641756635177688\n",
      "Val loss: 2.9024710655212402, Val acc: 0.868\n",
      "Epoch 67/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.086756800993895, Train acc: 0.8576388888888888\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0850960051911507, Train acc: 0.8583066239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.080253796020464, Train acc: 0.8632478632478633\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.078911883199317, Train acc: 0.8643830128205128\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0782562598204, Train acc: 0.8651175213675214\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.077662403257484, Train acc: 0.8660523504273504\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0769302521869815, Train acc: 0.8667582417582418\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0770484778361444, Train acc: 0.8669871794871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0768117113330766, Train acc: 0.8672542735042735\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.077088528820592, Train acc: 0.8670405982905983\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0775041375497376, Train acc: 0.866501554001554\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0778568247784235, Train acc: 0.8660746082621082\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0791683701760357, Train acc: 0.8647476988823143\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.078756929753901, Train acc: 0.8652129120879121\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0791350740992445, Train acc: 0.864957264957265\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0790189318040495, Train acc: 0.8650006677350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0790769573190575, Train acc: 0.8648975615887381\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.078805275112815, Train acc: 0.8651471984805318\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0792907648026486, Train acc: 0.8645833333333334\n",
      "Val loss: 2.901931047439575, Val acc: 0.87\n",
      "Epoch 68/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0723796400249515, Train acc: 0.8728632478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.07311253160493, Train acc: 0.8720619658119658\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0725239668136988, Train acc: 0.8727742165242165\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.071613682386203, Train acc: 0.8729300213675214\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0724683017812224, Train acc: 0.872008547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0724483927090964, Train acc: 0.8721064814814815\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.073205872624203, Train acc: 0.871031746031746\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0739355261763954, Train acc: 0.8703592414529915\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0737906146479697, Train acc: 0.8704594017094017\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0740138734507765, Train acc: 0.8700587606837606\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.074525035807766, Train acc: 0.8694638694638694\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.07456300375808, Train acc: 0.8694800569800569\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0746462496388203, Train acc: 0.8694732084155161\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.075209633261875, Train acc: 0.8688377594627594\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.075850612893064, Train acc: 0.8681267806267806\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0764462979048743, Train acc: 0.8674712873931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0762442380954056, Train acc: 0.867615635997989\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0763951071873237, Train acc: 0.8674620132953467\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.076444641352236, Train acc: 0.8674510796221323\n",
      "Val loss: 2.8860301971435547, Val acc: 0.888\n",
      "Epoch 69/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.071271244277302, Train acc: 0.874732905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0767139308472986, Train acc: 0.8681891025641025\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.079376528066108, Train acc: 0.8648504273504274\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0759953301177068, Train acc: 0.8682558760683761\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.075658855886541, Train acc: 0.8685897435897436\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.076910429191046, Train acc: 0.8671652421652422\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0773685642214486, Train acc: 0.8663766788766789\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.077649671043086, Train acc: 0.8661525106837606\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.07658067195617, Train acc: 0.8671355650522318\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0765771521462333, Train acc: 0.8672008547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.076418285232787, Train acc: 0.8674242424242424\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0769458841903936, Train acc: 0.866920405982906\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0767874553907397, Train acc: 0.86706936226167\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0764868511967314, Train acc: 0.8674068986568987\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0761657366385826, Train acc: 0.8676638176638176\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.075923436663599, Train acc: 0.8678886217948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0755664549502013, Train acc: 0.8683383609854198\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0753445818892913, Train acc: 0.8685452279202279\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.076025123967279, Train acc: 0.867886864597391\n",
      "Val loss: 2.8968091011047363, Val acc: 0.872\n",
      "Epoch 70/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.076837886093009, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0764983082428956, Train acc: 0.8672542735042735\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0741334620364373, Train acc: 0.8695690883190883\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.070973973498385, Train acc: 0.8726629273504274\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.071965268942026, Train acc: 0.872008547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.074431584431575, Train acc: 0.869613603988604\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.075842048076774, Train acc: 0.8679410866910867\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0761978312944755, Train acc: 0.8675213675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.076949643500057, Train acc: 0.8666310541310541\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0772731946064877, Train acc: 0.8663461538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0772579551761985, Train acc: 0.8662101787101787\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.076668398757266, Train acc: 0.8668981481481481\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.076545661096115, Train acc: 0.8671309993425378\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.077063583192371, Train acc: 0.8665483821733821\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0767144571342357, Train acc: 0.8668981481481481\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0765777023302183, Train acc: 0.8671374198717948\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0762651469492925, Train acc: 0.8674428104575164\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0763907259346075, Train acc: 0.8673284662867996\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.076283979190667, Train acc: 0.8674370220422852\n",
      "Val loss: 2.8886332511901855, Val acc: 0.89\n",
      "Epoch 71/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0799402324562397, Train acc: 0.8624465811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0783171679219627, Train acc: 0.8637820512820513\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0787129745184525, Train acc: 0.8638710826210826\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0778312586311602, Train acc: 0.8651842948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0754331211758474, Train acc: 0.8678418803418804\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0751953114811172, Train acc: 0.8685007122507122\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0759671600455913, Train acc: 0.8677503052503053\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.075354788802628, Train acc: 0.8681557158119658\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.076311677269786, Train acc: 0.8673433048433048\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0757559669323458, Train acc: 0.8678151709401709\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.075909950423815, Train acc: 0.8676670551670551\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.075826757484012, Train acc: 0.8678329772079773\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.075486848183481, Train acc: 0.8681788297172912\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.075362947106507, Train acc: 0.8683608058608059\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0756348485620615, Train acc: 0.868091168091168\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.076315959931439, Train acc: 0.8673878205128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.076546185516484, Train acc: 0.8670814479638009\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0761584488414973, Train acc: 0.8675213675213675\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0762586621513144, Train acc: 0.8674089068825911\n",
      "Val loss: 2.8974382877349854, Val acc: 0.872\n",
      "Epoch 72/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0743086103700166, Train acc: 0.8683226495726496\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.071256932539818, Train acc: 0.8723290598290598\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0738970788455755, Train acc: 0.8700142450142451\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0736129709288607, Train acc: 0.8699919871794872\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.074314311834482, Train acc: 0.8695512820512821\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0740162095113357, Train acc: 0.8697026353276354\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0731948117778995, Train acc: 0.8706120268620269\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0734418663713665, Train acc: 0.870292467948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0755060420982745, Train acc: 0.8681742640075973\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.075554454937959, Train acc: 0.8680555555555556\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.075738845912634, Train acc: 0.8679341491841492\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0752517410323152, Train acc: 0.8685007122507122\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0749692421536317, Train acc: 0.8687746548323472\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0749001086704313, Train acc: 0.8688377594627594\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.075073832392353, Train acc: 0.8686431623931624\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0750371008856683, Train acc: 0.8686231303418803\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.075096005168376, Train acc: 0.8686211664152841\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.075385127312098, Train acc: 0.8683226495726496\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.075019240701086, Train acc: 0.868688146648673\n",
      "Val loss: 2.889681816101074, Val acc: 0.884\n",
      "Epoch 73/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0749462109345655, Train acc: 0.8680555555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0771587093671164, Train acc: 0.8656517094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0768853898061987, Train acc: 0.8656517094017094\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.075977301495707, Train acc: 0.8669871794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.076380067809015, Train acc: 0.8668803418803419\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.075176601566141, Train acc: 0.8683226495726496\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0746132908286627, Train acc: 0.8689713064713065\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0750080201870356, Train acc: 0.8683560363247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0746931540660367, Train acc: 0.8687084520417854\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0750162249956374, Train acc: 0.8683760683760684\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0752192224378074, Train acc: 0.8681283993783994\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0757929551975, Train acc: 0.8676103988603988\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.075369605181015, Train acc: 0.8680144641683103\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0753577824622864, Train acc: 0.8681509462759462\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0755476059057774, Train acc: 0.8680021367521368\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0756237222216067, Train acc: 0.8679053151709402\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.075730523732273, Train acc: 0.8677884615384616\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.075458084076558, Train acc: 0.8679962013295347\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.075415692080585, Train acc: 0.8680133828160144\n",
      "Val loss: 2.8967697620391846, Val acc: 0.874\n",
      "Epoch 74/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0726985921207657, Train acc: 0.8696581196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.07420982611485, Train acc: 0.8679220085470085\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.079120259678941, Train acc: 0.8637820512820513\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.077928562449594, Train acc: 0.8651175213675214\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0765210137408006, Train acc: 0.8668269230769231\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0752591456782783, Train acc: 0.8681891025641025\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.075998866368854, Train acc: 0.8675976800976801\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0745772549993973, Train acc: 0.8690237713675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.075786947411576, Train acc: 0.8679071699905033\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0755542654257555, Train acc: 0.8681623931623932\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0755214960702926, Train acc: 0.8681526806526807\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.075773158219465, Train acc: 0.8679220085470085\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0756770406092255, Train acc: 0.8680144641683103\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.076029222381275, Train acc: 0.8677693833943834\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0757367820142, Train acc: 0.8680733618233618\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0754880954821906, Train acc: 0.8682558760683761\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.075425043964338, Train acc: 0.8684326294620413\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.075740231464618, Train acc: 0.8681297483380817\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0755404934816624, Train acc: 0.8683507647323436\n",
      "Val loss: 2.8910744190216064, Val acc: 0.882\n",
      "Epoch 75/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0740728928492618, Train acc: 0.8699252136752137\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0755676683197675, Train acc: 0.8677884615384616\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0736294475376096, Train acc: 0.8699252136752137\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.07262132030267, Train acc: 0.8704594017094017\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0728833561269644, Train acc: 0.8704059829059829\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0734686800557323, Train acc: 0.8699252136752137\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0738420055608318, Train acc: 0.8695436507936508\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.073206487883869, Train acc: 0.8702256944444444\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0730797232386053, Train acc: 0.8704594017094017\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0728920120459335, Train acc: 0.8705128205128205\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.073653024022859, Train acc: 0.8697552447552448\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0740758638266485, Train acc: 0.8692574786324786\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.073597788262101, Train acc: 0.8697608481262328\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0737699384508845, Train acc: 0.8696390415140415\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0735608225194815, Train acc: 0.8699252136752137\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0729533822363257, Train acc: 0.8704594017094017\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0733237168847647, Train acc: 0.8700194821518351\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0732032110435217, Train acc: 0.8702219848053181\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0731032939306853, Train acc: 0.8704031713900134\n",
      "Val loss: 2.8907177448272705, Val acc: 0.882\n",
      "Epoch 76/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.078698159283043, Train acc: 0.8627136752136753\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.081258908805684, Train acc: 0.8615117521367521\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.077389016110673, Train acc: 0.8654736467236467\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0756863449883256, Train acc: 0.8675213675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0771559859952355, Train acc: 0.8663995726495727\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0754487466947986, Train acc: 0.8682781339031339\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.07538090986501, Train acc: 0.8683608058608059\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.075020341282217, Train acc: 0.8687232905982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0762705111209256, Train acc: 0.8674620132953467\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0765210170012254, Train acc: 0.8672809829059829\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0760725945025893, Train acc: 0.8676913364413364\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.075613982741989, Train acc: 0.8680555555555556\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.075414038268772, Train acc: 0.8682610124917817\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.074864531975235, Train acc: 0.868780525030525\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.074493624817612, Train acc: 0.8689992877492877\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0744398298681292, Train acc: 0.8691239316239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.074080188110164, Train acc: 0.8694852941176471\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0741553898663714, Train acc: 0.8693316714150048\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.074465496819994, Train acc: 0.8690677013045434\n",
      "Val loss: 2.8893282413482666, Val acc: 0.882\n",
      "Epoch 77/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0766942378802176, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0738623223753057, Train acc: 0.8689903846153846\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0723104826745145, Train acc: 0.8711716524216524\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.070144847417489, Train acc: 0.8734642094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.071891253014915, Train acc: 0.871741452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0700897953109525, Train acc: 0.8737090455840456\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0722751311766796, Train acc: 0.8712606837606838\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0725247451128106, Train acc: 0.8709935897435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0727362965586518, Train acc: 0.8708155270655271\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0731679754379466, Train acc: 0.8702457264957265\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.073032125819072, Train acc: 0.8702894327894328\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0737100916880147, Train acc: 0.8694577991452992\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.07442048472217, Train acc: 0.8686513806706114\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.074300178168603, Train acc: 0.8687996031746031\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0736290903852197, Train acc: 0.8695512820512821\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0734122443912377, Train acc: 0.8697749732905983\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0737831813117977, Train acc: 0.8694381598793364\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.073512695850017, Train acc: 0.8697471509971509\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0735566490574886, Train acc: 0.8697705802968961\n",
      "Val loss: 2.8869688510894775, Val acc: 0.888\n",
      "Epoch 78/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0798676390933175, Train acc: 0.8611111111111112\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.072611207126552, Train acc: 0.8701923076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.070035537083944, Train acc: 0.8729522792022792\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.068597458111934, Train acc: 0.874732905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.068644946660751, Train acc: 0.8746794871794872\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.069942039302272, Train acc: 0.8733084045584045\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.070588191495856, Train acc: 0.8725961538461539\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0709836242290645, Train acc: 0.8724292200854701\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.070583431922246, Train acc: 0.8729819563152896\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.070609972619603, Train acc: 0.8729700854700855\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.070931829124249, Train acc: 0.8726204351204351\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0708524642506894, Train acc: 0.8727074430199431\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.070774094828017, Train acc: 0.872904339250493\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.07121421172942, Train acc: 0.872481684981685\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0712039903358175, Train acc: 0.8725783475783476\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0709033246733184, Train acc: 0.8728131677350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0705491243264613, Train acc: 0.8732088989441931\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0707794215038406, Train acc: 0.8729671177587844\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0710589016980006, Train acc: 0.872624269005848\n",
      "Val loss: 2.8912994861602783, Val acc: 0.882\n",
      "Epoch 79/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.070267043562017, Train acc: 0.8736645299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.068193882958502, Train acc: 0.8751335470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0722096686349634, Train acc: 0.8709045584045584\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.070108441460846, Train acc: 0.8727297008547008\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.069346893343151, Train acc: 0.8735576923076923\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0717470411561494, Train acc: 0.8712606837606838\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.071207481426197, Train acc: 0.8721764346764347\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0712121997633552, Train acc: 0.8722288995726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0711692337296967, Train acc: 0.8723884140550807\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0721535611356425, Train acc: 0.8713408119658119\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.071875590359229, Train acc: 0.8716249028749029\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0721703550414823, Train acc: 0.8712606837606838\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0728238818678397, Train acc: 0.8705621301775148\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0726166160843165, Train acc: 0.8708028083028083\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.072748038164231, Train acc: 0.8707264957264957\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0727352496777844, Train acc: 0.8708600427350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0727979317569685, Train acc: 0.8707579185520362\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0722479716099, Train acc: 0.8712903608736942\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0718413859511946, Train acc: 0.8717245838956366\n",
      "Val loss: 2.897467613220215, Val acc: 0.874\n",
      "Epoch 80/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0658999375807934, Train acc: 0.8776709401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0692709944187064, Train acc: 0.8739316239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.06951227860573, Train acc: 0.8736645299145299\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.070882386631436, Train acc: 0.8724626068376068\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.07123084740761, Train acc: 0.8719551282051282\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0693316043611945, Train acc: 0.8739761396011396\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0694312868828595, Train acc: 0.8740079365079365\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.069725904836614, Train acc: 0.8736979166666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.070487163911405, Train acc: 0.8726851851851852\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.070378248915713, Train acc: 0.8728365384615384\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.070152567538904, Train acc: 0.8729360916860917\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0707232786549463, Train acc: 0.8724180911680912\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0711359257450392, Train acc: 0.8720414201183432\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.071798979726612, Train acc: 0.8713942307692307\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0720172642982244, Train acc: 0.8711716524216524\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.072002928608503, Train acc: 0.8713107638888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.07210794868872, Train acc: 0.8713078179989945\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.072298424366193, Train acc: 0.8711271367521367\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0724090492730736, Train acc: 0.8710498200629779\n",
      "Val loss: 2.901657819747925, Val acc: 0.87\n",
      "Epoch 81/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.080635437598595, Train acc: 0.8645833333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0763647907819505, Train acc: 0.8684561965811965\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0723321400476657, Train acc: 0.8726851851851852\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0709478228520126, Train acc: 0.8735309829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0720501153897017, Train acc: 0.872008547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.072230034913772, Train acc: 0.8714832621082621\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0712729692459106, Train acc: 0.8724053724053724\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.07072847814132, Train acc: 0.8729634081196581\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.069460021351364, Train acc: 0.874198717948718\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0686944836225267, Train acc: 0.8749198717948717\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0695252616737085, Train acc: 0.8740044677544677\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0696432775581663, Train acc: 0.8738648504273504\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0696279873274253, Train acc: 0.8738494411571335\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.069700228789496, Train acc: 0.8736645299145299\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.069900388255758, Train acc: 0.8735398860398861\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0705513659960184, Train acc: 0.8727964743589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.071020950738841, Train acc: 0.8722819255907491\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.070690502638151, Train acc: 0.8726703466286799\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.070764508431847, Train acc: 0.8725820962663068\n",
      "Val loss: 2.8871829509735107, Val acc: 0.886\n",
      "Epoch 82/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0659039377147317, Train acc: 0.8771367521367521\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0711703213871036, Train acc: 0.8719284188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0678846978733683, Train acc: 0.8751780626780626\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.069966260948752, Train acc: 0.8730635683760684\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0680443327651066, Train acc: 0.8750534188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.069038215525809, Train acc: 0.8739761396011396\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.070206002729134, Train acc: 0.8728632478632479\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0706896394745917, Train acc: 0.8722288995726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0708504919765796, Train acc: 0.8720619658119658\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.070239424501729, Train acc: 0.8726228632478632\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.070228431052539, Train acc: 0.872693278943279\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0707951400354716, Train acc: 0.8723290598290598\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0714203195302288, Train acc: 0.8718770545693623\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.071383826898568, Train acc: 0.8718330280830281\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.07136545976003, Train acc: 0.872008547008547\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.071411868827975, Train acc: 0.8719951923076923\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0710282829791953, Train acc: 0.8724547511312217\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0711013020386835, Train acc: 0.8724032526115859\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.070630535691159, Train acc: 0.8728210751237067\n",
      "Val loss: 2.8912434577941895, Val acc: 0.884\n",
      "Epoch 83/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.065901240731916, Train acc: 0.8790064102564102\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0641292231714625, Train acc: 0.8800747863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.067238839942845, Train acc: 0.8770477207977208\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0679385881138663, Train acc: 0.8762686965811965\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.067612967124352, Train acc: 0.8765491452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0688650234472377, Train acc: 0.875267094017094\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0698791637525455, Train acc: 0.8742750305250305\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0699915784037013, Train acc: 0.8741319444444444\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0695591595551, Train acc: 0.8748219373219374\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0688645080623465, Train acc: 0.8756143162393163\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.06884396085465, Train acc: 0.8755827505827506\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0688511628201205, Train acc: 0.8755119301994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0695077715384027, Train acc: 0.8748767258382643\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.068653406386556, Train acc: 0.8758585164835165\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0686871093222896, Train acc: 0.875730056980057\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.068253531224198, Train acc: 0.8761351495726496\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0683760776658824, Train acc: 0.8759898190045249\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0690116261371747, Train acc: 0.8752967711301045\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.069395221840193, Train acc: 0.8749156545209177\n",
      "Val loss: 2.89312744140625, Val acc: 0.874\n",
      "Epoch 84/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.062527439533136, Train acc: 0.8803418803418803\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.064882590220525, Train acc: 0.8786057692307693\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0655757813711793, Train acc: 0.8773148148148148\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0656251665363965, Train acc: 0.8776709401709402\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0666066334797786, Train acc: 0.8767628205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0682897513408607, Train acc: 0.875534188034188\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0686736972922954, Train acc: 0.8751907814407814\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.06858440889762, Train acc: 0.8752337072649573\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.068777409147786, Train acc: 0.8749406457739791\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0691458812126746, Train acc: 0.8744123931623932\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0686693206348434, Train acc: 0.8749514374514374\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0691402705646307, Train acc: 0.8744435541310541\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0687797052306927, Train acc: 0.8746712689020382\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0685719440853787, Train acc: 0.8748855311355311\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.068583006940336, Train acc: 0.8748931623931624\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.06938565044831, Train acc: 0.8741319444444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0693857081875002, Train acc: 0.874198717948718\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.069127927648376, Train acc: 0.8744954890788225\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0691676774756416, Train acc: 0.8744517543859649\n",
      "Val loss: 2.8990981578826904, Val acc: 0.87\n",
      "Epoch 85/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.070902109146118, Train acc: 0.8720619658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0709655325636906, Train acc: 0.8719284188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0682957658061274, Train acc: 0.8748219373219374\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0665545089122577, Train acc: 0.8764022435897436\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0659985770527114, Train acc: 0.877457264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0658947360142004, Train acc: 0.8775373931623932\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0688923997611206, Train acc: 0.8741605616605617\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0698429585522056, Train acc: 0.8731637286324786\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0701529876685436, Train acc: 0.8728632478632479\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.06916889441319, Train acc: 0.8739049145299145\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0687612761984338, Train acc: 0.8742229992229992\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0687248691534386, Train acc: 0.8743545227920227\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0690968035867856, Train acc: 0.8739521696252466\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0691006984902827, Train acc: 0.8739888583638583\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.069021670227377, Train acc: 0.8741631054131054\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.069580678883781, Train acc: 0.8736144497863247\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0691912385792874, Train acc: 0.8740573152337858\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0691014606388887, Train acc: 0.8741542022792023\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0689981019126904, Train acc: 0.8742408906882592\n",
      "Val loss: 2.893209934234619, Val acc: 0.874\n",
      "Epoch 86/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0669830008449717, Train acc: 0.8774038461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0692011647754245, Train acc: 0.8743322649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.06958175894202, Train acc: 0.8739316239316239\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.069218492660767, Train acc: 0.8742654914529915\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0687705979387983, Train acc: 0.875\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0679034697703824, Train acc: 0.8759793447293447\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.067430633473891, Train acc: 0.8765644078144078\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0677017723647957, Train acc: 0.8760683760683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.067366066481653, Train acc: 0.8762761158594492\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0683853939048245, Train acc: 0.8750801282051283\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.068485527512818, Train acc: 0.8749271561771562\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.068447522543095, Train acc: 0.8748664529914529\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0684784687639146, Train acc: 0.8749383629191322\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.068838538413228, Train acc: 0.8745421245421245\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0686303659721657, Train acc: 0.874732905982906\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0685759622317095, Train acc: 0.874732905982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0687827908135707, Train acc: 0.8744815233785822\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0692091227030596, Train acc: 0.8739464624881291\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.06937729475791, Train acc: 0.8737488753936122\n",
      "Val loss: 2.8835229873657227, Val acc: 0.89\n",
      "Epoch 87/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.068157968358097, Train acc: 0.874198717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0655270947350397, Train acc: 0.8784722222222222\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.065112430485565, Train acc: 0.8787393162393162\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0657547623173804, Train acc: 0.8778712606837606\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.067597750517038, Train acc: 0.8761752136752137\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0684624597897217, Train acc: 0.875267094017094\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.069075018931658, Train acc: 0.8746565934065934\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.067689239087268, Train acc: 0.8760349893162394\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0680251307184085, Train acc: 0.8755638651471985\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.068341204040071, Train acc: 0.8753205128205128\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0678936586698637, Train acc: 0.8757770007770008\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0676578846242695, Train acc: 0.8760906339031339\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0678269147559423, Train acc: 0.8759861932938856\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.068611704022311, Train acc: 0.8751526251526252\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.068771281812945, Train acc: 0.874928774928775\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0691024584647937, Train acc: 0.8746160523504274\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0688792766189383, Train acc: 0.8748585972850679\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.068859674369055, Train acc: 0.8748516144349477\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0689764649171956, Train acc: 0.8748594242015295\n",
      "Val loss: 2.8831050395965576, Val acc: 0.894\n",
      "Epoch 88/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.065251426819043, Train acc: 0.8768696581196581\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.066561344342354, Train acc: 0.8762019230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0667122520952144, Train acc: 0.8762464387464387\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0676012721836057, Train acc: 0.8754006410256411\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0678608413435455, Train acc: 0.8753205128205128\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0680717353807214, Train acc: 0.875267094017094\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0673850527322046, Train acc: 0.875801282051282\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.068001308120214, Train acc: 0.8753672542735043\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0681221593818773, Train acc: 0.8750296771130105\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.068548885675577, Train acc: 0.8746527777777777\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.06822561643433, Train acc: 0.8750971250971251\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0677167164464283, Train acc: 0.8755787037037037\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.067487240151149, Train acc: 0.8759040105193951\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.067417722190838, Train acc: 0.8759539072039072\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0677785414236562, Train acc: 0.8755876068376068\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0696033096084228, Train acc: 0.8737646901709402\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.06946470403024, Train acc: 0.8740101809954751\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.068658666846193, Train acc: 0.8748219373219374\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.068363809767898, Train acc: 0.8751265182186235\n",
      "Val loss: 2.9027416706085205, Val acc: 0.87\n",
      "Epoch 89/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.06866138918787, Train acc: 0.875801282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0709298803256107, Train acc: 0.8732638888888888\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.07012753778713, Train acc: 0.8742877492877493\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0686248617294507, Train acc: 0.8757345085470085\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.067768339214162, Train acc: 0.8763888888888889\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0695718171929363, Train acc: 0.8742432336182336\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0703459059653557, Train acc: 0.8733211233211233\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0700792504681482, Train acc: 0.8734642094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.070096790054698, Train acc: 0.8735458214624882\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0695155188568637, Train acc: 0.8740117521367521\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.069587989286943, Train acc: 0.8739316239316239\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0688395860188367, Train acc: 0.8746883903133903\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.06843690947433, Train acc: 0.8749794543063774\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0686732270778756, Train acc: 0.874732905982906\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.068566553680985, Train acc: 0.874804131054131\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.068425528259359, Train acc: 0.8750166933760684\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0682890930027145, Train acc: 0.8752199597787833\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0682950596071263, Train acc: 0.875267094017094\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.06804962271889, Train acc: 0.8754498425551057\n",
      "Val loss: 2.8864946365356445, Val acc: 0.886\n",
      "Epoch 90/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.071819215758234, Train acc: 0.8731303418803419\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0703531473110885, Train acc: 0.874198717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0712791365436, Train acc: 0.8732193732193733\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.068602729811628, Train acc: 0.8756677350427351\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0691343708934946, Train acc: 0.8748397435897436\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0696311686453317, Train acc: 0.874198717948718\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0702498433645244, Train acc: 0.8735882173382173\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0674167005424824, Train acc: 0.8763020833333334\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.067820098212189, Train acc: 0.8760980531813866\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.067809577579172, Train acc: 0.8760416666666667\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0675062397791604, Train acc: 0.8760926573426573\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0672966302120446, Train acc: 0.8762909544159544\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.066859114726541, Train acc: 0.8767258382642998\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0669663072505715, Train acc: 0.8765262515262515\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.067000975771847, Train acc: 0.8764245014245015\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0674029959954767, Train acc: 0.8760182959401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0670506755810636, Train acc: 0.8763197586726998\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0669816816181426, Train acc: 0.8764393399810066\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0671374477141087, Train acc: 0.8762651821862348\n",
      "Val loss: 2.8952016830444336, Val acc: 0.878\n",
      "Epoch 91/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.065490279442225, Train acc: 0.8771367521367521\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0652483080187416, Train acc: 0.8783386752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.067989628878754, Train acc: 0.8748219373219374\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0677616810187316, Train acc: 0.8750667735042735\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0679504504570594, Train acc: 0.8750534188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0670984835706205, Train acc: 0.8758457977207977\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.066822637714018, Train acc: 0.8761828449328449\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.066945326761303, Train acc: 0.8763354700854701\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0671122976404535, Train acc: 0.876127730294397\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.06710245517584, Train acc: 0.8762019230769231\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.066298989241866, Train acc: 0.8770881895881896\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.065745485559148, Train acc: 0.8776486823361823\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.065886448042554, Train acc: 0.8775065746219592\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.066572447472294, Train acc: 0.8768315018315018\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0668201946465037, Train acc: 0.8766915954415955\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.066130571385734, Train acc: 0.8773370726495726\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0660671868355447, Train acc: 0.8773881347410759\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0665384841780376, Train acc: 0.8768844966761633\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.066948449670652, Train acc: 0.8765041610436347\n",
      "Val loss: 2.8872246742248535, Val acc: 0.888\n",
      "Epoch 92/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0654595224266377, Train acc: 0.8779380341880342\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.063150059973073, Train acc: 0.8800747863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.064040935956515, Train acc: 0.8792735042735043\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.065540269908742, Train acc: 0.8776041666666666\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0664287373550936, Train acc: 0.8768162393162393\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0675477860999583, Train acc: 0.875534188034188\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0683981130702445, Train acc: 0.874732905982906\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.067552366445207, Train acc: 0.8753672542735043\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0685799558844207, Train acc: 0.8743767806267806\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0675379850925544, Train acc: 0.8754807692307692\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0671037778246744, Train acc: 0.875971250971251\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0665747246511303, Train acc: 0.8764245014245015\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0665772071721755, Train acc: 0.8764792899408284\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.067127845808409, Train acc: 0.8759729853479854\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.06710239686178, Train acc: 0.8760149572649573\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.067255957513793, Train acc: 0.8759348290598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0669236592517657, Train acc: 0.8762254901960784\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0669399074792634, Train acc: 0.8762761158594492\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.066756945604469, Train acc: 0.8765041610436347\n",
      "Val loss: 2.890963315963745, Val acc: 0.882\n",
      "Epoch 93/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0682175353042083, Train acc: 0.874732905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0657376376991596, Train acc: 0.8776709401709402\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0652995418619224, Train acc: 0.8782051282051282\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0645840573005185, Train acc: 0.8790731837606838\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.064787814556024, Train acc: 0.878792735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.064030632673845, Train acc: 0.8794515669515669\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0644104144512077, Train acc: 0.8790827228327228\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0633450347898354, Train acc: 0.8800747863247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0638621849665046, Train acc: 0.8795999525166192\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0640967143906486, Train acc: 0.8793536324786325\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.063950161278109, Train acc: 0.879516317016317\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0638326886202876, Train acc: 0.8795405982905983\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.063629179311222, Train acc: 0.8797871466140696\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0639433161388534, Train acc: 0.8794452075702076\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.064287740484602, Train acc: 0.8790776353276353\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0641583220826254, Train acc: 0.8792234241452992\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.063906341056958, Train acc: 0.8794463298139769\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.064164316269294, Train acc: 0.8792289886039886\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0641892704403837, Train acc: 0.8791469860548808\n",
      "Val loss: 2.888148784637451, Val acc: 0.888\n",
      "Epoch 94/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.064625159288064, Train acc: 0.8787393162393162\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.068148152441041, Train acc: 0.8745993589743589\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0659982363382974, Train acc: 0.8766915954415955\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.065081130235623, Train acc: 0.8778712606837606\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0641994246050843, Train acc: 0.8788461538461538\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.065814747090353, Train acc: 0.8770477207977208\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.065154947204031, Train acc: 0.8778235653235653\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0659182778535743, Train acc: 0.8771367521367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0643884822061147, Train acc: 0.8786502849002849\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.065282194940453, Train acc: 0.8776976495726496\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0651070329692813, Train acc: 0.8779865967365967\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0647075591433763, Train acc: 0.8784499643874644\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.065031857415299, Train acc: 0.8782051282051282\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0647626022133925, Train acc: 0.8783768315018315\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.06457692435664, Train acc: 0.8786680911680912\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.064636999597916, Train acc: 0.878639155982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0649477146929267, Train acc: 0.8783465309200603\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0644128817891576, Train acc: 0.8788135090218424\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.064578743866849, Train acc: 0.8786127980206928\n",
      "Val loss: 2.8892159461975098, Val acc: 0.882\n",
      "Epoch 95/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.062650798732399, Train acc: 0.8819444444444444\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.065147763134068, Train acc: 0.8792735042735043\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.068032124103644, Train acc: 0.8757122507122507\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0655299677298617, Train acc: 0.8784054487179487\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0637127238461095, Train acc: 0.8803952991452991\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0639394360390146, Train acc: 0.8801638176638177\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.064069326689537, Train acc: 0.8798458485958486\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0639219966709104, Train acc: 0.8801415598290598\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.063995309817938, Train acc: 0.8801341405508072\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0640913215457886, Train acc: 0.8799679487179487\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0638533999221496, Train acc: 0.8801961926961926\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.064717685140436, Train acc: 0.8792289886039886\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0647155256509624, Train acc: 0.8791502301117686\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.064636136585976, Train acc: 0.8793307387057387\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.064421862075132, Train acc: 0.8794871794871795\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0642998712694545, Train acc: 0.8794237446581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0642949432029742, Train acc: 0.8794149069884364\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0641963093595512, Train acc: 0.8794664055080722\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.064102506347996, Train acc: 0.8795687134502924\n",
      "Val loss: 2.895451307296753, Val acc: 0.874\n",
      "Epoch 96/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.068797154304309, Train acc: 0.874732905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.071022615982936, Train acc: 0.8724626068376068\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0677292907339897, Train acc: 0.8761574074074074\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0668420595491033, Train acc: 0.8771367521367521\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0655731372344186, Train acc: 0.878525641025641\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.064434201289446, Train acc: 0.8793625356125356\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0653381046358046, Train acc: 0.8787011599511599\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0641723196221213, Train acc: 0.8796407585470085\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.063803789848842, Train acc: 0.8801044634377968\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0647594114654084, Train acc: 0.8791933760683761\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.065493681955078, Train acc: 0.8784236596736597\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.064480349294141, Train acc: 0.8793625356125356\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.064841683172694, Train acc: 0.8789242274819198\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.065488372311924, Train acc: 0.8783386752136753\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.065883705215237, Train acc: 0.8778133903133903\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.065391561223401, Train acc: 0.8783052884615384\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0656893081315024, Train acc: 0.8780480140774258\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0655472424068795, Train acc: 0.8781160968660968\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.065171794262868, Train acc: 0.8784581646423751\n",
      "Val loss: 2.8975141048431396, Val acc: 0.874\n",
      "Epoch 97/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0621138159026446, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.062747849867894, Train acc: 0.8799412393162394\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0632066057618186, Train acc: 0.8796296296296297\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.062594891868086, Train acc: 0.8804086538461539\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0642706791559857, Train acc: 0.8786858974358974\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0652073961377484, Train acc: 0.8778044871794872\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.064021703815577, Train acc: 0.8791590354090354\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.064001673561895, Train acc: 0.8792401175213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0641525144930237, Train acc: 0.8790657644824311\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.064093027665065, Train acc: 0.8792467948717949\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.063722725700017, Train acc: 0.8796377233877234\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0635752846033144, Train acc: 0.8798967236467237\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0639327916559775, Train acc: 0.8795405982905983\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0642499477580936, Train acc: 0.8791590354090354\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.063880705425882, Train acc: 0.8795584045584045\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0640405105092587, Train acc: 0.8793068910256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0641617402172137, Train acc: 0.879257792860734\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.064049063811162, Train acc: 0.8793625356125356\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.064112657262997, Train acc: 0.8793437921727395\n",
      "Val loss: 2.89890456199646, Val acc: 0.872\n",
      "Epoch 98/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.061563001738654, Train acc: 0.8830128205128205\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0630147971658626, Train acc: 0.8818108974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.062022235658434, Train acc: 0.8825676638176638\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.061652697559096, Train acc: 0.8827457264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0623330778545803, Train acc: 0.8818910256410256\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0618782143647176, Train acc: 0.8824341168091168\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.061858567999396, Train acc: 0.8822496947496947\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0628265997028756, Train acc: 0.8811765491452992\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.062848353997255, Train acc: 0.8812321937321937\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0620763312038193, Train acc: 0.8818910256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.061982710600455, Train acc: 0.8819444444444444\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0629945905120284, Train acc: 0.8807870370370371\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.062870510936489, Train acc: 0.8808349769888232\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.0629680463828155, Train acc: 0.8807234432234432\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0628622557023313, Train acc: 0.8807514245014245\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.063295548479272, Train acc: 0.8802918002136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.062723309565813, Train acc: 0.8807817998994469\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.062509173228417, Train acc: 0.8810244539411206\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0625245426347862, Train acc: 0.8810307017543859\n",
      "Val loss: 2.8879826068878174, Val acc: 0.88\n",
      "Epoch 99/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0702113565216718, Train acc: 0.8739316239316239\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.0701547540151157, Train acc: 0.8735309829059829\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0643909616008442, Train acc: 0.8793625356125356\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.063898871342341, Train acc: 0.8800080128205128\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0641306145578366, Train acc: 0.8797008547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0631469683429793, Train acc: 0.8806089743589743\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0636389746042982, Train acc: 0.8801510989010989\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.063365381115522, Train acc: 0.8804420405982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0633681695572217, Train acc: 0.8804902659069326\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0636475000626002, Train acc: 0.8801816239316239\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.06358525071296, Train acc: 0.8800990675990676\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0638472119457703, Train acc: 0.8797854344729344\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0635777453228803, Train acc: 0.8799926035502958\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.063569522748209, Train acc: 0.8800175518925519\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.0633537848790486, Train acc: 0.8802528490028491\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.0630422610885057, Train acc: 0.8806423611111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.0634177598958043, Train acc: 0.8802161890397184\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0630298781598735, Train acc: 0.8805496201329535\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0633050889192375, Train acc: 0.8801731893837157\n",
      "Val loss: 2.8997459411621094, Val acc: 0.872\n",
      "Epoch 100/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0588126875396466, Train acc: 0.8859508547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.060209781695635, Train acc: 0.8835470085470085\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0620083642481397, Train acc: 0.8823005698005698\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0631789098947477, Train acc: 0.8812767094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0621079952288897, Train acc: 0.8819444444444444\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0621344863179742, Train acc: 0.8819444444444444\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0631409477401568, Train acc: 0.8807615995115995\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0625805219269204, Train acc: 0.8811765491452992\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0631606240331393, Train acc: 0.8805199430199431\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.0634332130097937, Train acc: 0.8802617521367522\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.0630018971368336, Train acc: 0.8807789432789432\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.0628956265938587, Train acc: 0.8808092948717948\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.0630344853128437, Train acc: 0.8806911571334648\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.063286410656022, Train acc: 0.8804563492063492\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.063446607481041, Train acc: 0.8804131054131055\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.063120945906028, Train acc: 0.8807091346153846\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.062898501791266, Train acc: 0.8809389140271493\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.0629189778596926, Train acc: 0.8807870370370371\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.0627954186912705, Train acc: 0.8807354925775979\n",
      "Val loss: 2.894195556640625, Val acc: 0.876\n",
      "Tiempo total de entrenamiento: 2084.9157 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-5\n",
    "dropout_p = 0.7\n",
    "batch_size = 32\n",
    "criterion = perdida_regularizada_entropia\n",
    "epochs = 100\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    metadata_train,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    metadata_val,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    "    beta=0.3,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.85      0.90      0.87       100\n",
      "          SN       0.82      0.74      0.78       100\n",
      "          VS       0.95      0.87      0.91       100\n",
      "    asteroid       0.84      0.95      0.89       100\n",
      "       bogus       0.93      0.92      0.92       100\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.88      0.88      0.88       500\n",
      "weighted avg       0.88      0.88      0.88       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Asegúrate de tener tus datos de test preparados\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tercera combinación de hiperparámtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.2907093182588234, Train acc: 0.140758547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.223802490112109, Train acc: 0.1685363247863248\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.199030294717207, Train acc: 0.18037749287749288\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.18558989223252, Train acc: 0.18856837606837606\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.1773830303779014, Train acc: 0.1934294871794872\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.1709815722245436, Train acc: 0.1985844017094017\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.162469085756239, Train acc: 0.207226800976801\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.154498031251451, Train acc: 0.21661324786324787\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.1440743249818923, Train acc: 0.2289292497625831\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.130695722041986, Train acc: 0.24289529914529914\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.11846577630269, Train acc: 0.25497766122766125\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.1063417884019704, Train acc: 0.2666266025641026\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.0957044718063322, Train acc: 0.27712031558185407\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.0857965104891414, Train acc: 0.28668727106227104\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0769588693254692, Train acc: 0.29549501424501423\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.067738671715443, Train acc: 0.3043369391025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.0594902056554503, Train acc: 0.31229575163398693\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.0510200739812534, Train acc: 0.3210024928774929\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0432883869459433, Train acc: 0.32849752586594694\n",
      "Val loss: 3.260057210922241, Val acc: 0.552\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8882247423514342, Train acc: 0.49572649572649574\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8865916351986747, Train acc: 0.5004006410256411\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.880763764394994, Train acc: 0.5052528490028491\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8775839744470058, Train acc: 0.5063434829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.871998495525784, Train acc: 0.5111111111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.868706216839304, Train acc: 0.5145121082621082\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.865229332694495, Train acc: 0.5178952991452992\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.862709587965256, Train acc: 0.5201989850427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8608234250194555, Train acc: 0.5223765432098766\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.858249799410502, Train acc: 0.5246527777777777\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8560999855109674, Train acc: 0.526733682983683\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8531196706655018, Train acc: 0.5293358262108262\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8515553284757313, Train acc: 0.5305925378040762\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8498398448375846, Train acc: 0.5322039072039072\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.848150155483148, Train acc: 0.5338675213675214\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.846105007406993, Train acc: 0.5359241452991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8441717603087966, Train acc: 0.5377388134741076\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.842173530165626, Train acc: 0.5396783000949668\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8409626327331035, Train acc: 0.5408653846153846\n",
      "Val loss: 3.195110559463501, Val acc: 0.598\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8223396651765222, Train acc: 0.5598290598290598\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.819912537550315, Train acc: 0.5635683760683761\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.814066292553546, Train acc: 0.5690883190883191\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8141007968503184, Train acc: 0.5689770299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8095396420894523, Train acc: 0.572542735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8088652846480366, Train acc: 0.5725605413105413\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.807506204670311, Train acc: 0.5739087301587301\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.805033368177903, Train acc: 0.5761885683760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.804193720858321, Train acc: 0.5767153371320038\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8037077897634264, Train acc: 0.5772435897435897\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8026480682153707, Train acc: 0.5782342657342657\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8012724756515266, Train acc: 0.5795049857549858\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.800936483067168, Train acc: 0.5798405654174885\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8000541245689905, Train acc: 0.5805479242979243\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.7988935764019307, Train acc: 0.5819088319088319\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.7974669174251394, Train acc: 0.5832331730769231\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.796719249856718, Train acc: 0.5840717697335345\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.795523243412333, Train acc: 0.5853216999050332\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.793893767003803, Train acc: 0.5869742465137202\n",
      "Val loss: 3.164425849914551, Val acc: 0.636\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.7902609079312057, Train acc: 0.5870726495726496\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.7880014456235447, Train acc: 0.5914797008547008\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.780047623859851, Train acc: 0.5997150997150997\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.7768042245481768, Train acc: 0.6030982905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.7765243815560625, Train acc: 0.6033653846153846\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.776120145775993, Train acc: 0.6032318376068376\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.7708113135290087, Train acc: 0.6088598901098901\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.77031656080841, Train acc: 0.609107905982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.7700218264533585, Train acc: 0.6092711301044634\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.769664179769337, Train acc: 0.6099626068376068\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.7685489271070574, Train acc: 0.611013986013986\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.7667810417648053, Train acc: 0.6128249643874644\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.765694996936003, Train acc: 0.6138642340565418\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.7652772906644585, Train acc: 0.6142017704517705\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.7653384488532344, Train acc: 0.6138532763532764\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.7646931854323444, Train acc: 0.6145332532051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.7637172627532944, Train acc: 0.6154317496229261\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.7627101451696268, Train acc: 0.6165420227920227\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.7619869349724637, Train acc: 0.6169590643274854\n",
      "Val loss: 3.1201205253601074, Val acc: 0.672\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.7512174386244554, Train acc: 0.6266025641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.7543847377483663, Train acc: 0.6237980769230769\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.750132063515166, Train acc: 0.6282051282051282\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.7498079925520806, Train acc: 0.6274038461538461\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.748989039201003, Train acc: 0.6274038461538461\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.749563554413298, Train acc: 0.6269586894586895\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.74682295191419, Train acc: 0.6297313797313797\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.746215896983432, Train acc: 0.629440438034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.7437882763028485, Train acc: 0.6315586419753086\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.743647241592407, Train acc: 0.6314369658119658\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.7430660604226467, Train acc: 0.6316773504273504\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.7425462262243285, Train acc: 0.6319667022792023\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.741847225743481, Train acc: 0.6324375410913873\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.741358056900993, Train acc: 0.6328411172161172\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.7408165568979377, Train acc: 0.6331196581196581\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.7403569488953323, Train acc: 0.6334134615384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.740865330530687, Train acc: 0.6328714177978884\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.7410092250800426, Train acc: 0.6326715337132004\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.7406847398666128, Train acc: 0.6328160143949617\n",
      "Val loss: 3.104963779449463, Val acc: 0.672\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.738239106968937, Train acc: 0.6354166666666666\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.7359373732509775, Train acc: 0.6390224358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.7331162001672293, Train acc: 0.6419159544159544\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.7307403607246203, Train acc: 0.6430288461538461\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.731381271639441, Train acc: 0.6416666666666667\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.7322328623883063, Train acc: 0.6404914529914529\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.73424669499799, Train acc: 0.6381257631257631\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.7336738960355773, Train acc: 0.6385550213675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.7306127065946573, Train acc: 0.6420346628679962\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.7307256849403054, Train acc: 0.6416666666666667\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.7304166391457154, Train acc: 0.6414869852369852\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.7294917714561833, Train acc: 0.6424278846153846\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.729431032899334, Train acc: 0.6424432938856016\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.7289084417333824, Train acc: 0.6428952991452992\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.7284457722960034, Train acc: 0.6430555555555556\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.728149900706405, Train acc: 0.6430956196581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.727762599876024, Train acc: 0.6432095274007039\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.7276319522350487, Train acc: 0.6431178774928775\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.7267390282047903, Train acc: 0.6441183085919928\n",
      "Val loss: 3.093444585800171, Val acc: 0.688\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.71857384331206, Train acc: 0.6474358974358975\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.7164743405122023, Train acc: 0.6526442307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.7172546814649534, Train acc: 0.6517094017094017\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.7175751654510822, Train acc: 0.6513755341880342\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.7196378883133585, Train acc: 0.649465811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.7207048628744577, Train acc: 0.6480146011396012\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.7204768829438857, Train acc: 0.6482753357753358\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.71966298803305, Train acc: 0.648971688034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.7177140022161907, Train acc: 0.650670702754036\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.7178752954189593, Train acc: 0.6507745726495726\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.7180523677861497, Train acc: 0.6503739316239316\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.7176948585401575, Train acc: 0.6507523148148148\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.717392386926153, Train acc: 0.6509492110453649\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.717391729500413, Train acc: 0.6509081196581197\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.7173141046127363, Train acc: 0.6509259259259259\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.7173024491112456, Train acc: 0.6508747329059829\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.7168419688355687, Train acc: 0.6514265962795375\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.716249035860625, Train acc: 0.6519616571699905\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.71564796203651, Train acc: 0.652580971659919\n",
      "Val loss: 3.0907540321350098, Val acc: 0.68\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.7155332280020428, Train acc: 0.6509081196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.718987322261191, Train acc: 0.6477029914529915\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.7151499412677906, Train acc: 0.6513532763532763\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.7114963923764024, Train acc: 0.6549813034188035\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.712013181865725, Train acc: 0.6542200854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.711547643370778, Train acc: 0.6545584045584045\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.710211274096963, Train acc: 0.6561355311355311\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.7083942080155397, Train acc: 0.6582198183760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.707551244543715, Train acc: 0.658980294396961\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.7068261556136304, Train acc: 0.659375\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.707055488481203, Train acc: 0.6588723776223776\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.7073173020300363, Train acc: 0.6587651353276354\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.7075425570611498, Train acc: 0.658551117685733\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.706885119352003, Train acc: 0.6591498778998779\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.7074829222469927, Train acc: 0.6583867521367521\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.7080015383469753, Train acc: 0.6577023237179487\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.7082043364157085, Train acc: 0.657381221719457\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.707613183109396, Train acc: 0.6580603038936372\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.706961719935079, Train acc: 0.6586960188933874\n",
      "Val loss: 3.085559844970703, Val acc: 0.694\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.715210617098034, Train acc: 0.6501068376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.7028763691584268, Train acc: 0.6629273504273504\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.701196146826459, Train acc: 0.6647079772079773\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.7037833837362437, Train acc: 0.6629941239316239\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.7056778891473754, Train acc: 0.6605769230769231\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.704769988345285, Train acc: 0.6614138176638177\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.703823742266831, Train acc: 0.6617826617826618\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.701972024817752, Train acc: 0.6633279914529915\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.7032980837373652, Train acc: 0.6619776828110161\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.7036404012614845, Train acc: 0.6614850427350427\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.7034153110219603, Train acc: 0.6613976301476302\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.7034653994092914, Train acc: 0.6612134971509972\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.702930600450354, Train acc: 0.6617767915844839\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.702511704302824, Train acc: 0.662068833943834\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.701532896020134, Train acc: 0.6631054131054132\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.7008827997323794, Train acc: 0.6636618589743589\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.700746224117135, Train acc: 0.6637286324786325\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.7005705203991894, Train acc: 0.6637286324786325\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.700099534190457, Train acc: 0.6640941295546559\n",
      "Val loss: 3.080622911453247, Val acc: 0.694\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.6900472824390116, Train acc: 0.6754807692307693\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.6911843760400758, Train acc: 0.6738782051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.6915069923781263, Train acc: 0.6733440170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.693785151355287, Train acc: 0.6701388888888888\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.69328669075273, Train acc: 0.6705128205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.6934974563767087, Train acc: 0.6700498575498576\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.6941575814807224, Train acc: 0.6689560439560439\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.694690662826228, Train acc: 0.6681690705128205\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.6950668892403047, Train acc: 0.6676756885090218\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.6962138031282996, Train acc: 0.6665865384615385\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.6962463779790324, Train acc: 0.6663995726495726\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.695941647742888, Train acc: 0.6668224715099715\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.695730601071841, Train acc: 0.6672624917817226\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.695170650988708, Train acc: 0.6678113553113553\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.6955455815350566, Train acc: 0.6673611111111111\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.6965703924751687, Train acc: 0.6663327991452992\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.6964124894849375, Train acc: 0.6664309954751131\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.695660971168779, Train acc: 0.6672453703703703\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.6953380116459704, Train acc: 0.6675804093567251\n",
      "Val loss: 3.076352834701538, Val acc: 0.704\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.688383375477587, Train acc: 0.6746794871794872\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.691309788288214, Train acc: 0.6720085470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.693319464680816, Train acc: 0.6706730769230769\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.691893221476139, Train acc: 0.6719417735042735\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.6916087003854603, Train acc: 0.6720085470085471\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.690719231920704, Train acc: 0.6726317663817664\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.6915151022001647, Train acc: 0.6714743589743589\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.6907227074998055, Train acc: 0.6722088675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.6902437685561655, Train acc: 0.672334995251662\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.690174763223045, Train acc: 0.6722756410256411\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.6899468513450238, Train acc: 0.6724698912198912\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.689838741070185, Train acc: 0.6725649928774928\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.6906460916267108, Train acc: 0.6717619986850756\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.690995865980202, Train acc: 0.6715697496947497\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.691038455582752, Train acc: 0.6716702279202279\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.6911911284312224, Train acc: 0.6713241185897436\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.690782208008764, Train acc: 0.6717100301659125\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.6902130519562637, Train acc: 0.672142094017094\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.6904569681410417, Train acc: 0.6717836257309941\n",
      "Val loss: 3.0692827701568604, Val acc: 0.71\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.685194956950652, Train acc: 0.6773504273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.6867740225588155, Train acc: 0.6749465811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.6884500885281466, Train acc: 0.6726317663817664\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.689439863221258, Train acc: 0.672142094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.688690457792364, Train acc: 0.6728632478632479\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.6900569788071507, Train acc: 0.6712962962962963\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.690067237404531, Train acc: 0.6713217338217338\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.689285269405088, Train acc: 0.6723090277777778\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.6888680417313533, Train acc: 0.6725724121557455\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.6888381797024326, Train acc: 0.6725961538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.688083812443897, Train acc: 0.6733925796425796\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.687715269391693, Train acc: 0.673766915954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.687813671696429, Train acc: 0.6738576594345825\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.687142965703366, Train acc: 0.6745840964590964\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.6861544556087917, Train acc: 0.6754095441595441\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.6863725982670092, Train acc: 0.6754807692307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.684913432076686, Train acc: 0.6767848164906989\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.68479800111095, Train acc: 0.6769349477682811\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.685199534952024, Train acc: 0.6766194331983806\n",
      "Val loss: 3.0712156295776367, Val acc: 0.706\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.696649480069804, Train acc: 0.6653311965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.690270599137005, Train acc: 0.672676282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.68674362554849, Train acc: 0.6754807692307693\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.687504229382572, Train acc: 0.6734107905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.6877993225032446, Train acc: 0.6726495726495727\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.6869478086460687, Train acc: 0.6734330484330484\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.6859510139959055, Train acc: 0.6743742368742369\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.6850267164727564, Train acc: 0.6753138354700855\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.6834318780491495, Train acc: 0.6771130104463438\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.683592168082539, Train acc: 0.6769497863247863\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.6826281769848093, Train acc: 0.677763209013209\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.682845064887294, Train acc: 0.6775062321937322\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.682969429240581, Train acc: 0.6774326101249178\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.683308576052879, Train acc: 0.6770833333333334\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.6832382354301605, Train acc: 0.6772257834757834\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.6828743503389196, Train acc: 0.6777176816239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.6825644500055406, Train acc: 0.6780731523378583\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.682353745838176, Train acc: 0.678255579297246\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.6825973299029062, Train acc: 0.6780111336032388\n",
      "Val loss: 3.101257085800171, Val acc: 0.668\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.6987511133536315, Train acc: 0.6626602564102564\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.691163876117804, Train acc: 0.6702724358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.6867679204696264, Train acc: 0.6747685185185185\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.6846642647034082, Train acc: 0.6764155982905983\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.684214062160916, Train acc: 0.676923076923077\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.6825049426141288, Train acc: 0.6784633190883191\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.682945250009297, Train acc: 0.6778846153846154\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.6816242462040014, Train acc: 0.6790865384615384\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.6823151073111653, Train acc: 0.6786265432098766\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.682581103968824, Train acc: 0.6784722222222223\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.681525471049311, Train acc: 0.6795357420357421\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.6807782436028504, Train acc: 0.6805110398860399\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.6803468446211154, Train acc: 0.6808637409598948\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.6803862834588075, Train acc: 0.6807272588522588\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.679494524409628, Train acc: 0.6816061253561253\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.6797024633130455, Train acc: 0.6812900641025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.679447367899738, Train acc: 0.68151395173454\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.6790855023709232, Train acc: 0.6817426400759734\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.6785770313084476, Train acc: 0.6823268106162843\n",
      "Val loss: 3.0690677165985107, Val acc: 0.71\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.6693462836436734, Train acc: 0.6901709401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.674258099661933, Train acc: 0.6856303418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.676776540924681, Train acc: 0.6829594017094017\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.678995203258645, Train acc: 0.6813568376068376\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.6795583920601085, Train acc: 0.6807692307692308\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.678630227037305, Train acc: 0.6813123219373219\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.6750662606277746, Train acc: 0.6848290598290598\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.6763167697140293, Train acc: 0.6838274572649573\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.677399156666436, Train acc: 0.6829890788224121\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.678092758064596, Train acc: 0.6825320512820513\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.677028479831758, Train acc: 0.68368783993784\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.6774179502430124, Train acc: 0.6833823005698005\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.6765889883198133, Train acc: 0.684233234714004\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.677119410518325, Train acc: 0.683665293040293\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.677550749085907, Train acc: 0.6832264957264957\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.6773619732031455, Train acc: 0.6833099626068376\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.677252653437442, Train acc: 0.6834621669180493\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.6769050563502517, Train acc: 0.6838645536562203\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.6768342953140114, Train acc: 0.6838872019793072\n",
      "Val loss: 3.086246967315674, Val acc: 0.684\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.6759322402823686, Train acc: 0.6850961538461539\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.6789274460230117, Train acc: 0.6817574786324786\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.6756261149023333, Train acc: 0.6851851851851852\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.6738521344641333, Train acc: 0.6862313034188035\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.673050858831813, Train acc: 0.6871794871794872\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.6732353759966685, Train acc: 0.6870548433048433\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.6733352089976217, Train acc: 0.6870421245421245\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.6740045397200136, Train acc: 0.6864983974358975\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.673454635842913, Train acc: 0.6871438746438746\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.6745529698510455, Train acc: 0.685977564102564\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.6748894579945928, Train acc: 0.6857760295260296\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.6750761182219893, Train acc: 0.6854300213675214\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.6753603805451704, Train acc: 0.6849112426035503\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.6746484047036176, Train acc: 0.6855540293040293\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.67425206420768, Train acc: 0.6859508547008547\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.674316998730358, Train acc: 0.6861311431623932\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.674699481735882, Train acc: 0.6857246103569633\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.6745437912338708, Train acc: 0.685838081671415\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.674682822894578, Train acc: 0.6857990328385065\n",
      "Val loss: 3.064279556274414, Val acc: 0.714\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.66997758547465, Train acc: 0.6912393162393162\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.673004256354438, Train acc: 0.687767094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.673276692713767, Train acc: 0.6876780626780626\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.6724674431686726, Train acc: 0.6883680555555556\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.6700561413398156, Train acc: 0.690758547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.6713287500914005, Train acc: 0.6896367521367521\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.670966406008263, Train acc: 0.6897893772893773\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.6718239628861093, Train acc: 0.6890024038461539\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.6724889670115703, Train acc: 0.6883309591642925\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.673580061676156, Train acc: 0.6872863247863248\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.674787565822646, Train acc: 0.6860431235431236\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.675111121261901, Train acc: 0.6858529202279202\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.6757926320183207, Train acc: 0.6850139710716634\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.675421889885005, Train acc: 0.6852297008547008\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.674945064147993, Train acc: 0.6855947293447293\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.674214395829755, Train acc: 0.6862479967948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.673685315627438, Train acc: 0.6868558320764203\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.672292570895732, Train acc: 0.6882122507122507\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.6683056698797847, Train acc: 0.6923358074673864\n",
      "Val loss: 2.9321353435516357, Val acc: 0.852\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5870084619929647, Train acc: 0.780715811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5865719155368643, Train acc: 0.7800480769230769\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5819275358803253, Train acc: 0.7845441595441596\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5794084454194093, Train acc: 0.7871260683760684\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5775084214332775, Train acc: 0.788034188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5736608447512332, Train acc: 0.7921118233618234\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5700402108449785, Train acc: 0.795825702075702\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5685190278240757, Train acc: 0.7972088675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.56622181975717, Train acc: 0.7996794871794872\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5651445458077977, Train acc: 0.8003739316239317\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.563602426576355, Train acc: 0.8018648018648019\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5623481093648492, Train acc: 0.8030181623931624\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5609933913028375, Train acc: 0.8043639053254438\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.560203588634766, Train acc: 0.8053075396825397\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.559353303501749, Train acc: 0.805982905982906\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5590117609398995, Train acc: 0.806373530982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5586123226754567, Train acc: 0.8067339115133233\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5578284827392666, Train acc: 0.8074103751187085\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5569602847903687, Train acc: 0.8085779352226721\n",
      "Val loss: 2.906170129776001, Val acc: 0.872\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5428342656192617, Train acc: 0.8255876068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.540163807379894, Train acc: 0.8259882478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.538349440974048, Train acc: 0.8279024216524217\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5377232228588853, Train acc: 0.8285924145299145\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5384679447891365, Train acc: 0.8273504273504273\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.540009929243995, Train acc: 0.8254540598290598\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5412784493158735, Train acc: 0.8240613553113553\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5403165575276074, Train acc: 0.825020032051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5399530547523317, Train acc: 0.8253798670465338\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.539661118107983, Train acc: 0.8257211538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5401328506632748, Train acc: 0.8251991064491064\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.540239170575753, Train acc: 0.8250311609686609\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5395684453862657, Train acc: 0.8258547008547008\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5392692613077688, Train acc: 0.8259882478632479\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5393362302046554, Train acc: 0.8257834757834758\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5386533096560044, Train acc: 0.8264556623931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.538444495189124, Train acc: 0.8266402714932126\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.537791110624728, Train acc: 0.8271604938271605\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5375851158402924, Train acc: 0.8273448043184886\n",
      "Val loss: 2.9100282192230225, Val acc: 0.862\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5362897428691897, Train acc: 0.8309294871794872\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.530138469149924, Train acc: 0.8357371794871795\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5293627770198377, Train acc: 0.8346688034188035\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5301362235321956, Train acc: 0.8342013888888888\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5311920072278404, Train acc: 0.8327991452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.532319639483069, Train acc: 0.8320423789173789\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.533066962955927, Train acc: 0.8314636752136753\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5338610459087243, Train acc: 0.8305288461538461\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5343434210629656, Train acc: 0.8302172364672364\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.533919734832568, Train acc: 0.8308760683760684\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.533715864809891, Train acc: 0.8308809246309247\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5326642827091055, Train acc: 0.8320868945868946\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.532470444799018, Train acc: 0.8321416831032216\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5324100309821422, Train acc: 0.832226800976801\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5320316398924914, Train acc: 0.8325676638176638\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5317175045736833, Train acc: 0.8327991452991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.531159420718255, Train acc: 0.8332076420311715\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.531441520987979, Train acc: 0.83278430674264\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.53131716539878, Train acc: 0.8327850877192983\n",
      "Val loss: 2.9013800621032715, Val acc: 0.874\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5213139607356143, Train acc: 0.843215811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5241246182694392, Train acc: 0.8392094017094017\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5260897470675303, Train acc: 0.8366274928774928\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.527162448463277, Train acc: 0.8359375\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5281734173114483, Train acc: 0.8350961538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.526435493064402, Train acc: 0.8368055555555556\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.525815078481504, Train acc: 0.8371871184371185\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5250890012989697, Train acc: 0.8379073183760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5253505731693133, Train acc: 0.8373990978157645\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.525227736407875, Train acc: 0.8373931623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5248487666307167, Train acc: 0.8377525252525253\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.525149315171092, Train acc: 0.8373397435897436\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5252790521587847, Train acc: 0.8373808349769888\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.52513531712822, Train acc: 0.8374923687423688\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.52527271145769, Train acc: 0.8374287749287749\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5258233818488245, Train acc: 0.8369057158119658\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5256363158851727, Train acc: 0.8372454751131222\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.526014832349924, Train acc: 0.836968779677113\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5255430299207213, Train acc: 0.8374803193882141\n",
      "Val loss: 2.887582778930664, Val acc: 0.89\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5185647927797756, Train acc: 0.8464209401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.519104086435758, Train acc: 0.8450854700854701\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5208383761240207, Train acc: 0.8435719373219374\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.520107916277698, Train acc: 0.8436164529914529\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.520981068081326, Train acc: 0.8420940170940171\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.521611255797905, Train acc: 0.8409900284900285\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.520734921770946, Train acc: 0.842032967032967\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.52064065163971, Train acc: 0.8424813034188035\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5218575453599748, Train acc: 0.841553893637227\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.520922225357121, Train acc: 0.8422008547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5204307875525793, Train acc: 0.8425844988344988\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.519815475003332, Train acc: 0.8432603276353277\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5201554030356323, Train acc: 0.8428048980933597\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.519966021562234, Train acc: 0.8430441086691086\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5202881810332296, Train acc: 0.8425925925925926\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5207603150962763, Train acc: 0.8419971955128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5210667689761186, Train acc: 0.8416603821015586\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.521147242978088, Train acc: 0.841553893637227\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5213893191319996, Train acc: 0.8413461538461539\n",
      "Val loss: 2.886655330657959, Val acc: 0.894\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5173137799287453, Train acc: 0.8448183760683761\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.517956205922314, Train acc: 0.842948717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5184233725240768, Train acc: 0.8428596866096866\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.518743740697192, Train acc: 0.842948717948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.518493307146252, Train acc: 0.8430555555555556\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.518108730302577, Train acc: 0.8435719373219374\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5196305089817814, Train acc: 0.8418803418803419\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5197262822562814, Train acc: 0.8414463141025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.518723321436477, Train acc: 0.8427706552706553\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5184677368555315, Train acc: 0.8431356837606837\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.51921294563578, Train acc: 0.8423902486402487\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5193807599890947, Train acc: 0.8420806623931624\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5203133701572757, Train acc: 0.841387245233399\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.519970828214699, Train acc: 0.8417277167277167\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5201052301629656, Train acc: 0.8416488603988604\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5202500434258046, Train acc: 0.841529780982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.519806124565888, Train acc: 0.842084590246355\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5195543915457876, Train acc: 0.8424145299145299\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5200356035365536, Train acc: 0.8419506297795771\n",
      "Val loss: 2.8829541206359863, Val acc: 0.9\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5195638705522585, Train acc: 0.8448183760683761\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.518978059801281, Train acc: 0.843215811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.522024566970999, Train acc: 0.8402777777777778\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.519304003470983, Train acc: 0.84375\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5189170319809873, Train acc: 0.8436431623931624\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.518746277545592, Train acc: 0.84375\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.518978117149828, Train acc: 0.8436355311355311\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.519923210908205, Train acc: 0.8425480769230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.520142001643819, Train acc: 0.8420584045584045\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.520589753297659, Train acc: 0.841292735042735\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5210464330449436, Train acc: 0.8408119658119658\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.520951695591636, Train acc: 0.8408787393162394\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5209051045361983, Train acc: 0.8409557856673241\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5202926333279545, Train acc: 0.8417086385836385\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.519971401127655, Train acc: 0.8420584045584045\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.51977774551791, Train acc: 0.8423811431623932\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.519862440256931, Train acc: 0.8421631473102061\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5194463492119072, Train acc: 0.8425629154795822\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5187652718093125, Train acc: 0.8433845029239766\n",
      "Val loss: 2.8867242336273193, Val acc: 0.892\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.506795809819148, Train acc: 0.8557692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.509931229118608, Train acc: 0.8521634615384616\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.513388696219507, Train acc: 0.8490918803418803\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.515844597775712, Train acc: 0.8464209401709402\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.515387268147917, Train acc: 0.8472222222222222\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.516313958371806, Train acc: 0.8459312678062678\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5168902125841823, Train acc: 0.8448565323565324\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5164457602888093, Train acc: 0.8454527243589743\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5164644186086917, Train acc: 0.8454415954415955\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5162892806224333, Train acc: 0.8453525641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.516137338555313, Train acc: 0.8457653457653458\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5159003270996942, Train acc: 0.8460425569800569\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5164611264328514, Train acc: 0.8454758382642998\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5167167375958157, Train acc: 0.8450473137973138\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.516369341852998, Train acc: 0.8452457264957265\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.516382284780853, Train acc: 0.8450020032051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5162564954187117, Train acc: 0.8450383358471594\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5165642068596648, Train acc: 0.8448035375118709\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.516914000556215, Train acc: 0.8444107062528116\n",
      "Val loss: 2.896109104156494, Val acc: 0.88\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5094782552148542, Train acc: 0.8504273504273504\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.514321386304676, Train acc: 0.8460202991452992\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5144825538678726, Train acc: 0.8468660968660968\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5169108000575986, Train acc: 0.8447516025641025\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5146428340520615, Train acc: 0.8470619658119658\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5140480115542725, Train acc: 0.8478454415954416\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.514283119394957, Train acc: 0.8474130036630036\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5134750821142116, Train acc: 0.8481236645299145\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5132687621646457, Train acc: 0.8484686609686609\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5130853722238133, Train acc: 0.8484775641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5133573271640994, Train acc: 0.8480720668220668\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.51296885723402, Train acc: 0.8485131766381766\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.512173884170452, Train acc: 0.8494411571334648\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.512481497175382, Train acc: 0.8490155677655677\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5128425388933926, Train acc: 0.8486645299145299\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.512780081894663, Train acc: 0.8487246260683761\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5125586696459816, Train acc: 0.8489190548014077\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5124829573961858, Train acc: 0.8489434947768281\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.512347465155632, Train acc: 0.8490356500224921\n",
      "Val loss: 2.885324716567993, Val acc: 0.888\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.520112561364459, Train acc: 0.8405448717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5147242586836858, Train acc: 0.8472222222222222\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5137748575618124, Train acc: 0.8487357549857549\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.514402700795068, Train acc: 0.8476896367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5148210207621258, Train acc: 0.8471153846153846\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.514034667925278, Train acc: 0.8477564102564102\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.513600662368849, Train acc: 0.8482142857142857\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.513276563495652, Train acc: 0.8485243055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5134348085916054, Train acc: 0.8482609211775879\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5132304258835623, Train acc: 0.8481303418803419\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5136145696958647, Train acc: 0.8476592851592851\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5132672881808378, Train acc: 0.8480457621082621\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5133913375295203, Train acc: 0.8478591387245233\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5141482468198655, Train acc: 0.8471077533577533\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.513980860452027, Train acc: 0.847346866096866\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.513621993171863, Train acc: 0.8477397168803419\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.512838457744286, Train acc: 0.8484477124183006\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5130063381403374, Train acc: 0.8483202754036088\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5125743289600875, Train acc: 0.8488388439046334\n",
      "Val loss: 2.8875949382781982, Val acc: 0.884\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.511261485580705, Train acc: 0.8512286324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.508604345158634, Train acc: 0.8525641025641025\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5083766058299615, Train acc: 0.8523860398860399\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5102008463989973, Train acc: 0.8506944444444444\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.509693007591443, Train acc: 0.8511217948717948\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5089754762812557, Train acc: 0.8517628205128205\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.50893389669239, Train acc: 0.8519917582417582\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.509171004478748, Train acc: 0.8517628205128205\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5082834625063004, Train acc: 0.8524453941120608\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5087559663332426, Train acc: 0.8520032051282052\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5093384372604475, Train acc: 0.8517385392385393\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.51007538150858, Train acc: 0.8510950854700855\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5100037927771774, Train acc: 0.8509615384615384\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.510062529636099, Train acc: 0.8509615384615384\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5099161816458415, Train acc: 0.8511217948717948\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.510635573639829, Train acc: 0.8503271901709402\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5106789201513013, Train acc: 0.850270236299648\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.510688574112605, Train acc: 0.8502196106362773\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.510631630134325, Train acc: 0.8502586594691858\n",
      "Val loss: 2.882045030593872, Val acc: 0.89\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.513950767680111, Train acc: 0.8474893162393162\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5137823993324213, Train acc: 0.8453525641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5127233657402193, Train acc: 0.8465990028490028\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5113399492369757, Train acc: 0.8485576923076923\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.511108462015788, Train acc: 0.8491452991452991\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.511113785950207, Train acc: 0.8492254273504274\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5102305167760606, Train acc: 0.849969474969475\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.509989528065054, Train acc: 0.8507946047008547\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.508813391610315, Train acc: 0.8518221747388414\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5091809753678804, Train acc: 0.8512019230769231\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.50928983358607, Train acc: 0.8510586635586636\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5094982764320157, Train acc: 0.8510728276353277\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5093862764307104, Train acc: 0.8511875410913873\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5100731698293535, Train acc: 0.8504655067155067\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.510280556013102, Train acc: 0.850267094017094\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5107538821096096, Train acc: 0.8498597756410257\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.510175111247044, Train acc: 0.8503959276018099\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5099023911122016, Train acc: 0.850798314339981\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5096433764366326, Train acc: 0.8510458839406208\n",
      "Val loss: 2.8869290351867676, Val acc: 0.884\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5087628609094863, Train acc: 0.8520299145299145\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5093155956675863, Train acc: 0.8512286324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.509620881827808, Train acc: 0.8517628205128205\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.511746492141332, Train acc: 0.8501602564102564\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5106647931612454, Train acc: 0.8512286324786325\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5101605399042115, Train acc: 0.8516292735042735\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5101309751270744, Train acc: 0.8514575702075702\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.509830620044317, Train acc: 0.8514957264957265\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5094486781787784, Train acc: 0.8519705603038936\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.509795070102072, Train acc: 0.8514423076923077\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.508963838138595, Train acc: 0.8520299145299145\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.508915607745831, Train acc: 0.8518963675213675\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.509006327190813, Train acc: 0.8517011834319527\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5086001310593042, Train acc: 0.8518963675213675\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5086791982677927, Train acc: 0.8516203703703704\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5080416901753497, Train acc: 0.8523971688034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5080337862443423, Train acc: 0.852658371040724\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.507880061893155, Train acc: 0.8526234567901234\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.507421191082953, Train acc: 0.8530842330184435\n",
      "Val loss: 2.891935348510742, Val acc: 0.882\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5115110751910086, Train acc: 0.8488247863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.510470474887098, Train acc: 0.8512286324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5096142427873747, Train acc: 0.8515847578347578\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5067207721563487, Train acc: 0.8540331196581197\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5058798260158963, Train acc: 0.8551282051282051\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5067854960080225, Train acc: 0.8539440883190883\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.507705298098889, Train acc: 0.8529838217338217\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.507394360935586, Train acc: 0.8530649038461539\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.507378731244876, Train acc: 0.8532466761633428\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5071052408625936, Train acc: 0.8530448717948718\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.506115772600033, Train acc: 0.8540695415695416\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5069149720702755, Train acc: 0.8532540954415955\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.507189425726144, Train acc: 0.853077744904668\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.506908317334195, Train acc: 0.8534035409035409\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5069161080906532, Train acc: 0.8532941595441595\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.507124505109257, Train acc: 0.8530315170940171\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5066747950692463, Train acc: 0.8534439416792358\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.506634500178403, Train acc: 0.8534989316239316\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.506684438526979, Train acc: 0.8534497300944669\n",
      "Val loss: 2.8861827850341797, Val acc: 0.886\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5017212354219875, Train acc: 0.8595085470085471\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5017194900757227, Train acc: 0.8597756410256411\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5047745942390205, Train acc: 0.8560363247863247\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5061207943492465, Train acc: 0.8547008547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5057728918189675, Train acc: 0.8548611111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5070647473348853, Train acc: 0.8533208689458689\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.506761763413165, Train acc: 0.8533653846153846\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.506796483555411, Train acc: 0.8535323183760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5062168729044885, Train acc: 0.8542853751187085\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5064240600308803, Train acc: 0.8541132478632478\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5059916463672605, Train acc: 0.8546037296037297\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.506056870999839, Train acc: 0.8544782763532763\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5058052894083156, Train acc: 0.8547008547008547\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.505141002645714, Train acc: 0.8552159645909646\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5045727540964413, Train acc: 0.8559294871794871\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5044818428846507, Train acc: 0.8559361645299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.504406805671358, Train acc: 0.856052036199095\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5040235868093523, Train acc: 0.856644705603039\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.504439518680135, Train acc: 0.8561347278452541\n",
      "Val loss: 2.875697374343872, Val acc: 0.902\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4984020514365954, Train acc: 0.8611111111111112\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.499154510661068, Train acc: 0.8613782051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5008499961972577, Train acc: 0.8600427350427351\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5039142329468684, Train acc: 0.8567708333333334\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5023496016477926, Train acc: 0.8587072649572649\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5031164246067363, Train acc: 0.8576388888888888\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.503720135042519, Train acc: 0.8569139194139194\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5029575045292196, Train acc: 0.8576722756410257\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.502439927964242, Train acc: 0.8582324311490979\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.502871868141696, Train acc: 0.8578258547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5022501386036313, Train acc: 0.8583916083916084\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.502879676146385, Train acc: 0.8573717948717948\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.502763779519186, Train acc: 0.8576799802761341\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5028968799099673, Train acc: 0.85752442002442\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5033453339525096, Train acc: 0.8570868945868946\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5034921802261954, Train acc: 0.8569544604700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5031772622395665, Train acc: 0.8573246606334841\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5031959159195365, Train acc: 0.8572085707502374\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5033894495764564, Train acc: 0.8570625281151597\n",
      "Val loss: 2.8831422328948975, Val acc: 0.892\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.504335169099335, Train acc: 0.8557692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.506180312898424, Train acc: 0.8555021367521367\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.508895780286218, Train acc: 0.8519408831908832\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.506832588941623, Train acc: 0.8535657051282052\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5076126591772097, Train acc: 0.8525106837606837\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.507765669089097, Train acc: 0.8519408831908832\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5065519145993522, Train acc: 0.853021978021978\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.507307093367617, Train acc: 0.8519631410256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5065141147584544, Train acc: 0.8526531339031339\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5059867459484653, Train acc: 0.8533119658119658\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5062362686459556, Train acc: 0.8531954156954157\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.506026789163932, Train acc: 0.8533208689458689\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.505318238655256, Train acc: 0.8541050295857988\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5047198830943405, Train acc: 0.854739010989011\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.504215792169598, Train acc: 0.8553240740740741\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5038801827746577, Train acc: 0.8556857638888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5039301021666067, Train acc: 0.8556121166415284\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5040056124711647, Train acc: 0.8554576210826211\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5041285804194264, Train acc: 0.8554459064327485\n",
      "Val loss: 2.8818366527557373, Val acc: 0.894\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.501673920541747, Train acc: 0.8611111111111112\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5035404873709393, Train acc: 0.8565705128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.503147924727524, Train acc: 0.8577279202279202\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5007292516211157, Train acc: 0.8600427350427351\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4991804742405557, Train acc: 0.86116452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5002264079884586, Train acc: 0.8598201566951567\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4994966066800632, Train acc: 0.8605387667887668\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5001976220016804, Train acc: 0.8597756410256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5006420999510675, Train acc: 0.8593304843304843\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.5008576274937035, Train acc: 0.8591880341880341\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.501112355126275, Train acc: 0.8587801087801088\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.501301761705991, Train acc: 0.8585292022792023\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.5013159622415597, Train acc: 0.8585839907955293\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5015457529433625, Train acc: 0.8582875457875457\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.501553404636872, Train acc: 0.8582799145299145\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5006645309109974, Train acc: 0.8590912126068376\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5011672042254047, Train acc: 0.8586129964806435\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5013361809260486, Train acc: 0.8584846866096866\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.501375409004874, Train acc: 0.8584261133603239\n",
      "Val loss: 2.8728747367858887, Val acc: 0.904\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4992893822172766, Train acc: 0.8613782051282052\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.497794558859279, Train acc: 0.8633814102564102\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4992933381996263, Train acc: 0.8616452991452992\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.497951353717054, Train acc: 0.8629139957264957\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4977421210362363, Train acc: 0.8628205128205129\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4990256491549676, Train acc: 0.8614672364672364\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4984440937438146, Train acc: 0.8617979242979243\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4990132509643197, Train acc: 0.8610109508547008\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4986737372189167, Train acc: 0.8613782051282052\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.499950681588589, Train acc: 0.8598290598290599\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.500114988150667, Train acc: 0.8596299533799534\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4999804109589667, Train acc: 0.8596643518518519\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.499827898622421, Train acc: 0.8595085470085471\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5001583275486405, Train acc: 0.8592414529914529\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.500295050123818, Train acc: 0.8589565527065527\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.5005738959353194, Train acc: 0.8586404914529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.5006587712713197, Train acc: 0.8586601307189542\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.5004701064183164, Train acc: 0.858840811965812\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.5006146650833423, Train acc: 0.8586369770580297\n",
      "Val loss: 2.8846004009246826, Val acc: 0.892\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.499470258370424, Train acc: 0.8613782051282052\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.499898955353305, Train acc: 0.8605769230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5008175631194374, Train acc: 0.8598646723646723\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.500539424072983, Train acc: 0.8599759615384616\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.5010956731616942, Train acc: 0.8592414529914529\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.501154719934165, Train acc: 0.8589298433048433\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.499921136723333, Train acc: 0.8602335164835165\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.5008995685821924, Train acc: 0.8593082264957265\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5007430608229533, Train acc: 0.8593304843304843\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.500827366674048, Train acc: 0.8592147435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.501068498334314, Train acc: 0.8588529526029526\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.5003927137777, Train acc: 0.8596866096866097\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.500971131070831, Train acc: 0.8589949046679816\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.5005749398535424, Train acc: 0.8596230158730159\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.5000204917712088, Train acc: 0.8600783475783476\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.499886451750739, Train acc: 0.8602597489316239\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.499294047981366, Train acc: 0.8608754399195576\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4986676843757305, Train acc: 0.8614672364672364\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.498909522325565, Train acc: 0.8611673414304993\n",
      "Val loss: 2.8807883262634277, Val acc: 0.892\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.501510345018827, Train acc: 0.8565705128205128\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5014274232407923, Train acc: 0.8577724358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.498817295090765, Train acc: 0.8605769230769231\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5008649764916835, Train acc: 0.8587072649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.501509332249307, Train acc: 0.8584935897435897\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5010395739492868, Train acc: 0.8589743589743589\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.502347359581599, Train acc: 0.8574099511599511\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.502134766843584, Train acc: 0.8575721153846154\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.5026207144557016, Train acc: 0.8572530864197531\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.501539769539466, Train acc: 0.8582264957264957\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.5008116683574637, Train acc: 0.85880439005439\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.500490463357366, Train acc: 0.8587740384615384\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4995958841137007, Train acc: 0.85961127547666\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4988590455608346, Train acc: 0.8603098290598291\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4982917511225424, Train acc: 0.8610042735042736\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4980622259979572, Train acc: 0.8612446581196581\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4984170395624345, Train acc: 0.8608597285067874\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.498105785106322, Train acc: 0.8612891737891738\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4976545842016704, Train acc: 0.8617296446243815\n",
      "Val loss: 2.8750228881835938, Val acc: 0.904\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5052260802342343, Train acc: 0.8536324786324786\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5029749574824276, Train acc: 0.8560363247863247\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.502921061638074, Train acc: 0.8560363247863247\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.5029903517829046, Train acc: 0.8554353632478633\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.50280164205111, Train acc: 0.8555555555555555\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.5021927000110984, Train acc: 0.8563924501424501\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.5012513857621412, Train acc: 0.8574099511599511\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.500454066655575, Train acc: 0.8581063034188035\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.500161026278113, Train acc: 0.8583214624881291\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.500243663787842, Train acc: 0.8582532051282051\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.499517086965684, Train acc: 0.8592171717171717\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.498732706250628, Train acc: 0.8601985398860399\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.498479681215656, Train acc: 0.8603920118343196\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4982197551063567, Train acc: 0.8607295482295483\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4979383325984337, Train acc: 0.8611289173789174\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.498256063868857, Train acc: 0.8608106303418803\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.498048750223857, Train acc: 0.86103255404726\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.498554326077359, Train acc: 0.8606066001899335\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4988345580759534, Train acc: 0.8603238866396761\n",
      "Val loss: 2.881596326828003, Val acc: 0.892\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.506259188692794, Train acc: 0.8517628205128205\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.5021387450715418, Train acc: 0.8563034188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.5005209656522482, Train acc: 0.8581730769230769\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4999611392999306, Train acc: 0.8587740384615384\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4980291986057903, Train acc: 0.8607371794871795\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4973785615714528, Train acc: 0.8614672364672364\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.496584608150198, Train acc: 0.8625992063492064\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4975665323754663, Train acc: 0.8616786858974359\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4977385440782944, Train acc: 0.8616156220322887\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.4975920440804247, Train acc: 0.8616185897435897\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.497635579424119, Train acc: 0.8617424242424242\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4976933770030314, Train acc: 0.8618456196581197\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4971892901112733, Train acc: 0.862405489809336\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4974205399316456, Train acc: 0.8622176434676435\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4972444681020884, Train acc: 0.8624109686609687\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.497549090375248, Train acc: 0.8620626335470085\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4972487379403017, Train acc: 0.8623523127199598\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4974215317089445, Train acc: 0.8621498100664767\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.497510742186642, Train acc: 0.8620248538011696\n",
      "Val loss: 2.8828206062316895, Val acc: 0.892\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.494268152448866, Train acc: 0.8635149572649573\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.497354892583994, Train acc: 0.8605769230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4931185259099022, Train acc: 0.8653846153846154\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4936030094440165, Train acc: 0.8649172008547008\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.49361511784741, Train acc: 0.8653846153846154\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.493632149152946, Train acc: 0.8655181623931624\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4943005954244053, Train acc: 0.8650412087912088\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4943743348121643, Train acc: 0.8652176816239316\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.494967171722441, Train acc: 0.8645833333333334\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.495320278762752, Train acc: 0.8643429487179487\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4953456945723125, Train acc: 0.8643405205905206\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4952075435565066, Train acc: 0.8643384971509972\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.495102861380593, Train acc: 0.8644806048652203\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4949202735491, Train acc: 0.8645260989010989\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4950207168220455, Train acc: 0.8645121082621082\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4951186122802587, Train acc: 0.8643496260683761\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.495277296783578, Train acc: 0.8641119909502263\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4954210227710907, Train acc: 0.8639749525166192\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.495190903779037, Train acc: 0.8641897210976158\n",
      "Val loss: 2.8937900066375732, Val acc: 0.878\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.510935473645854, Train acc: 0.8477564102564102\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.502597644797757, Train acc: 0.8572382478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.497221629504125, Train acc: 0.8621794871794872\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4951759770385222, Train acc: 0.8639823717948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4944320144816343, Train acc: 0.8651175213675214\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.492810830091819, Train acc: 0.8668981481481481\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4929969316582685, Train acc: 0.8666437728937729\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.493563212135918, Train acc: 0.8661525106837606\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4935977196308508, Train acc: 0.8661858974358975\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.4938851254618064, Train acc: 0.8659989316239316\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.494374710918862, Train acc: 0.8654817404817405\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.494423524776415, Train acc: 0.8654736467236467\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.49446836260884, Train acc: 0.865302432610125\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4950533447393712, Train acc: 0.8647550366300366\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4951149465012077, Train acc: 0.8646545584045584\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.495007200882985, Train acc: 0.8648337339743589\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.49490812101695, Train acc: 0.8649132730015083\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4945054606721158, Train acc: 0.8653697768281101\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4944687156702985, Train acc: 0.8654830184435448\n",
      "Val loss: 2.87682843208313, Val acc: 0.898\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.490238660421127, Train acc: 0.8685897435897436\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4883522640945563, Train acc: 0.8703258547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4918438675057173, Train acc: 0.8674323361823362\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.492466255130931, Train acc: 0.8671207264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4932440537672775, Train acc: 0.8661324786324787\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4950030641338423, Train acc: 0.8641381766381766\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4960734419188073, Train acc: 0.8630570818070818\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4952563240996795, Train acc: 0.8638488247863247\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.495938628713856, Train acc: 0.8628917378917379\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.4957451117344394, Train acc: 0.8630341880341881\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.495545644767542, Train acc: 0.8632964257964258\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4947775193089434, Train acc: 0.8640491452991453\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4949475152958076, Train acc: 0.8639875082182774\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4943689727957867, Train acc: 0.8645642551892552\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4939680187790483, Train acc: 0.8650819088319088\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.493948670151906, Train acc: 0.8650674412393162\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4938238346738513, Train acc: 0.8652275012569131\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.493709905987565, Train acc: 0.865295584045584\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4936926751001645, Train acc: 0.865286212325686\n",
      "Val loss: 2.8808953762054443, Val acc: 0.886\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.495748252950163, Train acc: 0.8648504273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4944137681243768, Train acc: 0.8660523504273504\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.493293143405534, Train acc: 0.8663639601139601\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4921769779971523, Train acc: 0.8676549145299145\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.493127843253633, Train acc: 0.8665064102564103\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4933221581315044, Train acc: 0.8662749287749287\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4932147705511296, Train acc: 0.86626221001221\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.492459375124711, Train acc: 0.8669871794871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4920332549530784, Train acc: 0.8675510446343779\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.4910399412497495, Train acc: 0.8684294871794872\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.490661038848533, Train acc: 0.8687597125097125\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4902093799365552, Train acc: 0.8694355413105413\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.490526977767292, Train acc: 0.8691855687047995\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.490915413159008, Train acc: 0.8686278998778999\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4914833377229524, Train acc: 0.8680377492877492\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4919529768646274, Train acc: 0.8675380608974359\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4920610158871384, Train acc: 0.8674428104575164\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4922744597238466, Train acc: 0.8672691120607787\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.49246803490471, Train acc: 0.8670434098065677\n",
      "Val loss: 2.8891713619232178, Val acc: 0.886\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.5016412327432227, Train acc: 0.8587072649572649\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.497917834510151, Train acc: 0.8620459401709402\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.497883419705252, Train acc: 0.8612001424501424\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4953169929675565, Train acc: 0.8636485042735043\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4941604818034375, Train acc: 0.8649038461538462\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.494582302210338, Train acc: 0.8643607549857549\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4935923580430512, Train acc: 0.865460927960928\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.492978247312399, Train acc: 0.8660857371794872\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4932553227017973, Train acc: 0.8658000949667616\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.4927068035826725, Train acc: 0.8666132478632479\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4920197755862503, Train acc: 0.8673999611499611\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4928622398620996, Train acc: 0.8666087962962963\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4934429316674307, Train acc: 0.8660831689677844\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.493583537283398, Train acc: 0.8658806471306472\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4935577566467457, Train acc: 0.8658653846153846\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.493216493063503, Train acc: 0.8662025908119658\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.493221274448316, Train acc: 0.866091628959276\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.493318063241464, Train acc: 0.8659781576448243\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4926725415243953, Train acc: 0.866593567251462\n",
      "Val loss: 2.879699230194092, Val acc: 0.896\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4798687890044646, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4866339314697137, Train acc: 0.8728632478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4882507555165523, Train acc: 0.8709045584045584\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.489835031521626, Train acc: 0.8691907051282052\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4895027661934876, Train acc: 0.8694978632478633\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4901984284746, Train acc: 0.8685452279202279\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4902701922243184, Train acc: 0.8683226495726496\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4897057610189814, Train acc: 0.8689236111111112\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.48914665963009, Train acc: 0.8696877967711301\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.489781797441662, Train acc: 0.8690972222222222\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.489818109684242, Train acc: 0.8692939005439005\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.491441606286584, Train acc: 0.8676771723646723\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4908803498407948, Train acc: 0.8682815581854043\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.490647698933388, Train acc: 0.868379884004884\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.490510832006775, Train acc: 0.8686431623931624\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.490563860561094, Train acc: 0.8686565170940171\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4906018012848974, Train acc: 0.8686997234791353\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.490770727820546, Train acc: 0.8685007122507122\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.490829445465266, Train acc: 0.8683367071524967\n",
      "Val loss: 2.8755266666412354, Val acc: 0.902\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4882259022476325, Train acc: 0.8728632478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4856584805708666, Train acc: 0.8729967948717948\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4865961713329, Train acc: 0.8722400284900285\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.489080978764428, Train acc: 0.8707264957264957\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.489500437434922, Train acc: 0.8699786324786325\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4898406701889475, Train acc: 0.8695245726495726\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.490601866527646, Train acc: 0.8683608058608059\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.490814229871473, Train acc: 0.8678886217948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.49084375069918, Train acc: 0.8679368471035138\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.49140943767678, Train acc: 0.8672275641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.491212011031986, Train acc: 0.8675942113442113\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4909398837646526, Train acc: 0.8679442663817664\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4910353233279716, Train acc: 0.8679322813938198\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.491228786143628, Train acc: 0.8678075396825397\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4914716003287554, Train acc: 0.8677350427350428\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4912205847919497, Train acc: 0.8679553952991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.491496738804951, Train acc: 0.8676784816490699\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4914778843451315, Train acc: 0.8677439458689459\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.49166756259714, Train acc: 0.8675073099415205\n",
      "Val loss: 2.878385543823242, Val acc: 0.894\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.47850515292241, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4823824788770104, Train acc: 0.8764690170940171\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.484760994924779, Train acc: 0.8743767806267806\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4846800519869876, Train acc: 0.8739316239316239\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4863091798929067, Train acc: 0.8725961538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.488172305275572, Train acc: 0.8703258547008547\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.488306089198633, Train acc: 0.8703067765567766\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.488144541143352, Train acc: 0.8704260149572649\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4869827404547507, Train acc: 0.8716464862298196\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.48641718941876, Train acc: 0.8721688034188034\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4870883834945574, Train acc: 0.8715034965034965\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4873735247514186, Train acc: 0.8713942307692307\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.487517118924233, Train acc: 0.8712195923734385\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.487373389895, Train acc: 0.8714514652014652\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4872913522258444, Train acc: 0.8716168091168092\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.487254942838962, Train acc: 0.8716112446581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4871191974858533, Train acc: 0.871810583207642\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4874934671271562, Train acc: 0.8713200379867047\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4879353764997747, Train acc: 0.8709233018443545\n",
      "Val loss: 2.8777003288269043, Val acc: 0.896\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.478608989308023, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4830174007986345, Train acc: 0.8762019230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4817574554019504, Train acc: 0.8774038461538461\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.484028870733375, Train acc: 0.8749332264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4867695930676583, Train acc: 0.8719551282051282\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.48612387261839, Train acc: 0.8725961538461539\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.486212193602025, Train acc: 0.8727106227106227\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4875279387347717, Train acc: 0.871360844017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4881810016668306, Train acc: 0.8707858499525166\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.489133636564271, Train acc: 0.8699252136752137\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4895174344010194, Train acc: 0.8695852758352758\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4890533132091206, Train acc: 0.870147792022792\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.488939164893397, Train acc: 0.8701717619986851\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.488867198300158, Train acc: 0.8704021672771672\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.489725609178896, Train acc: 0.8696225071225071\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4895457369394793, Train acc: 0.8697582799145299\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4890001878599355, Train acc: 0.8701294620412268\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.48841856529004, Train acc: 0.8707561728395061\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4885130354374634, Train acc: 0.8705859199280251\n",
      "Val loss: 2.8855364322662354, Val acc: 0.886\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4920130550351915, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4874124476033397, Train acc: 0.8717948717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.486392641339207, Train acc: 0.8734864672364673\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.487562087356535, Train acc: 0.8722622863247863\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4871703946692314, Train acc: 0.8723824786324786\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4870748346687384, Train acc: 0.8724626068376068\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4877525566552876, Train acc: 0.8721001221001221\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.488352994378815, Train acc: 0.871360844017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.488515461725161, Train acc: 0.8709935897435898\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.488123824657538, Train acc: 0.8712606837606838\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4875015608543745, Train acc: 0.871819153069153\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.487478335528632, Train acc: 0.8717058404558404\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4870706443172157, Train acc: 0.8720825115055885\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4874706953873127, Train acc: 0.8715659340659341\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.48796159779584, Train acc: 0.8711182336182336\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.487786068238764, Train acc: 0.8712439903846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4878935913394717, Train acc: 0.8711507038712921\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4882094776302095, Train acc: 0.8707561728395061\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4879918935006127, Train acc: 0.8710357624831309\n",
      "Val loss: 2.8805131912231445, Val acc: 0.892\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.492217218773997, Train acc: 0.8651175213675214\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4864280682343702, Train acc: 0.8721955128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4856586415543513, Train acc: 0.8732193732193733\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.484913111751915, Train acc: 0.8736645299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.484556360733815, Train acc: 0.8739316239316239\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.484561620954095, Train acc: 0.8739316239316239\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.48468347521492, Train acc: 0.8740460927960928\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.484267224868139, Train acc: 0.8747662927350427\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.484533423598562, Train acc: 0.8746735517568851\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.484718013421083, Train acc: 0.8744123931623932\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.483298670161854, Train acc: 0.8759955322455323\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.483051857377729, Train acc: 0.8762464387464387\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4839689897754176, Train acc: 0.8753287310979618\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.484149751063523, Train acc: 0.8751526251526252\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4848280156779494, Train acc: 0.8744480056980057\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4852510728897195, Train acc: 0.8739149305555556\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.484872757459538, Train acc: 0.8744186777275013\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.484744507136621, Train acc: 0.8745251661918328\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.485436731987154, Train acc: 0.8737910481331534\n",
      "Val loss: 2.8731541633605957, Val acc: 0.902\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4855219421223698, Train acc: 0.8720619658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.4814112736628604, Train acc: 0.8775373931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.483775902337838, Train acc: 0.8750890313390314\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4831608024417844, Train acc: 0.8757345085470085\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.484179430333977, Train acc: 0.8744123931623932\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4848131325170186, Train acc: 0.8740206552706553\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.485823168422713, Train acc: 0.872748778998779\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4859624077112246, Train acc: 0.8725961538461539\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.486265236609116, Train acc: 0.8723884140550807\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.4848140539267125, Train acc: 0.8739316239316239\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4848180941675833, Train acc: 0.8738344988344988\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4850998430849818, Train acc: 0.8735754985754985\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.4849110967935815, Train acc: 0.8738905325443787\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.4853658171072692, Train acc: 0.8734355921855922\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4851252809888615, Train acc: 0.8738069800569801\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.485119816202384, Train acc: 0.8737646901709402\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4858677263772924, Train acc: 0.8731146304675717\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4858933229844906, Train acc: 0.8730413105413105\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.486242300532071, Train acc: 0.8726945569050832\n",
      "Val loss: 2.880628824234009, Val acc: 0.894\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.491753680074317, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.487981737169445, Train acc: 0.8713942307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4895341898980643, Train acc: 0.8701032763532763\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4887490348938184, Train acc: 0.8703926282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.489082776990711, Train acc: 0.8703525641025641\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4884017906297644, Train acc: 0.8705929487179487\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.487283554123726, Train acc: 0.871947496947497\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4876000713079405, Train acc: 0.8714276175213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4881414751268407, Train acc: 0.8706374643874644\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.487538976343269, Train acc: 0.8711805555555555\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4868719885306665, Train acc: 0.8718434343434344\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4871611739495543, Train acc: 0.87150551994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.486354223291471, Train acc: 0.8723496055226825\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.48656244461353, Train acc: 0.872214590964591\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4866177753165917, Train acc: 0.8721509971509972\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.486780928622963, Train acc: 0.8720285790598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4870388676857695, Train acc: 0.8717477375565611\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.4870872105854755, Train acc: 0.8716168091168092\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4872551167059522, Train acc: 0.8714856050382366\n",
      "Val loss: 2.876737594604492, Val acc: 0.896\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.4912853016812577, Train acc: 0.8656517094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.494909443406977, Train acc: 0.8629807692307693\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.491519668503025, Train acc: 0.8665420227920227\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.490584182943034, Train acc: 0.8676549145299145\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.4890811691936263, Train acc: 0.8693910256410257\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4891445008438198, Train acc: 0.8689013532763533\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.489575901310959, Train acc: 0.8686660561660562\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4890755155147652, Train acc: 0.8689903846153846\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.4889757789437925, Train acc: 0.8693613485280152\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.4881704055345977, Train acc: 0.870219017094017\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.4875061708359913, Train acc: 0.8709935897435898\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.486803973162616, Train acc: 0.8718839031339032\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.485861577617738, Train acc: 0.8728427021696252\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.486224470703302, Train acc: 0.8723672161172161\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.485902516726415, Train acc: 0.872738603988604\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4858276576567917, Train acc: 0.8727964743589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4864718958383354, Train acc: 0.8720148315736551\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.48647925973731, Train acc: 0.8720619658119658\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.4862456267018387, Train acc: 0.8723290598290598\n",
      "Val loss: 2.8891525268554688, Val acc: 0.884\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.483090031860221, Train acc: 0.875\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.483787579414172, Train acc: 0.8756677350427351\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.4828253906336943, Train acc: 0.8766915954415955\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.4843756357828775, Train acc: 0.875267094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.485054185247829, Train acc: 0.8746260683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.4853108516785496, Train acc: 0.8740651709401709\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.4854320262960172, Train acc: 0.8734737484737485\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.4859762528003793, Train acc: 0.8728966346153846\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.485633992061995, Train acc: 0.8732490503323836\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.485881285382132, Train acc: 0.8729166666666667\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.485542169169298, Train acc: 0.8732517482517482\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.4853108155082095, Train acc: 0.8735977564102564\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.485426970017262, Train acc: 0.8733768902038133\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.485159910642184, Train acc: 0.8736072954822954\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.4850711830660828, Train acc: 0.8736823361823362\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.4847042019295897, Train acc: 0.8740651709401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.4841204791167324, Train acc: 0.8746857717445953\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.483901692931129, Train acc: 0.8750445156695157\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.483423360446204, Train acc: 0.8754357849752586\n",
      "Val loss: 2.882305145263672, Val acc: 0.888\n",
      "Early stopping at epoch 55 due to no improvement after 20 epochs.\n",
      "Tiempo total de entrenamiento: 1126.1966 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-5\n",
    "dropout_p = 0.6\n",
    "batch_size = 32\n",
    "criterion = perdida_regularizada_entropia\n",
    "epochs = 100\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    metadata_train,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    metadata_val,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    "    beta=0.2,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.91      0.86      0.88       100\n",
      "          SN       0.89      0.78      0.83       100\n",
      "          VS       0.92      0.93      0.93       100\n",
      "    asteroid       0.86      0.95      0.90       100\n",
      "       bogus       0.90      0.94      0.92       100\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.89      0.89      0.89       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Asegúrate de tener tus datos de test preparados\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.700225257465982, Train acc: 0.13087606837606838\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.630207084183, Train acc: 0.17240918803418803\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.603817605564737, Train acc: 0.18892450142450143\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.592459190605033, Train acc: 0.19584668803418803\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.582884201636681, Train acc: 0.20448717948717948\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.5727740763938667, Train acc: 0.21505519943019943\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.5580977147606556, Train acc: 0.23034951159951159\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.544392617085041, Train acc: 0.24315571581196582\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.532533741178449, Train acc: 0.25418447293447294\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.521983132810674, Train acc: 0.2639423076923077\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.5122064275156184, Train acc: 0.2728486790986791\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.50441490517043, Train acc: 0.2798477564102564\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.4967412573973022, Train acc: 0.2869206114398422\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.4899369532080944, Train acc: 0.29311660561660563\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.4842664058391866, Train acc: 0.29830840455840457\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.4787129152279634, Train acc: 0.3036024305555556\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.4723126183447977, Train acc: 0.309907616892911\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.4653988215092353, Train acc: 0.3165212488129155\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.4589352447923325, Train acc: 0.32288855150697254\n",
      "Val loss: 3.3317480087280273, Val acc: 0.442\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.3410617225190515, Train acc: 0.43696581196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.330402524043352, Train acc: 0.4484508547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.328448917791035, Train acc: 0.45085470085470086\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.325593987591246, Train acc: 0.452991452991453\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.3229116207514053, Train acc: 0.455982905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.3223457445106614, Train acc: 0.45699786324786323\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.318731419769399, Train acc: 0.4609279609279609\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.3151280989504266, Train acc: 0.4658119658119658\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.311323809374658, Train acc: 0.4705899810066477\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.307558600107829, Train acc: 0.47462606837606836\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.304033415208237, Train acc: 0.4785839160839161\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.3005381163708503, Train acc: 0.4824608262108262\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.297640472866061, Train acc: 0.4854947403024326\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.2931624368579104, Train acc: 0.49021291208791207\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.2908949072204763, Train acc: 0.49266381766381767\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.2887298794638395, Train acc: 0.49494190705128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.2853408927831174, Train acc: 0.49846028154851685\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.283346509888188, Train acc: 0.5007567663817664\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.280811393470095, Train acc: 0.5033597615834458\n",
      "Val loss: 3.2023911476135254, Val acc: 0.592\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.23938638328487, Train acc: 0.5520833333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.2291943558260927, Train acc: 0.5614316239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.2278948733609627, Train acc: 0.5623219373219374\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.225305794650673, Train acc: 0.5663060897435898\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.223894762789082, Train acc: 0.5676816239316239\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.2232613349572206, Train acc: 0.5687767094017094\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.220386895213517, Train acc: 0.5723443223443223\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.218979386947094, Train acc: 0.5736511752136753\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.2173367868914338, Train acc: 0.5751721272554606\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.2159399619469275, Train acc: 0.5764423076923076\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.2135431279298774, Train acc: 0.5787441724941725\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.2104253721373035, Train acc: 0.5818198005698005\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.2067551311891695, Train acc: 0.5856755424063116\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.204309065845569, Train acc: 0.5884844322344323\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.200923751765846, Train acc: 0.5920762108262109\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.197949412286791, Train acc: 0.5952357104700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.195748104528904, Train acc: 0.5976307189542484\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.1936952406637804, Train acc: 0.5997150997150997\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.190881676894826, Train acc: 0.6023251237067027\n",
      "Val loss: 3.059607982635498, Val acc: 0.736\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.1134566608657184, Train acc: 0.6901709401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.111321573583489, Train acc: 0.6919070512820513\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.1111150310929343, Train acc: 0.6898148148148148\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.1057841614780264, Train acc: 0.6943108974358975\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.1027132401099573, Train acc: 0.697275641025641\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.1020637057785296, Train acc: 0.6983173076923077\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.1009154715671934, Train acc: 0.6993666056166056\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.1005976202650967, Train acc: 0.6996861645299145\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.098984346317317, Train acc: 0.7015372744539411\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.098357904466808, Train acc: 0.7020299145299145\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0960424292800774, Train acc: 0.7043269230769231\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.094186704722565, Train acc: 0.7061520655270656\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.0929247289014286, Train acc: 0.7076347797501643\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.092020283397446, Train acc: 0.7085050366300366\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.090708290953242, Train acc: 0.7094729344729345\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0897234581474566, Train acc: 0.7105201655982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.0884250437811787, Train acc: 0.7116013071895425\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.087606421789439, Train acc: 0.7124436134852802\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.085814833694678, Train acc: 0.7142234592892488\n",
      "Val loss: 3.016059637069702, Val acc: 0.774\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0504771664611297, Train acc: 0.7483974358974359\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.0586574892712455, Train acc: 0.7407852564102564\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.058754013474511, Train acc: 0.7406517094017094\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.0574231570602484, Train acc: 0.742454594017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.057291126251221, Train acc: 0.7422542735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.0563707691312176, Train acc: 0.7423433048433048\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.0556494461893307, Train acc: 0.7432844932844933\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.0541227928593626, Train acc: 0.7447248931623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0534169857318583, Train acc: 0.7454297245963912\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.05223491191864, Train acc: 0.7466613247863247\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0510590124611903, Train acc: 0.7477418414918415\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.050109386953533, Train acc: 0.7485532407407407\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.049316974159607, Train acc: 0.7496712689020382\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.0485287497186255, Train acc: 0.7503434065934066\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0476396625877444, Train acc: 0.7513532763532763\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0469653968118195, Train acc: 0.7516526442307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.04588523745357, Train acc: 0.7524981146304676\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.0451282268462583, Train acc: 0.7530715811965812\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0445626290381838, Train acc: 0.7536549707602339\n",
      "Val loss: 2.9693708419799805, Val acc: 0.828\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0260859248984575, Train acc: 0.7689636752136753\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.024790704759777, Train acc: 0.7721688034188035\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.028251675119427, Train acc: 0.7677172364672364\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.0283807490625954, Train acc: 0.766159188034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.0277860405098678, Train acc: 0.7663995726495727\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.02753852745067, Train acc: 0.7658030626780626\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.027928596887833, Train acc: 0.7659111721611722\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.0262798172795873, Train acc: 0.7672275641025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0258340031785957, Train acc: 0.7677469135802469\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.0246824625210884, Train acc: 0.7686698717948718\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0233009404328413, Train acc: 0.7692550505050505\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.022281672200586, Train acc: 0.7700543091168092\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.021145008176193, Train acc: 0.7713469756738988\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.020780843110603, Train acc: 0.7715010683760684\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0204122118121197, Train acc: 0.7719373219373219\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.019892397854063, Train acc: 0.7724025106837606\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.01937181162079, Train acc: 0.7728915284062343\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.018600249222541, Train acc: 0.7737862060778727\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0179177753784825, Train acc: 0.7744461313540261\n",
      "Val loss: 2.9402644634246826, Val acc: 0.848\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0115596901657233, Train acc: 0.7777777777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.0063319420203185, Train acc: 0.7835202991452992\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.0049523553277693, Train acc: 0.7850783475783476\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.003363382103097, Train acc: 0.7879273504273504\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.004380329653748, Train acc: 0.7862179487179487\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.0047855299082915, Train acc: 0.7850783475783476\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.005586149346115, Train acc: 0.7839590964590964\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.004408723523474, Train acc: 0.7847556089743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0037533573502038, Train acc: 0.7855235042735043\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.003011825145819, Train acc: 0.7864850427350427\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0023737772546633, Train acc: 0.7868346930846931\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.0032557344164945, Train acc: 0.7858796296296297\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.0034390879962727, Train acc: 0.7852769559500329\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.0034427222169215, Train acc: 0.7850465506715507\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0032979477504718, Train acc: 0.7852920227920228\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0031241345354633, Train acc: 0.7854734241452992\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.002174940224327, Train acc: 0.7865133232780291\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.001220983544193, Train acc: 0.7874376780626781\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0007943239396506, Train acc: 0.7878430049482681\n",
      "Val loss: 2.9295547008514404, Val acc: 0.852\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9908503141158667, Train acc: 0.7943376068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9899338310600347, Train acc: 0.7947382478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.989572035960662, Train acc: 0.7971866096866097\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9917703637709985, Train acc: 0.7956730769230769\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9931231519095918, Train acc: 0.7935363247863247\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.992425243399422, Train acc: 0.7939369658119658\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9935689283960176, Train acc: 0.7927350427350427\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.994348974054695, Train acc: 0.7915665064102564\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.993403566528929, Train acc: 0.7923492402659069\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.992079275082319, Train acc: 0.7936965811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9922483182268356, Train acc: 0.7937062937062938\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9915462326460074, Train acc: 0.7944711538461539\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9914391655266717, Train acc: 0.7942965154503616\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9910965003664534, Train acc: 0.7945283882783882\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9911701525718057, Train acc: 0.7944266381766382\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9907442546553082, Train acc: 0.7948551014957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9905802628213642, Train acc: 0.7952017345399698\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9902991519467896, Train acc: 0.7952872744539411\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.990130084836156, Train acc: 0.7955746738641476\n",
      "Val loss: 2.9175567626953125, Val acc: 0.862\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9832685442052336, Train acc: 0.7994123931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9868713158827562, Train acc: 0.7948717948717948\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9856016289474616, Train acc: 0.7954950142450142\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.985151416216141, Train acc: 0.7964075854700855\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.984174108097696, Train acc: 0.7982371794871795\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.985261335671797, Train acc: 0.7970975783475783\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.98393141131698, Train acc: 0.7985729548229549\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.983897587682447, Train acc: 0.7987112713675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.984514790269611, Train acc: 0.7978395061728395\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9845028512498253, Train acc: 0.7979700854700855\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.983485051616737, Train acc: 0.7991210178710179\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.983428656375646, Train acc: 0.7993678774928775\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9832864995852337, Train acc: 0.7995356673241288\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.983091129488124, Train acc: 0.7996794871794872\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9830577522940787, Train acc: 0.7996260683760684\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9828779592982726, Train acc: 0.7996627938034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.982683391772424, Train acc: 0.7998680241327301\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9826837411293616, Train acc: 0.7998575498575499\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.982251652929089, Train acc: 0.8001855600539811\n",
      "Val loss: 2.914170742034912, Val acc: 0.874\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9825942332928, Train acc: 0.7980769230769231\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9806862234050393, Train acc: 0.8010149572649573\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9797489507245882, Train acc: 0.8021723646723646\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.981026346866901, Train acc: 0.8006810897435898\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9810090765993817, Train acc: 0.8006410256410257\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9811839153963615, Train acc: 0.8006143162393162\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9814166105710544, Train acc: 0.8004044566544567\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.981324976326054, Train acc: 0.8005475427350427\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9809952115061615, Train acc: 0.8009852801519468\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9805188649739973, Train acc: 0.8017361111111111\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9801589305213745, Train acc: 0.8021318958818959\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.979612590920212, Train acc: 0.80252849002849\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9783683848334017, Train acc: 0.8038091715976331\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.978493548429347, Train acc: 0.8035905067155067\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9781162045959735, Train acc: 0.8037393162393163\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9778122142848806, Train acc: 0.8039196047008547\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.977576854302093, Train acc: 0.803921568627451\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9781534289136347, Train acc: 0.8032704178537512\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.978175820716587, Train acc: 0.8031517094017094\n",
      "Val loss: 2.909843683242798, Val acc: 0.874\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.971783788795145, Train acc: 0.8076923076923077\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.970896946059333, Train acc: 0.8082264957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9729220717720835, Train acc: 0.8068910256410257\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.973108453118903, Train acc: 0.8067574786324786\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.972489826903384, Train acc: 0.8076388888888889\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.972136087227411, Train acc: 0.8077813390313391\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.971228759367387, Train acc: 0.8087225274725275\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9719128743705587, Train acc: 0.8079594017094017\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.972388693862944, Train acc: 0.8074845679012346\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9720540933119945, Train acc: 0.8079059829059829\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9718588728634256, Train acc: 0.8084450271950272\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9729486576512327, Train acc: 0.8071358618233618\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9724320305718317, Train acc: 0.8077744904667982\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.972479403819502, Train acc: 0.8076350732600732\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.971893422623985, Train acc: 0.8085292022792023\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9718868154236393, Train acc: 0.8085269764957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9719870227134786, Train acc: 0.8082107843137255\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.97249628426569, Train acc: 0.8076774691358025\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9727645232449014, Train acc: 0.8074673864147548\n",
      "Val loss: 2.911822557449341, Val acc: 0.86\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9676888396597314, Train acc: 0.811965811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9628535223822308, Train acc: 0.8177083333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9669169591702627, Train acc: 0.8141025641025641\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.967900255296984, Train acc: 0.8128338675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.967876251334818, Train acc: 0.8124465811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.967495467927721, Train acc: 0.8120103276353277\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.967810582765293, Train acc: 0.8118131868131868\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.968640615288009, Train acc: 0.8111979166666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9690285978833493, Train acc: 0.8104522792022792\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9697607906455668, Train acc: 0.8095886752136752\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9695457687837354, Train acc: 0.8098290598290598\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9684403351908735, Train acc: 0.8109642094017094\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9680455964619514, Train acc: 0.8111439842209073\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9681497905426117, Train acc: 0.8112026862026862\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.967612061948858, Train acc: 0.8119480056980057\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.96732972536841, Train acc: 0.8125667735042735\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9669945258001036, Train acc: 0.8128613624937154\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9663662887819586, Train acc: 0.8133754748338081\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9665389843905126, Train acc: 0.8130623031938822\n",
      "Val loss: 2.9108927249908447, Val acc: 0.868\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.97649607495365, Train acc: 0.8010149572649573\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.965290544379471, Train acc: 0.8157051282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9619987500019564, Train acc: 0.8180199430199431\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.963353461179978, Train acc: 0.8160389957264957\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9634089971200015, Train acc: 0.8159188034188034\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9638141906499182, Train acc: 0.8154380341880342\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9642500111762713, Train acc: 0.8150946275946276\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9636552861103644, Train acc: 0.8159722222222222\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.963288185150422, Train acc: 0.8165064102564102\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9639126814328707, Train acc: 0.8158119658119658\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9642018026961034, Train acc: 0.8153894716394716\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9641723325449516, Train acc: 0.8152377136752137\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.964602576205011, Train acc: 0.8146778435239974\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9641168677908745, Train acc: 0.8151327838827839\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.963771023166146, Train acc: 0.8155270655270656\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9638885668455024, Train acc: 0.815254407051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.964126189310027, Train acc: 0.8150138260432378\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9640994490834496, Train acc: 0.8150225546058879\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9639025883153383, Train acc: 0.8150444219523167\n",
      "Val loss: 2.9123740196228027, Val acc: 0.856\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.956869302651821, Train acc: 0.8215811965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9591161532279773, Train acc: 0.8181089743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9584280840012425, Train acc: 0.8193554131054132\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9580252909252787, Train acc: 0.8195779914529915\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9583595165839562, Train acc: 0.8196047008547008\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.958592197833917, Train acc: 0.8187321937321937\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9589051510388162, Train acc: 0.8183760683760684\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9588476556998033, Train acc: 0.8184762286324786\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.959947771043406, Train acc: 0.8178122032288699\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.960117602144551, Train acc: 0.817815170940171\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9596573698863375, Train acc: 0.8182060994560995\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.959200394119632, Train acc: 0.8189992877492878\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.95934737159734, Train acc: 0.8188075279421433\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9582936957351165, Train acc: 0.8199213980463981\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.957782661677086, Train acc: 0.8203347578347578\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9576413310491123, Train acc: 0.8204627403846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.956938846078933, Train acc: 0.8213926596279537\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9568529320351873, Train acc: 0.8213141025641025\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9570699701210534, Train acc: 0.8210329509671614\n",
      "Val loss: 2.9080145359039307, Val acc: 0.866\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.953850324337299, Train acc: 0.8223824786324786\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9576743994003687, Train acc: 0.8205128205128205\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.955917871236122, Train acc: 0.8215811965811965\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9544392003972306, Train acc: 0.8225160256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.953420623551067, Train acc: 0.8240384615384615\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.954476040313047, Train acc: 0.8230947293447294\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.95557469879169, Train acc: 0.8217338217338217\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9553979569011264, Train acc: 0.8222823183760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.956123401409993, Train acc: 0.8214328110161444\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.956783558364607, Train acc: 0.8209401709401709\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.956117397884852, Train acc: 0.8216297591297591\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.956121032054608, Train acc: 0.8216034544159544\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9566407658252802, Train acc: 0.8210880999342538\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9570960943224374, Train acc: 0.8204746642246642\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.957615765139588, Train acc: 0.8198717948717948\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9569589894296775, Train acc: 0.8206129807692307\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.956830410456286, Train acc: 0.820811337355455\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9560981397954826, Train acc: 0.8216405508072174\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9565760619566346, Train acc: 0.821173526765632\n",
      "Val loss: 2.9049785137176514, Val acc: 0.874\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9570448520856027, Train acc: 0.8189102564102564\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.956470468105414, Train acc: 0.8199786324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9582778741831115, Train acc: 0.8177528490028491\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.955618997414907, Train acc: 0.8212473290598291\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.954969062560644, Train acc: 0.8220619658119658\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.95232259853613, Train acc: 0.8246972934472935\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9532538703364186, Train acc: 0.8239468864468864\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.952829862761701, Train acc: 0.8247195512820513\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.952861805587073, Train acc: 0.8249940645773979\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9531048324373033, Train acc: 0.8246794871794871\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9524109674778294, Train acc: 0.8252719502719502\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9530738782339285, Train acc: 0.8244524572649573\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.95309260344521, Train acc: 0.8244781393819856\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.953762328959501, Train acc: 0.823756105006105\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9532857644931543, Train acc: 0.8243055555555555\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9530065940995502, Train acc: 0.8244190705128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9525964076702413, Train acc: 0.824739190548014\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.952116181368162, Train acc: 0.825201804368471\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.952041897964649, Train acc: 0.8251658794421952\n",
      "Val loss: 2.902162551879883, Val acc: 0.874\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9513472919790154, Train acc: 0.8237179487179487\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.950324201176309, Train acc: 0.8262553418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.951662151222555, Train acc: 0.8246972934472935\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.950277434964465, Train acc: 0.8263221153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9475126788147494, Train acc: 0.8294337606837607\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9489473264101904, Train acc: 0.8280359686609686\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9497682972559853, Train acc: 0.827075702075702\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9491032137830033, Train acc: 0.8279580662393162\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.948506197924854, Train acc: 0.8287333808167141\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9475088800120557, Train acc: 0.8298611111111112\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.948266415792375, Train acc: 0.8289384226884227\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.948206693018943, Train acc: 0.8288149928774928\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.948950536309066, Train acc: 0.827827087442472\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.948539935334407, Train acc: 0.8281440781440782\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.948939536439727, Train acc: 0.8276887464387465\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.949387999299245, Train acc: 0.8271567841880342\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9495695965442303, Train acc: 0.8267816742081447\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9496218960056506, Train acc: 0.8266708214624882\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.949657139239738, Train acc: 0.8265716374269005\n",
      "Val loss: 2.902263641357422, Val acc: 0.876\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9473541455391126, Train acc: 0.8258547008547008\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9483608241774077, Train acc: 0.8246527777777778\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.946916766995378, Train acc: 0.8272792022792023\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.946273795559875, Train acc: 0.8291266025641025\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9473144070714965, Train acc: 0.8282585470085471\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9471034385539867, Train acc: 0.8279469373219374\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9482888514596755, Train acc: 0.8270375457875457\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.947582674586875, Train acc: 0.8278245192307693\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9478374428672103, Train acc: 0.8274869420702754\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.948121216358283, Train acc: 0.8271634615384615\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.948900546412553, Train acc: 0.8265588578088578\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9489353530087703, Train acc: 0.8266114672364673\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9490402152395654, Train acc: 0.8266148915187377\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9487682809468736, Train acc: 0.827075702075702\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9490589664872218, Train acc: 0.8267094017094017\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9486630133584013, Train acc: 0.8271233974358975\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.948944095992275, Train acc: 0.8268130970336852\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.948808042185712, Train acc: 0.8269379154795822\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9489303280842822, Train acc: 0.8269371345029239\n",
      "Val loss: 2.905210018157959, Val acc: 0.866\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9423856348054023, Train acc: 0.8338675213675214\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.944791588008913, Train acc: 0.8310630341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.945404120659896, Train acc: 0.8309294871794872\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.945190378743359, Train acc: 0.8309962606837606\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.94429977001288, Train acc: 0.8316773504273505\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9463387843210813, Train acc: 0.8289707977207977\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9464389221135514, Train acc: 0.8290979853479854\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9466200565171037, Train acc: 0.8292267628205128\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9471451384842338, Train acc: 0.8288224121557455\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9472016803219785, Train acc: 0.8288995726495727\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.947141992462265, Train acc: 0.8288655788655789\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.947375750100171, Train acc: 0.8285033831908832\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9465203396510637, Train acc: 0.8296351084812623\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9466226376019993, Train acc: 0.8295940170940171\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.945792608858853, Train acc: 0.8305021367521368\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9455736838599558, Train acc: 0.8306790865384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9460737576971345, Train acc: 0.8300810708898945\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.94564315305929, Train acc: 0.8304843304843305\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.945227746431537, Train acc: 0.8310278902384165\n",
      "Val loss: 2.9017417430877686, Val acc: 0.87\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9488365833575907, Train acc: 0.8253205128205128\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.949072808281988, Train acc: 0.8262553418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9482058684031167, Train acc: 0.8270121082621082\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.945672035217285, Train acc: 0.8296607905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9442977265415027, Train acc: 0.8313568376068377\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.941768087892451, Train acc: 0.8341346153846154\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.941571469126458, Train acc: 0.8343253968253969\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9427401568645086, Train acc: 0.8331997863247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9436308350431726, Train acc: 0.8321462488129154\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9435504988727406, Train acc: 0.8320779914529914\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.943555400088117, Train acc: 0.8319978632478633\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9437141277511576, Train acc: 0.8320646367521367\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9429619039539285, Train acc: 0.8328813280736358\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9437842277380137, Train acc: 0.8321314102564102\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.943786074021603, Train acc: 0.831980056980057\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.94395154587224, Train acc: 0.8317307692307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9443546129028184, Train acc: 0.8311965811965812\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.944276136776887, Train acc: 0.8313449667616334\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.944097157056622, Train acc: 0.8315339631129105\n",
      "Val loss: 2.8958675861358643, Val acc: 0.876\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9424246478284526, Train acc: 0.8306623931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9489292395420565, Train acc: 0.8247863247863247\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.946999937041193, Train acc: 0.8270121082621082\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.943679246637556, Train acc: 0.8303285256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9451245633964866, Train acc: 0.8291666666666667\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.944194102898622, Train acc: 0.8298611111111112\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9444666997562545, Train acc: 0.8294795482295483\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.943819145616303, Train acc: 0.8303285256410257\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.943131020718943, Train acc: 0.8311965811965812\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9425891633726593, Train acc: 0.8321314102564102\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.94336552868171, Train acc: 0.8314393939393939\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9423722908367798, Train acc: 0.8325097934472935\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9424746083554902, Train acc: 0.8323676857330703\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9433043832743997, Train acc: 0.8315590659340659\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9433061466597423, Train acc: 0.8314636752136753\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9433169017235437, Train acc: 0.8313468215811965\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9434426734650057, Train acc: 0.8312280040221217\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.943446564651736, Train acc: 0.8311965811965812\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9431204660701367, Train acc: 0.8316604813315339\n",
      "Val loss: 2.9013383388519287, Val acc: 0.87\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.941806652607062, Train acc: 0.8333333333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9415280564218507, Train acc: 0.8342681623931624\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.942134102525195, Train acc: 0.834045584045584\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9410034397728424, Train acc: 0.8352697649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9412774954086696, Train acc: 0.8349893162393163\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9430773461985793, Train acc: 0.833244301994302\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.942346133009709, Train acc: 0.8339819902319903\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9413257075680628, Train acc: 0.8348357371794872\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.941500056956002, Train acc: 0.8345204178537512\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.940919222994747, Train acc: 0.8350160256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9408952161704467, Train acc: 0.8349358974358975\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.941428420720277, Train acc: 0.8344684829059829\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9414898107743124, Train acc: 0.8345660749506904\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.941004856747731, Train acc: 0.8349168192918193\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9410254697174767, Train acc: 0.8347934472934473\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.940936095821552, Train acc: 0.8348858173076923\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.940698032465456, Train acc: 0.8351244343891403\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9402363810896985, Train acc: 0.8355739553656221\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9400886003894953, Train acc: 0.8357231219073324\n",
      "Val loss: 2.8979995250701904, Val acc: 0.868\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9246484886886726, Train acc: 0.8512286324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9342165084985585, Train acc: 0.8402777777777778\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.939893435209225, Train acc: 0.8342236467236467\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9400681894049687, Train acc: 0.8345352564102564\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.940147679483789, Train acc: 0.8346688034188035\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.940919883910068, Train acc: 0.8335559116809117\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9398896388518505, Train acc: 0.8342872405372406\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.939077334271537, Train acc: 0.8355034722222222\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9398254333851117, Train acc: 0.8348765432098766\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9396780933070388, Train acc: 0.8351228632478632\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9386787872121793, Train acc: 0.8360042735042735\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9388330636540707, Train acc: 0.8356481481481481\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.939802073868069, Train acc: 0.8348331689677844\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9400474486624657, Train acc: 0.8345924908424909\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9396276764720253, Train acc: 0.8349002849002849\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9394498720892472, Train acc: 0.8351028311965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.939304442427277, Train acc: 0.8353129713423831\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.939885995440107, Train acc: 0.8348765432098766\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9397690623288106, Train acc: 0.8351045883940621\n",
      "Val loss: 2.896892786026001, Val acc: 0.878\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.940141561703804, Train acc: 0.8322649572649573\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.938647167295472, Train acc: 0.8344017094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9373376484949705, Train acc: 0.8366274928774928\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.936315895145775, Train acc: 0.8382077991452992\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9379353906354333, Train acc: 0.8371260683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9389837246674757, Train acc: 0.8367165242165242\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9396334011767227, Train acc: 0.836195054945055\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.939441788655061, Train acc: 0.8362713675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.938740426658565, Train acc: 0.8370132953466287\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9378399828560333, Train acc: 0.837900641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.938118598400018, Train acc: 0.8378253690753691\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9388976395979225, Train acc: 0.8369168447293447\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9387270723966137, Train acc: 0.8370110124917817\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.93834880827693, Train acc: 0.8373206654456654\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.938453756098734, Train acc: 0.8371794871794872\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.93840107971277, Train acc: 0.8371394230769231\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.93857148006491, Train acc: 0.8369155354449472\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9383776246765514, Train acc: 0.8370874881291548\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.937901210420259, Train acc: 0.8375646648672964\n",
      "Val loss: 2.897573232650757, Val acc: 0.88\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.944136919119419, Train acc: 0.8298611111111112\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9430697728426027, Train acc: 0.8313301282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9439474497085962, Train acc: 0.8301282051282052\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.942683420629583, Train acc: 0.8313969017094017\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.940472315519284, Train acc: 0.8344551282051282\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9393645385731317, Train acc: 0.8355591168091168\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.939212317286248, Train acc: 0.8358134920634921\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9382545678024616, Train acc: 0.8368055555555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9375208787202607, Train acc: 0.8373990978157645\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.936786925283253, Train acc: 0.8380608974358974\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9365107738980973, Train acc: 0.8383109945609946\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.935722603247716, Train acc: 0.8391871438746439\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9356860711651365, Train acc: 0.839086127547666\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.935904875925318, Train acc: 0.8387515262515263\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.935560060430456, Train acc: 0.8390847578347579\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.935477153230936, Train acc: 0.8393763354700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9354991899656255, Train acc: 0.8393979386626446\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9349320595986708, Train acc: 0.8400997150997151\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9349207779442037, Train acc: 0.8399966261808367\n",
      "Val loss: 2.884551763534546, Val acc: 0.896\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9235525681422305, Train acc: 0.8525641025641025\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9288594437460613, Train acc: 0.8473557692307693\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.928572136452395, Train acc: 0.8461538461538461\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9327473777991075, Train acc: 0.8420806623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.931801616636097, Train acc: 0.8435363247863248\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.932697757696494, Train acc: 0.8422364672364673\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.932922872431549, Train acc: 0.842032967032967\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9337524033000326, Train acc: 0.8412126068376068\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9345685104359247, Train acc: 0.8402184235517569\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.935006433063083, Train acc: 0.8397702991452991\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.934093353631613, Train acc: 0.840836247086247\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9354608344216633, Train acc: 0.8394542378917379\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9354202618808984, Train acc: 0.8394764957264957\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.934650804242517, Train acc: 0.8403731684981685\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.934331663653382, Train acc: 0.840829772079772\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9342084530836496, Train acc: 0.8409288194444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9341247985565584, Train acc: 0.8408748114630468\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9338570921509355, Train acc: 0.8411087369420702\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.933875319446147, Train acc: 0.8410931174089069\n",
      "Val loss: 2.89705491065979, Val acc: 0.878\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.93309000822214, Train acc: 0.8416132478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9328452868339343, Train acc: 0.8422809829059829\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9308213915919987, Train acc: 0.8449074074074074\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.933351238568624, Train acc: 0.8418135683760684\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9328614426474284, Train acc: 0.8417200854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.932750910096019, Train acc: 0.8424145299145299\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9315231370401906, Train acc: 0.8440934065934066\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.93123227587113, Train acc: 0.8440504807692307\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.93181412647932, Train acc: 0.8432454890788225\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.931150709461962, Train acc: 0.8438034188034188\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.931323947884264, Train acc: 0.8437014374514374\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9308739455676824, Train acc: 0.8442619301994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.931445035395224, Train acc: 0.8435445430637738\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9312909416517785, Train acc: 0.8438072344322345\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9314351017998153, Train acc: 0.8437678062678062\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9309128696083, Train acc: 0.8443008814102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9309413775899054, Train acc: 0.844347033685269\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9317564700516994, Train acc: 0.8434680674264008\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9321480980339856, Train acc: 0.842948717948718\n",
      "Val loss: 2.8859121799468994, Val acc: 0.888\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9370179298596506, Train acc: 0.8376068376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.934999109333397, Train acc: 0.8390758547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9328395158816605, Train acc: 0.8419693732193733\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.932967466166896, Train acc: 0.8420806623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.932921163037292, Train acc: 0.8419337606837607\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.932529214780215, Train acc: 0.8424590455840456\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.93375408693111, Train acc: 0.841231684981685\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9331608172665296, Train acc: 0.8418135683760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9325927329538892, Train acc: 0.8423848528015194\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9331353896703476, Train acc: 0.8415598290598291\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9325175309422042, Train acc: 0.8424388111888111\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.932486225227345, Train acc: 0.8425258190883191\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9323053168748885, Train acc: 0.8425583497698882\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9321535467664837, Train acc: 0.8426625457875457\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.931862526809388, Train acc: 0.843019943019943\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9314944272876806, Train acc: 0.8434328258547008\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9315848459605296, Train acc: 0.8432943690296631\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9317595345115843, Train acc: 0.8430525878442545\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.93148537085392, Train acc: 0.8432579847053532\n",
      "Val loss: 2.8887271881103516, Val acc: 0.886\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.928921275668674, Train acc: 0.84375\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9293150840661464, Train acc: 0.8438835470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.929134222856614, Train acc: 0.84375\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.930369078094124, Train acc: 0.8426816239316239\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9306788570860514, Train acc: 0.8430555555555556\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.929199094106669, Train acc: 0.8448183760683761\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9285029296618825, Train acc: 0.8459249084249084\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9287708243753157, Train acc: 0.8455862713675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.928208425853327, Train acc: 0.8463022317188984\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9275206667745217, Train acc: 0.8472489316239317\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.928087235238076, Train acc: 0.846663752913753\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9281118420454173, Train acc: 0.8465099715099715\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.927928344424346, Train acc: 0.8468113083497699\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9278159578204592, Train acc: 0.8468788156288156\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9280328561777402, Train acc: 0.8465633903133903\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9281870786450868, Train acc: 0.8463875534188035\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.928864301779331, Train acc: 0.8456667923579688\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.929003249432853, Train acc: 0.8455157882241215\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9294042090708965, Train acc: 0.8451698155645524\n",
      "Val loss: 2.895110845565796, Val acc: 0.876\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9249466728960347, Train acc: 0.8482905982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9247475567027035, Train acc: 0.8493589743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.929319349109617, Train acc: 0.8446403133903134\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9303541698007503, Train acc: 0.8433493589743589\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9286755142048895, Train acc: 0.8455662393162393\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.929142295804798, Train acc: 0.8453970797720798\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.929253925479521, Train acc: 0.8450091575091575\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9292465810082917, Train acc: 0.8448851495726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.929669621323588, Train acc: 0.8442248338081672\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.930193131397932, Train acc: 0.8436431623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9293154147399334, Train acc: 0.8444298756798757\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.928792347255935, Train acc: 0.8451967592592593\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9294261714488874, Train acc: 0.8445718277449047\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9292175584951456, Train acc: 0.8449137667887668\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9286762478005173, Train acc: 0.8455128205128205\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.928607625074876, Train acc: 0.8456697382478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9287368641120217, Train acc: 0.8454311211664153\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9282580232574955, Train acc: 0.8459609449192782\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.928058903977724, Train acc: 0.8461819613135403\n",
      "Val loss: 2.8899178504943848, Val acc: 0.886\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9202821478884444, Train acc: 0.8565705128205128\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9241345825358334, Train acc: 0.8509615384615384\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.92233590047244, Train acc: 0.85372150997151\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9241328606238732, Train acc: 0.852096688034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9234001208574343, Train acc: 0.8526175213675213\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.924664751416937, Train acc: 0.8510950854700855\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9256750806638463, Train acc: 0.8497405372405372\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.924787813017511, Train acc: 0.8503605769230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9254081280935638, Train acc: 0.849596391263058\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.925487711490729, Train acc: 0.849599358974359\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.92527160092482, Train acc: 0.8499174436674437\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9254729173801564, Train acc: 0.8498486467236467\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9253619710996226, Train acc: 0.8499342537804077\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9255573599620908, Train acc: 0.8494734432234432\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9258516876785845, Train acc: 0.8491631054131055\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9259344453995046, Train acc: 0.8489249465811965\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9262908170305946, Train acc: 0.8484005781799899\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9264564105474937, Train acc: 0.8482312440645774\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.926641128538323, Train acc: 0.8479672739541161\n",
      "Val loss: 2.898418664932251, Val acc: 0.88\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9279803822183204, Train acc: 0.8474893162393162\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.924132345069168, Train acc: 0.8501602564102564\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9263729261197255, Train acc: 0.8478454415954416\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9257827523427133, Train acc: 0.8483573717948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9266963278126514, Train acc: 0.8477029914529914\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.926255167719306, Train acc: 0.8484241452991453\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.925090002926278, Train acc: 0.8494352869352869\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.924610866440667, Train acc: 0.8497262286324786\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9241563585522283, Train acc: 0.8505460588793922\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9235889605986767, Train acc: 0.8511752136752136\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9234847433361417, Train acc: 0.8513014763014763\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9233049108431888, Train acc: 0.8515402421652422\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.923254617180658, Train acc: 0.8514546351084813\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9231265147235947, Train acc: 0.8514957264957265\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9238810672379625, Train acc: 0.8507122507122508\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9239763748696728, Train acc: 0.8506944444444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.923895643607284, Train acc: 0.8506315987933635\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9238322879287706, Train acc: 0.850798314339981\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.92314498487808, Train acc: 0.851355150697256\n",
      "Val loss: 2.8958475589752197, Val acc: 0.882\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.92306365722265, Train acc: 0.8509615384615384\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.918948687039889, Train acc: 0.8555021367521367\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9223173377860308, Train acc: 0.8524750712250713\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.921765322359199, Train acc: 0.8533653846153846\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9214482274829834, Train acc: 0.8533653846153846\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9210500985468895, Train acc: 0.8536324786324786\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9199372729393445, Train acc: 0.8545863858363858\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.921648752485585, Train acc: 0.8527644230769231\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.922131371067913, Train acc: 0.852059591642925\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9221952030801366, Train acc: 0.8521367521367521\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9228042468787536, Train acc: 0.851277195027195\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9231258922832306, Train acc: 0.8508947649572649\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9228810188724834, Train acc: 0.8511669953977646\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9231483226002934, Train acc: 0.8510569291819292\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9234331098377195, Train acc: 0.8508190883190884\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9233046363816304, Train acc: 0.8510450053418803\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9229970535721352, Train acc: 0.8512757667169432\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.92333143694788, Train acc: 0.8508428300094967\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9235418453122173, Train acc: 0.850567926225821\n",
      "Val loss: 2.8850700855255127, Val acc: 0.888\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.920214110969478, Train acc: 0.8555021367521367\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9227342738045587, Train acc: 0.8526976495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9243654036453988, Train acc: 0.8500712250712251\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.923328129654257, Train acc: 0.8517628205128205\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.924118999334482, Train acc: 0.8510149572649572\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9246593129600895, Train acc: 0.8505608974358975\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9250887689136325, Train acc: 0.8496260683760684\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.925727822841742, Train acc: 0.8490251068376068\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.925926039814383, Train acc: 0.8484389838556505\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.925450666745504, Train acc: 0.8487446581196582\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9251810585516966, Train acc: 0.8487519425019425\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9240783509026227, Train acc: 0.8499599358974359\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9243342606822256, Train acc: 0.8497082511505588\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9240311018858782, Train acc: 0.8499313186813187\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9233694981306027, Train acc: 0.850462962962963\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.923117662469546, Train acc: 0.8506777510683761\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9232818283226814, Train acc: 0.8505844645550528\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9234459166966045, Train acc: 0.8504570275403609\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.923179243150403, Train acc: 0.8507787899235267\n",
      "Val loss: 2.886298418045044, Val acc: 0.888\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.918520428176619, Train acc: 0.8568376068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9192643858428693, Train acc: 0.8552350427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9192066613765184, Train acc: 0.8549679487179487\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.921470262046553, Train acc: 0.8525641025641025\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9215532596294698, Train acc: 0.8521901709401709\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9225328977291403, Train acc: 0.8511841168091168\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.919686881904928, Train acc: 0.8546245421245421\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.920557203201147, Train acc: 0.8537326388888888\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.920168536793925, Train acc: 0.8540479582146249\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9200622827578813, Train acc: 0.8543803418803418\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9196549343710827, Train acc: 0.8547979797979798\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9204013328946212, Train acc: 0.8540998931623932\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.919623209409695, Train acc: 0.8548241288625904\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9199257275414845, Train acc: 0.8544909951159951\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.920217948489719, Train acc: 0.8542378917378918\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9201100653308067, Train acc: 0.8544671474358975\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9201047283851542, Train acc: 0.8544494720965309\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9204465223287017, Train acc: 0.8540034425451092\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9204376109591275, Train acc: 0.8540120332883491\n",
      "Val loss: 2.878328800201416, Val acc: 0.9\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.929157754294893, Train acc: 0.8421474358974359\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9256298919009347, Train acc: 0.8473557692307693\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9270617527160208, Train acc: 0.8466880341880342\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9251598129924545, Train acc: 0.8488247863247863\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9260909162016, Train acc: 0.8479700854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9249820736398724, Train acc: 0.849136396011396\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9240302748441405, Train acc: 0.850236568986569\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.922811917260162, Train acc: 0.8513621794871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.921410792686774, Train acc: 0.8528311965811965\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.921032219259148, Train acc: 0.853258547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9206801136983116, Train acc: 0.8537296037296037\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9207066277153473, Train acc: 0.8537882834757835\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9211604432163076, Train acc: 0.8532010190664037\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.921383481613737, Train acc: 0.8529838217338217\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.920872398115631, Train acc: 0.8534009971509972\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9209534162894273, Train acc: 0.8532819177350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.920763818386993, Train acc: 0.8535382101558572\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.920694612483127, Train acc: 0.8535879629629629\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9209043345792414, Train acc: 0.8533653846153846\n",
      "Val loss: 2.87821626663208, Val acc: 0.896\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9138882160186768, Train acc: 0.8608440170940171\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9129398735160503, Train acc: 0.8612446581196581\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9164324486017907, Train acc: 0.8571047008547008\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.916068837683425, Train acc: 0.8573717948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9168593598227215, Train acc: 0.8568910256410256\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.916728329115104, Train acc: 0.8573717948717948\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.916839106761201, Train acc: 0.8572191697191697\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9176920094551186, Train acc: 0.8563034188034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9175362011872354, Train acc: 0.856451804368471\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9181379886773917, Train acc: 0.8558226495726495\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.918566067643006, Train acc: 0.8552836052836053\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.919761214670632, Train acc: 0.8542111823361823\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9196639513985096, Train acc: 0.8544132149901381\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.919361788711268, Train acc: 0.8546626984126984\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9191904354638862, Train acc: 0.8548076923076923\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9199214820933137, Train acc: 0.854049813034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.919932118248496, Train acc: 0.8541038210155857\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9194915637671097, Train acc: 0.8544634377967711\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.91953139856384, Train acc: 0.854518106162843\n",
      "Val loss: 2.8889000415802, Val acc: 0.886\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9156473824101634, Train acc: 0.8584401709401709\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.919021108211615, Train acc: 0.8549679487179487\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9172342835668146, Train acc: 0.8577279202279202\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9163469178044896, Train acc: 0.8584401709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.917565481071798, Train acc: 0.8573717948717948\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9175733592775135, Train acc: 0.8572827635327636\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9165109432369505, Train acc: 0.8582875457875457\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.917067167086479, Train acc: 0.8572048611111112\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9164867059231256, Train acc: 0.8577279202279202\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.91677198715699, Train acc: 0.8573450854700855\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.91804464213498, Train acc: 0.8560363247863247\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9171426275856476, Train acc: 0.8568821225071225\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9171668514462383, Train acc: 0.8567348783694938\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.917780581120315, Train acc: 0.8560172466422467\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9174529037584267, Train acc: 0.8562678062678063\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9179597961851673, Train acc: 0.8557024572649573\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9179690328898173, Train acc: 0.8556121166415284\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.918131413962427, Train acc: 0.8554724596391263\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.918089480106218, Train acc: 0.8556145973909132\n",
      "Val loss: 2.8889975547790527, Val acc: 0.886\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9143714110056558, Train acc: 0.8608440170940171\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9148338156887608, Train acc: 0.8587072649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.914303451521784, Train acc: 0.8594195156695157\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9149884219862456, Train acc: 0.8590411324786325\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.916443171460404, Train acc: 0.8577457264957264\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9179194666381574, Train acc: 0.8560808404558404\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9168462569896993, Train acc: 0.8571810134310134\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.916970264962596, Train acc: 0.8571380876068376\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9172971194286292, Train acc: 0.8568376068376068\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.916636085917807, Train acc: 0.8574786324786324\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.917184589440821, Train acc: 0.8565705128205128\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.917326988997283, Train acc: 0.8563701923076923\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.917957182858509, Train acc: 0.8557281393819856\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9179453062341616, Train acc: 0.8557310744810744\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.918501782756925, Train acc: 0.8552172364672365\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9183083109111867, Train acc: 0.8554520566239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9182010986746225, Train acc: 0.8556906737053795\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9179301693228914, Train acc: 0.8560214862298196\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.918223092645018, Train acc: 0.8556848852901484\n",
      "Val loss: 2.8847780227661133, Val acc: 0.89\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9159450266096325, Train acc: 0.8576388888888888\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9147843805133786, Train acc: 0.8587072649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9154094793857674, Train acc: 0.8585292022792023\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9158809719941554, Train acc: 0.8577056623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9156192685803797, Train acc: 0.8580128205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9152855411214365, Train acc: 0.8581730769230769\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9156606016869366, Train acc: 0.8578296703296703\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9158128079695578, Train acc: 0.8578392094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.916281525112851, Train acc: 0.8572827635327636\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9168515264478505, Train acc: 0.8567574786324786\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9170160510323266, Train acc: 0.8565947940947941\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9174197209187045, Train acc: 0.8561253561253561\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9172745754184257, Train acc: 0.8562417817225509\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9176176798067104, Train acc: 0.8557501526251526\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9174973182189157, Train acc: 0.8558226495726495\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.917280243630083, Train acc: 0.8560363247863247\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9168177062150162, Train acc: 0.8565233785822021\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.916785295306221, Train acc: 0.8564963200379867\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.916420734094919, Train acc: 0.8568376068376068\n",
      "Val loss: 2.8811941146850586, Val acc: 0.886\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.922982851664225, Train acc: 0.8504273504273504\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.921818860575684, Train acc: 0.8526976495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9261169936242606, Train acc: 0.8482015669515669\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.92460327117871, Train acc: 0.8497596153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9219586902194554, Train acc: 0.8521367521367521\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.920467613089798, Train acc: 0.8533208689458689\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.919349084406982, Train acc: 0.8544337606837606\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.91834436242397, Train acc: 0.85546875\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.917390305998205, Train acc: 0.8564221272554606\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.917216772910876, Train acc: 0.8567040598290598\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.917628671739485, Train acc: 0.8563277000777001\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.917247626516554, Train acc: 0.8565927706552706\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.916970417422734, Train acc: 0.8568376068376068\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9174624530095903, Train acc: 0.856341575091575\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9175545739991713, Train acc: 0.8561787749287749\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.917090081761026, Train acc: 0.8567541399572649\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9174349201330894, Train acc: 0.8563819758672699\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.917250951244394, Train acc: 0.8565111585944919\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.917040337971401, Train acc: 0.8567532613585245\n",
      "Val loss: 2.8815011978149414, Val acc: 0.896\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.932038900179741, Train acc: 0.8402777777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9257654797317634, Train acc: 0.8486912393162394\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9201888447133904, Train acc: 0.8540776353276354\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9182977666202774, Train acc: 0.8552350427350427\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.917500644667536, Train acc: 0.8559294871794871\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9173440250576053, Train acc: 0.8559472934472935\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9172446966462373, Train acc: 0.8558836996336996\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9183952698850226, Train acc: 0.8550347222222222\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9169029915547893, Train acc: 0.856451804368471\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.917192197457338, Train acc: 0.8560096153846154\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.917280459459567, Train acc: 0.8559391996891997\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.91725406161061, Train acc: 0.8559250356125356\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.916864322879297, Train acc: 0.8562623274161736\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9163461395235726, Train acc: 0.8567231379731379\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9160677433013915, Train acc: 0.857051282051282\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9154187196340318, Train acc: 0.8577891292735043\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.91491569425066, Train acc: 0.8583616138763197\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9153735206337736, Train acc: 0.8578614672364673\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.915084960167868, Train acc: 0.8581449617633828\n",
      "Val loss: 2.8769500255584717, Val acc: 0.896\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9050716314560328, Train acc: 0.8693910256410257\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.909259936748407, Train acc: 0.8648504273504274\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9123067665643503, Train acc: 0.8615562678062678\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.912645967088194, Train acc: 0.8615785256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9122094472249347, Train acc: 0.8619123931623932\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.913174290942331, Train acc: 0.8612001424501424\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9137735494909416, Train acc: 0.8602716727716728\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.915206799140343, Train acc: 0.8586404914529915\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.91454386801688, Train acc: 0.8594195156695157\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.913893043077909, Train acc: 0.8600694444444444\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9141535746004568, Train acc: 0.859678515928516\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.91407804849141, Train acc: 0.8596866096866097\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.914339578755195, Train acc: 0.8593852728468113\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.913670999517662, Train acc: 0.8601381257631258\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.913619324828145, Train acc: 0.8600071225071225\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9138279513925567, Train acc: 0.8597589476495726\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9141554984691576, Train acc: 0.8594771241830066\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9144921062791904, Train acc: 0.8591969373219374\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9146586805112826, Train acc: 0.8590305892937472\n",
      "Val loss: 2.8953590393066406, Val acc: 0.874\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9163583343864508, Train acc: 0.8557692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.916832234105493, Train acc: 0.8560363247863247\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.916593104685813, Train acc: 0.8558582621082621\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.917403730062338, Train acc: 0.8553685897435898\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9181168861878226, Train acc: 0.8545405982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.918046970313091, Train acc: 0.8547453703703703\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.915828325227358, Train acc: 0.857257326007326\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.916415085649898, Train acc: 0.8566038995726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.915545554808628, Train acc: 0.8576092117758785\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9149430533759615, Train acc: 0.8582264957264957\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.914361849624053, Train acc: 0.8588772338772339\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.914032135424111, Train acc: 0.8592859686609686\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9138881624097657, Train acc: 0.859344181459566\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9133785728715424, Train acc: 0.8599282661782662\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9133059102245884, Train acc: 0.8600071225071225\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9128523020662813, Train acc: 0.8604266826923077\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9132046253612858, Train acc: 0.8600584464555053\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9128271925483333, Train acc: 0.860502730294397\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9125597775116514, Train acc: 0.8608440170940171\n",
      "Val loss: 2.883885622024536, Val acc: 0.892\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.902007979205531, Train acc: 0.8720619658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.906635850922674, Train acc: 0.8663194444444444\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.907902852762119, Train acc: 0.8648504273504274\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9096515891898393, Train acc: 0.8633814102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.909964277805426, Train acc: 0.8637286324786325\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.910897642119318, Train acc: 0.8628472222222222\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.91108796593616, Train acc: 0.8625228937728938\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.912426937849094, Train acc: 0.8610109508547008\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.911779688294457, Train acc: 0.8617046533713201\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.911498742022066, Train acc: 0.8620192307692308\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9114339155287547, Train acc: 0.8620580808080808\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9113001133981253, Train acc: 0.8621794871794872\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.911986250849165, Train acc: 0.861439842209073\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.911452358895606, Train acc: 0.8618742368742369\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.911545064850071, Train acc: 0.8617521367521368\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9109906094451237, Train acc: 0.8623798076923077\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9112834116512354, Train acc: 0.862100930115636\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9110651809831856, Train acc: 0.8622536799620133\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9116911478424417, Train acc: 0.8616312415654521\n",
      "Val loss: 2.88828444480896, Val acc: 0.88\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9203529011489997, Train acc: 0.8547008547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.92016065833915, Train acc: 0.8543002136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.918317779176935, Train acc: 0.8547008547008547\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9162693365007386, Train acc: 0.8571047008547008\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9147248239598724, Train acc: 0.8584401709401709\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9142316471137892, Train acc: 0.8590633903133903\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9144578897036038, Train acc: 0.8587454212454212\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.913536691512817, Train acc: 0.859642094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.912661385105999, Train acc: 0.8604878917378918\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.913105983000535, Train acc: 0.8600694444444444\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9126764918448114, Train acc: 0.8603583916083916\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9126281172801285, Train acc: 0.8602653133903134\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.913132993427254, Train acc: 0.8598167324128863\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9128213813744477, Train acc: 0.8602144383394383\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.911946706146936, Train acc: 0.8612713675213676\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.912702834631643, Train acc: 0.8604600694444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9127209344780463, Train acc: 0.8604669431875315\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9122680955462985, Train acc: 0.8609182098765432\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.912315246064653, Train acc: 0.8609424201529464\n",
      "Val loss: 2.8802709579467773, Val acc: 0.896\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9071554200262084, Train acc: 0.8653846153846154\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.909143967506213, Train acc: 0.8640491452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9102525045389465, Train acc: 0.8630698005698005\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.910455011913919, Train acc: 0.8631143162393162\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.908441959690844, Train acc: 0.864957264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9088565484750646, Train acc: 0.8647168803418803\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9097660382588706, Train acc: 0.8635149572649573\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9092380817119894, Train acc: 0.8638822115384616\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9097743181534756, Train acc: 0.863366571699905\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.909383999588143, Train acc: 0.8635683760683761\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.909468052270529, Train acc: 0.8637092074592074\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.909898441401642, Train acc: 0.8631365740740741\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.910110127212655, Train acc: 0.8630424063116371\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9098392225738263, Train acc: 0.8633050976800977\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.910224370195655, Train acc: 0.8630341880341881\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.910367966335044, Train acc: 0.8629306891025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.910146655959661, Train acc: 0.8632007290095526\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.909796343116905, Train acc: 0.8636336657169991\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9103037681442308, Train acc: 0.8632056905083221\n",
      "Val loss: 2.880352258682251, Val acc: 0.89\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.904713844641661, Train acc: 0.8693910256410257\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9046108630987315, Train acc: 0.8693910256410257\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.905449354410851, Train acc: 0.8689458689458689\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.907062276815757, Train acc: 0.866920405982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9090576770978096, Train acc: 0.864957264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9089048384261607, Train acc: 0.8652065527065527\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.909602889502296, Train acc: 0.8642017704517705\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9101288721092744, Train acc: 0.8635817307692307\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.91017550190403, Train acc: 0.8635149572649573\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.910519139175741, Train acc: 0.8631410256410257\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9110632620769223, Train acc: 0.8626165501165501\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9109661495923316, Train acc: 0.8627581908831908\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9113647260609463, Train acc: 0.8623233070348455\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.911609750673157, Train acc: 0.8618742368742369\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9105968565003484, Train acc: 0.8628739316239317\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.909950806799098, Train acc: 0.8635650373931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.910145970191591, Train acc: 0.8633578431372549\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.910102975900583, Train acc: 0.8634259259259259\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9102512253226323, Train acc: 0.8633040935672515\n",
      "Val loss: 2.8873438835144043, Val acc: 0.89\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9034711193834615, Train acc: 0.8691239316239316\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9080846605137882, Train acc: 0.8649839743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.90821490056834, Train acc: 0.8651175213675214\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.909210606517955, Train acc: 0.8639155982905983\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9103884729564697, Train acc: 0.8627136752136753\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.911373834324698, Train acc: 0.8620014245014245\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9091219928238417, Train acc: 0.8645451770451771\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.909677513134785, Train acc: 0.8640491452991453\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.909458738220157, Train acc: 0.8642568850902185\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.909558840490814, Train acc: 0.8640758547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9093509761510936, Train acc: 0.864291958041958\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9093587447775056, Train acc: 0.8640714031339032\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9089608971497323, Train acc: 0.864439513477975\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9095242896796147, Train acc: 0.8640491452991453\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9090180307372004, Train acc: 0.8646011396011396\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.908546110503694, Train acc: 0.8652009882478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9085075266700464, Train acc: 0.8652432126696833\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9086987166210005, Train acc: 0.8649394586894587\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9091564279068067, Train acc: 0.8644427575348628\n",
      "Val loss: 2.8810226917266846, Val acc: 0.896\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9041788598411102, Train acc: 0.8691239316239316\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9030336233285756, Train acc: 0.8701923076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9048614977431773, Train acc: 0.8679665242165242\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.906168918833773, Train acc: 0.8670539529914529\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9078535369318774, Train acc: 0.8657051282051282\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.907284237720348, Train acc: 0.8664975071225072\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9072197170630187, Train acc: 0.8662240537240538\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.908440422298562, Train acc: 0.8648504273504274\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9083744202583945, Train acc: 0.8650581671415005\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9076967196586803, Train acc: 0.8657852564102564\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9078454296998304, Train acc: 0.8654817404817405\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9079874485646218, Train acc: 0.8654513888888888\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9076467314337053, Train acc: 0.865836620644313\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9075265818608695, Train acc: 0.865995115995116\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.90805227532346, Train acc: 0.8653490028490028\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9085165153965993, Train acc: 0.8648170405982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9082408984622026, Train acc: 0.8651175213675214\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.907845380639079, Train acc: 0.8655181623931624\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.908396552287937, Train acc: 0.8649910031488979\n",
      "Val loss: 2.8939619064331055, Val acc: 0.882\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.913577990654187, Train acc: 0.8579059829059829\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9093455738491483, Train acc: 0.8623130341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.910488082472755, Train acc: 0.8611111111111112\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.911215039399954, Train acc: 0.8609107905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.911795499997261, Train acc: 0.8603632478632479\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9094284969856936, Train acc: 0.8630698005698005\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9093818606328905, Train acc: 0.8633623321123322\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9086557142754907, Train acc: 0.8639155982905983\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.908933849189809, Train acc: 0.8634852801519468\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9097661880346446, Train acc: 0.8627403846153846\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9095600277486473, Train acc: 0.8631021756021756\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.909186330276337, Train acc: 0.8634704415954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.909274519783035, Train acc: 0.8633505917159763\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.909286235278343, Train acc: 0.8634004884004884\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9088176609104517, Train acc: 0.8638710826210826\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.908324653903643, Train acc: 0.8643997061965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9080195305873664, Train acc: 0.8647875816993464\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9084305895699396, Train acc: 0.8644052706552706\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.908283581778749, Train acc: 0.864498987854251\n",
      "Val loss: 2.8819684982299805, Val acc: 0.894\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9106910534394093, Train acc: 0.8632478632478633\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9129596469748735, Train acc: 0.8620459401709402\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9093562727979787, Train acc: 0.8651175213675214\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9079994549099197, Train acc: 0.8657184829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9084894225128695, Train acc: 0.864957264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.908375649031071, Train acc: 0.8653400997150997\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9089214469923643, Train acc: 0.8646978021978022\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9079068700472512, Train acc: 0.8657184829059829\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9078676917953707, Train acc: 0.865889126305793\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9077565478463456, Train acc: 0.8659188034188035\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9081516601174213, Train acc: 0.8655060217560218\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9084517183127225, Train acc: 0.8652510683760684\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9087176293154284, Train acc: 0.8649531558185405\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9083762521126624, Train acc: 0.8653846153846154\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.907965467186735, Train acc: 0.865758547008547\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.907977769517491, Train acc: 0.8656684027777778\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9077215345736542, Train acc: 0.8658559577677225\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.907847324780702, Train acc: 0.8656517094017094\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9076555830586135, Train acc: 0.8658204003598741\n",
      "Val loss: 2.8771066665649414, Val acc: 0.898\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9057113125792937, Train acc: 0.8667200854700855\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.90859682030148, Train acc: 0.8633814102564102\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.910249714837794, Train acc: 0.8613782051282052\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9084381675108886, Train acc: 0.8631143162393162\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.908932139730861, Train acc: 0.8630341880341881\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.908524767965333, Train acc: 0.8636039886039886\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.908508237901625, Train acc: 0.8636675824175825\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.907905935222267, Train acc: 0.8645165598290598\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9076975557992033, Train acc: 0.8646130104463438\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.908173400112706, Train acc: 0.8642895299145299\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.907797804523459, Train acc: 0.8647047397047397\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.907764210150792, Train acc: 0.8649617165242165\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.907956211759729, Train acc: 0.8647887902695595\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9083212055129444, Train acc: 0.8643925518925519\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9076729803003816, Train acc: 0.8649394586894587\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.907844732205073, Train acc: 0.8649005074786325\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.908034231388251, Train acc: 0.8647718702865762\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9072443401938037, Train acc: 0.8655775166191833\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.907088593861781, Train acc: 0.8657501124606388\n",
      "Val loss: 2.8755736351013184, Val acc: 0.898\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.908023326824873, Train acc: 0.8637820512820513\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9080908481891337, Train acc: 0.8645833333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9049300841796093, Train acc: 0.8680555555555556\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9042073055210276, Train acc: 0.8689903846153846\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.904504076232258, Train acc: 0.8686965811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9029372258064075, Train acc: 0.870147792022792\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9025643788851223, Train acc: 0.8708028083028083\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.903017724681104, Train acc: 0.8703258547008547\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9043835547574903, Train acc: 0.8687381291547959\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9046591334872778, Train acc: 0.8684027777777777\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9051707307775536, Train acc: 0.8678613053613053\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9046120735315175, Train acc: 0.8683226495726496\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.904613944592247, Train acc: 0.868302103879027\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.904415871343042, Train acc: 0.8683989621489622\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.903729930019107, Train acc: 0.8692307692307693\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.903841006195443, Train acc: 0.8692407852564102\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9041849720591935, Train acc: 0.8688411261940674\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9038265250686908, Train acc: 0.8693019943019943\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.903623743953868, Train acc: 0.8694613135402609\n",
      "Val loss: 2.8908472061157227, Val acc: 0.88\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9073924610757422, Train acc: 0.8659188034188035\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9021299129877334, Train acc: 0.8700587606837606\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.906813675182158, Train acc: 0.8656517094017094\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.903842274959271, Train acc: 0.8689236111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.904187343059442, Train acc: 0.8686431623931624\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.904422194190175, Train acc: 0.869079415954416\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9049016299847428, Train acc: 0.8687423687423688\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.904042005029499, Train acc: 0.8695913461538461\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.904148368301102, Train acc: 0.869420702754036\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.904130138698806, Train acc: 0.869417735042735\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.903859338375053, Train acc: 0.8697066822066822\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.90410931429632, Train acc: 0.8694800569800569\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9037045314859045, Train acc: 0.8697608481262328\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.903486117775187, Train acc: 0.8700396825396826\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9035727810655905, Train acc: 0.8699786324786325\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.903590471826048, Train acc: 0.8698417467948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9040934720669878, Train acc: 0.8693753142282554\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.904442756151089, Train acc: 0.868886514719848\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9044492709116523, Train acc: 0.8689411830859199\n",
      "Val loss: 2.8883562088012695, Val acc: 0.884\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9059797841259556, Train acc: 0.8667200854700855\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.907860990263458, Train acc: 0.8643162393162394\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9095880937712146, Train acc: 0.8629807692307693\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9088533867118707, Train acc: 0.8638488247863247\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.907315316159501, Train acc: 0.8656517094017094\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9059576825198965, Train acc: 0.8669871794871795\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.905063853886829, Train acc: 0.8677884615384616\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.905273164439405, Train acc: 0.8675881410256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9054361139154388, Train acc: 0.8673729819563153\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9054475655922523, Train acc: 0.8676282051282052\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9050604291173405, Train acc: 0.8679827117327117\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9057431275348717, Train acc: 0.8673210470085471\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9052835766380354, Train acc: 0.8677268244575936\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9051907941680835, Train acc: 0.8678838522588522\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9046774921254217, Train acc: 0.8683226495726496\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9051676819212418, Train acc: 0.8677383814102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.905203168268717, Train acc: 0.867615635997989\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.905163282342786, Train acc: 0.8676103988603988\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.904979624621674, Train acc: 0.8678306342780027\n",
      "Val loss: 2.872995376586914, Val acc: 0.9\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.902572124432295, Train acc: 0.8707264957264957\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9020880185640774, Train acc: 0.8715277777777778\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9027689990834293, Train acc: 0.8709935897435898\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.900131196547777, Train acc: 0.8739983974358975\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9017919389610616, Train acc: 0.8721153846153846\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9015940831937, Train acc: 0.8721509971509972\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.90210753337222, Train acc: 0.8712988400488401\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.902477882356725, Train acc: 0.8709268162393162\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9019570230645217, Train acc: 0.8714387464387464\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9019160020045747, Train acc: 0.8715544871794871\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.902349686381793, Train acc: 0.8711149961149961\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9021322480973355, Train acc: 0.8712829415954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9018457017393193, Train acc: 0.8715483234714004\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9026068686274527, Train acc: 0.8707455738705738\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.902470533256857, Train acc: 0.8708333333333333\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9022318732279997, Train acc: 0.8710102831196581\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.902973029528388, Train acc: 0.8702237305178482\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.902616864595658, Train acc: 0.8705484330484331\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.902535438323096, Train acc: 0.870543747188484\n",
      "Val loss: 2.8733952045440674, Val acc: 0.9\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8922033595223713, Train acc: 0.8795405982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.894860384810684, Train acc: 0.8775373931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8984484475562375, Train acc: 0.874198717948718\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.899627792020129, Train acc: 0.8727964743589743\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.901559540349194, Train acc: 0.8709935897435898\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9026209442024555, Train acc: 0.8699697293447294\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.903033354925731, Train acc: 0.8694673382173382\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9028547296666694, Train acc: 0.8698918269230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.902054682529663, Train acc: 0.8705484330484331\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9016426467487957, Train acc: 0.870940170940171\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9020050526035788, Train acc: 0.8705565268065268\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9019268892089864, Train acc: 0.8707042378917379\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9024215222659038, Train acc: 0.8702539447731755\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.902901695033715, Train acc: 0.869696275946276\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9031765352287184, Train acc: 0.8694266381766381\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.902473581652356, Train acc: 0.8701422275641025\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.902465911292742, Train acc: 0.870208019105078\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.902493181957705, Train acc: 0.8702368233618234\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.902359543970668, Train acc: 0.87027665317139\n",
      "Val loss: 2.875997543334961, Val acc: 0.894\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.895308822648138, Train acc: 0.8782051282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.899353871997605, Train acc: 0.8740651709401709\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.902318647444418, Train acc: 0.8712606837606838\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9005863997671337, Train acc: 0.8733306623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8993039265657083, Train acc: 0.874465811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9009009827236163, Train acc: 0.8724626068376068\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.901038466210185, Train acc: 0.8721764346764347\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9021778269710703, Train acc: 0.8710603632478633\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9026676605456463, Train acc: 0.8704594017094017\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9022381053011643, Train acc: 0.8707532051282051\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9018764840417255, Train acc: 0.8712121212121212\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.901664645583541, Train acc: 0.8713719729344729\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.90256982316789, Train acc: 0.8705004930966469\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.902710570374979, Train acc: 0.8703067765567766\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.902748615313799, Train acc: 0.8703347578347579\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.902843510365894, Train acc: 0.8702256944444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.90233738162279, Train acc: 0.870679361488185\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9021737693268577, Train acc: 0.8707710113960114\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.902224370503393, Train acc: 0.8707546108861899\n",
      "Val loss: 2.884049654006958, Val acc: 0.888\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9017879168192544, Train acc: 0.8691239316239316\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.899838035942143, Train acc: 0.8716613247863247\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.901445960726833, Train acc: 0.8701923076923077\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.900500280225379, Train acc: 0.8715945512820513\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9002464106959156, Train acc: 0.8722222222222222\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.899634078017667, Train acc: 0.8728187321937322\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.899152960387196, Train acc: 0.8733974358974359\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.899330504429646, Train acc: 0.8732305021367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9000169644441813, Train acc: 0.8725071225071225\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.899657336667053, Train acc: 0.8731036324786324\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8992491018522037, Train acc: 0.8735188422688422\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8992319174981187, Train acc: 0.8736200142450142\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8990823515616775, Train acc: 0.8738288954635108\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.900200675404261, Train acc: 0.8728060134310134\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9002180876555266, Train acc: 0.872738603988604\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.900558037762968, Train acc: 0.8725293803418803\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9007944837774446, Train acc: 0.8722190799396682\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9013937591713943, Train acc: 0.8715574548907882\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.901276575462592, Train acc: 0.8717105263157895\n",
      "Val loss: 2.878535032272339, Val acc: 0.896\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9014964185209355, Train acc: 0.8720619658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.899906927703792, Train acc: 0.8727297008547008\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9022489395576323, Train acc: 0.8709045584045584\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9005578267268644, Train acc: 0.8725961538461539\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8999901506635877, Train acc: 0.8732905982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8996607126333775, Train acc: 0.8739316239316239\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9005819662441117, Train acc: 0.873015873015873\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.901373258003822, Train acc: 0.8722622863247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9004452617985796, Train acc: 0.8729226020892688\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9007586006425385, Train acc: 0.8725961538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9000567760037748, Train acc: 0.8733488733488733\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9004956040626917, Train acc: 0.8730413105413105\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.900393720365997, Train acc: 0.8732741617357002\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.900866989542131, Train acc: 0.8726724664224664\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9001635865268542, Train acc: 0.8733084045584045\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9007450334536724, Train acc: 0.8727964743589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.900809504987008, Train acc: 0.8727846907993967\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9005542685616503, Train acc: 0.8729967948717948\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.900541175553995, Train acc: 0.8730459964012596\n",
      "Val loss: 2.8764231204986572, Val acc: 0.896\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8912940106840215, Train acc: 0.8822115384615384\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.897165520578368, Train acc: 0.8764690170940171\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8982025551320483, Train acc: 0.8757122507122507\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.89917585788629, Train acc: 0.8743322649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9026802715073283, Train acc: 0.8706730769230769\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9027514984125427, Train acc: 0.8709490740740741\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.902456534214509, Train acc: 0.8713751526251526\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9018186893728046, Train acc: 0.8719284188034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.901746544742856, Train acc: 0.8718542260208927\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9010662496599378, Train acc: 0.8723023504273504\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9006930812903864, Train acc: 0.8724990287490287\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.900966969140914, Train acc: 0.8722845441595442\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9016846652723784, Train acc: 0.8715277777777778\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9016250776575014, Train acc: 0.8715850122100122\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9015750266887523, Train acc: 0.8716168091168092\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.901680678511277, Train acc: 0.8715110844017094\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9008690049865247, Train acc: 0.8722819255907491\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9004167854955734, Train acc: 0.8727000237416904\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9002024622259515, Train acc: 0.872919478182636\n",
      "Val loss: 2.876248598098755, Val acc: 0.896\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.902319309038994, Train acc: 0.8701923076923077\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.901289224624634, Train acc: 0.8713942307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9027739223251996, Train acc: 0.8697471509971509\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.901591246454125, Train acc: 0.8711271367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9001522871164176, Train acc: 0.8726495726495727\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.899951773830968, Train acc: 0.8727297008547008\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9010907529474617, Train acc: 0.8712988400488401\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.901302403364426, Train acc: 0.871360844017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.901481458830584, Train acc: 0.8710826210826211\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9012299187163, Train acc: 0.8715544871794871\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9011933853576233, Train acc: 0.8716977466977467\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9007739536782617, Train acc: 0.8720174501424501\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9013859843204557, Train acc: 0.8713634122287969\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9007041826935187, Train acc: 0.8719284188034188\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.900369669976737, Train acc: 0.8721866096866097\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9005370536166377, Train acc: 0.8720285790598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.900709659984928, Train acc: 0.8718262946204123\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9009262501117963, Train acc: 0.8717355175688509\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.900902894612534, Train acc: 0.8717808142150247\n",
      "Val loss: 2.879070997238159, Val acc: 0.892\n",
      "Epoch 64/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8945973551171456, Train acc: 0.8763354700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8964828395435953, Train acc: 0.875267094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.898348984894929, Train acc: 0.8738425925925926\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8984871175554066, Train acc: 0.8731971153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.898209475248288, Train acc: 0.8737713675213675\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8981824712535933, Train acc: 0.874198717948718\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8976676327259403, Train acc: 0.8748092185592186\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.896619407284973, Train acc: 0.8757011217948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.896230018603043, Train acc: 0.8762761158594492\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8969158441592486, Train acc: 0.8756410256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.897386070361134, Train acc: 0.8752185314685315\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.897765737313491, Train acc: 0.874732905982906\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8980114894192286, Train acc: 0.874465811965812\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.898095840790624, Train acc: 0.8744085775335775\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8984899787142067, Train acc: 0.8740740740740741\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8986295690903296, Train acc: 0.8739983974358975\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.898313690874072, Train acc: 0.8743244092508798\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8990782209497796, Train acc: 0.8735754985754985\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.899264359227505, Train acc: 0.8733693207377418\n",
      "Val loss: 2.874854803085327, Val acc: 0.896\n",
      "Epoch 65/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.904429223802355, Train acc: 0.8688568376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9016135829126735, Train acc: 0.8708600427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.900485099211038, Train acc: 0.8722400284900285\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8995725644959345, Train acc: 0.8729300213675214\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.901278867884579, Train acc: 0.8710470085470086\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9022729108136605, Train acc: 0.8702368233618234\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.902299324380318, Train acc: 0.8698870573870574\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.90241538383003, Train acc: 0.8698250534188035\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9017529184209203, Train acc: 0.8704297245963912\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9002583573007175, Train acc: 0.8719551282051282\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.900125083204564, Train acc: 0.8720376845376845\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.899998777612322, Train acc: 0.8722400284900285\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9000137309977916, Train acc: 0.8721441485864563\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8996851124024072, Train acc: 0.8725579975579976\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8995842814105868, Train acc: 0.8726851851851852\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8995043268570533, Train acc: 0.8728131677350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8997304421564394, Train acc: 0.8726589994972348\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8996629647040297, Train acc: 0.8726851851851852\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.899688859378439, Train acc: 0.8727507872244714\n",
      "Val loss: 2.8831775188446045, Val acc: 0.892\n",
      "Epoch 66/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8934289838513756, Train acc: 0.8798076923076923\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8959504406676335, Train acc: 0.8778044871794872\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8947392413419197, Train acc: 0.8786502849002849\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8958773674109044, Train acc: 0.8772702991452992\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8950323149689243, Train acc: 0.8785790598290598\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.895164536614703, Train acc: 0.8784277065527065\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8961473952282915, Train acc: 0.8774420024420024\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.897416720023522, Train acc: 0.8761017628205128\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.898748735643407, Train acc: 0.8747032288698955\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8981625139203846, Train acc: 0.8752136752136752\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.898968104901199, Train acc: 0.8742958430458431\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8985293964375116, Train acc: 0.8747996794871795\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8980698978955775, Train acc: 0.8752465483234714\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.897771228946318, Train acc: 0.8754960317460317\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8973254297533604, Train acc: 0.8759615384615385\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8975903194429526, Train acc: 0.8756677350427351\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.896974019994683, Train acc: 0.8763197586726998\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8968072010014922, Train acc: 0.8765135327635327\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8966531374944635, Train acc: 0.8767009671614935\n",
      "Val loss: 2.879688024520874, Val acc: 0.892\n",
      "Epoch 67/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.901109349014413, Train acc: 0.8717948717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8999249109855065, Train acc: 0.8729967948717948\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8987001971981123, Train acc: 0.8741096866096866\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8982560104793973, Train acc: 0.8749332264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.899572956867707, Train acc: 0.8732905982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8984953419775024, Train acc: 0.8743767806267806\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.897661844307104, Train acc: 0.8751526251526252\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8976220328074236, Train acc: 0.8751335470085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.897461916759596, Train acc: 0.8753561253561254\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.898851844184419, Train acc: 0.8740918803418803\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8989688197864334, Train acc: 0.8740530303030303\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8995706148636646, Train acc: 0.8734419515669516\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.899319860983804, Train acc: 0.8737467126890204\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8988749724750265, Train acc: 0.8740651709401709\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8983104962569017, Train acc: 0.8746260683760684\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.898225580270474, Train acc: 0.8746327457264957\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8985974150964995, Train acc: 0.8742458521870287\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8983416388630303, Train acc: 0.8745548433048433\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.898271052407296, Train acc: 0.8746063877642825\n",
      "Val loss: 2.880652904510498, Val acc: 0.892\n",
      "Epoch 68/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.89530269916241, Train acc: 0.8768696581196581\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.893247229421241, Train acc: 0.8796741452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8925417053733455, Train acc: 0.8800747863247863\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8966290950775146, Train acc: 0.8758680555555556\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8958031597300473, Train acc: 0.8772970085470085\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8951438291799647, Train acc: 0.8779825498575499\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8952392888447593, Train acc: 0.8778617216117216\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.894888223745884, Train acc: 0.8779714209401709\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8960990550284826, Train acc: 0.8765432098765432\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.895470825627319, Train acc: 0.8772168803418804\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8965863016287114, Train acc: 0.8759469696969697\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.896998028130273, Train acc: 0.8755787037037037\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8965579891267534, Train acc: 0.8760478303747534\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8963880668484103, Train acc: 0.8761828449328449\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8957256288609954, Train acc: 0.8767806267806267\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8957124724347367, Train acc: 0.8767361111111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8958278440721914, Train acc: 0.8764768728004022\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8959087957570584, Train acc: 0.8763799857549858\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8957943678223974, Train acc: 0.8764901034637876\n",
      "Val loss: 2.8863189220428467, Val acc: 0.886\n",
      "Epoch 69/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.890798040944287, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.890818554112035, Train acc: 0.8818108974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8942720149656984, Train acc: 0.8777599715099715\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8949617366505485, Train acc: 0.8772702991452992\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8956267344645963, Train acc: 0.8767094017094017\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8951239242852584, Train acc: 0.8771367521367521\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.89581402433952, Train acc: 0.8765262515262515\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8954147734703164, Train acc: 0.8768362713675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8949153921429813, Train acc: 0.8773444919278253\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8951889981571424, Train acc: 0.8771634615384616\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8965719248216653, Train acc: 0.875801282051282\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.897071667036779, Train acc: 0.8753116096866097\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.89752867251615, Train acc: 0.8749178172255095\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8973229928476614, Train acc: 0.8750190781440782\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8973400755825205, Train acc: 0.875\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.897390655345387, Train acc: 0.8749165331196581\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8967868333608435, Train acc: 0.8755656108597285\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.896491728843334, Train acc: 0.8758309591642925\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.896560460664215, Train acc: 0.8757591093117408\n",
      "Val loss: 2.884505033493042, Val acc: 0.888\n",
      "Epoch 70/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9015653683589053, Train acc: 0.8696581196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.898138793105753, Train acc: 0.8735309829059829\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8962068578116913, Train acc: 0.8753561253561254\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8994614232299676, Train acc: 0.8719951923076923\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8998675366752167, Train acc: 0.8718482905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.899875869438519, Train acc: 0.8718839031339032\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8995777445689517, Train acc: 0.8720619658119658\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.898566207060447, Train acc: 0.8735643696581197\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8981525264008208, Train acc: 0.8741096866096866\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8986591320771438, Train acc: 0.8736912393162393\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8984339752212085, Train acc: 0.8739801864801865\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8974731387915433, Train acc: 0.8748887108262108\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8965434001355796, Train acc: 0.8758834648257725\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8965506003453183, Train acc: 0.8758966727716728\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8966237106214563, Train acc: 0.8758190883190883\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.896362156822131, Train acc: 0.876151842948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8959149611901256, Train acc: 0.8766025641025641\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.896236934326766, Train acc: 0.8763503086419753\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8965468183595533, Train acc: 0.8761105488079173\n",
      "Val loss: 2.874769449234009, Val acc: 0.894\n",
      "Epoch 71/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8901164307553544, Train acc: 0.8811431623931624\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.895865808185349, Train acc: 0.8770032051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.892657785334139, Train acc: 0.8803418803418803\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8923317504744244, Train acc: 0.8807425213675214\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.892724637496166, Train acc: 0.8803952991452991\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8926949195372753, Train acc: 0.8804754273504274\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8925392048408405, Train acc: 0.8807234432234432\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8932658357497973, Train acc: 0.8799078525641025\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.893339448052141, Train acc: 0.8798373694207028\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8942695619713548, Train acc: 0.8788194444444445\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8944401035219918, Train acc: 0.8786907536907537\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.894384001394962, Train acc: 0.8787170584045584\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8948301734460333, Train acc: 0.878266765285996\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8948482979493844, Train acc: 0.8781288156288156\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.895435197536762, Train acc: 0.8775997150997151\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8953237978056965, Train acc: 0.8777377136752137\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8954506800245436, Train acc: 0.8775452488687783\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8953406944347355, Train acc: 0.8775522317188984\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.895148670678733, Train acc: 0.8777552856500225\n",
      "Val loss: 2.876518726348877, Val acc: 0.894\n",
      "Epoch 72/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8950187079926843, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.898947682136144, Train acc: 0.8754006410256411\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8971675746461267, Train acc: 0.8766915954415955\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.895500317088559, Train acc: 0.8782051282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.893763847840138, Train acc: 0.8799145299145299\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8931889269087048, Train acc: 0.8806089743589743\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8944077570359785, Train acc: 0.8791590354090354\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.893943718610666, Train acc: 0.8795405982905983\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.894754878702553, Train acc: 0.8786206077872745\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8945831920346645, Train acc: 0.8789797008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8944299085805283, Train acc: 0.879079254079254\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8946511465260105, Train acc: 0.8788283475783476\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.895046452955538, Train acc: 0.8783489480604865\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.895000746544173, Train acc: 0.8783959096459096\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.895067769781477, Train acc: 0.8782407407407408\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.894764893966862, Train acc: 0.8784889155982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8943879683452853, Train acc: 0.8788021618903972\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8944817328611556, Train acc: 0.8787689933523267\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.894343722794327, Train acc: 0.8788517768780927\n",
      "Val loss: 2.8813564777374268, Val acc: 0.888\n",
      "Epoch 73/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8938167421226826, Train acc: 0.8779380341880342\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.896831661208063, Train acc: 0.8762019230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.893713851939579, Train acc: 0.8792735042735043\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8938091896538043, Train acc: 0.8794738247863247\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.892298098099537, Train acc: 0.88125\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8919009112904215, Train acc: 0.8811876780626781\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8925483322842216, Train acc: 0.8804945054945055\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.893142705799168, Train acc: 0.8797743055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.892499817289405, Train acc: 0.8806980056980057\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.89248621443398, Train acc: 0.8806623931623931\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.893029077532871, Train acc: 0.8801233488733489\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.893433751373889, Train acc: 0.8796518874643875\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8935450222208496, Train acc: 0.8795200525969756\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8931183929990647, Train acc: 0.8798458485958486\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8934635571265153, Train acc: 0.8794871794871795\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.894169220302859, Train acc: 0.8787560096153846\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8939387893005493, Train acc: 0.8790692559074912\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.893841346784195, Train acc: 0.8792586657169991\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8938828467035744, Train acc: 0.8793156770130455\n",
      "Val loss: 2.8851277828216553, Val acc: 0.884\n",
      "Epoch 74/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.899318676728469, Train acc: 0.8723290598290598\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.894912911276532, Train acc: 0.8763354700854701\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8937198232721397, Train acc: 0.8774928774928775\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.892556534363673, Train acc: 0.8790064102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.893000370824439, Train acc: 0.8789529914529914\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.892215366716738, Train acc: 0.8798076923076923\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8921940676021927, Train acc: 0.879769536019536\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8941085756334486, Train acc: 0.8777043269230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8941118257564242, Train acc: 0.8779380341880342\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.893789017913688, Train acc: 0.878258547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.893699373306002, Train acc: 0.8783508158508159\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.893375251707528, Train acc: 0.8786502849002849\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.892974127757557, Train acc: 0.8790885930309007\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8938154178661306, Train acc: 0.8782814407814408\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8943798186092975, Train acc: 0.8777065527065527\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8945211154273434, Train acc: 0.8776208600427351\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8944976514401057, Train acc: 0.8777180744092509\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.894803918438193, Train acc: 0.8773444919278253\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.895134447074612, Train acc: 0.8769399460188934\n",
      "Val loss: 2.8710289001464844, Val acc: 0.904\n",
      "Epoch 75/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8998484366979356, Train acc: 0.8733974358974359\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8945370168767424, Train acc: 0.8782051282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.893922230456969, Train acc: 0.8791844729344729\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8931981172317114, Train acc: 0.8798076923076923\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.891742251469539, Train acc: 0.8808760683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8922247631937013, Train acc: 0.8804754273504274\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8940865457858505, Train acc: 0.8785103785103785\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8953472956632957, Train acc: 0.8774038461538461\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.895185468996126, Train acc: 0.8775225546058879\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8947534328851945, Train acc: 0.8780448717948718\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8941789981275914, Train acc: 0.8786907536907537\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.893590697535762, Train acc: 0.8793625356125356\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.893489551262037, Train acc: 0.8794378698224852\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.893696086165087, Train acc: 0.8792735042735043\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.893367601867415, Train acc: 0.8797186609686609\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8932438891412864, Train acc: 0.879707532051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.892772295775876, Train acc: 0.8801376319758673\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8930425342784423, Train acc: 0.879852207977208\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.892727106581708, Train acc: 0.8800747863247863\n",
      "Val loss: 2.8767330646514893, Val acc: 0.898\n",
      "Epoch 76/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.894423564275106, Train acc: 0.8779380341880342\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.896888795062008, Train acc: 0.875534188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8962080376779933, Train acc: 0.8765135327635327\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.896516506488507, Train acc: 0.8765357905982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8958043929858084, Train acc: 0.8768696581196581\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.896479603232142, Train acc: 0.8761574074074074\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.897257589449667, Train acc: 0.8753815628815629\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.896359808424599, Train acc: 0.8762019230769231\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8955890645555167, Train acc: 0.8771367521367521\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8956506562029194, Train acc: 0.8769497863247864\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.894880252747732, Train acc: 0.8777195027195027\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8950038487754997, Train acc: 0.8774038461538461\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8949417687026706, Train acc: 0.8775271203155819\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.89485736032982, Train acc: 0.8776137057387058\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.894717937181478, Train acc: 0.8777777777777778\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8941840816002626, Train acc: 0.8783553685897436\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8945256439868743, Train acc: 0.8780794369029663\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8946562489213425, Train acc: 0.8779677113010447\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8942643626552416, Train acc: 0.8784300494826811\n",
      "Val loss: 2.8763370513916016, Val acc: 0.894\n",
      "Epoch 77/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8964976551186323, Train acc: 0.8768696581196581\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.891675414183201, Train acc: 0.8827457264957265\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8936547329622795, Train acc: 0.8804309116809117\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.893724644795442, Train acc: 0.8799412393162394\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8934769108764127, Train acc: 0.8800213675213675\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8943700056809645, Train acc: 0.8789173789173789\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8948841406457735, Train acc: 0.8780906593406593\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.895371813549955, Train acc: 0.8777711004273504\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8966322318780797, Train acc: 0.8764541785375118\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.895420658894074, Train acc: 0.877590811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8947930130369457, Train acc: 0.8781565656565656\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8938722525566734, Train acc: 0.8790286680911681\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.89408128955974, Train acc: 0.8788214990138067\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.894738875029288, Train acc: 0.8782242063492064\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.894486697922405, Train acc: 0.8783475783475784\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8942479603310938, Train acc: 0.8786057692307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.893810967334615, Train acc: 0.8790692559074912\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8937218499432715, Train acc: 0.878991571699905\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.893955457977384, Train acc: 0.8787674313990104\n",
      "Val loss: 2.8721559047698975, Val acc: 0.898\n",
      "Epoch 78/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8927293883429632, Train acc: 0.8792735042735043\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8916882724843473, Train acc: 0.8803418803418803\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.891568132954785, Train acc: 0.8810541310541311\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.894122900106968, Train acc: 0.8784054487179487\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.893950563006931, Train acc: 0.8785790598290598\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8931970997074052, Train acc: 0.8793625356125356\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.892801714176369, Train acc: 0.8798076923076923\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.893379313314063, Train acc: 0.8789396367521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8931174814870895, Train acc: 0.8792141500474834\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.892798981299767, Train acc: 0.8796474358974359\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.893289503980813, Train acc: 0.879249222999223\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.892825524861317, Train acc: 0.8796964031339032\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8926134755312023, Train acc: 0.879930966469428\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8925248051155474, Train acc: 0.8800557081807082\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8922136669484977, Train acc: 0.8804843304843305\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8923975467427163, Train acc: 0.8802250267094017\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8921722559068237, Train acc: 0.8804204374057315\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8917610533217077, Train acc: 0.8807276828110161\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.891916953064116, Train acc: 0.8805527440395862\n",
      "Val loss: 2.877736806869507, Val acc: 0.894\n",
      "Epoch 79/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8838061715802574, Train acc: 0.8883547008547008\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.886788937780592, Train acc: 0.8852831196581197\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8873322465141276, Train acc: 0.8854166666666666\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.889044156441322, Train acc: 0.8839476495726496\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.891180432556022, Train acc: 0.8814102564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8902700572271973, Train acc: 0.8822560541310541\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.89018751529838, Train acc: 0.8826312576312576\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8902262260771203, Train acc: 0.8827123397435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.890526880226244, Train acc: 0.8826566951566952\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.890327768244295, Train acc: 0.8827457264957265\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.891288195827161, Train acc: 0.8816287878787878\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.891836306648037, Train acc: 0.8810986467236467\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.891711188648031, Train acc: 0.8810815253122946\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.891434870068989, Train acc: 0.8813911782661783\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.891245128490307, Train acc: 0.8814102564102564\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8912966758267493, Train acc: 0.881393563034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8917686561653517, Train acc: 0.8809389140271493\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8920719439034674, Train acc: 0.8806238129154795\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8918157031393457, Train acc: 0.8808901259559154\n",
      "Val loss: 2.884535312652588, Val acc: 0.89\n",
      "Epoch 80/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9003571608127694, Train acc: 0.874465811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.896483907332787, Train acc: 0.8770032051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.894081034212031, Train acc: 0.8791844729344729\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8939687421179223, Train acc: 0.8789396367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8930102111946825, Train acc: 0.8799145299145299\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8935156160610016, Train acc: 0.8794515669515669\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8923518223931355, Train acc: 0.8803800366300366\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.892070642648599, Train acc: 0.8807091346153846\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.892438692471467, Train acc: 0.8802825261158594\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8921937852843196, Train acc: 0.8804220085470086\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8927843837471277, Train acc: 0.8797105672105672\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.892484345836857, Train acc: 0.8801638176638177\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.891516055032503, Train acc: 0.8811226166995397\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.891729171168382, Train acc: 0.880818833943834\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.891478001765716, Train acc: 0.8809650997150997\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8918230751386056, Train acc: 0.8806089743589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8914187937420537, Train acc: 0.8809703368526898\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.891717515666487, Train acc: 0.880579297245964\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.89144691056729, Train acc: 0.8809182411156096\n",
      "Val loss: 2.8802568912506104, Val acc: 0.892\n",
      "Epoch 81/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9004272701393843, Train acc: 0.8709935897435898\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.899378976251325, Train acc: 0.8727297008547008\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.897408132199888, Train acc: 0.875\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.898247405504569, Train acc: 0.8739983974358975\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8953398358108653, Train acc: 0.8771367521367521\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8947518177521534, Train acc: 0.8776264245014245\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8938586851616046, Train acc: 0.8784722222222222\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8944339864274378, Train acc: 0.8779046474358975\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.893792693997607, Train acc: 0.8785612535612536\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8934847850065966, Train acc: 0.8788194444444445\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8931449685989654, Train acc: 0.8794434731934732\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.89263788926975, Train acc: 0.8798299501424501\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8927161072838237, Train acc: 0.8797460552268245\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8925285508198906, Train acc: 0.8798840048840049\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8931109822373786, Train acc: 0.8793803418803419\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8925782775777016, Train acc: 0.8799245459401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8921201197929634, Train acc: 0.8803418803418803\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8919135221162526, Train acc: 0.8805199430199431\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.892130441082205, Train acc: 0.8802997076023392\n",
      "Val loss: 2.8750059604644775, Val acc: 0.902\n",
      "Epoch 82/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.892946968730698, Train acc: 0.8806089743589743\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8887487548029323, Train acc: 0.8842147435897436\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8890319964145323, Train acc: 0.8839921652421653\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8892807425596776, Train acc: 0.8832131410256411\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.889109089028122, Train acc: 0.8833333333333333\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8899486445293805, Train acc: 0.8823450854700855\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8899253625718373, Train acc: 0.882554945054945\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8897951550972767, Train acc: 0.8827457264957265\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.889125112567985, Train acc: 0.8834283000949668\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8892037528192893, Train acc: 0.8834134615384616\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.888781640913103, Train acc: 0.8839112276612276\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8893141367836215, Train acc: 0.8833689458689459\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8895636851030457, Train acc: 0.8832388231426693\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.889943780625405, Train acc: 0.8829174297924298\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.88959394710356, Train acc: 0.883244301994302\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8892244132410765, Train acc: 0.8835637019230769\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.88914677640072, Train acc: 0.8835941427853192\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8891261920177245, Train acc: 0.8835618471035138\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8893507401041227, Train acc: 0.8833361448493028\n",
      "Val loss: 2.8810927867889404, Val acc: 0.892\n",
      "Epoch 83/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.890621511345236, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8912151583239565, Train acc: 0.8812767094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8911254711640186, Train acc: 0.8814992877492878\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.889871443948175, Train acc: 0.8824786324786325\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.887775942403027, Train acc: 0.8844017094017094\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8873038387026884, Train acc: 0.8850605413105413\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8869711290203464, Train acc: 0.8853021978021978\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8866090558023534, Train acc: 0.8856503739316239\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.886529582404909, Train acc: 0.8859508547008547\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8865834668151336, Train acc: 0.8859508547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8869559020151345, Train acc: 0.8856594794094794\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8865241899109972, Train acc: 0.8859731125356125\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.887254826259174, Train acc: 0.885293392504931\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.887256537601625, Train acc: 0.8853021978021978\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8878817176547145, Train acc: 0.8846331908831909\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.888946206523822, Train acc: 0.8835470085470085\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8892581986685744, Train acc: 0.8831385118149824\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8886122865441406, Train acc: 0.8837695868945868\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8886501756917773, Train acc: 0.8837438146648673\n",
      "Val loss: 2.8802270889282227, Val acc: 0.892\n",
      "Epoch 84/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.887937046523787, Train acc: 0.8840811965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.890781451494266, Train acc: 0.8815438034188035\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8904582543930095, Train acc: 0.8815883190883191\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8919993073512344, Train acc: 0.8799412393162394\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.890545146485679, Train acc: 0.8815705128205128\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8893885843434566, Train acc: 0.8830128205128205\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8904658895126922, Train acc: 0.8819062881562881\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.891080869568719, Train acc: 0.8813434829059829\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8909018531031303, Train acc: 0.8816179962013295\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8907500226273495, Train acc: 0.8817574786324787\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.890153623127437, Train acc: 0.8824057886557887\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.889868724889565, Train acc: 0.8827457264957265\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.889758962145924, Train acc: 0.8828073635765944\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8901320575066682, Train acc: 0.8824213980463981\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.890237813933283, Train acc: 0.8822293447293448\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8898785340989757, Train acc: 0.8825787927350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.889882814830245, Train acc: 0.8825414781297134\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8900657355615555, Train acc: 0.8824637939221273\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8901462546351575, Train acc: 0.8824505173189384\n",
      "Val loss: 2.876650810241699, Val acc: 0.894\n",
      "Epoch 85/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8841981561774883, Train acc: 0.8867521367521367\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.888178529902401, Train acc: 0.8832799145299145\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8911646173210905, Train acc: 0.8807870370370371\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.890651598954812, Train acc: 0.8810763888888888\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8908541532663197, Train acc: 0.8809294871794872\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.890753016512618, Train acc: 0.8810096153846154\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.890721149933644, Train acc: 0.8812576312576312\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8899298809532428, Train acc: 0.8822449252136753\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8892435017700775, Train acc: 0.8829534662867996\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.88878369738913, Train acc: 0.8833867521367521\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8881784061544518, Train acc: 0.8839112276612276\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.887942943647716, Train acc: 0.8843037749287749\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.888183628548767, Train acc: 0.8841222879684418\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8882074913553555, Train acc: 0.8841575091575091\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.888348912377643, Train acc: 0.8839565527065527\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8880210593979583, Train acc: 0.8843315972222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.888063454280372, Train acc: 0.8842068878833584\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.887949528857174, Train acc: 0.8843928062678063\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.888254492013668, Train acc: 0.8840952541610436\n",
      "Val loss: 2.874518871307373, Val acc: 0.896\n",
      "Epoch 86/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8911534293085084, Train acc: 0.8814102564102564\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8838629600329275, Train acc: 0.8883547008547008\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8851743878802005, Train acc: 0.8871082621082621\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8858585760124726, Train acc: 0.8863514957264957\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8872205330775333, Train acc: 0.8849358974358974\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.886394724207386, Train acc: 0.8858618233618234\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.887208027717395, Train acc: 0.8846916971916972\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8875562405993795, Train acc: 0.8843482905982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8873720635489293, Train acc: 0.8844966761633428\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.887244292609712, Train acc: 0.8846153846153846\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8881146566943783, Train acc: 0.8838869463869464\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8880821955509677, Train acc: 0.8841257122507122\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.888227257424479, Train acc: 0.8839990138067061\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8886191181210807, Train acc: 0.8835851648351648\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8885229893219777, Train acc: 0.8838141025641025\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.888703905364387, Train acc: 0.8836972489316239\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.888694729260789, Train acc: 0.8838298139768728\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8884647126890655, Train acc: 0.8840811965811965\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.887892877524681, Train acc: 0.8847419028340081\n",
      "Val loss: 2.8689699172973633, Val acc: 0.902\n",
      "Epoch 87/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.88352600325886, Train acc: 0.8888888888888888\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.884698558057475, Train acc: 0.8875534188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8861719448002656, Train acc: 0.8865740740740741\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8876229534801254, Train acc: 0.8845486111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8876926177587263, Train acc: 0.8844017094017094\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8889400242400645, Train acc: 0.8829683048433048\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.889972302212092, Train acc: 0.8819444444444444\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.889482353997027, Train acc: 0.8826121794871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.889836013147294, Train acc: 0.8824489553656221\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8898426009039593, Train acc: 0.882318376068376\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8899332804557605, Train acc: 0.8822115384615384\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8890728391813076, Train acc: 0.8832131410256411\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.889699791388477, Train acc: 0.8824169953977646\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8897085485295353, Train acc: 0.8824213980463981\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.889198872166821, Train acc: 0.8830306267806268\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.889346439232174, Train acc: 0.8829460470085471\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8892032484003742, Train acc: 0.8830128205128205\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8889708869930457, Train acc: 0.8833392687559354\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.889078994833345, Train acc: 0.8832658569500674\n",
      "Val loss: 2.8728458881378174, Val acc: 0.9\n",
      "Epoch 88/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.894784654307569, Train acc: 0.8766025641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8916328768444877, Train acc: 0.8804754273504274\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8860834174686008, Train acc: 0.8864850427350427\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8866820911056976, Train acc: 0.8858173076923077\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8900621410109038, Train acc: 0.8827457264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.889597016861636, Train acc: 0.8834134615384616\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.888741940951318, Train acc: 0.8843482905982906\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8896801087081943, Train acc: 0.883346688034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.88900889381271, Train acc: 0.8840811965811965\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.888960995225825, Train acc: 0.8839743589743589\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8886313521778666, Train acc: 0.8842511655011654\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8878288839617348, Train acc: 0.8851718304843305\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8878488167581553, Train acc: 0.8850468441814595\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8873696512063924, Train acc: 0.8856074481074481\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8873424786787765, Train acc: 0.885505698005698\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8869908165473204, Train acc: 0.8858340010683761\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.887155475060265, Train acc: 0.8856052036199095\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.887245083919391, Train acc: 0.8854908594491928\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.887274251239449, Train acc: 0.8855291273054431\n",
      "Val loss: 2.872534990310669, Val acc: 0.898\n",
      "Epoch 89/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8942686268407054, Train acc: 0.8790064102564102\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8910313141651645, Train acc: 0.8818108974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8884834876427283, Train acc: 0.8848824786324786\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.888891339302063, Train acc: 0.8842815170940171\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8879860625307785, Train acc: 0.8848824786324786\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8876529120312115, Train acc: 0.8850160256410257\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8885309233333603, Train acc: 0.8840430402930403\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8874885969691806, Train acc: 0.8849492521367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.887537228076207, Train acc: 0.8849121557454891\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8876249859475682, Train acc: 0.884775641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.888396895756162, Train acc: 0.8839840714840714\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.888188398291922, Train acc: 0.8842147435897436\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.887969599986531, Train acc: 0.8843071992110454\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.887843480185857, Train acc: 0.8843482905982906\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8876970425630226, Train acc: 0.8845975783475784\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8875744878991036, Train acc: 0.8847823183760684\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8873552371054334, Train acc: 0.8849924585218703\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.88743116962038, Train acc: 0.8848973171889839\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.887393818103243, Train acc: 0.8850371120107963\n",
      "Val loss: 2.8749730587005615, Val acc: 0.896\n",
      "Epoch 90/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.889768900015415, Train acc: 0.8835470085470085\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.893473533483652, Train acc: 0.8798076923076923\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8921791977352567, Train acc: 0.8807870370370371\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.890660526915493, Train acc: 0.8820779914529915\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8896962952410052, Train acc: 0.8829594017094017\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8894003622891895, Train acc: 0.8831908831908832\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8887184396913783, Train acc: 0.8839285714285714\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8880258082834063, Train acc: 0.8846487713675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.888740044826569, Train acc: 0.883843779677113\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8881453534476775, Train acc: 0.8842147435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.888430237306952, Train acc: 0.883935508935509\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.888274122676958, Train acc: 0.8840366809116809\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8888205386242687, Train acc: 0.8835059171597633\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.889100034012754, Train acc: 0.8832608363858364\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8891734240061875, Train acc: 0.8830840455840456\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8894591671534076, Train acc: 0.8829126602564102\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8890521226665253, Train acc: 0.8834370286576169\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.888531497061422, Train acc: 0.8839624881291548\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8883982506948067, Train acc: 0.8840530814215025\n",
      "Val loss: 2.874990701675415, Val acc: 0.896\n",
      "Epoch 91/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.882764840737367, Train acc: 0.8918269230769231\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8867780927918916, Train acc: 0.8870192307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.888137190430253, Train acc: 0.8853276353276354\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.891240771509643, Train acc: 0.8820779914529915\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.890004642600687, Train acc: 0.8827457264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8897760393273115, Train acc: 0.8830128205128205\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8901071865771133, Train acc: 0.8825167887667887\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.890070770031367, Train acc: 0.8824786324786325\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.889300509622181, Train acc: 0.8833689458689459\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8886634967265983, Train acc: 0.8837606837606837\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.889062159359687, Train acc: 0.883498445998446\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8881657019979254, Train acc: 0.8843482905982906\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8881850168941683, Train acc: 0.8843482905982906\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.887906429677365, Train acc: 0.8845772283272283\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.888014869744282, Train acc: 0.8844551282051282\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.887631550176531, Train acc: 0.8849158653846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.887836532055762, Train acc: 0.8846310960281548\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.88767581515842, Train acc: 0.8847934472934473\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8878792620327256, Train acc: 0.8845591542959964\n",
      "Val loss: 2.8755407333374023, Val acc: 0.898\n",
      "Epoch 92/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8894366015735855, Train acc: 0.8824786324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.888385267339201, Train acc: 0.8830128205128205\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.890836358410001, Train acc: 0.8810541310541311\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.889426506482638, Train acc: 0.8824786324786325\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.888824192682902, Train acc: 0.8830128205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8882902147423506, Train acc: 0.8837695868945868\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.886985685653593, Train acc: 0.884806166056166\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.886861910677364, Train acc: 0.8853498931623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.886769880030343, Train acc: 0.8854760208926875\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8865563231655673, Train acc: 0.8856837606837606\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8858343628644016, Train acc: 0.8865336052836053\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.885846116603949, Train acc: 0.8864850427350427\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8851086703042776, Train acc: 0.8872863247863247\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8857641412224964, Train acc: 0.8866758241758241\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8854224286527717, Train acc: 0.8870192307692307\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8860773738378134, Train acc: 0.8864015758547008\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8859585877816962, Train acc: 0.8864536199095022\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.885660603741522, Train acc: 0.8868263295346629\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8856630742523084, Train acc: 0.8868786549707602\n",
      "Val loss: 2.87392258644104, Val acc: 0.896\n",
      "Epoch 93/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8795299856071797, Train acc: 0.8931623931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.884401480356852, Train acc: 0.8895566239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8848396251004647, Train acc: 0.8887998575498576\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8854836510796833, Train acc: 0.8876201923076923\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8854045835315674, Train acc: 0.8875534188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8854136789626206, Train acc: 0.8873308404558404\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8864935029557337, Train acc: 0.8860653235653235\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.88658985750288, Train acc: 0.8861511752136753\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.886278524244839, Train acc: 0.8865147198480532\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.887107436066, Train acc: 0.8856837606837606\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.886650635275437, Train acc: 0.885975135975136\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.886917008633627, Train acc: 0.8857505341880342\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.886819269059913, Train acc: 0.8858686719263642\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.887106947846465, Train acc: 0.8856074481074481\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8869773957124805, Train acc: 0.8857549857549858\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8870908294478035, Train acc: 0.8855335202991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.886735049847564, Train acc: 0.8860294117647058\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8870071884800113, Train acc: 0.8856540835707503\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8868540678268824, Train acc: 0.885768106162843\n",
      "Val loss: 2.8676555156707764, Val acc: 0.904\n",
      "Epoch 94/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8900277940636006, Train acc: 0.8811431623931624\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8893480514868712, Train acc: 0.8818108974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.887876804058368, Train acc: 0.8831908831908832\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.88830703496933, Train acc: 0.8827457264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8863911918085865, Train acc: 0.8849893162393162\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8864981217941326, Train acc: 0.8848824786324786\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8871905800769326, Train acc: 0.8844246031746031\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.886664079549985, Train acc: 0.8849158653846154\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.886292491543327, Train acc: 0.8853869895536562\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.886290167947101, Train acc: 0.8854700854700854\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.886021881948262, Train acc: 0.8859022921522921\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8864445178597062, Train acc: 0.8854834401709402\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8864274811854416, Train acc: 0.8856221236028928\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8858744057397994, Train acc: 0.8861797924297924\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.88579597405219, Train acc: 0.8862535612535613\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8857498838860765, Train acc: 0.8863681891025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.885652051612083, Train acc: 0.8864064856711915\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8856442922200913, Train acc: 0.8864405270655271\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8858246262739544, Train acc: 0.8862320062977957\n",
      "Val loss: 2.8690552711486816, Val acc: 0.902\n",
      "Epoch 95/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8788029271313267, Train acc: 0.8928952991452992\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8884400710081444, Train acc: 0.8834134615384616\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.884641472770278, Train acc: 0.8871972934472935\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8846969303921757, Train acc: 0.8876201923076923\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.884893683083037, Train acc: 0.8870726495726495\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.884841653356525, Train acc: 0.8870192307692307\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8841395034603727, Train acc: 0.8877442002442002\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.885518454843097, Train acc: 0.8863181089743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.885485710921111, Train acc: 0.8864553656220323\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8853451745122927, Train acc: 0.8866987179487179\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8853122517408654, Train acc: 0.886776418026418\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.885486820144871, Train acc: 0.8866853632478633\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.885324961726874, Train acc: 0.8867521367521367\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8851450268311085, Train acc: 0.8869619963369964\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8851795109588534, Train acc: 0.8870370370370371\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.885028970292491, Train acc: 0.8872362446581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8851173251282933, Train acc: 0.8870035193564605\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.884883154717832, Train acc: 0.8872863247863247\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.884436506223314, Train acc: 0.8877361673414305\n",
      "Val loss: 2.8747012615203857, Val acc: 0.898\n",
      "Epoch 96/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8832017111982036, Train acc: 0.8904914529914529\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8850451779161763, Train acc: 0.8872863247863247\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8869775007253358, Train acc: 0.8853276353276354\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.886562383073008, Train acc: 0.8859508547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8864342750647127, Train acc: 0.8858974358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8853530914355545, Train acc: 0.8871527777777778\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8856429765512654, Train acc: 0.8865995115995116\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8858298804515448, Train acc: 0.8863848824786325\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8845663113698206, Train acc: 0.8877314814814815\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.88452653212425, Train acc: 0.8878472222222222\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8838735097308628, Train acc: 0.8885975135975136\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8832508244066157, Train acc: 0.8892004985754985\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.883108893481624, Train acc: 0.889258711374096\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8830190501079165, Train acc: 0.8893658424908425\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8833321497990534, Train acc: 0.8890847578347578\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.883372281988462, Train acc: 0.8890057425213675\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.883340226159496, Train acc: 0.8890931372549019\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.883291312998403, Train acc: 0.8892004985754985\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.883731641481977, Train acc: 0.8887061403508771\n",
      "Val loss: 2.8721933364868164, Val acc: 0.9\n",
      "Epoch 97/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8794409026447525, Train acc: 0.8936965811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8803590016487317, Train acc: 0.8924946581196581\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8828476011922897, Train acc: 0.8902243589743589\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8825056675152902, Train acc: 0.8898904914529915\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8832189718882244, Train acc: 0.8887820512820512\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.882553062208018, Train acc: 0.8892450142450142\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8834539900769243, Train acc: 0.8883547008547008\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8834364732615967, Train acc: 0.8883213141025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.883808215459188, Train acc: 0.8879095441595442\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.884826786701496, Train acc: 0.8868589743589743\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8848827781840267, Train acc: 0.8868978243978244\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8838733663925757, Train acc: 0.8879763176638177\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.883716122560169, Train acc: 0.8881697896120972\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8838697361858774, Train acc: 0.8880494505494505\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8837714072985525, Train acc: 0.888301282051282\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8835228797462253, Train acc: 0.8885717147435898\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8835670745450686, Train acc: 0.8885589492207139\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8837323282745375, Train acc: 0.8882805080721747\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8841512353367706, Train acc: 0.8878486279802069\n",
      "Val loss: 2.8721649646759033, Val acc: 0.9\n",
      "Epoch 98/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.891267413766975, Train acc: 0.8819444444444444\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.88860034229409, Train acc: 0.8839476495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.886465683961526, Train acc: 0.8858618233618234\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8867131746732273, Train acc: 0.8858173076923077\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.888288308412601, Train acc: 0.8845619658119658\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8876537492132592, Train acc: 0.8853276353276354\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.887202553987794, Train acc: 0.8857982295482295\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8873589711311536, Train acc: 0.8853165064102564\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.886847407729537, Train acc: 0.8855947293447294\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.885410780376858, Train acc: 0.8872329059829059\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8852895558482468, Train acc: 0.8873591686091686\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8847420271645245, Train acc: 0.887931801994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8843452300310606, Train acc: 0.8883341551610783\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8844420246152214, Train acc: 0.8881639194139194\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8843526855153576, Train acc: 0.8882122507122507\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.884618772782831, Train acc: 0.8878372061965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.884561109926426, Train acc: 0.8878048014077425\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8844388348650956, Train acc: 0.8879392212725546\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8844947142478747, Train acc: 0.8877923976608187\n",
      "Val loss: 2.8645567893981934, Val acc: 0.906\n",
      "Epoch 99/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8814934339278784, Train acc: 0.8915598290598291\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8751711865775604, Train acc: 0.8979700854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8820171756961748, Train acc: 0.8904024216524217\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8811272153487573, Train acc: 0.8909588675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8811953556843295, Train acc: 0.8908653846153847\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8816615974801216, Train acc: 0.8903133903133903\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.88151969228472, Train acc: 0.8905677655677655\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.882313631538652, Train acc: 0.8897903311965812\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8825430575712225, Train acc: 0.8896011396011396\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.883100088437398, Train acc: 0.8889423076923076\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.882332933903111, Train acc: 0.889787296037296\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.882279538191282, Train acc: 0.8898014601139601\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8820137891386937, Train acc: 0.8899983563445102\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.882101342209384, Train acc: 0.8899763431013431\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.88212749326331, Train acc: 0.8899928774928775\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.882290567470412, Train acc: 0.8899238782051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.882646251168306, Train acc: 0.8895959024635495\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.882899646632239, Train acc: 0.8892746913580247\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.883025428168794, Train acc: 0.8891138101664418\n",
      "Val loss: 2.8753836154937744, Val acc: 0.896\n",
      "Epoch 100/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8741649807008924, Train acc: 0.8985042735042735\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8832034972997813, Train acc: 0.8890224358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.880909302975038, Train acc: 0.8908475783475783\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.880326076450511, Train acc: 0.8915598290598291\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8810531628437532, Train acc: 0.8910256410256411\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8811358006251844, Train acc: 0.8908475783475783\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8808899254734843, Train acc: 0.8909493284493285\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.88153699804575, Train acc: 0.8902243589743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8827155521679013, Train acc: 0.8891559829059829\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.882879532300509, Train acc: 0.8890491452991452\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.882723983351167, Train acc: 0.889253108003108\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8834741127457035, Train acc: 0.8884214743589743\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8833108815137374, Train acc: 0.8886628862590401\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.883236291353228, Train acc: 0.8887553418803419\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8836083198884275, Train acc: 0.8884615384615384\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.883460721016949, Train acc: 0.8887219551282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.882966131346498, Train acc: 0.8892031171442936\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8826977761722357, Train acc: 0.8894527540360874\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8826337837711735, Train acc: 0.8895917678812416\n",
      "Val loss: 2.865480661392212, Val acc: 0.908\n",
      "Tiempo total de entrenamiento: 2052.0049 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-5\n",
    "dropout_p = 0.6\n",
    "batch_size = 32\n",
    "criterion = perdida_regularizada_entropia\n",
    "epochs = 100\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    metadata_train,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    metadata_val,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    "    beta=0.1,\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.88      0.91      0.90       100\n",
      "          SN       0.83      0.82      0.82       100\n",
      "          VS       0.96      0.91      0.93       100\n",
      "    asteroid       0.86      0.95      0.90       100\n",
      "       bogus       0.98      0.91      0.94       100\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.90      0.90       500\n",
      "weighted avg       0.90      0.90      0.90       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Asegúrate de tener tus datos de test preparados\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.7173804156800623, Train acc: 0.05422008547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.6469257723571906, Train acc: 0.12393162393162394\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.6216087042436302, Train acc: 0.14948361823361822\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.6073600868893485, Train acc: 0.16346153846153846\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.5985176155709815, Train acc: 0.17291666666666666\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.590837864454655, Train acc: 0.18126780626780628\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.584566124192961, Train acc: 0.18757631257631258\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.578264730863082, Train acc: 0.19521233974358973\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.572277098979026, Train acc: 0.20278371320037986\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.5671333451556344, Train acc: 0.21006944444444445\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.56223762118733, Train acc: 0.21690462315462317\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.5563504352868454, Train acc: 0.2255386396011396\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.549929739690626, Train acc: 0.2351249178172255\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.5421551594076286, Train acc: 0.2463560744810745\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.5328329957108893, Train acc: 0.2584757834757835\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.5231699477403593, Train acc: 0.27024906517094016\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.5134533175635783, Train acc: 0.2812185771744595\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.5038007380049905, Train acc: 0.29191892212725545\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.4940270251429837, Train acc: 0.30240665766981556\n",
      "Val loss: 3.2738184928894043, Val acc: 0.508\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.308710829824464, Train acc: 0.4970619658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.308405469625424, Train acc: 0.4987980769230769\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.3069559889301616, Train acc: 0.4983974358974359\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.303434016358139, Train acc: 0.5010683760683761\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.300838112627339, Train acc: 0.5036324786324786\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.295984226753909, Train acc: 0.5076566951566952\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.2927683782519295, Train acc: 0.5106074481074481\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.2901525989047484, Train acc: 0.5125868055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.287395936245026, Train acc: 0.5151353276353277\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.2850704219606186, Train acc: 0.5171207264957265\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.282462354769703, Train acc: 0.5199592074592074\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.27977082685188, Train acc: 0.5223691239316239\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.277899298050308, Train acc: 0.5241822813938198\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.2764301762912735, Train acc: 0.5250496031746031\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.2741349847907695, Train acc: 0.5273504273504274\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.2720852594854484, Train acc: 0.5293469551282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.271427087534487, Train acc: 0.5297574157868276\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.2703274115990824, Train acc: 0.5304932336182336\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.2690523296018146, Train acc: 0.5314889788573999\n",
      "Val loss: 3.208071231842041, Val acc: 0.572\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.222151257034041, Train acc: 0.5715811965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.2237440541259246, Train acc: 0.5727831196581197\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.2256058029979044, Train acc: 0.5706018518518519\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.229271484236432, Train acc: 0.5666399572649573\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.2323305178911257, Train acc: 0.5628205128205128\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.2310180694628983, Train acc: 0.5645922364672364\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.2277007894783813, Train acc: 0.5684905372405372\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.2272033895182815, Train acc: 0.5685430021367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.226309800306503, Train acc: 0.5698005698005698\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.2255931640282656, Train acc: 0.5707532051282052\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.2254610841553992, Train acc: 0.5702457264957265\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.2248220846184297, Train acc: 0.5706241096866097\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.223777835264087, Train acc: 0.5713757396449705\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.2225007970108943, Train acc: 0.5725160256410257\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.2227336484142857, Train acc: 0.5718304843304843\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.2220011880764594, Train acc: 0.5726161858974359\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.221770980718928, Train acc: 0.5726181498240321\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.2209154233860042, Train acc: 0.5737476258309592\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.2197650363731642, Train acc: 0.5749128430049483\n",
      "Val loss: 3.1594789028167725, Val acc: 0.62\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.1989178412999864, Train acc: 0.5974893162393162\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.1998058883552876, Train acc: 0.5948183760683761\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.199940815270796, Train acc: 0.5930377492877493\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.2037676649215894, Train acc: 0.5872729700854701\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.2023717684623523, Train acc: 0.5890491452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.20034680611048, Train acc: 0.5905003561253561\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.200348368494502, Train acc: 0.5914224664224664\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.1993624190998893, Train acc: 0.5929821047008547\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.196808716057599, Train acc: 0.5955899810066477\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.1949465321679402, Train acc: 0.5977297008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.1934691842619354, Train acc: 0.5988733488733489\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.1897351808018155, Train acc: 0.6028311965811965\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.1853861014048257, Train acc: 0.6072279750164365\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.180670769368918, Train acc: 0.6119887057387058\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.1758596238247687, Train acc: 0.6166488603988604\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.1714745469582386, Train acc: 0.6211605235042735\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.167972939646142, Train acc: 0.624732905982906\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.164090731657921, Train acc: 0.6288135090218424\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.160404378961348, Train acc: 0.632605150697256\n",
      "Val loss: 3.0118467807769775, Val acc: 0.774\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0989270699329867, Train acc: 0.6981837606837606\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.09412064205887, Train acc: 0.702323717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.091455990772302, Train acc: 0.7047720797720798\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.0886334277625775, Train acc: 0.7078659188034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.088611821639232, Train acc: 0.7074786324786325\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.086248293901101, Train acc: 0.7103810541310541\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.083351334663829, Train acc: 0.7129884004884005\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.08241465407559, Train acc: 0.7135416666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0810950751091792, Train acc: 0.7148326210826211\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.0794944801901143, Train acc: 0.7161858974358974\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0774942579988185, Train acc: 0.7183857808857809\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.0767281315265556, Train acc: 0.7192396723646723\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.0765682222967006, Train acc: 0.7190376397107167\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.0753250547090003, Train acc: 0.7205051892551892\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.074344243622913, Train acc: 0.7213319088319088\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0732060162684856, Train acc: 0.7223557692307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.0726586723519427, Train acc: 0.7227564102564102\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.072031616711775, Train acc: 0.723142212725546\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0712568545416565, Train acc: 0.7239372469635628\n",
      "Val loss: 2.977602958679199, Val acc: 0.804\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0488878845149636, Train acc: 0.7478632478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.050898101594713, Train acc: 0.7450587606837606\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.0514807707903393, Train acc: 0.7431445868945868\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.054322381814321, Train acc: 0.7393162393162394\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.0540270837963135, Train acc: 0.7388354700854701\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.053111999123185, Train acc: 0.7399839743589743\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.052013084856436, Train acc: 0.7417200854700855\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.051238948972816, Train acc: 0.741653311965812\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0506015385431215, Train acc: 0.7416013770180437\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.0504136592913897, Train acc: 0.7414797008547008\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0489266535618924, Train acc: 0.7429584304584305\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.048943855483987, Train acc: 0.7425213675213675\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.048335025688287, Train acc: 0.7426446416831032\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.047718735113831, Train acc: 0.7433226495726496\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0473619162187275, Train acc: 0.7438390313390313\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0470844246128683, Train acc: 0.7440571581196581\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.046387089982951, Train acc: 0.7449880593262946\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.0457672891680216, Train acc: 0.7454445631528965\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.045364303198474, Train acc: 0.7456562078272605\n",
      "Val loss: 2.961091995239258, Val acc: 0.822\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0381990070016975, Train acc: 0.7558760683760684\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.03410685266185, Train acc: 0.7573450854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.036246167288886, Train acc: 0.7537393162393162\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.0358071887595024, Train acc: 0.7548076923076923\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.03644578436501, Train acc: 0.7530982905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.0375541441800586, Train acc: 0.7516025641025641\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.03569604188968, Train acc: 0.7535866910866911\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.035052960499739, Train acc: 0.7540397970085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0346801142860116, Train acc: 0.7540064102564102\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.0343812186493833, Train acc: 0.7539529914529914\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0347042845123577, Train acc: 0.7539335664335665\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.0333787235099705, Train acc: 0.7550970441595442\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.0337518800801937, Train acc: 0.7545611439842209\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.0336631012486888, Train acc: 0.7544833638583639\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0332891609594013, Train acc: 0.7549501424501425\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0337643744344387, Train acc: 0.7540731837606838\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.0328368571249307, Train acc: 0.7549176721970839\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.03203052265352, Train acc: 0.7556386514719848\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0316728909595883, Train acc: 0.7559885290148448\n",
      "Val loss: 2.9486124515533447, Val acc: 0.838\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0320531262291803, Train acc: 0.7566773504273504\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.022751057249868, Train acc: 0.766426282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.021238822882671, Train acc: 0.7676282051282052\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.023613346947564, Train acc: 0.765090811965812\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.023687064749563, Train acc: 0.7643696581196581\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.0246214839467975, Train acc: 0.7636663105413105\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.0239200953016643, Train acc: 0.764041514041514\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.024471069503034, Train acc: 0.7630542200854701\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0243064170775815, Train acc: 0.7632656695156695\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.0244096988286726, Train acc: 0.7631143162393162\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.023813295105028, Train acc: 0.7635975135975136\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.023313740549604, Train acc: 0.7636885683760684\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.0232793581007655, Train acc: 0.7634368836291914\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.0228764951156317, Train acc: 0.763640873015873\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.022505688599372, Train acc: 0.7639423076923076\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0223512943738546, Train acc: 0.7639556623931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.021911575423826, Train acc: 0.7641245600804424\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.021317706262058, Train acc: 0.7646011396011396\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0211890654364417, Train acc: 0.764704228520018\n",
      "Val loss: 2.9458725452423096, Val acc: 0.838\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0156141692756586, Train acc: 0.7727029914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.0165536851964445, Train acc: 0.7704326923076923\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.019556918375173, Train acc: 0.7670940170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.0169063511057796, Train acc: 0.7696314102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.014413003024892, Train acc: 0.7718482905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.0149836580977483, Train acc: 0.7714120370370371\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.0144771903716894, Train acc: 0.7720924908424909\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.0151027400269466, Train acc: 0.7710670405982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0140941387567763, Train acc: 0.77127849002849\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.0147117213306265, Train acc: 0.7702724358974359\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.0142786890013604, Train acc: 0.7706876456876457\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.0142268565645245, Train acc: 0.7703881766381766\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.014165426538933, Train acc: 0.7702991452991453\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.0135358478851226, Train acc: 0.7709859584859585\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0138326551160244, Train acc: 0.7705840455840456\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.01307327968952, Train acc: 0.7715511485042735\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.0126132692986243, Train acc: 0.7718859979889392\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.0122347704478933, Train acc: 0.7720352564102564\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.0116538914591677, Train acc: 0.7726186459739092\n",
      "Val loss: 2.9412100315093994, Val acc: 0.834\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0000577421269865, Train acc: 0.7828525641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.0038087052157803, Train acc: 0.7801816239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.0053634439778123, Train acc: 0.7784009971509972\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.0056663904434595, Train acc: 0.7791132478632479\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.0067010602380475, Train acc: 0.7780448717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.0077384669556575, Train acc: 0.7764868233618234\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.0071402285998556, Train acc: 0.776747557997558\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 3.006601836182113, Train acc: 0.7773771367521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 3.0064585313045287, Train acc: 0.7773029439696106\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 3.0062998516946777, Train acc: 0.7774038461538462\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 3.00587102675864, Train acc: 0.777802059052059\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 3.0057176044184257, Train acc: 0.77775551994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 3.005997753237361, Train acc: 0.7774901380670611\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 3.00525853427193, Train acc: 0.7782356532356532\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.0059245396203806, Train acc: 0.7775462962962963\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 3.0054166631566153, Train acc: 0.7778278579059829\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 3.005224237135153, Train acc: 0.7779191804927099\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 3.0046908497697156, Train acc: 0.7784009971509972\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 3.004091782125867, Train acc: 0.7790148448043185\n",
      "Val loss: 2.9427194595336914, Val acc: 0.832\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0033834510379367, Train acc: 0.7783119658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.0047932537192974, Train acc: 0.7765758547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.002972692505926, Train acc: 0.7783119658119658\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9998147467262726, Train acc: 0.7817174145299145\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.998660290546906, Train acc: 0.7824252136752137\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.998065936599362, Train acc: 0.7823628917378918\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9978117215036617, Train acc: 0.7828525641025641\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9985272102376337, Train acc: 0.7821514423076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.998004883442849, Train acc: 0.7829712725546059\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.998443654867319, Train acc: 0.7824252136752137\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.998777055147552, Train acc: 0.782051282051282\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.999007868291306, Train acc: 0.7817619301994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.999662316370606, Train acc: 0.7810034516765286\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.999976656521342, Train acc: 0.7808112026862026\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 3.000001718108131, Train acc: 0.7808404558404558\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.999941600693597, Train acc: 0.7808159722222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9997095338899085, Train acc: 0.7812028657616893\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.999827060604367, Train acc: 0.7811609686609686\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9998028386674163, Train acc: 0.7813202878992352\n",
      "Val loss: 2.9334208965301514, Val acc: 0.85\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 3.0100052091810436, Train acc: 0.7719017094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 3.0050071539022984, Train acc: 0.7772435897435898\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 3.0047642641257695, Train acc: 0.7761752136752137\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 3.002449165042649, Train acc: 0.7786458333333334\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 3.002368513743083, Train acc: 0.7779380341880342\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 3.0008701072459205, Train acc: 0.7798700142450142\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 3.0009410940247143, Train acc: 0.779532967032967\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9997617301777897, Train acc: 0.7808827457264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.998982439240386, Train acc: 0.781784188034188\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9982842769378273, Train acc: 0.7824252136752137\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.997968189644091, Train acc: 0.7826825951825952\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.997892979033652, Train acc: 0.7826967592592593\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.998515912531866, Train acc: 0.7819896449704142\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9978468607342434, Train acc: 0.7826427045177046\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.997899524982159, Train acc: 0.7825676638176638\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9974360106847224, Train acc: 0.7829527243589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.996525503391594, Train acc: 0.7839052287581699\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.996100264510311, Train acc: 0.7843364197530864\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.995885184151173, Train acc: 0.7845535312640576\n",
      "Val loss: 2.932530403137207, Val acc: 0.844\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9957052010756273, Train acc: 0.7863247863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.992714730083433, Train acc: 0.7865918803418803\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9945997030307083, Train acc: 0.7856125356125356\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.995950560284476, Train acc: 0.7837873931623932\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9941132716643506, Train acc: 0.7860042735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.994925261902334, Train acc: 0.7850338319088319\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9954151283399235, Train acc: 0.7843406593406593\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9947171165392947, Train acc: 0.784889155982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9931037994305068, Train acc: 0.786176400759734\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9927562858304406, Train acc: 0.7863782051282051\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9930288767573807, Train acc: 0.7857905982905983\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.992651942448738, Train acc: 0.7860131766381766\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.991667457507834, Train acc: 0.7872082511505588\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9916867362710584, Train acc: 0.7872023809523809\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.99083956870598, Train acc: 0.7879451566951567\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.99025777542693, Train acc: 0.7885450053418803\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.991090560331004, Train acc: 0.7878173705379587\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9905153851450224, Train acc: 0.7883725071225072\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9903928567523415, Train acc: 0.7886864597390914\n",
      "Val loss: 2.9213004112243652, Val acc: 0.854\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9756259836702266, Train acc: 0.8055555555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.979301150028522, Train acc: 0.8011485042735043\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.983670512495557, Train acc: 0.7969195156695157\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.98636236659482, Train acc: 0.7929353632478633\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.985552998485728, Train acc: 0.7939102564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9854026404541103, Train acc: 0.7944711538461539\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.986805013975672, Train acc: 0.7926587301587301\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.987237979968389, Train acc: 0.7920005341880342\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9881037085371025, Train acc: 0.79122150997151\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.989109148327102, Train acc: 0.7900373931623932\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.989146200185982, Train acc: 0.7902340714840714\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9886793382147436, Train acc: 0.7905092592592593\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.988456283565574, Train acc: 0.7906393819855358\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9883905259098618, Train acc: 0.790521978021978\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.98822572591298, Train acc: 0.790758547008547\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9880555022476067, Train acc: 0.7909488514957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9878707662067443, Train acc: 0.7912267471091\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9877950365160717, Train acc: 0.791028608736942\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9876859035783667, Train acc: 0.7910481331533963\n",
      "Val loss: 2.9241647720336914, Val acc: 0.854\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9719886290721402, Train acc: 0.8066239316239316\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9792334116422214, Train acc: 0.7990117521367521\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9839854023055814, Train acc: 0.7946047008547008\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9853413701057434, Train acc: 0.7924679487179487\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.985886319478353, Train acc: 0.7917735042735042\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.984024765484693, Train acc: 0.7938479344729344\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.985137705692296, Train acc: 0.7930784493284493\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9849381260892267, Train acc: 0.7930355235042735\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9841363744744775, Train acc: 0.7941892212725546\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9840231932126557, Train acc: 0.7948985042735043\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9835164811876087, Train acc: 0.7953088578088578\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9839436048456065, Train acc: 0.7948272792022792\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9840754843479234, Train acc: 0.7943581525312294\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.983857408402458, Train acc: 0.7944902319902319\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9839352556103655, Train acc: 0.7943198005698006\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9838322362838645, Train acc: 0.7943209134615384\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9835854601536402, Train acc: 0.7946047008547008\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9831069394280996, Train acc: 0.7951388888888888\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9835282204336697, Train acc: 0.7947733918128655\n",
      "Val loss: 2.9145703315734863, Val acc: 0.864\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9856654456538014, Train acc: 0.7889957264957265\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9857788839910784, Train acc: 0.7909989316239316\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9846720023032947, Train acc: 0.7923789173789174\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9818109279004936, Train acc: 0.7956063034188035\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9813269957517967, Train acc: 0.7961004273504273\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9811406485375516, Train acc: 0.7962962962962963\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9808963471716576, Train acc: 0.7967032967032966\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.980118393388569, Train acc: 0.797409188034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9811114454314693, Train acc: 0.7962072649572649\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9816788763062565, Train acc: 0.7956997863247863\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.981519186135494, Train acc: 0.796037296037296\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9809330151631284, Train acc: 0.7968082264957265\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9801117558764596, Train acc: 0.7978303747534516\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9804668688512113, Train acc: 0.7975045787545788\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.979913539560432, Train acc: 0.7979166666666667\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9800513363801517, Train acc: 0.7979266826923077\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9799398309085285, Train acc: 0.797998366013072\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.980947251673098, Train acc: 0.7969491927825261\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.980733467017108, Train acc: 0.7972194107062528\n",
      "Val loss: 2.938246250152588, Val acc: 0.834\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9912829623263106, Train acc: 0.7857905982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9840332891187096, Train acc: 0.7927350427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.983801780602871, Train acc: 0.7918447293447294\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.981684592035082, Train acc: 0.7957398504273504\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9812952412499323, Train acc: 0.7964743589743589\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9796256502809007, Train acc: 0.7980769230769231\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9807306803189793, Train acc: 0.7970085470085471\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.980776183370851, Train acc: 0.7966746794871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.980345729635878, Train acc: 0.7970085470085471\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9806909899426324, Train acc: 0.7966613247863248\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9801299911001067, Train acc: 0.7972756410256411\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.979646204713403, Train acc: 0.7978543447293447\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.979643932069155, Train acc: 0.797707100591716\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9788015393838196, Train acc: 0.798458485958486\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.977958884605995, Train acc: 0.7992521367521368\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9786437949818425, Train acc: 0.7985109508547008\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.97873206591714, Train acc: 0.7984539969834088\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.978687421322322, Train acc: 0.7984182098765432\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.978508026386455, Train acc: 0.7984564777327935\n",
      "Val loss: 2.9222829341888428, Val acc: 0.854\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9750126590076675, Train acc: 0.8031517094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9746174934582834, Train acc: 0.8032852564102564\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9766835840339336, Train acc: 0.8001246438746439\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9795766956785803, Train acc: 0.7964075854700855\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9785301440801377, Train acc: 0.7979700854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.977399500346931, Train acc: 0.7991898148148148\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.976698876882793, Train acc: 0.8000610500610501\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9760940706628, Train acc: 0.8008814102564102\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9773571790113746, Train acc: 0.7996794871794872\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.977093065294445, Train acc: 0.7998130341880342\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9758450536646395, Train acc: 0.8012820512820513\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.975961744955123, Train acc: 0.8011485042735043\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.976540031752878, Train acc: 0.8005013149243918\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9763721632287905, Train acc: 0.8005380036630036\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.97636390669733, Train acc: 0.8004451566951567\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.976910642069629, Train acc: 0.7997963408119658\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9761729294358337, Train acc: 0.800732151835093\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9757726507195947, Train acc: 0.8012226970560304\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9762298181364786, Train acc: 0.8007056905083221\n",
      "Val loss: 2.9208765029907227, Val acc: 0.856\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9723092205504065, Train acc: 0.8052884615384616\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9718203585372014, Train acc: 0.8048878205128205\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9733165200279648, Train acc: 0.8033297720797721\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.972943869411436, Train acc: 0.8038194444444444\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.970765373441908, Train acc: 0.8055555555555556\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.970884626407569, Train acc: 0.8052884615384616\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9710715902885094, Train acc: 0.8051739926739927\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.971343894290109, Train acc: 0.8050213675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9717062296237926, Train acc: 0.8049323361823362\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.97171574291001, Train acc: 0.8052350427350428\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9725880528385544, Train acc: 0.8043172105672106\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.972849560599042, Train acc: 0.8041087962962963\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.973332129521718, Train acc: 0.8036653517422748\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9732637029864413, Train acc: 0.8038194444444444\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9729687243784935, Train acc: 0.8038283475783475\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.973657028033183, Train acc: 0.8031016292735043\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9735210538809476, Train acc: 0.8031988436400201\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9731778561899125, Train acc: 0.8038639601139601\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.973778699466467, Train acc: 0.8032219973009447\n",
      "Val loss: 2.912156343460083, Val acc: 0.862\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9749433973915558, Train acc: 0.7994123931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9811843725351186, Train acc: 0.7951388888888888\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9773219080052824, Train acc: 0.7991452991452992\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9758041297268663, Train acc: 0.8006810897435898\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9765114307403566, Train acc: 0.7991452991452992\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.977277874606967, Train acc: 0.7987001424501424\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.977170209162692, Train acc: 0.7985347985347986\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.97828719427443, Train acc: 0.797142094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9777037757753986, Train acc: 0.7980175688509021\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9770378956427943, Train acc: 0.7985576923076924\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.976980346621889, Train acc: 0.798513986013986\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9761167464093266, Train acc: 0.7993901353276354\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9758379258739565, Train acc: 0.7996589414858646\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.975051730689257, Train acc: 0.8006524725274725\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9740234968669053, Train acc: 0.8017628205128206\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.973684158972186, Train acc: 0.8023337339743589\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.973107517994247, Train acc: 0.8029945952740071\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9730223091239605, Train acc: 0.802988485280152\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9735003698531752, Train acc: 0.8023644849302745\n",
      "Val loss: 2.9129021167755127, Val acc: 0.866\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9676786915868774, Train acc: 0.811965811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9683984803338337, Train acc: 0.8096955128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9646396609792682, Train acc: 0.812767094017094\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9659498440913663, Train acc: 0.8112313034188035\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9668437680627546, Train acc: 0.8103632478632479\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9682176299923846, Train acc: 0.8086716524216524\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9705136483114427, Train acc: 0.8064331501831502\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.969793547677179, Train acc: 0.8070913461538461\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9702726268587174, Train acc: 0.806653608736942\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.968767727134574, Train acc: 0.8083066239316239\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9690538284476635, Train acc: 0.8078379953379954\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.96906266847567, Train acc: 0.8078035968660968\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.969056478070241, Train acc: 0.8077333990795529\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.96925858118013, Train acc: 0.8075015262515263\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.969107581543447, Train acc: 0.8076388888888889\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9691367505962014, Train acc: 0.8076589209401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.96856027275189, Train acc: 0.8084150326797386\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.968621977940131, Train acc: 0.80818198005698\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9685025531538853, Train acc: 0.8082405533063428\n",
      "Val loss: 2.9078009128570557, Val acc: 0.866\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.975485269839947, Train acc: 0.7991452991452992\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9686619501847487, Train acc: 0.8067574786324786\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.972176844577844, Train acc: 0.8043091168091168\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9707826951630096, Train acc: 0.8056223290598291\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.969425096267309, Train acc: 0.8070512820512821\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9692466690329744, Train acc: 0.807380698005698\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.968076425012069, Train acc: 0.8087225274725275\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9691295985482697, Train acc: 0.8074919871794872\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9693871554259674, Train acc: 0.8070394112060779\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.969997523788713, Train acc: 0.8064369658119658\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.969372311950008, Train acc: 0.8069638694638694\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.969706257523974, Train acc: 0.8065349002849003\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.969707513623611, Train acc: 0.8065622945430638\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.969749545730805, Train acc: 0.8066239316239316\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.969355609953573, Train acc: 0.8070868945868945\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9693061928463798, Train acc: 0.8070245726495726\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.969091734255661, Train acc: 0.8072523881347411\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9690518256945486, Train acc: 0.8072174738841406\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9688953176790993, Train acc: 0.8073830409356725\n",
      "Val loss: 2.910174608230591, Val acc: 0.87\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9742714217585378, Train acc: 0.8015491452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9709366142240343, Train acc: 0.8042200854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.968004798617458, Train acc: 0.8068019943019943\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9661867353651257, Train acc: 0.8094951923076923\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9645477845118595, Train acc: 0.8118055555555556\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.965736428217331, Train acc: 0.8099626068376068\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9660058071034006, Train acc: 0.8096001221001221\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9661997276493626, Train acc: 0.8094618055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9654683088644958, Train acc: 0.8099180911680912\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9661698856924334, Train acc: 0.8094284188034188\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.966093797135372, Train acc: 0.8094405594405595\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.967037235739564, Train acc: 0.8085603632478633\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.96737306639993, Train acc: 0.8081648586456279\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9668624404003623, Train acc: 0.8085126678876678\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9668765081639306, Train acc: 0.8084935897435898\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9662696599450884, Train acc: 0.8091947115384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.966675115147088, Train acc: 0.808776395173454\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.966613710096419, Train acc: 0.8086716524216524\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9666135387274792, Train acc: 0.8087325686009896\n",
      "Val loss: 2.9171946048736572, Val acc: 0.858\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.961573209518041, Train acc: 0.8149038461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9619682415937767, Train acc: 0.8137019230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.963824897070556, Train acc: 0.811698717948718\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.963084599401197, Train acc: 0.8129674145299145\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.962093464125935, Train acc: 0.8137286324786325\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9638661485791546, Train acc: 0.8121883903133903\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.964793342664856, Train acc: 0.8111645299145299\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9647623096775804, Train acc: 0.8112313034188035\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9646220340801213, Train acc: 0.8113425925925926\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.964378524845482, Train acc: 0.8114583333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9648227741668274, Train acc: 0.8110431235431236\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.965782518230612, Train acc: 0.8101406695156695\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.965380299804557, Train acc: 0.8105276134122288\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9658751244655606, Train acc: 0.809981684981685\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.965963566473067, Train acc: 0.8098468660968661\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.96514028756537, Train acc: 0.8105301816239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9656836944587748, Train acc: 0.8100175967823027\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.965479383554667, Train acc: 0.810303893637227\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9653672822359387, Train acc: 0.8104757085020243\n",
      "Val loss: 2.908139944076538, Val acc: 0.868\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9633163916759, Train acc: 0.812232905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9611016134930472, Train acc: 0.8150373931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9607740542148253, Train acc: 0.8151709401709402\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.960468753790244, Train acc: 0.8159054487179487\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9616642503656894, Train acc: 0.8147435897435897\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.962096259804533, Train acc: 0.8141025641025641\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9638919172415075, Train acc: 0.8120421245421245\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.963579810327954, Train acc: 0.8126001602564102\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9630228954615867, Train acc: 0.8130045109211775\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9625342619724764, Train acc: 0.8135149572649573\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9635561875993184, Train acc: 0.8123057498057498\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9634896228116463, Train acc: 0.8122774216524217\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9635230523046108, Train acc: 0.8121918145956607\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9627401517278837, Train acc: 0.813034188034188\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9634123500595746, Train acc: 0.8123397435897436\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9632335398186984, Train acc: 0.8125\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.963039859687341, Train acc: 0.8126256913021619\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.963081006310944, Train acc: 0.8125741927825261\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9629355364965524, Train acc: 0.8128092667566352\n",
      "Val loss: 2.916140556335449, Val acc: 0.858\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.96247560550005, Train acc: 0.8138354700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9645949663260045, Train acc: 0.8118322649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9632476116517332, Train acc: 0.8133903133903134\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.962242293052184, Train acc: 0.8141025641025641\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9609351598299467, Train acc: 0.8146901709401709\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.962766885078191, Train acc: 0.8122774216524217\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9615400734110775, Train acc: 0.8135302197802198\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9602400874480224, Train acc: 0.8148036858974359\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9608280737968364, Train acc: 0.8142509496676164\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.960917694752033, Train acc: 0.8140491452991453\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.960474990371965, Train acc: 0.8147581585081585\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.960875646001593, Train acc: 0.8143251424501424\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9603748431259045, Train acc: 0.814965483234714\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.96087286207411, Train acc: 0.8144459706959707\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9610539801779634, Train acc: 0.8143162393162393\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9610449018386693, Train acc: 0.8143028846153846\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9608077011616643, Train acc: 0.8146367521367521\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9607773578857537, Train acc: 0.8146515906932573\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9597859489880216, Train acc: 0.8157191857849753\n",
      "Val loss: 2.924612045288086, Val acc: 0.85\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9669293338416987, Train acc: 0.8090277777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.960918864633283, Train acc: 0.8151709401709402\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9575100766967166, Train acc: 0.8183760683760684\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9603500707536683, Train acc: 0.8151709401709402\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.961132284718701, Train acc: 0.8141559829059829\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.960753766559807, Train acc: 0.8145922364672364\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9621595576569275, Train acc: 0.813301282051282\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.960608073024668, Train acc: 0.8146367521367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.960001004727138, Train acc: 0.8154083570750238\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9604587856520954, Train acc: 0.814823717948718\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9607844219341146, Train acc: 0.8143939393939394\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9611079674161056, Train acc: 0.8141470797720798\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9604770174785164, Train acc: 0.814801117685733\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9603564941548313, Train acc: 0.8148084554334555\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.959258541829905, Train acc: 0.816025641025641\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.959615570994524, Train acc: 0.8156383547008547\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.959302025982457, Train acc: 0.8160822021116139\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.959604353080561, Train acc: 0.8156606125356125\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9597702779267965, Train acc: 0.8154802069275754\n",
      "Val loss: 2.919650077819824, Val acc: 0.852\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.955005340087108, Train acc: 0.8197115384615384\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9518656852917795, Train acc: 0.8233173076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.953347891484231, Train acc: 0.8221153846153846\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9547453242489414, Train acc: 0.820579594017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9533556673261856, Train acc: 0.8216346153846154\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9521949036508546, Train acc: 0.8229166666666666\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9524903760288224, Train acc: 0.8227640415140415\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9538815159064074, Train acc: 0.8215144230769231\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.953630175685611, Train acc: 0.821610873694207\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9539242760747926, Train acc: 0.8213675213675213\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9537612004343075, Train acc: 0.8214112276612276\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.954471309640129, Train acc: 0.8206018518518519\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.954088336930786, Train acc: 0.8211086456278764\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9544666154044017, Train acc: 0.8207036019536019\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.954091122890809, Train acc: 0.8210292022792023\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.95402456412458, Train acc: 0.8209635416666666\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.954442025909117, Train acc: 0.82048139768728\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9547559753102792, Train acc: 0.8201121794871795\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.954671574775775, Train acc: 0.8201754385964912\n",
      "Val loss: 2.926790237426758, Val acc: 0.846\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.950954979301518, Train acc: 0.8237179487179487\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9512429298498692, Train acc: 0.8233173076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9558893309699164, Train acc: 0.8181089743589743\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.955283042202648, Train acc: 0.8189770299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9551134186932164, Train acc: 0.8193376068376068\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9575344996574597, Train acc: 0.8170405982905983\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.958309645472283, Train acc: 0.8161630036630036\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.957650914406165, Train acc: 0.8170072115384616\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9562297786629324, Train acc: 0.8185541310541311\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.956425755044334, Train acc: 0.8184561965811966\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.957615351991868, Train acc: 0.8173562548562548\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.958302910850938, Train acc: 0.8165731837606838\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.95795741495061, Train acc: 0.8170816896778436\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9573870812871372, Train acc: 0.8175175518925519\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.957456665772658, Train acc: 0.8173433048433049\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9566045209892793, Train acc: 0.8183259882478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.956348115682242, Train acc: 0.8185174710910005\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.95585964615868, Train acc: 0.818880579297246\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9554560652092348, Train acc: 0.819331983805668\n",
      "Val loss: 2.9072868824005127, Val acc: 0.864\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9561350345611572, Train acc: 0.8181089743589743\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9536719301826935, Train acc: 0.8209134615384616\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.954480716645548, Train acc: 0.8190883190883191\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9526752243694077, Train acc: 0.8215811965811965\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9529155877920297, Train acc: 0.8215811965811965\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.953748275072147, Train acc: 0.8206463675213675\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9551484002589596, Train acc: 0.8190247252747253\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9556485167425923, Train acc: 0.8187099358974359\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.95481010991284, Train acc: 0.8198005698005698\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9547798943315815, Train acc: 0.8200587606837607\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.955502063800127, Train acc: 0.8192987567987567\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.955132484605849, Train acc: 0.819511217948718\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9556881759136933, Train acc: 0.8190540762656148\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.955864272129259, Train acc: 0.8188911782661783\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9559733411185762, Train acc: 0.8187678062678063\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9558212862819686, Train acc: 0.8189603365384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9558257976328686, Train acc: 0.818753142282554\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.955277353270441, Train acc: 0.8193702516619183\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9555166821945265, Train acc: 0.8191070625281152\n",
      "Val loss: 2.9083049297332764, Val acc: 0.872\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9638363907479834, Train acc: 0.8090277777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.957407710898636, Train acc: 0.8177083333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.952736201788965, Train acc: 0.8222934472934473\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9533012657084017, Train acc: 0.8215144230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9546163770887586, Train acc: 0.8202991452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.954544736788823, Train acc: 0.8203792735042735\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9533352412292517, Train acc: 0.8213141025641025\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.953472184574502, Train acc: 0.8210803952991453\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9528479938380285, Train acc: 0.821610873694207\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9530391715530655, Train acc: 0.8212339743589744\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.953123170827467, Train acc: 0.8210712898212899\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9544931668841263, Train acc: 0.8196670227920227\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.954162440591545, Train acc: 0.819896449704142\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.953602006147196, Train acc: 0.8206654456654456\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.954043699329735, Train acc: 0.8201745014245014\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9538675198952355, Train acc: 0.8203959668803419\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.954157175641374, Train acc: 0.8200729009552539\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9543516197775164, Train acc: 0.8199786324786325\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.953894295351386, Train acc: 0.8204425326135852\n",
      "Val loss: 2.910071611404419, Val acc: 0.866\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9603318740160036, Train acc: 0.8125\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.96304869855571, Train acc: 0.8094284188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9580253834738013, Train acc: 0.8148148148148148\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9569433616776752, Train acc: 0.8163060897435898\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.956435117150983, Train acc: 0.8171474358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.954688635986415, Train acc: 0.8187767094017094\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.953812441400847, Train acc: 0.8200167887667887\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.953329764879667, Train acc: 0.8204460470085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.954121342185329, Train acc: 0.8196521842355176\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.953618554172353, Train acc: 0.8199786324786325\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.95380339422426, Train acc: 0.8198329448329449\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9539660682365767, Train acc: 0.819778311965812\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9542903611724274, Train acc: 0.8194649901380671\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.953706719790914, Train acc: 0.8203601953601953\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.954203446026881, Train acc: 0.8198717948717948\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.953747016369787, Train acc: 0.8202624198717948\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.953987512833033, Train acc: 0.8201357466063348\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.953557003031203, Train acc: 0.820616690408357\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.952930224676471, Train acc: 0.8213000449842555\n",
      "Val loss: 2.9070546627044678, Val acc: 0.872\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.955726841576079, Train acc: 0.8183760683760684\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.955427796412737, Train acc: 0.8175747863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.95356924988945, Train acc: 0.8194444444444444\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9507943116701565, Train acc: 0.8223824786324786\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.949631209251208, Train acc: 0.8239850427350427\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9501637661898577, Train acc: 0.8233173076923077\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9501342016553123, Train acc: 0.8233745421245421\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.950371588142509, Train acc: 0.8233506944444444\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9499547257382646, Train acc: 0.823539886039886\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9499937725882246, Train acc: 0.8237179487179487\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.951038616427916, Train acc: 0.8225281662781663\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9506679559365296, Train acc: 0.8228944088319088\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9510609449954663, Train acc: 0.8225879355687048\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9515720371507173, Train acc: 0.8220772283272283\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9503862092977235, Train acc: 0.8234152421652422\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9499269056523967, Train acc: 0.823951655982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9506035768068752, Train acc: 0.8233094519859225\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9505736986343223, Train acc: 0.8234360161443495\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9510416392533565, Train acc: 0.8230994152046783\n",
      "Val loss: 2.910322904586792, Val acc: 0.86\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.955272996527517, Train acc: 0.8173076923076923\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9503699286371217, Train acc: 0.8233173076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.943446111814928, Train acc: 0.8312856125356125\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9449207772556534, Train acc: 0.8299278846153846\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.94723344093714, Train acc: 0.8271367521367521\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9470343494686984, Train acc: 0.8275017806267806\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.947158345663795, Train acc: 0.827342796092796\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.946893309426104, Train acc: 0.8275574252136753\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.947210040640401, Train acc: 0.8275166191832859\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.947369653546912, Train acc: 0.8274839743589744\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.949003715707798, Train acc: 0.8256361693861693\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9486819921735345, Train acc: 0.8259437321937322\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.949060691805908, Train acc: 0.8255876068376068\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.949169403904087, Train acc: 0.8254540598290598\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.949092416274242, Train acc: 0.825462962962963\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9483059423092084, Train acc: 0.8262720352564102\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.948350118835587, Train acc: 0.826263197586727\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9480130375846727, Train acc: 0.8266708214624882\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.948259278991123, Train acc: 0.826417004048583\n",
      "Val loss: 2.9083516597747803, Val acc: 0.864\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9432792296776404, Train acc: 0.8319978632478633\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.943986583978702, Train acc: 0.8299946581196581\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.941569976996832, Train acc: 0.8316417378917379\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9458323635606685, Train acc: 0.8272569444444444\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9463708840883696, Train acc: 0.826602564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.946758516154058, Train acc: 0.8264779202279202\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.946110587125878, Train acc: 0.8271138583638583\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.94575146630279, Train acc: 0.827323717948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9457368271076896, Train acc: 0.827665004748338\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.944850491050981, Train acc: 0.8287927350427351\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.945268813824598, Train acc: 0.8285256410256411\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9459580293408147, Train acc: 0.827857905982906\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9456156435019087, Train acc: 0.8284023668639053\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.946216703189016, Train acc: 0.8277625152625152\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9463986012331103, Train acc: 0.8277421652421653\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.946895153604002, Train acc: 0.8273404113247863\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9469216632267647, Train acc: 0.8271901709401709\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9470952456040487, Train acc: 0.8269675925925926\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.947322108967584, Train acc: 0.8265716374269005\n",
      "Val loss: 2.9167587757110596, Val acc: 0.85\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.935380793025351, Train acc: 0.8397435897435898\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.941208187331501, Train acc: 0.8338675213675214\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9450812815261362, Train acc: 0.8295940170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9438890213640327, Train acc: 0.8309294871794872\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.945859567120544, Train acc: 0.8288461538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9463308180159653, Train acc: 0.8283030626780626\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9458280483008306, Train acc: 0.8287927350427351\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9459179484436655, Train acc: 0.828659188034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9454812704667748, Train acc: 0.828852089268756\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9458837816857883, Train acc: 0.8284455128205128\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.945776747656869, Train acc: 0.8283799533799534\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9461441031548374, Train acc: 0.8280582264957265\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.945692507708096, Train acc: 0.8285256410256411\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9456782543324436, Train acc: 0.8286973443223443\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.945641250284309, Train acc: 0.82872150997151\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9456845943489647, Train acc: 0.8286925747863247\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9452958626025523, Train acc: 0.8290441176470589\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9456765021127627, Train acc: 0.8285701566951567\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.945282670114151, Train acc: 0.8290176563202879\n",
      "Val loss: 2.934088945388794, Val acc: 0.84\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9491544788719244, Train acc: 0.8234508547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.948704908036778, Train acc: 0.8234508547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9482086282170394, Train acc: 0.8246972934472935\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.948238364651672, Train acc: 0.8251869658119658\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9493697313161995, Train acc: 0.8243055555555555\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9491138084661586, Train acc: 0.8245192307692307\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.948912308184073, Train acc: 0.8247481684981685\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.94861755029768, Train acc: 0.8249198717948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9482957413393547, Train acc: 0.8253501899335233\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9475822108423606, Train acc: 0.8262019230769231\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9486940144195795, Train acc: 0.8248591686091686\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9489953555272854, Train acc: 0.8245414886039886\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.949089995345969, Train acc: 0.8247246877054569\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.949238161318759, Train acc: 0.8246909340659341\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.948907846195406, Train acc: 0.8249109686609687\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9478784879812827, Train acc: 0.8258713942307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.947336553987874, Train acc: 0.8263574660633484\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.946950010085038, Train acc: 0.826730175688509\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9469774585557422, Train acc: 0.8268106162843005\n",
      "Val loss: 2.9070022106170654, Val acc: 0.864\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.945223160279103, Train acc: 0.8287927350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.950067060625451, Train acc: 0.8249198717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9491824893190652, Train acc: 0.8257656695156695\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9489280897327976, Train acc: 0.8261217948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.949312619266347, Train acc: 0.8257478632478632\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9496681829463385, Train acc: 0.8252759971509972\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.950018304317134, Train acc: 0.8249389499389499\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.949465254178414, Train acc: 0.8253538995726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9476799285649573, Train acc: 0.8271011396011396\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9477145064590324, Train acc: 0.8267895299145299\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9473952683905993, Train acc: 0.8269473581973582\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.947629566206212, Train acc: 0.8266337250712251\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9465057834208914, Train acc: 0.8276421761998685\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.946419810491895, Train acc: 0.827857905982906\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.946333483345488, Train acc: 0.8278311965811965\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9462461709721475, Train acc: 0.8279580662393162\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.946322469150319, Train acc: 0.827928607340372\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9459934903685525, Train acc: 0.8282288698955366\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9459263782913063, Train acc: 0.8281460863697706\n",
      "Val loss: 2.9095664024353027, Val acc: 0.864\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9427303269378142, Train acc: 0.8317307692307693\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.94235670974112, Train acc: 0.8327991452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9435108604594173, Train acc: 0.8311075498575499\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.942425448160905, Train acc: 0.8326655982905983\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.94510928585998, Train acc: 0.82991452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.945302910614557, Train acc: 0.8293714387464387\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9471499858758388, Train acc: 0.8275335775335775\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9462833111612206, Train acc: 0.8283253205128205\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.945462399172081, Train acc: 0.8292082146248813\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9456806645434126, Train acc: 0.8289529914529915\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9459720860179672, Train acc: 0.8285499222999223\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9448669315063714, Train acc: 0.8295717592592593\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9434704694365452, Train acc: 0.8309089414858646\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9430229168962936, Train acc: 0.8313682844932845\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.942801136576552, Train acc: 0.8315349002849003\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9429749302629733, Train acc: 0.8311965811965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9421671080553335, Train acc: 0.8319193061840121\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9417347022724063, Train acc: 0.8323836657169991\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9420295117730446, Train acc: 0.8320400359874044\n",
      "Val loss: 2.904585123062134, Val acc: 0.866\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9510971480964594, Train acc: 0.8242521367521367\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9429104226267238, Train acc: 0.8314636752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9441190411222626, Train acc: 0.8291488603988604\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9417869927536726, Train acc: 0.8314636752136753\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.941248292596931, Train acc: 0.8322649572649573\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.940986001593435, Train acc: 0.8327991452991453\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9405873290493956, Train acc: 0.8333333333333334\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.941796544015917, Train acc: 0.8323317307692307\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9424643240310075, Train acc: 0.8317901234567902\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9420035588435636, Train acc: 0.8321047008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.942369810629002, Train acc: 0.8317064879564879\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9419096806789735, Train acc: 0.8322872150997151\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9417993233910837, Train acc: 0.8323676857330703\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.941668400543222, Train acc: 0.8325129731379731\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9422225946714398, Train acc: 0.8319978632478633\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.941794082012951, Train acc: 0.8323818108974359\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9420432964121654, Train acc: 0.8320607088989442\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9422077552998283, Train acc: 0.8319681861348528\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.942070544114587, Train acc: 0.8319275753486279\n",
      "Val loss: 2.9073619842529297, Val acc: 0.87\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9387732469118557, Train acc: 0.8341346153846154\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.944751288136865, Train acc: 0.827590811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9434913712689, Train acc: 0.8295940170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.944464960668841, Train acc: 0.8287927350427351\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.943973800056001, Train acc: 0.8295940170940171\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.943728142314487, Train acc: 0.8296830484330484\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9416191705999504, Train acc: 0.8319978632478633\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9413768124376607, Train acc: 0.8321981837606838\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.940847834290942, Train acc: 0.8327397910731245\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.940871894053924, Train acc: 0.8326388888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.940591954036378, Train acc: 0.8328477078477079\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9401030604995553, Train acc: 0.8333333333333334\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.939409688928267, Train acc: 0.8340935239973701\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9391642766703323, Train acc: 0.8342300061050061\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9390722987998243, Train acc: 0.8343482905982906\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9393140744959187, Train acc: 0.8343015491452992\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9394672577437473, Train acc: 0.8341660382101559\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.939924532531673, Train acc: 0.8336301044634378\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.940168501608982, Train acc: 0.8334036212325686\n",
      "Val loss: 2.9137797355651855, Val acc: 0.858\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9370652773441415, Train acc: 0.8376068376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9400557330530934, Train acc: 0.8331997863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9405935028000094, Train acc: 0.8328881766381766\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.940731869294093, Train acc: 0.8336004273504274\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9403937543559278, Train acc: 0.8343482905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9400326105264516, Train acc: 0.8347578347578347\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9389235958802686, Train acc: 0.8356608669108669\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.938921311726937, Train acc: 0.8355368589743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9394895256075086, Train acc: 0.834906220322887\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9391132817309127, Train acc: 0.8350961538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9386303732352563, Train acc: 0.8356643356643356\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.939010276244237, Train acc: 0.8353587962962963\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9396506728975496, Train acc: 0.8346071663379355\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.938931844027779, Train acc: 0.8353174603174603\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.93896875938459, Train acc: 0.8352207977207977\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.939226019204172, Train acc: 0.8349192040598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9393263002206718, Train acc: 0.8347787833081951\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9392530636230427, Train acc: 0.8346984805318138\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.939668986907801, Train acc: 0.8342189608636977\n",
      "Val loss: 2.902329444885254, Val acc: 0.866\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.929519916192079, Train acc: 0.8450854700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9279083013534546, Train acc: 0.8469551282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9338706538208528, Train acc: 0.8404558404558404\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9333247853140545, Train acc: 0.8412793803418803\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9353337039295426, Train acc: 0.8387286324786325\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9358374335487345, Train acc: 0.8385416666666666\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.938162382996854, Train acc: 0.8363858363858364\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.938571726664519, Train acc: 0.8358039529914529\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9382370530370294, Train acc: 0.8363307217473884\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9377757475926325, Train acc: 0.8366452991452992\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9378325592758308, Train acc: 0.8365141802641802\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9380433934706227, Train acc: 0.836204594017094\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9380384903844448, Train acc: 0.8360453648915187\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9387243858041634, Train acc: 0.8353746947496947\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.938637496671106, Train acc: 0.8354166666666667\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9391342290701012, Train acc: 0.8349025106837606\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.939270685880612, Train acc: 0.8348573403720463\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9390781921991707, Train acc: 0.8349952516619183\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9386561453154747, Train acc: 0.8353716824111561\n",
      "Val loss: 2.9169373512268066, Val acc: 0.852\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.937576391758063, Train acc: 0.8360042735042735\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9383657783524604, Train acc: 0.8348023504273504\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9376750535774776, Train acc: 0.8354700854700855\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.937488853931427, Train acc: 0.8358707264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9400578091287204, Train acc: 0.8335470085470086\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.938921650250753, Train acc: 0.8348023504273504\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.939465822026552, Train acc: 0.8340583028083028\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.939209123961946, Train acc: 0.8340678418803419\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9394963933305296, Train acc: 0.8339862298195632\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9391662884981202, Train acc: 0.8344818376068376\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.939585721668756, Train acc: 0.8339889277389277\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9390666732760917, Train acc: 0.8344907407407407\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9393789371638452, Train acc: 0.8341346153846154\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9392932988202904, Train acc: 0.8342872405372406\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.938372149834266, Train acc: 0.8353098290598291\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9377391835053763, Train acc: 0.8361378205128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9377562052763664, Train acc: 0.8361299648064354\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9380121092963876, Train acc: 0.8358410493827161\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9379097620646157, Train acc: 0.8359621007647323\n",
      "Val loss: 2.9078774452209473, Val acc: 0.866\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9362898993695903, Train acc: 0.8352029914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9419396973063803, Train acc: 0.8314636752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9408405710149697, Train acc: 0.8326210826210826\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9403007514456396, Train acc: 0.8326655982905983\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9409600412743724, Train acc: 0.8322649572649573\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9407777412664515, Train acc: 0.8327546296296297\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.941135550011063, Train acc: 0.832493894993895\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.940261730780968, Train acc: 0.8334668803418803\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9383715417197176, Train acc: 0.8355887939221273\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9372294705138247, Train acc: 0.8366452991452992\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.936829698298824, Train acc: 0.8369755244755245\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9359683234807092, Train acc: 0.8378516737891738\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.937091639761389, Train acc: 0.8364973701512163\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9365271204702728, Train acc: 0.837263431013431\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.936284458399498, Train acc: 0.8374643874643874\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9363787783007336, Train acc: 0.8374565972222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.936495923228813, Train acc: 0.8372140522875817\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9365853972358016, Train acc: 0.8371765194681862\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.935638959805707, Train acc: 0.8381269680611786\n",
      "Val loss: 2.91115403175354, Val acc: 0.86\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9364386175432777, Train acc: 0.8357371794871795\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9359820215111103, Train acc: 0.8370726495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9361564345509237, Train acc: 0.8373397435897436\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9359311334088316, Train acc: 0.8377403846153846\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.935711322686611, Train acc: 0.8379807692307693\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9368064729576435, Train acc: 0.8371616809116809\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9363843658177116, Train acc: 0.8374923687423688\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.936073439243512, Train acc: 0.8376736111111112\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9353781853645957, Train acc: 0.8384081196581197\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.934773484458271, Train acc: 0.8389423076923077\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.934530581998195, Train acc: 0.8391365578865578\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9351277176471178, Train acc: 0.8385639245014245\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.935721096124091, Train acc: 0.8381410256410257\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9361133523039764, Train acc: 0.8377976190476191\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.937009443924298, Train acc: 0.8368233618233618\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9372817319937243, Train acc: 0.8363715277777778\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9373994350673085, Train acc: 0.8362713675213675\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9376395640776254, Train acc: 0.8361229819563153\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9374410635278507, Train acc: 0.8363557130004499\n",
      "Val loss: 2.909261703491211, Val acc: 0.862\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9401241591852956, Train acc: 0.8325320512820513\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9323565725587373, Train acc: 0.8417467948717948\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9336570769633323, Train acc: 0.8396545584045584\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9332974476692004, Train acc: 0.8406784188034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9335245458488792, Train acc: 0.8402777777777778\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9327933210932633, Train acc: 0.8407674501424501\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.932851642916054, Train acc: 0.8408882783882784\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.93390834891898, Train acc: 0.8399772970085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.935302903735966, Train acc: 0.8386455365622032\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.935518081575377, Train acc: 0.8383547008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.935253826332537, Train acc: 0.8384081196581197\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9348335014788853, Train acc: 0.8387642450142451\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.935464023838382, Train acc: 0.838120479947403\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9354676232378707, Train acc: 0.8381028693528694\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9351819627984637, Train acc: 0.8383368945868945\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9347321501909156, Train acc: 0.8388254540598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.934265898004017, Train acc: 0.8392879587732529\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9346335435751385, Train acc: 0.8388384377967711\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.935164538257786, Train acc: 0.8382675438596491\n",
      "Val loss: 2.916476249694824, Val acc: 0.85\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9312967744647946, Train acc: 0.8421474358974359\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9259189000496497, Train acc: 0.8470886752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9299588142297206, Train acc: 0.8433938746438746\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9305230567597937, Train acc: 0.8428151709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9323051065461248, Train acc: 0.840758547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.931015668431578, Train acc: 0.8423700142450142\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9319033474275917, Train acc: 0.8413461538461539\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9324690959392448, Train acc: 0.8406784188034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9320476643833113, Train acc: 0.8412274453941121\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9330610862145057, Train acc: 0.8402510683760683\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.933113050534916, Train acc: 0.8401078088578089\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.933165720385364, Train acc: 0.8402110042735043\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.933209522863033, Train acc: 0.840051775147929\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.932447432889461, Train acc: 0.8409264346764347\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.932194467419573, Train acc: 0.8411502849002849\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9324423358735876, Train acc: 0.840995592948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9321302964856364, Train acc: 0.8413147310206134\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.932128236051525, Train acc: 0.8413461538461539\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9322363955986375, Train acc: 0.8413039811066126\n",
      "Val loss: 2.9142839908599854, Val acc: 0.86\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9264931678771973, Train acc: 0.8482905982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.930159757279942, Train acc: 0.8430822649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9321266579152514, Train acc: 0.8400106837606838\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.932744490285205, Train acc: 0.8392094017094017\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9327620265830276, Train acc: 0.8397970085470086\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.934379345671064, Train acc: 0.8387197293447294\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.933924003107353, Train acc: 0.8394383394383395\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9343071821917834, Train acc: 0.8391426282051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9353684120141996, Train acc: 0.8379629629629629\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9351999763749603, Train acc: 0.8381143162393162\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.935426300963229, Train acc: 0.8378739316239316\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9358573785194984, Train acc: 0.8374287749287749\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.934978688727234, Train acc: 0.8385724852071006\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.935100795585157, Train acc: 0.8385035103785103\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.935078275984848, Train acc: 0.8385327635327635\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.93493769553482, Train acc: 0.8385416666666666\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.935112325913826, Train acc: 0.8384552538964304\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.934687300398592, Train acc: 0.8386752136752137\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9350888508158763, Train acc: 0.8382956590193432\n",
      "Val loss: 2.9028756618499756, Val acc: 0.866\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9260012777442608, Train acc: 0.8480235042735043\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9267397864252076, Train acc: 0.8468215811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.928302605946859, Train acc: 0.8449074074074074\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9304633925103736, Train acc: 0.8426148504273504\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.930718630603236, Train acc: 0.8426816239316239\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.931471881363806, Train acc: 0.8419248575498576\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.931410168844556, Train acc: 0.842032967032967\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9308389825189214, Train acc: 0.8424145299145299\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9308625463067295, Train acc: 0.8425332383665717\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.931500150403406, Train acc: 0.8419871794871795\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9310419326061967, Train acc: 0.8424873737373737\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.931789313965713, Train acc: 0.8416132478632479\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9312569537027975, Train acc: 0.8421679815910585\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.931498601905301, Train acc: 0.8418612637362637\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.931506174446171, Train acc: 0.8418981481481481\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9315403101281223, Train acc: 0.8417467948717948\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9315923352287068, Train acc: 0.8416603821015586\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.931386465473845, Train acc: 0.8417913105413105\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.931515161485968, Train acc: 0.841655420602789\n",
      "Val loss: 2.913820266723633, Val acc: 0.858\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.93237808219388, Train acc: 0.8392094017094017\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9299457501142454, Train acc: 0.8426816239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9341220651936326, Train acc: 0.8386752136752137\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.933776009796012, Train acc: 0.8396768162393162\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.934297575094761, Train acc: 0.8391025641025641\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.93409091183263, Train acc: 0.8392094017094017\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.934289486853631, Train acc: 0.8392857142857143\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9331431623198028, Train acc: 0.8407118055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9327141178978815, Train acc: 0.8409900284900285\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.932158518652631, Train acc: 0.8415598290598291\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.932158639658859, Train acc: 0.8414189976689976\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9317545211552893, Train acc: 0.8417022792022792\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.931956440739378, Train acc: 0.8415721564760026\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.932328351105817, Train acc: 0.841231684981685\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9320965425920624, Train acc: 0.8413995726495727\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9315751919634323, Train acc: 0.8419638087606838\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9318570617217405, Train acc: 0.8417232277526395\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9306728379112816, Train acc: 0.8428745251661919\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9310749680728887, Train acc: 0.8426816239316239\n",
      "Val loss: 2.8916070461273193, Val acc: 0.884\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.940825639626919, Train acc: 0.8298611111111112\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9299363794489803, Train acc: 0.8426816239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9308795521401954, Train acc: 0.8419693732193733\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.93247547159847, Train acc: 0.8402777777777778\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9315807884574956, Train acc: 0.8417200854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.933176840472425, Train acc: 0.8399216524216524\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9312513517954994, Train acc: 0.8419184981684982\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.931007384744465, Train acc: 0.8421474358974359\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9313898532592106, Train acc: 0.8419693732193733\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9315710102391037, Train acc: 0.8416666666666667\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9318863863896842, Train acc: 0.8413947163947164\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.931904889579512, Train acc: 0.8413906695156695\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.931134788399069, Train acc: 0.8421268902038133\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9311084764781015, Train acc: 0.8421283577533577\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9314703422394235, Train acc: 0.8417913105413105\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9314464970022187, Train acc: 0.8418302617521367\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.931501589569391, Train acc: 0.8416132478632479\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9317160221020155, Train acc: 0.8413906695156695\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.931367334030364, Train acc: 0.8417257085020243\n",
      "Val loss: 2.8960018157958984, Val acc: 0.876\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.918892024928688, Train acc: 0.8563034188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.924974059447264, Train acc: 0.8493589743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.926148288270347, Train acc: 0.8481125356125356\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9265545168493547, Train acc: 0.8475560897435898\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.925663839242397, Train acc: 0.8483974358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.927094323003394, Train acc: 0.8468215811965812\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.927017920474284, Train acc: 0.8469169719169719\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9267863794269724, Train acc: 0.8470886752136753\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.926911759127466, Train acc: 0.8468067426400759\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9274946771116337, Train acc: 0.8462873931623932\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.927866344777947, Train acc: 0.8458624708624709\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.928465813822896, Train acc: 0.8451745014245015\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.92901916475691, Train acc: 0.8444896449704142\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9284022884927827, Train acc: 0.8450091575091575\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.928950219140773, Train acc: 0.8444088319088319\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9283930950949335, Train acc: 0.8449519230769231\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9284501201606625, Train acc: 0.8448969331322272\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.928362143798205, Train acc: 0.8449222459639126\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9280075748064363, Train acc: 0.8452822762033289\n",
      "Val loss: 2.9020490646362305, Val acc: 0.868\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.928777103750115, Train acc: 0.84375\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.927439741599254, Train acc: 0.8452190170940171\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9266763670831666, Train acc: 0.8463319088319088\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.927820398257329, Train acc: 0.8454861111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9289620419852755, Train acc: 0.844017094017094\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9302173166872767, Train acc: 0.8428596866096866\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9296371532156065, Train acc: 0.8435210622710623\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9291536825844364, Train acc: 0.8438835470085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.927861397762244, Train acc: 0.8453525641025641\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9273698876046725, Train acc: 0.8458600427350428\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9276483429432285, Train acc: 0.8455468142968143\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9268689551244775, Train acc: 0.8463986823361823\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.926719810522206, Train acc: 0.8465442143326759\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9267232434071317, Train acc: 0.8465735653235653\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9268639652817336, Train acc: 0.8464387464387464\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9283246687080107, Train acc: 0.8450186965811965\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.928086485798003, Train acc: 0.8452582956259427\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9280404042654227, Train acc: 0.845278371320038\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.928221162353289, Train acc: 0.8450995276653172\n",
      "Val loss: 2.9017138481140137, Val acc: 0.866\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9307666150932636, Train acc: 0.8416132478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9301786096686993, Train acc: 0.8430822649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9306778126632387, Train acc: 0.8426816239316239\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9300152007331195, Train acc: 0.8438167735042735\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9313688465672683, Train acc: 0.8423611111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9304949493489714, Train acc: 0.8430822649572649\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.928988086143838, Train acc: 0.8446275946275946\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.929332938968626, Train acc: 0.8441172542735043\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9292392948074557, Train acc: 0.8441654795821463\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9281749089558917, Train acc: 0.845272435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.928286451652426, Train acc: 0.8451097513597513\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9284218714447783, Train acc: 0.8448628917378918\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.927910098114114, Train acc: 0.8453731097961867\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9274376134441593, Train acc: 0.8457532051282052\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9271704309686295, Train acc: 0.8461182336182336\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.926604106894925, Train acc: 0.8468048878205128\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.926358751222199, Train acc: 0.8469865510306687\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9256295170652673, Train acc: 0.8476673789173789\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9258409143834836, Train acc: 0.847419028340081\n",
      "Val loss: 2.8958351612091064, Val acc: 0.874\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9215706250606437, Train acc: 0.8509615384615384\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9183412566144242, Train acc: 0.8545673076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9211002272418423, Train acc: 0.8510505698005698\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.923464990579165, Train acc: 0.8490251068376068\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.922873409792908, Train acc: 0.8498931623931624\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.925550312398166, Train acc: 0.8472667378917379\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.925716762577658, Train acc: 0.8471840659340659\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9267086883385978, Train acc: 0.8465878739316239\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9270341047647443, Train acc: 0.8462428774928775\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9269868015224096, Train acc: 0.846073717948718\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.927528725174294, Train acc: 0.8454496891996892\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9274712437578074, Train acc: 0.8455306267806267\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9272990001023875, Train acc: 0.8457223865877712\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.927094475108043, Train acc: 0.8459821428571429\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.926831215535134, Train acc: 0.8461716524216524\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9259452046746883, Train acc: 0.8469885149572649\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9256294978330217, Train acc: 0.8473164906988436\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9252686809610435, Train acc: 0.8477118945868946\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9253082455572006, Train acc: 0.8477564102564102\n",
      "Val loss: 2.892805337905884, Val acc: 0.882\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9234408191126637, Train acc: 0.8525641025641025\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9270739127428103, Train acc: 0.8470886752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9263183255480905, Train acc: 0.8471331908831908\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9234257355714455, Train acc: 0.8500267094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.923034198467548, Train acc: 0.8503205128205128\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9244186990281458, Train acc: 0.8490473646723646\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9252345850179484, Train acc: 0.8486721611721612\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.925415794818829, Train acc: 0.8485243055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.92591982055367, Train acc: 0.847875118708452\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.925903345377017, Train acc: 0.8476495726495726\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.92533443616913, Train acc: 0.848266317016317\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9248183792472906, Train acc: 0.8486467236467237\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9256463444287806, Train acc: 0.8476536817882971\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9260683120825353, Train acc: 0.8472413003663004\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.925221471161584, Train acc: 0.8481659544159544\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.925504507162632, Train acc: 0.847923344017094\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9250687954712298, Train acc: 0.8483220211161387\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.925158647164094, Train acc: 0.8481125356125356\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9250918372064576, Train acc: 0.8480797345928925\n",
      "Val loss: 2.892512321472168, Val acc: 0.884\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9210226067111025, Train acc: 0.8528311965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.918781401764633, Train acc: 0.8541666666666666\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.918834110950133, Train acc: 0.854255698005698\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.921839504160433, Train acc: 0.8513621794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.921182757565099, Train acc: 0.8519230769230769\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9215868480524785, Train acc: 0.8515847578347578\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9220211421468174, Train acc: 0.8511904761904762\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.921637927874541, Train acc: 0.8515958867521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9213801863526347, Train acc: 0.851792497625831\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9219208348510612, Train acc: 0.8513087606837607\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9219103389316134, Train acc: 0.8513500388500389\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9224575890435114, Train acc: 0.8507389601139601\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.922248194830579, Train acc: 0.8508588099934253\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9227639478932663, Train acc: 0.8503510378510378\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9228763440395693, Train acc: 0.8503561253561254\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9233862353950486, Train acc: 0.8497429220085471\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9233416620362878, Train acc: 0.8496889140271493\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.923540790083288, Train acc: 0.8494925213675214\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9230224977888826, Train acc: 0.8499775078722447\n",
      "Val loss: 2.9006733894348145, Val acc: 0.872\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.926949344129644, Train acc: 0.8453525641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9254884709659805, Train acc: 0.8461538461538461\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9236843830499892, Train acc: 0.8482015669515669\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9243076647448745, Train acc: 0.8477564102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.92321198210757, Train acc: 0.8489316239316239\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9233737878310375, Train acc: 0.8490473646723646\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9235878104255315, Train acc: 0.8488629426129426\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9238025327014108, Train acc: 0.8489249465811965\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.924364307327488, Train acc: 0.8483499525166192\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.924106825518812, Train acc: 0.8487446581196582\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9242638272653743, Train acc: 0.8487276612276612\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9240792799539377, Train acc: 0.8487802706552706\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9242991664392393, Train acc: 0.8485166009204471\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9249995657230325, Train acc: 0.847985347985348\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.92425798323759, Train acc: 0.8486645299145299\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9244649091846924, Train acc: 0.8484909188034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9240930201481072, Train acc: 0.8488404977375565\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9243101972913244, Train acc: 0.8486615622032289\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.924189575931482, Train acc: 0.8488388439046334\n",
      "Val loss: 2.8917834758758545, Val acc: 0.88\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.917043274284428, Train acc: 0.8560363247863247\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9171257161686563, Train acc: 0.8561698717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.920067563695446, Train acc: 0.8533653846153846\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9235646092993584, Train acc: 0.8494257478632479\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9243504222641645, Train acc: 0.8486645299145299\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9242514185076764, Train acc: 0.8489583333333334\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.923620390222477, Train acc: 0.8496260683760684\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9240348109832177, Train acc: 0.8492254273504274\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9236257397777563, Train acc: 0.8496557454890789\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.923191516012208, Train acc: 0.8498931623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.922725341814659, Train acc: 0.8505730380730381\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9220722096937672, Train acc: 0.851028311965812\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.921976147945738, Train acc: 0.8511669953977646\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9218065208858914, Train acc: 0.8513431013431013\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.921934658849341, Train acc: 0.8511752136752136\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.921505333903508, Train acc: 0.8515791933760684\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.921249281287253, Train acc: 0.8518099547511312\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.92139983041334, Train acc: 0.8516737891737892\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9216530160007315, Train acc: 0.8513973234367971\n",
      "Val loss: 2.89145565032959, Val acc: 0.882\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.921130504363622, Train acc: 0.8528311965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9206453245929165, Train acc: 0.8522970085470085\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9208052674589675, Train acc: 0.8520299145299145\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.92029539285562, Train acc: 0.8522970085470085\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.916623646988828, Train acc: 0.8561431623931623\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.916756858513226, Train acc: 0.8562143874643875\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9185555936070444, Train acc: 0.8538995726495726\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9185593843969526, Train acc: 0.8541666666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9188776853983445, Train acc: 0.8538698955365622\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9194347915486394, Train acc: 0.8534188034188034\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9194132481421624, Train acc: 0.8534625097125097\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.919695062005622, Train acc: 0.8529870014245015\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.919876924084645, Train acc: 0.8527695595003287\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.920361192235143, Train acc: 0.8523923992673993\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9204615459822523, Train acc: 0.8524216524216525\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.920893301184361, Train acc: 0.8520132211538461\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9208075598653926, Train acc: 0.8520927601809954\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9206294251529807, Train acc: 0.8522376543209876\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9204933338748726, Train acc: 0.8523813540260908\n",
      "Val loss: 2.893277645111084, Val acc: 0.88\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.924917871116573, Train acc: 0.8480235042735043\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9258916693874912, Train acc: 0.8466880341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9250659134313253, Train acc: 0.8480235042735043\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9235593635811763, Train acc: 0.8494257478632479\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.921123739796826, Train acc: 0.8518162393162393\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9211066217503996, Train acc: 0.8518963675213675\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.920415416014209, Train acc: 0.8525641025641025\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.921427559394103, Train acc: 0.8517294337606838\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9221365906913737, Train acc: 0.8510505698005698\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.92180259981726, Train acc: 0.8513888888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9216226959006213, Train acc: 0.8513500388500389\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9212247822699045, Train acc: 0.8517405626780626\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9210228318058293, Train acc: 0.8520093688362919\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9205344264760558, Train acc: 0.8524305555555556\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9202005633601438, Train acc: 0.8527599715099715\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.919780825957274, Train acc: 0.8531483707264957\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.919797866412777, Train acc: 0.8532239819004525\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.919736701651969, Train acc: 0.853261514719848\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9197665147828564, Train acc: 0.8533372694556906\n",
      "Val loss: 2.8923797607421875, Val acc: 0.872\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9250672148843098, Train acc: 0.8480235042735043\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9218220160557675, Train acc: 0.8508279914529915\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.919930432936405, Train acc: 0.8526531339031339\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.919318423821376, Train acc: 0.8532318376068376\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9194463615743524, Train acc: 0.853258547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.919877582805449, Train acc: 0.8530092592592593\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.920098821758787, Train acc: 0.8530982905982906\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.92039911257915, Train acc: 0.8527310363247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.920799157331925, Train acc: 0.852326685660019\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.920274225055662, Train acc: 0.852857905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.920356042642601, Train acc: 0.8526369463869464\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9202919632960587, Train acc: 0.8527421652421653\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9204977375359067, Train acc: 0.8525846482577252\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.920177328863132, Train acc: 0.8528502747252747\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.91908529124029, Train acc: 0.8540420227920228\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9190241681714344, Train acc: 0.8539663461538461\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.919225796315465, Train acc: 0.8538053041729512\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.918839108117512, Train acc: 0.8541815052231719\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.918405196391083, Train acc: 0.8546305668016194\n",
      "Val loss: 2.8893189430236816, Val acc: 0.886\n",
      "Epoch 64/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9239571176023564, Train acc: 0.8488247863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9191065007804804, Train acc: 0.8538995726495726\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9191350495373762, Train acc: 0.8536324786324786\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9181114761238423, Train acc: 0.8549679487179487\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.917549561639117, Train acc: 0.8554487179487179\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9169906778552934, Train acc: 0.8556801994301995\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9170829472524344, Train acc: 0.8557692307692307\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9178413359018474, Train acc: 0.8550013354700855\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9179915899564737, Train acc: 0.8549679487179487\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9178803250320957, Train acc: 0.8550747863247863\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.919200300892472, Train acc: 0.8536081973581974\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.919664649372427, Train acc: 0.8529202279202279\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9203635288805336, Train acc: 0.8523175542406312\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.919817150148571, Train acc: 0.8528311965811965\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9197983229601823, Train acc: 0.8529024216524217\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.919427886987344, Train acc: 0.8532986111111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.919889278661192, Train acc: 0.8527526395173454\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.920007912408479, Train acc: 0.8526234567901234\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.919468654687481, Train acc: 0.8531404633378318\n",
      "Val loss: 2.8912389278411865, Val acc: 0.882\n",
      "Epoch 65/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.927989686656202, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9240263007644915, Train acc: 0.8476228632478633\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9262718875863274, Train acc: 0.8459757834757835\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.923761305646, Train acc: 0.8486244658119658\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9214832477080517, Train acc: 0.8508547008547008\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9208898415253035, Train acc: 0.8512286324786325\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9216148669903097, Train acc: 0.8502747252747253\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9214914206765656, Train acc: 0.8503271901709402\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.920045619903014, Train acc: 0.8519112060778727\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9194892932207157, Train acc: 0.8525106837606837\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9192552720287, Train acc: 0.8528311965811965\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9198313256953856, Train acc: 0.852363782051282\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.919141526915069, Train acc: 0.8531188362919132\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.919280613735045, Train acc: 0.853155525030525\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9188397013563714, Train acc: 0.8536858974358974\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9181301484250617, Train acc: 0.8543502938034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.918617726571, Train acc: 0.8539152840623428\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9182676209343805, Train acc: 0.854255698005698\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9184724575648477, Train acc: 0.8540120332883491\n",
      "Val loss: 2.892965078353882, Val acc: 0.878\n",
      "Epoch 66/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.921746111323691, Train acc: 0.8522970085470085\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.920388848353655, Train acc: 0.8522970085470085\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9196168676740424, Train acc: 0.8538995726495726\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9189809005484624, Train acc: 0.8541666666666666\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.918357697511331, Train acc: 0.8544871794871794\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.916807739483325, Train acc: 0.8559918091168092\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9175502843472545, Train acc: 0.855273199023199\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9164833416286697, Train acc: 0.8563034188034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.916188077030019, Train acc: 0.8566892212725546\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9163317397109463, Train acc: 0.8564636752136752\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9171250029506846, Train acc: 0.8557692307692307\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9175944163588716, Train acc: 0.8553240740740741\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9174572937117054, Train acc: 0.8555637738330046\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9173147489154148, Train acc: 0.8557119963369964\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9167936066956264, Train acc: 0.85625\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9168410554655595, Train acc: 0.8561698717948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9171505992049367, Train acc: 0.8558634992458521\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9173923436506293, Train acc: 0.8555763295346629\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9170514594527233, Train acc: 0.8559379217273954\n",
      "Val loss: 2.887275218963623, Val acc: 0.886\n",
      "Epoch 67/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.910111826709193, Train acc: 0.8632478632478633\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.912174821918846, Train acc: 0.8600427350427351\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9121771619530485, Train acc: 0.8603988603988604\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9128763736822667, Train acc: 0.8597088675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9127885467985757, Train acc: 0.8597222222222223\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.91415949866303, Train acc: 0.858573717948718\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9143908370254388, Train acc: 0.8581349206349206\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.914269665112862, Train acc: 0.8583400106837606\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.914215472575493, Train acc: 0.858647910731244\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9145556101432213, Train acc: 0.8584935897435897\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9150727797406724, Train acc: 0.8579788267288267\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9155787937661524, Train acc: 0.8574608262108262\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.916577649069491, Train acc: 0.8566732412886259\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.91665316443158, Train acc: 0.8565323565323565\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9169288047018895, Train acc: 0.8562678062678063\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.916116240697029, Train acc: 0.8570379273504274\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.916213021199268, Train acc: 0.856853318250377\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9163290373167308, Train acc: 0.8567634140550807\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.915857821394611, Train acc: 0.8572031039136302\n",
      "Val loss: 2.890900135040283, Val acc: 0.882\n",
      "Epoch 68/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9139746193192964, Train acc: 0.8605769230769231\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9136432195321107, Train acc: 0.8600427350427351\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9162562464037514, Train acc: 0.8570156695156695\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.916128043435578, Train acc: 0.8569711538461539\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9158131843958146, Train acc: 0.8572649572649572\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.917144063191536, Train acc: 0.8556356837606838\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.91667431381887, Train acc: 0.8561889499389499\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9177778837008352, Train acc: 0.855201655982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.916959475474706, Train acc: 0.8560363247863247\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9170510206467064, Train acc: 0.8560096153846154\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9161558986729026, Train acc: 0.856764763014763\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.915902737878327, Train acc: 0.8570156695156695\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.916598702166756, Train acc: 0.856344510190664\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.915852227781573, Train acc: 0.8572191697191697\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9157871253130443, Train acc: 0.8574074074074074\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.914955699418345, Train acc: 0.8582565438034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9152593051686004, Train acc: 0.8578588486676721\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9159639231499783, Train acc: 0.8571937321937322\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9157838426728104, Train acc: 0.8572733918128655\n",
      "Val loss: 2.905813694000244, Val acc: 0.866\n",
      "Epoch 69/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.914970518177391, Train acc: 0.8573717948717948\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9139839390404205, Train acc: 0.8597756410256411\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.912882657472225, Train acc: 0.8604878917378918\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9138699229965863, Train acc: 0.8587740384615384\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.913074810484536, Train acc: 0.8595619658119659\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.912751369326882, Train acc: 0.8600427350427351\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.912775177658696, Train acc: 0.8598519536019537\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.913686900312065, Train acc: 0.858840811965812\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9140207414273864, Train acc: 0.8586182336182336\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9145304158202605, Train acc: 0.8582264957264957\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9139460876364067, Train acc: 0.8589015151515151\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.913708392880921, Train acc: 0.8592637108262108\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.913325262414555, Train acc: 0.8596318211702827\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.913803702454573, Train acc: 0.859107905982906\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9137848731799005, Train acc: 0.8592058404558405\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9138563410339193, Train acc: 0.8590745192307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9134920253533103, Train acc: 0.8594928355957768\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9135938173006064, Train acc: 0.8593601614434948\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.913847782780231, Train acc: 0.8591008771929824\n",
      "Val loss: 2.8933119773864746, Val acc: 0.88\n",
      "Epoch 70/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9125453936748014, Train acc: 0.8600427350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9192469344179854, Train acc: 0.8532318376068376\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.919946710608284, Train acc: 0.8519408831908832\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9165253904130726, Train acc: 0.8556356837606838\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9153286322569234, Train acc: 0.8572115384615384\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9148042178221916, Train acc: 0.8576834045584045\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.914043515593141, Train acc: 0.8582875457875457\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9144214442652516, Train acc: 0.8579059829059829\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9134051315238785, Train acc: 0.858915004748338\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.912836043039958, Train acc: 0.8595085470085471\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9134382293867156, Train acc: 0.8590957653457654\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.91462633871285, Train acc: 0.8579504985754985\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9142138956409784, Train acc: 0.8584401709401709\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9145486960335383, Train acc: 0.8580013736263736\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.914767548161694, Train acc: 0.8578881766381766\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9142157464710055, Train acc: 0.8584067841880342\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9140838111927305, Train acc: 0.858581573655103\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9143027586814685, Train acc: 0.8583956552706553\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9140610444990838, Train acc: 0.8585948043184886\n",
      "Val loss: 2.891580581665039, Val acc: 0.882\n",
      "Epoch 71/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9175797894469695, Train acc: 0.8549679487179487\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9166371730657725, Train acc: 0.8571047008547008\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9145166228639434, Train acc: 0.8585292022792023\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.914361837073269, Train acc: 0.8584401709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.914921057937492, Train acc: 0.857852564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.914145244152797, Train acc: 0.8585292022792023\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.915388424609025, Train acc: 0.8572191697191697\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9150495536816425, Train acc: 0.8574385683760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9144929696125637, Train acc: 0.8580246913580247\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9138867476047614, Train acc: 0.8587339743589744\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.913499834376337, Train acc: 0.8592657342657343\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9145642987683287, Train acc: 0.8580840455840456\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9152094490194225, Train acc: 0.8573717948717948\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9154345227976277, Train acc: 0.8570665445665445\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9151114158141307, Train acc: 0.8574074074074074\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9144842425982156, Train acc: 0.8580562232905983\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9140529270565287, Train acc: 0.8585972850678733\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.913743202163283, Train acc: 0.8589743589743589\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9132648409887634, Train acc: 0.8594101439496177\n",
      "Val loss: 2.8859333992004395, Val acc: 0.89\n",
      "Epoch 72/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9155526079683223, Train acc: 0.8568376068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9099888923840647, Train acc: 0.8620459401709402\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.912632051696125, Train acc: 0.8588853276353277\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.915702611972124, Train acc: 0.8561698717948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9124210997524425, Train acc: 0.8595619658119659\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.913474565217977, Train acc: 0.8584401709401709\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.912970217447432, Train acc: 0.8591269841269841\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9131529718382745, Train acc: 0.8590745192307693\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9127679973812413, Train acc: 0.8593898385565052\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9119489790027977, Train acc: 0.8603365384615385\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.911722877179363, Train acc: 0.8606497668997669\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9116418039017593, Train acc: 0.8607327279202279\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9115462782820303, Train acc: 0.8608440170940171\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9109863189841656, Train acc: 0.8614735958485958\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.910914208067109, Train acc: 0.8616274928774929\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9112959737196946, Train acc: 0.861328125\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.911378887084455, Train acc: 0.8612210910005028\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9109785146975904, Train acc: 0.8616601377018044\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.91086699508945, Train acc: 0.8617155870445344\n",
      "Val loss: 2.8973031044006348, Val acc: 0.878\n",
      "Epoch 73/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9058232388944707, Train acc: 0.8653846153846154\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9078511784219336, Train acc: 0.8643162393162394\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9101993847436716, Train acc: 0.8624465811965812\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9094025258325105, Train acc: 0.8633814102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.910698977087298, Train acc: 0.8621794871794872\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9133053559523363, Train acc: 0.8595530626780626\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9118005056229848, Train acc: 0.8610729548229549\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9117352600790496, Train acc: 0.8609775641025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.912354471337082, Train acc: 0.86036918328585\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9125124672539213, Train acc: 0.8601228632478632\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9123507661234065, Train acc: 0.8601155788655789\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.912528345048258, Train acc: 0.8599982193732194\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9122129502381093, Train acc: 0.8602687376725838\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.912388653545589, Train acc: 0.8600618131868132\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9119459423923764, Train acc: 0.8604700854700855\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.912161051462858, Train acc: 0.8602764423076923\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9122017921066092, Train acc: 0.8603255404725993\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9120180165326155, Train acc: 0.8605175688509021\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9119173656751913, Train acc: 0.8607456140350878\n",
      "Val loss: 2.8990299701690674, Val acc: 0.874\n",
      "Epoch 74/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.917322698821369, Train acc: 0.8565705128205128\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9159119139369736, Train acc: 0.8575053418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9119005257587487, Train acc: 0.8606659544159544\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.909719091704768, Train acc: 0.8628472222222222\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9088064401577682, Train acc: 0.8637820512820513\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9101934521286577, Train acc: 0.8624910968660968\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9102522983364714, Train acc: 0.8623702686202687\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9099258828876367, Train acc: 0.8626802884615384\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9096589815243017, Train acc: 0.8632478632478633\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9104437021108773, Train acc: 0.8624732905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9103096517001186, Train acc: 0.8625922688422688\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9104175557438126, Train acc: 0.8624910968660968\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9096830571505055, Train acc: 0.8632889546351085\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9100794399177636, Train acc: 0.8627327533577533\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.910921254932371, Train acc: 0.8619123931623932\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.91080483666852, Train acc: 0.8619290865384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.910744507987635, Train acc: 0.8619752388134742\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9114954459135123, Train acc: 0.8612891737891738\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9116774286389404, Train acc: 0.8611954565901935\n",
      "Val loss: 2.8944411277770996, Val acc: 0.88\n",
      "Epoch 75/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9137527473971376, Train acc: 0.8579059829059829\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.911496516985771, Train acc: 0.8605769230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9125712603924963, Train acc: 0.8595975783475783\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9125801742586317, Train acc: 0.859909188034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9133810059637084, Train acc: 0.8591346153846153\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.912654877727867, Train acc: 0.8599537037037037\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9118205968276922, Train acc: 0.8606532356532357\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9120009936328626, Train acc: 0.8604433760683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.912029098581385, Train acc: 0.86036918328585\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.912321243530665, Train acc: 0.860176282051282\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9117629613261307, Train acc: 0.8606012043512044\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9112612479432696, Train acc: 0.8610220797720798\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9110746391504554, Train acc: 0.8610905654174885\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9106953719014506, Train acc: 0.861492673992674\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9109021253395624, Train acc: 0.86130698005698\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9110904486260862, Train acc: 0.8610777243589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.911120466397239, Train acc: 0.8610796882855707\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9107174831464646, Train acc: 0.8614375593542261\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9104029150948674, Train acc: 0.8617296446243815\n",
      "Val loss: 2.8919124603271484, Val acc: 0.876\n",
      "Epoch 76/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9087401496039496, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9111244301510673, Train acc: 0.8621794871794872\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.910417780916915, Train acc: 0.8628027065527065\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.909590984511579, Train acc: 0.8637152777777778\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.908451625628349, Train acc: 0.864690170940171\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.909676535177095, Train acc: 0.8633368945868946\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.910173421086555, Train acc: 0.8627136752136753\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9087282633170104, Train acc: 0.8638488247863247\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9083623071002145, Train acc: 0.8643459164292497\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9084488838146894, Train acc: 0.8642361111111111\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9091423334034268, Train acc: 0.8635392385392385\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9079680590548067, Train acc: 0.8646055911680912\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.907994722865429, Train acc: 0.8646038790269559\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9090711521723915, Train acc: 0.8634958791208791\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9091917505291454, Train acc: 0.8634615384615385\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.909122492131005, Train acc: 0.8633981036324786\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.908848116300764, Train acc: 0.8637034942182001\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9081969774007117, Train acc: 0.8643607549857549\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9081254272486676, Train acc: 0.8644146423751687\n",
      "Val loss: 2.9039127826690674, Val acc: 0.868\n",
      "Epoch 77/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.915827979389419, Train acc: 0.8576388888888888\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9112795224556556, Train acc: 0.8617788461538461\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.907697637536247, Train acc: 0.86502849002849\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.907633461503901, Train acc: 0.8650507478632479\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9051270965837004, Train acc: 0.8673076923076923\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.90587856932583, Train acc: 0.8667646011396012\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.906687472183917, Train acc: 0.86626221001221\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9071109468101435, Train acc: 0.8658520299145299\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.906636868220562, Train acc: 0.8664529914529915\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9070154764713387, Train acc: 0.8659455128205128\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9070776292494127, Train acc: 0.8659430846930847\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.907335767888615, Train acc: 0.8656294515669516\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.907854643564017, Train acc: 0.864973701512163\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9085920768197493, Train acc: 0.8642017704517705\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9080551167838595, Train acc: 0.8646545584045584\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9078489729227166, Train acc: 0.8647168803418803\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.90755047206006, Train acc: 0.8650075414781297\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9073425488367834, Train acc: 0.8651768755935423\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9074463608347507, Train acc: 0.8650894062078273\n",
      "Val loss: 2.89634370803833, Val acc: 0.876\n",
      "Epoch 78/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9098003195901203, Train acc: 0.8629807692307693\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9065223229237094, Train acc: 0.8656517094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.907012707827098, Train acc: 0.8651175213675214\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9072666412744765, Train acc: 0.8647168803418803\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9077659647688905, Train acc: 0.8638888888888889\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.90853261641967, Train acc: 0.8632478632478633\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.908392068085071, Train acc: 0.863476800976801\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9082617431114883, Train acc: 0.8638488247863247\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.908479134920995, Train acc: 0.8638117283950617\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9079056332254, Train acc: 0.8643696581196582\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9074930811261797, Train acc: 0.8647533022533023\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.906798691491456, Train acc: 0.8655181623931624\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9062941474086905, Train acc: 0.86620644312952\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9065495675591175, Train acc: 0.8658997252747253\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9064288370290035, Train acc: 0.8661680911680911\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.906800127691693, Train acc: 0.8658687232905983\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9071156047228413, Train acc: 0.8655417295123178\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9070666731819017, Train acc: 0.8655330009496676\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.907170779952797, Train acc: 0.8653986729644625\n",
      "Val loss: 2.885457992553711, Val acc: 0.89\n",
      "Epoch 79/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9092857226347313, Train acc: 0.8648504273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9042999224785047, Train acc: 0.8692574786324786\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.906029613608988, Train acc: 0.8674323361823362\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.905240567321451, Train acc: 0.8682558760683761\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.903621595333784, Train acc: 0.8694444444444445\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9032735090989332, Train acc: 0.869613603988604\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.903712397821075, Train acc: 0.8692384004884005\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9050884297770314, Train acc: 0.8678552350427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9052531766755627, Train acc: 0.8675807217473884\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.906235034445412, Train acc: 0.8666399572649572\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.906227011595387, Train acc: 0.8667200854700855\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.906373726336705, Train acc: 0.8665642806267806\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.906599041978283, Train acc: 0.866370808678501\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9070051963803825, Train acc: 0.8659569597069597\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9067813467096397, Train acc: 0.8661680911680911\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9066782382308927, Train acc: 0.8662359775641025\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9066110261425173, Train acc: 0.8662173202614379\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.906581880699875, Train acc: 0.8662155745489079\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9069818732870414, Train acc: 0.8658204003598741\n",
      "Val loss: 2.891101360321045, Val acc: 0.882\n",
      "Epoch 80/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.912123174748869, Train acc: 0.8608440170940171\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.910141333555564, Train acc: 0.8615117521367521\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9106235334336588, Train acc: 0.8611111111111112\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9097487325342293, Train acc: 0.8621794871794872\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.907782973998632, Train acc: 0.864423076923077\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9083557077962108, Train acc: 0.863960113960114\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9095288359347427, Train acc: 0.8627518315018315\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9091797178117638, Train acc: 0.863014155982906\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9090363392915934, Train acc: 0.8631588319088319\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.909485688372555, Train acc: 0.8624732905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9091735208506906, Train acc: 0.8626893939393939\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9085623652507095, Train acc: 0.8633591524216524\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9088439049491908, Train acc: 0.8630629520052597\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9078376978744953, Train acc: 0.864144536019536\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9081151554727147, Train acc: 0.8638710826210826\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.907978878571437, Train acc: 0.8640157585470085\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.907188284511, Train acc: 0.8648818501759679\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.907387677647336, Train acc: 0.8646130104463438\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.90714677947092, Train acc: 0.8648223121907332\n",
      "Val loss: 2.89748477935791, Val acc: 0.874\n",
      "Epoch 81/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9043917207636385, Train acc: 0.8667200854700855\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9062636071800165, Train acc: 0.8645833333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.906259999315963, Train acc: 0.8656517094017094\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.906018976472382, Train acc: 0.8664529914529915\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9049054492233144, Train acc: 0.867948717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.906326557835962, Train acc: 0.8663639601139601\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.906188231248122, Train acc: 0.8666437728937729\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.905962611365522, Train acc: 0.8667534722222222\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.906213114177852, Train acc: 0.8664529914529915\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9059083863201303, Train acc: 0.866693376068376\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9064403869611124, Train acc: 0.8663801476301476\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.907124853881336, Train acc: 0.8656071937321937\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9076508452122654, Train acc: 0.8648504273504274\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9078851721371195, Train acc: 0.8646978021978022\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9074936573322003, Train acc: 0.8651887464387464\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.906859759082142, Train acc: 0.8658019497863247\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9067860307736635, Train acc: 0.8658088235294118\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.906825180520133, Train acc: 0.8657852564102564\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9068692766542217, Train acc: 0.8656938821412505\n",
      "Val loss: 2.8938252925872803, Val acc: 0.876\n",
      "Epoch 82/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8946065637800427, Train acc: 0.8768696581196581\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.897421067596501, Train acc: 0.8739316239316239\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.900042570554293, Train acc: 0.8706374643874644\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9030235562569056, Train acc: 0.8679220085470085\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9061553730923904, Train acc: 0.864957264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9068880740054315, Train acc: 0.8643162393162394\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.907626411999247, Train acc: 0.8636294261294262\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.907218984559051, Train acc: 0.8641493055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9070450994703503, Train acc: 0.8645239791073125\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9072903404887924, Train acc: 0.8643963675213675\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.906840443518519, Train acc: 0.8649232711732712\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.906449537331562, Train acc: 0.8653623575498576\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9067134029531383, Train acc: 0.865240795529257\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9068143867019915, Train acc: 0.8653083028083028\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.90652349219363, Train acc: 0.865758547008547\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.906965697932447, Train acc: 0.8653011485042735\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9067432236707407, Train acc: 0.8656674208144797\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9067380308312454, Train acc: 0.8656517094017094\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9064815045475583, Train acc: 0.8659750337381916\n",
      "Val loss: 2.9132518768310547, Val acc: 0.858\n",
      "Epoch 83/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9061958137740436, Train acc: 0.8680555555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9053500381290402, Train acc: 0.8673878205128205\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.905042960772827, Train acc: 0.8671652421652422\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.904736431235941, Train acc: 0.8675213675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9056893104161974, Train acc: 0.8666132478632479\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.906436514990282, Train acc: 0.8659633190883191\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9065221925067086, Train acc: 0.8660714285714286\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.905493699842029, Train acc: 0.8671541132478633\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9062078273081258, Train acc: 0.8662749287749287\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9056586387829904, Train acc: 0.8667200854700855\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9047314824294745, Train acc: 0.8677884615384616\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9046159619279734, Train acc: 0.8678774928774928\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.904974301342585, Train acc: 0.8673775476660092\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.905486718930022, Train acc: 0.8668536324786325\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9058650461017574, Train acc: 0.8665420227920227\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.906102172839336, Train acc: 0.8662526709401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9057363406565395, Train acc: 0.8667357968828557\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.905120596700018, Train acc: 0.8673136277302944\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.904843769682671, Train acc: 0.8675635402609086\n",
      "Val loss: 2.8982954025268555, Val acc: 0.88\n",
      "Epoch 84/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9116874882298656, Train acc: 0.8600427350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.904320640441699, Train acc: 0.8675213675213675\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9049739647455026, Train acc: 0.8670762108262108\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.904606907795637, Train acc: 0.8669871794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9029766351748734, Train acc: 0.8688034188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.902536268247838, Train acc: 0.8693019943019943\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9016486774579655, Train acc: 0.8705738705738706\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9013624193831387, Train acc: 0.8710269764957265\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.902213033674336, Train acc: 0.8702219848053181\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9036501446340837, Train acc: 0.8687767094017094\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9043403498035305, Train acc: 0.8679341491841492\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.904405879159259, Train acc: 0.8679665242165242\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9044652533797666, Train acc: 0.8678911900065747\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.904798418319851, Train acc: 0.8676167582417582\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9051765022114813, Train acc: 0.867218660968661\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.905535637568205, Train acc: 0.8669370993589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9056876979921857, Train acc: 0.8667986425339367\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9056430512796894, Train acc: 0.866690408357075\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9058164235336847, Train acc: 0.8664670490328386\n",
      "Val loss: 2.8981802463531494, Val acc: 0.87\n",
      "Epoch 85/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9066642634889, Train acc: 0.8664529914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.903046704765059, Train acc: 0.8695245726495726\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.903399632527278, Train acc: 0.8692129629629629\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9040510506711454, Train acc: 0.8685229700854701\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.904752759444408, Train acc: 0.8677350427350428\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9053232299636234, Train acc: 0.8671652421652422\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.906111136752025, Train acc: 0.8667200854700855\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9051581850418677, Train acc: 0.8674212072649573\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9047915756645817, Train acc: 0.8677587844254511\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9041211784395395, Train acc: 0.8685363247863248\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.904121054913892, Train acc: 0.8684926184926185\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9037077912577876, Train acc: 0.8688568376068376\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9034189180979832, Train acc: 0.8692472057856673\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9032299074643406, Train acc: 0.86944826007326\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9039698507031826, Train acc: 0.8687678062678063\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.903932403422828, Train acc: 0.8687900641025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9037667177861035, Train acc: 0.8689039718451483\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9036829695742354, Train acc: 0.8690052231718899\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.903553371755486, Train acc: 0.8691239316239316\n",
      "Val loss: 2.892184257507324, Val acc: 0.878\n",
      "Epoch 86/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.910481579283364, Train acc: 0.8613782051282052\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9014427865672316, Train acc: 0.8704594017094017\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9026385297802437, Train acc: 0.8693019943019943\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9031359675603037, Train acc: 0.8691907051282052\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9028549055767874, Train acc: 0.8694444444444445\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.902860077697667, Train acc: 0.8693019943019943\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9036773237989935, Train acc: 0.8687042124542125\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9034929983636255, Train acc: 0.8688902243589743\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.90401576523088, Train acc: 0.8683820037986705\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9047837300178334, Train acc: 0.8676549145299145\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.904848311238263, Train acc: 0.8674970862470862\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.90408180310176, Train acc: 0.8683003917378918\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.904312730697642, Train acc: 0.8679528270874425\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9036436647140356, Train acc: 0.8685897435897436\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.90347160409998, Train acc: 0.8687856125356125\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9036083483797874, Train acc: 0.8685897435897436\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9037239563531045, Train acc: 0.8684483408748115\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9034008280164496, Train acc: 0.8687974833808167\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.903244260941547, Train acc: 0.8689411830859199\n",
      "Val loss: 2.8927416801452637, Val acc: 0.876\n",
      "Epoch 87/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.901475113681239, Train acc: 0.8717948717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9024215681940064, Train acc: 0.8709935897435898\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9012993407724927, Train acc: 0.8727742165242165\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.901241306056324, Train acc: 0.8725961538461539\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.900299280932826, Train acc: 0.8731837606837607\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.900561034849227, Train acc: 0.8723735754985755\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9001793742034314, Train acc: 0.8729014041514042\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9006111754311457, Train acc: 0.8723958333333334\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9001741692324763, Train acc: 0.872803893637227\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8998902530751676, Train acc: 0.8730235042735043\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.900676230080107, Train acc: 0.8723290598290598\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9008063947373306, Train acc: 0.8721064814814815\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.900920753334793, Train acc: 0.8719797830374754\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.901281948636885, Train acc: 0.8716040903540904\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.901344302033427, Train acc: 0.8714387464387464\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.901673574732919, Train acc: 0.87109375\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.901609581040641, Train acc: 0.8711821266968326\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.90176442590987, Train acc: 0.8709639126305793\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.901572169669834, Train acc: 0.8712325686009896\n",
      "Val loss: 2.8980836868286133, Val acc: 0.876\n",
      "Epoch 88/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.89517454815726, Train acc: 0.8792735042735043\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.899338070144001, Train acc: 0.8736645299145299\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9011637492057605, Train acc: 0.8717058404558404\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8996752974314566, Train acc: 0.8731303418803419\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9014041774293298, Train acc: 0.871207264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9011885936443624, Train acc: 0.8714832621082621\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.9019742832952367, Train acc: 0.8708409645909646\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9022846025788884, Train acc: 0.8705929487179487\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9027349677180974, Train acc: 0.8701032763532763\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9021948459820868, Train acc: 0.8706196581196581\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.903117334370291, Train acc: 0.8695609945609946\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.902025469517776, Train acc: 0.8706597222222222\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.90204528975377, Train acc: 0.8705004930966469\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9019026524854667, Train acc: 0.870764652014652\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9019138768187953, Train acc: 0.870744301994302\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9019039734306498, Train acc: 0.8708600427350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.902095304535165, Train acc: 0.8705850930115636\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9021405558980087, Train acc: 0.8704594017094017\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9025623276058856, Train acc: 0.8700517318938371\n",
      "Val loss: 2.9072630405426025, Val acc: 0.858\n",
      "Epoch 89/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.900354711418478, Train acc: 0.8728632478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9022936535696697, Train acc: 0.8711271367521367\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.902941312545385, Train acc: 0.8701923076923077\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9023454958557062, Train acc: 0.8707932692307693\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9015777269999186, Train acc: 0.871474358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9016223025118184, Train acc: 0.8714387464387464\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.901763739021124, Train acc: 0.8710699023199023\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.90410999660818, Train acc: 0.8684895833333334\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9036816169506916, Train acc: 0.8689755460588794\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9036873124603533, Train acc: 0.8689369658119658\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9035304907966974, Train acc: 0.8690753690753691\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.9033789266208636, Train acc: 0.8692574786324786\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.902695812103703, Train acc: 0.8699868507560815\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9024187439963933, Train acc: 0.8701732295482295\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.902025977838413, Train acc: 0.8704950142450143\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9017983333677306, Train acc: 0.8707264957264957\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9018246160193626, Train acc: 0.8705850930115636\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.901565988405704, Train acc: 0.8709787511870846\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.901240030280974, Train acc: 0.871330971659919\n",
      "Val loss: 2.8826019763946533, Val acc: 0.896\n",
      "Epoch 90/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.900079181051662, Train acc: 0.8712606837606838\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9023873653167334, Train acc: 0.8692574786324786\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.899781296395848, Train acc: 0.8716168091168092\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.901151394742167, Train acc: 0.8703926282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9020493087605534, Train acc: 0.8696047008547009\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9018464601277625, Train acc: 0.8700142450142451\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.901917217415331, Train acc: 0.86996336996337\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9019317323835487, Train acc: 0.8701923076923077\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.901553719358453, Train acc: 0.8707264957264957\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9028001610030474, Train acc: 0.8694978632478633\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9024561807547973, Train acc: 0.8698523698523698\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.902524616643574, Train acc: 0.8697471509971509\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9016444452650685, Train acc: 0.87060322156476\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.901887185262091, Train acc: 0.8704212454212454\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.9015409379942803, Train acc: 0.8709045584045584\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.901750107224171, Train acc: 0.8707431891025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.9016661435410627, Train acc: 0.8708836098541981\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.901783184561408, Train acc: 0.8707710113960114\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9020082785425774, Train acc: 0.870529689608637\n",
      "Val loss: 2.9088568687438965, Val acc: 0.86\n",
      "Epoch 91/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.901111380666749, Train acc: 0.8728632478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.9043549767926207, Train acc: 0.8680555555555556\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.903352143078448, Train acc: 0.8689458689458689\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.9046054110567794, Train acc: 0.867454594017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9022498053363246, Train acc: 0.8700854700854701\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.902715241807139, Train acc: 0.8697471509971509\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.903191335035331, Train acc: 0.8694673382173382\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9032538061977453, Train acc: 0.8695245726495726\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9026129809539882, Train acc: 0.8701032763532763\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.902460043043153, Train acc: 0.8704059829059829\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.9019080247634497, Train acc: 0.8708964646464646\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.901682207557211, Train acc: 0.8711493945868946\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.9022723774154433, Train acc: 0.8705210387902695\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.9016859314526102, Train acc: 0.8711080586080586\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.901628885513697, Train acc: 0.8710470085470086\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.9009872803575973, Train acc: 0.8716446314102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.900886247120891, Train acc: 0.8717320261437909\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.9003592226919626, Train acc: 0.8723142212725546\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.9005072026761116, Train acc: 0.8721603688708952\n",
      "Val loss: 2.8899025917053223, Val acc: 0.88\n",
      "Epoch 92/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8959848758501883, Train acc: 0.875267094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8954165032786183, Train acc: 0.8763354700854701\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.897673171469968, Train acc: 0.874465811965812\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.898280296060774, Train acc: 0.8740651709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8990550090105107, Train acc: 0.8736111111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.89789088567098, Train acc: 0.8749554843304843\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.898817609081338, Train acc: 0.8740079365079365\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8989812255415144, Train acc: 0.8737646901709402\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8980354492933773, Train acc: 0.8747625830959165\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8984267416163387, Train acc: 0.874465811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.897463220538515, Train acc: 0.8754613442113443\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8979331555529537, Train acc: 0.8750222578347578\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.898048980388102, Train acc: 0.875\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8978828130624232, Train acc: 0.8750953907203908\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.898207352304051, Train acc: 0.874661680911681\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8986356613727717, Train acc: 0.874198717948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.899304536073396, Train acc: 0.8734445701357466\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.89929770834652, Train acc: 0.8734567901234568\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.899089774407624, Train acc: 0.8736082995951417\n",
      "Val loss: 2.8941807746887207, Val acc: 0.88\n",
      "Epoch 93/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.90239683787028, Train acc: 0.8701923076923077\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8965766572544718, Train acc: 0.8751335470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.897014351651879, Train acc: 0.874465811965812\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8987452764796395, Train acc: 0.8729967948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.89939392122448, Train acc: 0.8724893162393162\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.900613636033148, Train acc: 0.8711716524216524\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.901115476284563, Train acc: 0.8704594017094017\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.9010011665841455, Train acc: 0.8707264957264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9006610086500815, Train acc: 0.8711716524216524\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.9000394725392007, Train acc: 0.8718482905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.89917951808387, Train acc: 0.8727904040404041\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.898263277830901, Train acc: 0.8737758190883191\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.89852582209209, Train acc: 0.8735823471400395\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8993379649079616, Train acc: 0.872748778998779\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8993197214229833, Train acc: 0.8728454415954416\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.899104522079484, Train acc: 0.8730301816239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.899217708917525, Train acc: 0.8729260935143288\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.899011267782503, Train acc: 0.873070987654321\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8990547539037035, Train acc: 0.873158457040036\n",
      "Val loss: 2.9075050354003906, Val acc: 0.86\n",
      "Epoch 94/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9042821052746897, Train acc: 0.8680555555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8987331135660157, Train acc: 0.8733974358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8975922463626262, Train acc: 0.8745548433048433\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8991084093721504, Train acc: 0.8733306623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.899516390939044, Train acc: 0.8727029914529915\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8981656871969546, Train acc: 0.8740206552706553\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8981759510343035, Train acc: 0.8740079365079365\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.898116728434196, Train acc: 0.8740985576923077\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8965498959576643, Train acc: 0.8756232193732194\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8966965832261957, Train acc: 0.8754006410256411\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8967403957986426, Train acc: 0.8752428127428128\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8972597697861175, Train acc: 0.874732905982906\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.897166726308618, Train acc: 0.8749383629191322\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8972792814648343, Train acc: 0.8747519841269841\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.897171410672006, Train acc: 0.8748753561253562\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8976879214119706, Train acc: 0.8743322649572649\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.897602448336379, Train acc: 0.874402966314731\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.897818942015667, Train acc: 0.8741542022792023\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.898243257498559, Train acc: 0.8737207602339181\n",
      "Val loss: 2.9058995246887207, Val acc: 0.864\n",
      "Epoch 95/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.9066516782483482, Train acc: 0.8640491452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.904892810389527, Train acc: 0.8655181623931624\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9056277057723783, Train acc: 0.8648504273504274\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.904718226347214, Train acc: 0.8662526709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.9049648549821643, Train acc: 0.8665598290598291\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.9031387030908524, Train acc: 0.8688123219373219\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.900766315332117, Train acc: 0.8714514652014652\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8997556095958776, Train acc: 0.8724959935897436\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.9003625966658055, Train acc: 0.8719432573599241\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.899149128718254, Train acc: 0.8732638888888888\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8984689731049555, Train acc: 0.8740530303030303\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.897862840921451, Train acc: 0.8745993589743589\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8979534983713324, Train acc: 0.8744863576594346\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8972947147739676, Train acc: 0.8751907814407814\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.897921911739556, Train acc: 0.8745192307692308\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8973656985749545, Train acc: 0.8751001602564102\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.897617957411009, Train acc: 0.8748114630467572\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.897730112302224, Train acc: 0.8747625830959165\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.897988993468394, Train acc: 0.8745220422852001\n",
      "Val loss: 2.8944053649902344, Val acc: 0.878\n",
      "Epoch 96/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.900105570116614, Train acc: 0.8733974358974359\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.895476900614225, Train acc: 0.8779380341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.9015956067631388, Train acc: 0.8717948717948718\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8995505459288244, Train acc: 0.8735309829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8984597377288037, Train acc: 0.8746794871794872\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.900029136923983, Train acc: 0.8730858262108262\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.899393452247275, Train acc: 0.8740460927960928\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8985612186101766, Train acc: 0.8749666132478633\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.898364411686447, Train acc: 0.8749109686609686\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.897634266788124, Train acc: 0.8756410256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.897092065240583, Train acc: 0.8759955322455323\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8972044288263024, Train acc: 0.8757567663817664\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8977511661135784, Train acc: 0.8751232741617357\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8976115963543436, Train acc: 0.875267094017094\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8982549050594666, Train acc: 0.874661680911681\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.898120542239939, Train acc: 0.8747162126068376\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.898128032444589, Train acc: 0.874670060331825\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8982989034082136, Train acc: 0.8744806505223172\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.898180980806784, Train acc: 0.8746485605038237\n",
      "Val loss: 2.912879228591919, Val acc: 0.856\n",
      "Epoch 97/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.8932583230173488, Train acc: 0.8784722222222222\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8957153864395924, Train acc: 0.8766025641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.895495174277542, Train acc: 0.8767806267806267\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8959417811825743, Train acc: 0.8762019230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.896965175204807, Train acc: 0.8751068376068376\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.8964308527460125, Train acc: 0.8754451566951567\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8958467124291536, Train acc: 0.8762210012210012\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8966273517690153, Train acc: 0.8757011217948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8972151358695903, Train acc: 0.8751780626780626\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8977886898904783, Train acc: 0.8747061965811965\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8978309449801265, Train acc: 0.8746114996114996\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8966469886975412, Train acc: 0.8757567663817664\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.897142068090445, Train acc: 0.8752465483234714\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.897320986958505, Train acc: 0.8750190781440782\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.897332426082035, Train acc: 0.8749821937321938\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8977938332618813, Train acc: 0.8745158920940171\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8972723686617186, Train acc: 0.8750942684766214\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8973691387620293, Train acc: 0.8749554843304843\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8974520541288484, Train acc: 0.8748594242015295\n",
      "Val loss: 2.877243757247925, Val acc: 0.898\n",
      "Epoch 98/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.900960645105085, Train acc: 0.8709935897435898\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8958337001311474, Train acc: 0.8763354700854701\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.894889124438294, Train acc: 0.8773148148148148\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.894981069951995, Train acc: 0.8772035256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.894686131599622, Train acc: 0.8777777777777778\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.893999277696311, Train acc: 0.8784722222222222\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.8954759997762602, Train acc: 0.8769459706959707\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.896090966768754, Train acc: 0.8762353098290598\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.895192403965413, Train acc: 0.8773148148148148\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8945188667020227, Train acc: 0.8778846153846154\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.894433806826185, Train acc: 0.878010878010878\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.89431321077537, Train acc: 0.8780715811965812\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8942966509774513, Train acc: 0.8781845825115056\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.8942057530376357, Train acc: 0.8784340659340659\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8949250585333233, Train acc: 0.8777777777777778\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8949979519487448, Train acc: 0.8777377136752137\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8949020819906135, Train acc: 0.8778123428858723\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8952375820219687, Train acc: 0.8774335232668566\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8954489693360665, Train acc: 0.8771789248762933\n",
      "Val loss: 2.8925247192382812, Val acc: 0.88\n",
      "Epoch 99/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.899116615963797, Train acc: 0.8717948717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.8961535753347936, Train acc: 0.875801282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.894737476636881, Train acc: 0.8772257834757835\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.8931547724283657, Train acc: 0.8783386752136753\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.8929901518373407, Train acc: 0.8787393162393162\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.894277317571504, Train acc: 0.8772702991452992\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.895884862312904, Train acc: 0.8756105006105006\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.8953072724179325, Train acc: 0.8764022435897436\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8950051310395244, Train acc: 0.8768399810066477\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.8956973689234156, Train acc: 0.8760683760683761\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.896650728738484, Train acc: 0.8754370629370629\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.896386179489288, Train acc: 0.8759570868945868\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.8955336626543176, Train acc: 0.8769518408941486\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.895668308959048, Train acc: 0.87685057997558\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.895871224090924, Train acc: 0.8765669515669515\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.8963089799269652, Train acc: 0.876151842948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8961627932875045, Train acc: 0.8764140271493213\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.895811960341697, Train acc: 0.8767806267806267\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.895790579079831, Train acc: 0.8767290823211876\n",
      "Val loss: 2.8818609714508057, Val acc: 0.892\n",
      "Epoch 100/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.898146136194213, Train acc: 0.8739316239316239\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.895760279435378, Train acc: 0.8770032051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.8973785173519384, Train acc: 0.8762464387464387\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.895171214372684, Train acc: 0.8782051282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.893235814673269, Train acc: 0.8796474358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.893073103027126, Train acc: 0.8796296296296297\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.893912056572417, Train acc: 0.8787011599511599\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.893678379109782, Train acc: 0.87890625\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.8937393889920777, Train acc: 0.8787986704653371\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 2.894893514193021, Train acc: 0.8776175213675214\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 2.8939364727097328, Train acc: 0.8786179098679099\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 2.8939877753583794, Train acc: 0.8785835113960114\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 2.893844862661418, Train acc: 0.878698224852071\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 2.894098966695159, Train acc: 0.8783768315018315\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 2.8946784952766875, Train acc: 0.8778311965811966\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 2.894587918351858, Train acc: 0.8779046474358975\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 2.8948494246882253, Train acc: 0.8776866515837104\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 2.8954038741128962, Train acc: 0.8769735280151947\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 2.8961359002383187, Train acc: 0.8762089518668466\n",
      "Val loss: 2.895501136779785, Val acc: 0.876\n",
      "Tiempo total de entrenamiento: 2056.1718 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-5\n",
    "dropout_p = 0.7\n",
    "batch_size = 32\n",
    "criterion = perdida_regularizada_entropia\n",
    "epochs = 100\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    metadata_train,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    metadata_val,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    "    beta=0.1,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.91      0.91      0.91       100\n",
      "          SN       0.90      0.72      0.80       100\n",
      "          VS       0.89      0.93      0.91       100\n",
      "    asteroid       0.86      0.95      0.90       100\n",
      "       bogus       0.90      0.95      0.93       100\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.89      0.89      0.89       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Asegúrate de tener tus datos de test preparados\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 2.0789039318378153, Train acc: 0.02216880341880342\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 2.078725120960138, Train acc: 0.026709401709401708\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 2.0785311674460387, Train acc: 0.03178418803418803\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 2.0776742024299426, Train acc: 0.04720886752136752\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 2.0659328941606048, Train acc: 0.07425213675213675\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 2.0419857060467757, Train acc: 0.09602029914529915\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 2.0240663442856226, Train acc: 0.11046245421245421\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 2.0099913006664343, Train acc: 0.1219951923076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 2.0000503866987236, Train acc: 0.12995607787274455\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.9916433371030366, Train acc: 0.13640491452991452\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.9840478424703603, Train acc: 0.1427496114996115\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.9778846787591267, Train acc: 0.14763621794871795\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.9728326474577875, Train acc: 0.15152449046679817\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.9687782955402566, Train acc: 0.15476190476190477\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.964847572304924, Train acc: 0.15762108262108263\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.9610412916821292, Train acc: 0.16075721153846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.958312082074886, Train acc: 0.16272310206133736\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.9557973013983831, Train acc: 0.16466346153846154\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.9533610035050735, Train acc: 0.16669478182636077\n",
      "Val loss: 3.5518252849578857, Val acc: 0.2\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.901338575232742, Train acc: 0.2110042735042735\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.9047014662343213, Train acc: 0.20686431623931623\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.9064794931656275, Train acc: 0.20584045584045585\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.9093535909285912, Train acc: 0.20219017094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.9084650382017478, Train acc: 0.2032051282051282\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.9085206465843396, Train acc: 0.20281339031339032\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.9072007940802382, Train acc: 0.20436507936507936\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.9066328210198982, Train acc: 0.20512820512820512\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.9071189544366183, Train acc: 0.20429724596391263\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.9064826720800154, Train acc: 0.2049145299145299\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.9054996596812832, Train acc: 0.20568667443667443\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.9048232770713307, Train acc: 0.20608529202279202\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.9046181314168675, Train acc: 0.20613494411571334\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.9047300170775008, Train acc: 0.20592948717948717\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.9040274533111485, Train acc: 0.20665954415954416\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.903610538595762, Train acc: 0.2067474626068376\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.90368810654166, Train acc: 0.20688788335847158\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.9035669952376275, Train acc: 0.20720560303893637\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.9029776904157192, Train acc: 0.20789754835807467\n",
      "Val loss: 3.5291836261749268, Val acc: 0.33\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.8961435505467603, Train acc: 0.2152777777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.9011990901751397, Train acc: 0.2076655982905983\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.902459977359174, Train acc: 0.20539529914529914\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.9006466651574159, Train acc: 0.20793269230769232\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.8994534011579987, Train acc: 0.20966880341880342\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.8983342307924884, Train acc: 0.21006944444444445\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.897745765318073, Train acc: 0.21127136752136752\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.8984137875402076, Train acc: 0.21047008547008547\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.8971777659648956, Train acc: 0.21210232668566\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.8967470291333322, Train acc: 0.21215277777777777\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.8965312562066636, Train acc: 0.2125097125097125\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.896057232153042, Train acc: 0.21307425213675213\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.8954844057834281, Train acc: 0.2133259368836292\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.8946471562170137, Train acc: 0.2142094017094017\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.8936734999686564, Train acc: 0.21511752136752138\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.8930438094668918, Train acc: 0.21596220619658119\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.8929811353477297, Train acc: 0.21570198592257417\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.8924101246954257, Train acc: 0.2162571225071225\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.8920872437755536, Train acc: 0.21679599640125957\n",
      "Val loss: 3.5096588134765625, Val acc: 0.364\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.8874356176099207, Train acc: 0.22409188034188035\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.8855772660328791, Train acc: 0.22449252136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.8849718000134852, Train acc: 0.22631766381766383\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.8824837564403176, Train acc: 0.23190438034188035\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.879531211934538, Train acc: 0.234241452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.878764403511656, Train acc: 0.23504273504273504\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.8774813135610542, Train acc: 0.23588217338217338\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.8768796869832227, Train acc: 0.23594417735042736\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.8764305150973037, Train acc: 0.23652659069325735\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.8758354347995203, Train acc: 0.23709935897435896\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.875379936771052, Train acc: 0.23778651903651904\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.8739471462717083, Train acc: 0.23960559116809116\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.8734070395733635, Train acc: 0.240138067061144\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.8727742781423677, Train acc: 0.240861568986569\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.871661148315821, Train acc: 0.24228988603988605\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.8712419932469344, Train acc: 0.24263822115384615\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.8703142513037327, Train acc: 0.2439039718451483\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.8688683018498724, Train acc: 0.2458452041785375\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.8678548042161798, Train acc: 0.2474555780476833\n",
      "Val loss: 3.4456329345703125, Val acc: 0.492\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.8458394555964022, Train acc: 0.28552350427350426\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.846937299793602, Train acc: 0.2780448717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.8429709302733766, Train acc: 0.2835648148148148\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.840557558923705, Train acc: 0.28505608974358976\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.8370497084071493, Train acc: 0.2920405982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.835251134005707, Train acc: 0.29407051282051283\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.8307708407205248, Train acc: 0.298992673992674\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.8280340124908676, Train acc: 0.3031517094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.8244921936948075, Train acc: 0.307247150997151\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.8209149440129597, Train acc: 0.31175213675213675\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.8168325798402207, Train acc: 0.31631216006216006\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.813983435134942, Train acc: 0.31982282763532766\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.8114410249282467, Train acc: 0.3227317554240631\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.8074136799217289, Train acc: 0.3274381868131868\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.8044675763176377, Train acc: 0.33101851851851855\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.8008023887617974, Train acc: 0.33486912393162394\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.7976769872380598, Train acc: 0.338298139768728\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.7943191082276058, Train acc: 0.34186550332383664\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.791408554691481, Train acc: 0.34476214574898784\n",
      "Val loss: 3.332990884780884, Val acc: 0.438\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.739677622786954, Train acc: 0.390758547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.736315003827087, Train acc: 0.3954326923076923\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.7352470359910928, Train acc: 0.3995726495726496\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.7313303962731972, Train acc: 0.40411324786324787\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.728685963867057, Train acc: 0.40737179487179487\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.7265572799237026, Train acc: 0.40900997150997154\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.7249912671262673, Train acc: 0.4103708791208791\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.7236287064022489, Train acc: 0.41139155982905984\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.7201241774436755, Train acc: 0.4151531339031339\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.7189188818646293, Train acc: 0.4155181623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.717584821727726, Train acc: 0.4172494172494173\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.7146266411851954, Train acc: 0.41980502136752135\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.7131089458804158, Train acc: 0.4211867192636423\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.7112184393245804, Train acc: 0.4230960012210012\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.7095250154152895, Train acc: 0.42467948717948717\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.707525587744183, Train acc: 0.4265992254273504\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.7061000567935352, Train acc: 0.42801030668677725\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.7049492260216534, Train acc: 0.42926460113960113\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.7029661234573796, Train acc: 0.4312162618083671\n",
      "Val loss: 3.275606870651245, Val acc: 0.522\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.6812800668243668, Train acc: 0.4545940170940171\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.6810658691275833, Train acc: 0.4512553418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.6761562036313222, Train acc: 0.45601851851851855\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.6712955868142283, Train acc: 0.46153846153846156\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.6660258606967764, Train acc: 0.4669337606837607\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.663719506005616, Train acc: 0.46897257834757833\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.6616209601307963, Train acc: 0.47046703296703296\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.6611495496880295, Train acc: 0.4713541666666667\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.6588489232692736, Train acc: 0.4744183285849953\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.6573314591350718, Train acc: 0.47630876068376066\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.6553219648507924, Train acc: 0.4788752913752914\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.6535662745478485, Train acc: 0.4807247150997151\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.653077393750624, Train acc: 0.48101577909270216\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.6519633364182544, Train acc: 0.48193299755799757\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.6506144689358877, Train acc: 0.48331552706552705\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.6498389459318585, Train acc: 0.4837239583333333\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.6487912260269388, Train acc: 0.48452425842131724\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.6477515551439377, Train acc: 0.48576982431149096\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.6461388627205462, Train acc: 0.4876152721547458\n",
      "Val loss: 3.220111608505249, Val acc: 0.558\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.6100222371582291, Train acc: 0.5267094017094017\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.6081769160735302, Train acc: 0.5271100427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.6107363116707218, Train acc: 0.5244836182336182\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.612832293041751, Train acc: 0.5234375\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.6119092374785333, Train acc: 0.5237713675213675\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.6105053825595779, Train acc: 0.5244836182336182\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.6089495055113665, Train acc: 0.5263278388278388\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.61031681452042, Train acc: 0.5240384615384616\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.6100964598279845, Train acc: 0.5238307217473884\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.6088447546347593, Train acc: 0.5254273504273504\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.6082092613421768, Train acc: 0.5261509324009324\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.6074667162025755, Train acc: 0.5272881054131054\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.606476001877443, Train acc: 0.5282297830374754\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.605540200176402, Train acc: 0.529265873015873\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.6041436352960745, Train acc: 0.5308404558404558\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.603099276876857, Train acc: 0.5316673344017094\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.6026978238615455, Train acc: 0.5319413021618904\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.6023163496598898, Train acc: 0.5323925688509021\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.6018327623136506, Train acc: 0.5326135852451642\n",
      "Val loss: 3.1951537132263184, Val acc: 0.57\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.593506242474939, Train acc: 0.5357905982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.5879705614513822, Train acc: 0.5413995726495726\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.5825275804242518, Train acc: 0.5475427350427351\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.5800730713412292, Train acc: 0.5491452991452992\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.5806731713123812, Train acc: 0.5495192307692308\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.5811913516107108, Train acc: 0.5482104700854701\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.5784605978754995, Train acc: 0.5517017704517705\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.5779448319704106, Train acc: 0.5529513888888888\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.5778384059243054, Train acc: 0.5534484805318138\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.5762582730024288, Train acc: 0.5554220085470085\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.5754168581944061, Train acc: 0.5565268065268065\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.5750269387182687, Train acc: 0.5567797364672364\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.5751669737635907, Train acc: 0.5565828402366864\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.5743051119630882, Train acc: 0.5576350732600732\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.574039286969394, Train acc: 0.5582977207977208\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.573567106300949, Train acc: 0.5589443108974359\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.5725035901163138, Train acc: 0.5600804424333836\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.5722366613313796, Train acc: 0.5603187321937322\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.5716354801909, Train acc: 0.56092555105713\n",
      "Val loss: 3.169703960418701, Val acc: 0.612\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.5505295268490784, Train acc: 0.5803952991452992\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.5497308633266351, Train acc: 0.5837339743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.5532790982825124, Train acc: 0.5811965811965812\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.5541128368459196, Train acc: 0.5803285256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.5559084729251698, Train acc: 0.5794337606837607\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.554680897639348, Train acc: 0.5813746438746439\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.5535072462317125, Train acc: 0.5829136141636142\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.5510270564984052, Train acc: 0.5855368589743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.5512506119546048, Train acc: 0.5852029914529915\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.5505428621911594, Train acc: 0.5858707264957265\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.5495648326666298, Train acc: 0.5873640248640248\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.549218642745602, Train acc: 0.587829415954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.5478351075425107, Train acc: 0.5891066732412886\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.54698203203879, Train acc: 0.5903350122100122\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.5469120411451724, Train acc: 0.5907763532763532\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.5465813015515988, Train acc: 0.5910456730769231\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.5464426417516188, Train acc: 0.5913618652589241\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.5455845888750053, Train acc: 0.5922364672364673\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.5445299851052459, Train acc: 0.5936094242015295\n",
      "Val loss: 3.1303110122680664, Val acc: 0.68\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.5297574344863238, Train acc: 0.6116452991452992\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.526007157105666, Train acc: 0.6168536324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.5260155234921013, Train acc: 0.6162749287749287\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.5251231504301739, Train acc: 0.6153846153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.5236428004044753, Train acc: 0.6174145299145299\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.5217877650193, Train acc: 0.6186342592592593\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.5224402751386965, Train acc: 0.6177884615384616\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.5208125618787913, Train acc: 0.6190571581196581\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.5210711263183854, Train acc: 0.6185600664767331\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.5198090076446533, Train acc: 0.6197115384615385\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.520077541806296, Train acc: 0.6192210567210568\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.5198848376586567, Train acc: 0.6192574786324786\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.5189035699055589, Train acc: 0.6202950361604208\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.5185501940812238, Train acc: 0.6202876984126984\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.518709971762111, Train acc: 0.6200320512820513\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.5182941818339193, Train acc: 0.6204093215811965\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.5170752517897739, Train acc: 0.6215906234288587\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.5161537672379757, Train acc: 0.6225813152896487\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.5158221300153436, Train acc: 0.6225820962663068\n",
      "Val loss: 3.115948438644409, Val acc: 0.676\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.516491290850517, Train acc: 0.6207264957264957\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.5057521131303575, Train acc: 0.6336805555555556\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.5030571945712097, Train acc: 0.6366631054131054\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.5033997129171321, Train acc: 0.6342147435897436\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.5043047941648042, Train acc: 0.6327457264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.502574310003862, Train acc: 0.6338586182336182\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.5018070297217923, Train acc: 0.6351114163614163\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.500496560946489, Train acc: 0.6357505341880342\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.5013300854029932, Train acc: 0.6348528015194682\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.501322503375192, Train acc: 0.6348290598290598\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.5000993216232263, Train acc: 0.635440947940948\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.500491680582704, Train acc: 0.6348602207977208\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.5003796158300897, Train acc: 0.6351701183431953\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.5002301657738413, Train acc: 0.6353021978021978\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4997145186801921, Train acc: 0.635844017094017\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4994699832720635, Train acc: 0.6356837606837606\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.49909364344068, Train acc: 0.6357466063348416\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4992326280669948, Train acc: 0.635772792022792\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4990349963501772, Train acc: 0.6361336032388664\n",
      "Val loss: 3.106627941131592, Val acc: 0.68\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4929425492245927, Train acc: 0.6402243589743589\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4945790482382488, Train acc: 0.6390224358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4917664521100513, Train acc: 0.6428062678062678\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4937188385898232, Train acc: 0.640892094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4940615886296982, Train acc: 0.6402777777777777\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.489676072047307, Train acc: 0.6459223646723646\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4912036408434857, Train acc: 0.6439636752136753\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4914365958454263, Train acc: 0.6438969017094017\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4911795717585348, Train acc: 0.6441417378917379\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.490652768020956, Train acc: 0.6450053418803419\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4913049570422998, Train acc: 0.6441579254079254\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4905332705573817, Train acc: 0.6449207621082621\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4907580506715392, Train acc: 0.6444978632478633\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4911508616946993, Train acc: 0.6440209096459096\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4908592580050823, Train acc: 0.6442307692307693\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4906364615656371, Train acc: 0.6444143963675214\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4908832311030906, Train acc: 0.6440579436902967\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4905197186800603, Train acc: 0.6445868945868946\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4902232525534207, Train acc: 0.6448211875843455\n",
      "Val loss: 3.1068990230560303, Val acc: 0.68\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4960820063566551, Train acc: 0.6380876068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4902632939509857, Train acc: 0.6442307692307693\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4847157083005986, Train acc: 0.6491274928774928\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4840069346957736, Train acc: 0.6507077991452992\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4869375803531744, Train acc: 0.6462606837606838\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4854043647434636, Train acc: 0.6475694444444444\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4855902526841496, Train acc: 0.6471306471306472\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4856056461476872, Train acc: 0.6471688034188035\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4857536233954507, Train acc: 0.6471688034188035\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4845556012585632, Train acc: 0.6486912393162393\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4845380375527928, Train acc: 0.6488927738927739\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.484476049910923, Train acc: 0.6490162037037037\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4842049434418587, Train acc: 0.6493055555555556\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4835484048240206, Train acc: 0.6502213064713065\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4840437485621525, Train acc: 0.6496082621082621\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4839113284634728, Train acc: 0.6499232104700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4831386141108172, Train acc: 0.650593891402715\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.482016862627448, Train acc: 0.6517539173789174\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.481923275577341, Train acc: 0.6517656320287899\n",
      "Val loss: 3.0965044498443604, Val acc: 0.696\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.466022742100251, Train acc: 0.6693376068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4747956349299505, Train acc: 0.6603899572649573\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4756246027443822, Train acc: 0.6584757834757835\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4805167271540716, Train acc: 0.6527777777777778\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4811436318943643, Train acc: 0.6517094017094017\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4823987035669832, Train acc: 0.6497952279202279\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4815478802309514, Train acc: 0.6513659951159951\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4795447753535376, Train acc: 0.6536124465811965\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4804161652314132, Train acc: 0.6531339031339032\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4797715382698255, Train acc: 0.6538728632478632\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4805296630755633, Train acc: 0.6526320901320901\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.480594463328011, Train acc: 0.6527777777777778\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.480353365596543, Train acc: 0.652983234714004\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.479259325180007, Train acc: 0.6540941697191697\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.478861057113039, Train acc: 0.6543803418803419\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.477568680149877, Train acc: 0.6555321848290598\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4773939415819983, Train acc: 0.6557472347913524\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4770126755987025, Train acc: 0.6561906457739791\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4768861624983265, Train acc: 0.6562921727395412\n",
      "Val loss: 3.089359998703003, Val acc: 0.708\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4776735672583947, Train acc: 0.6541132478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4785240154999952, Train acc: 0.6533119658119658\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4730911995270992, Train acc: 0.6590099715099715\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4724131975418482, Train acc: 0.6604567307692307\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4696721541575897, Train acc: 0.6634615384615384\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.469542302297391, Train acc: 0.6633279914529915\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.469987859073867, Train acc: 0.6631944444444444\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4689937631289165, Train acc: 0.6649639423076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4686348594491638, Train acc: 0.6657763532763533\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4685277975522555, Train acc: 0.6661324786324786\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.467947727965123, Train acc: 0.666958041958042\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4673916458064675, Train acc: 0.6676014957264957\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.467533690611208, Train acc: 0.6674268573307035\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4677154089213873, Train acc: 0.6668574481074481\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.468412478900703, Train acc: 0.6661146723646724\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4682619060970779, Train acc: 0.6664162660256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4674940521600799, Train acc: 0.6672008547008547\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4677755116057871, Train acc: 0.6670969848053181\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4680652579797906, Train acc: 0.6668634727845254\n",
      "Val loss: 3.081709384918213, Val acc: 0.71\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4562182100410135, Train acc: 0.6792200854700855\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4530407047679281, Train acc: 0.6850961538461539\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4533029845637133, Train acc: 0.6819800569800569\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4588332395268302, Train acc: 0.6760149572649573\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4569934429266513, Train acc: 0.6776175213675214\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.457922739860339, Train acc: 0.6755698005698005\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.460185362451388, Train acc: 0.6736111111111112\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.461130222194215, Train acc: 0.6726095085470085\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4615073570838342, Train acc: 0.6721866096866097\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4624187883148845, Train acc: 0.6711805555555556\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.461747290463807, Train acc: 0.6719599844599845\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4618843296314576, Train acc: 0.6716524216524217\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.461449514636078, Train acc: 0.6723167324128863\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.461744082425249, Train acc: 0.672142094017094\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4621760069474876, Train acc: 0.6717236467236467\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4631180978483624, Train acc: 0.6707732371794872\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4636443766473586, Train acc: 0.6703431372549019\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4636813615009203, Train acc: 0.6705543684710351\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4640917278226302, Train acc: 0.6700826585695007\n",
      "Val loss: 3.0721542835235596, Val acc: 0.72\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4610459784157255, Train acc: 0.6744123931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.451871990138649, Train acc: 0.6845619658119658\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4531588615515294, Train acc: 0.6817129629629629\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4534529360950503, Train acc: 0.6814236111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.454831507267096, Train acc: 0.6801282051282052\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.455186869004513, Train acc: 0.6798878205128205\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4579910285889157, Train acc: 0.676510989010989\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.458320180575053, Train acc: 0.6766826923076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4582042642015565, Train acc: 0.6767568850902185\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4581485379455437, Train acc: 0.6773771367521367\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4586646417915683, Train acc: 0.6767676767676768\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.458723340109203, Train acc: 0.6767494658119658\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4590604733198116, Train acc: 0.675973865877712\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.458758787066654, Train acc: 0.6761485042735043\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4591824490799863, Train acc: 0.6756766381766381\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.459205725381517, Train acc: 0.6756143162393162\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4588233521980758, Train acc: 0.6759521116138764\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4585930818845743, Train acc: 0.6759259259259259\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4584484009178598, Train acc: 0.6758884390463338\n",
      "Val loss: 3.0733702182769775, Val acc: 0.712\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4545705379583898, Train acc: 0.6776175213675214\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4557776522432637, Train acc: 0.6786858974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4548421367960438, Train acc: 0.6780626780626781\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4551697067725353, Train acc: 0.6775507478632479\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4548245593013927, Train acc: 0.6788461538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4553380039682415, Train acc: 0.6777510683760684\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4546452459398207, Train acc: 0.6786858974358975\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4516409770545797, Train acc: 0.6816573183760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4507607544249619, Train acc: 0.68284069325736\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4516835396106427, Train acc: 0.6817040598290598\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4528289413674451, Train acc: 0.6809440559440559\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.453213048456741, Train acc: 0.6810007122507122\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4542576401426477, Train acc: 0.6800419132149902\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4543842460064078, Train acc: 0.6798878205128205\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4542205523901175, Train acc: 0.6798789173789174\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.454233935246101, Train acc: 0.6798711271367521\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.454558863119602, Train acc: 0.6797542735042735\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4547951418450076, Train acc: 0.6797691120607787\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4546901360321731, Train acc: 0.6799932523616734\n",
      "Val loss: 3.0694310665130615, Val acc: 0.73\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4541126907381237, Train acc: 0.6797542735042735\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.450747013092041, Train acc: 0.6849626068376068\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4547140299424826, Train acc: 0.6797542735042735\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4552313970704365, Train acc: 0.6794871794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4567731042193552, Train acc: 0.6775641025641026\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.455758524076891, Train acc: 0.6790865384615384\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.455866965327653, Train acc: 0.6786095848595849\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4552684306588948, Train acc: 0.6791199252136753\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4530773984740601, Train acc: 0.6819503798670465\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4540392013696524, Train acc: 0.6805822649572649\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.452850922745332, Train acc: 0.6819881507381508\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4520996379376816, Train acc: 0.6826255341880342\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4522079864980986, Train acc: 0.6826101249178173\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4522254950252063, Train acc: 0.6823679792429792\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.451422515418115, Train acc: 0.6832086894586895\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4514196499800072, Train acc: 0.6832098023504274\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4508043387956269, Train acc: 0.6839963549522373\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.449742335652807, Train acc: 0.6851406695156695\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4498387052301775, Train acc: 0.6850399235267657\n",
      "Val loss: 3.066598892211914, Val acc: 0.722\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4437694284651015, Train acc: 0.6899038461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4397361054379716, Train acc: 0.6952457264957265\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4389742201889342, Train acc: 0.6948896011396012\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4416948215574281, Train acc: 0.6929754273504274\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4440189227079734, Train acc: 0.6901175213675214\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4465227762178818, Train acc: 0.6876780626780626\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4480854492629986, Train acc: 0.6864316239316239\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4480937892555172, Train acc: 0.6863314636752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4483670370984054, Train acc: 0.6862832383665717\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4478989861969256, Train acc: 0.6868322649572649\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4479877294823469, Train acc: 0.6868201243201243\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4479559333915384, Train acc: 0.6865651709401709\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4476917801053801, Train acc: 0.686801446416831\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4472327683580373, Train acc: 0.6872901404151404\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4471982087844457, Train acc: 0.6871260683760684\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.447162287357526, Train acc: 0.6872829861111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.446911521746682, Train acc: 0.6875785570638512\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4463676532335996, Train acc: 0.6880787037037037\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4458755851596312, Train acc: 0.688554318488529\n",
      "Val loss: 3.0675668716430664, Val acc: 0.724\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.446563859271188, Train acc: 0.6891025641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.445991372450804, Train acc: 0.6901709401709402\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.446714947366307, Train acc: 0.6893696581196581\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4451451780449631, Train acc: 0.6895699786324786\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4436224358713525, Train acc: 0.6916666666666667\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.443446138645509, Train acc: 0.6918625356125356\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4441379863263923, Train acc: 0.6903617216117216\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.444111965660356, Train acc: 0.6905048076923077\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4437413489603474, Train acc: 0.6907644824311491\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.444393793741862, Train acc: 0.6901442307692308\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4440872521018833, Train acc: 0.6904865967365967\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4434155629910634, Train acc: 0.6910167378917379\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4426304993074557, Train acc: 0.6917118671926364\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.442514747199267, Train acc: 0.691868894993895\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4425846271025828, Train acc: 0.6916844729344729\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4432955377886438, Train acc: 0.6911892361111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.442981887067485, Train acc: 0.6917263700351935\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4430430374707026, Train acc: 0.6916696343779677\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4436422919454845, Train acc: 0.6912533738191633\n",
      "Val loss: 3.0599496364593506, Val acc: 0.736\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4462367184141762, Train acc: 0.6909722222222222\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.443001206104572, Train acc: 0.6937767094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4394390263788381, Train acc: 0.6979166666666666\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4411202944242036, Train acc: 0.695045405982906\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.439549777446649, Train acc: 0.6956196581196581\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.440454305406989, Train acc: 0.6947560541310541\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4414934040717007, Train acc: 0.6935286935286935\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4405538697018583, Train acc: 0.6945779914529915\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4403684861526416, Train acc: 0.6951270180436847\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.440585227501698, Train acc: 0.6951388888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4407904614935387, Train acc: 0.6951728826728827\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.439330171146284, Train acc: 0.6966479700854701\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4395372718670587, Train acc: 0.6961086456278764\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4391180982810965, Train acc: 0.6965621184371185\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4390675092354799, Train acc: 0.6963853276353277\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4389026406993213, Train acc: 0.6966145833333334\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4386065406617117, Train acc: 0.6968325791855203\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4387259198050213, Train acc: 0.6965663580246914\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4386393162671156, Train acc: 0.6964406207827261\n",
      "Val loss: 3.0597126483917236, Val acc: 0.734\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.447800575158535, Train acc: 0.6920405982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.440446505179772, Train acc: 0.6952457264957265\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.440853175274667, Train acc: 0.6944444444444444\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.438321857370882, Train acc: 0.6962473290598291\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4374735860743075, Train acc: 0.6976495726495726\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4374226956625609, Train acc: 0.6984063390313391\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4369325661106134, Train acc: 0.6991376678876678\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4371139697539501, Train acc: 0.6991185897435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4364542259223554, Train acc: 0.6997863247863247\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4358577709931595, Train acc: 0.7001602564102564\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4350929473293041, Train acc: 0.7006847319347319\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4347014990966884, Train acc: 0.7010327635327636\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4352623384928405, Train acc: 0.7001561472715319\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4353688819941146, Train acc: 0.6998626373626373\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4354705449183103, Train acc: 0.6997150997150997\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4354030503931208, Train acc: 0.6996861645299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4357483745400423, Train acc: 0.69947209653092\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4354728402575196, Train acc: 0.6998160018993352\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4347664604195591, Train acc: 0.7004892037786775\n",
      "Val loss: 3.05112361907959, Val acc: 0.744\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4240811726985834, Train acc: 0.7134081196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4270024584908771, Train acc: 0.7088675213675214\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4276147391381766, Train acc: 0.7073539886039886\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4275356215289516, Train acc: 0.7080662393162394\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4289478436494485, Train acc: 0.7067841880341881\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4299890652001752, Train acc: 0.7052617521367521\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4296100439460577, Train acc: 0.7060821123321124\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.430819970420283, Train acc: 0.7044604700854701\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4310149827234426, Train acc: 0.7041488603988604\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4307425340016684, Train acc: 0.7043803418803419\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4308217216482808, Train acc: 0.7040355477855478\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4316190209483828, Train acc: 0.7028356481481481\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.431333334331525, Train acc: 0.7035256410256411\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.431033823370788, Train acc: 0.7038690476190477\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.431277486741373, Train acc: 0.7036146723646723\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4316451856468477, Train acc: 0.7032752403846154\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4321721479963454, Train acc: 0.702661513323278\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4324989127524106, Train acc: 0.7021159781576448\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4326732420352897, Train acc: 0.701951192082771\n",
      "Val loss: 3.048800230026245, Val acc: 0.742\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4354762990250547, Train acc: 0.7032585470085471\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4325127540490565, Train acc: 0.7029914529914529\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.432759286331655, Train acc: 0.7011217948717948\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4331349414637966, Train acc: 0.6998530982905983\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4322520688048794, Train acc: 0.7019764957264957\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4327578177818885, Train acc: 0.7016114672364673\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.430799575661274, Train acc: 0.7038308913308914\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4308226505915325, Train acc: 0.7038595085470085\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4312936317773512, Train acc: 0.7033475783475783\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4301378849225166, Train acc: 0.7047809829059829\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4300105297574723, Train acc: 0.7047154234654235\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.430158268692147, Train acc: 0.7044159544159544\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4302877426774465, Train acc: 0.7044091058514136\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.430212272683634, Train acc: 0.7047275641025641\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.429932084178653, Train acc: 0.7052350427350428\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.429299098558915, Train acc: 0.7060296474358975\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.429381715646037, Train acc: 0.7059451985922575\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4280345571233564, Train acc: 0.7074133428300095\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.428844486439062, Train acc: 0.7066042510121457\n",
      "Val loss: 3.0446372032165527, Val acc: 0.75\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.421300967534383, Train acc: 0.7134081196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4231960294593093, Train acc: 0.7115384615384616\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4236093171980986, Train acc: 0.7119836182336182\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.426009135368543, Train acc: 0.7089342948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4258879486312215, Train acc: 0.7094551282051282\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4268785786424947, Train acc: 0.7089565527065527\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4273019407549474, Train acc: 0.7082570207570208\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4273078999458215, Train acc: 0.707832532051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4272086674671227, Train acc: 0.7082739791073125\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4267696044383904, Train acc: 0.7085737179487179\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4271113100029649, Train acc: 0.7078719891219891\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4264869778244584, Train acc: 0.70877849002849\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.426224974877422, Train acc: 0.7091757067718606\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.426073397035564, Train acc: 0.7089629120879121\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4254064959338588, Train acc: 0.7097222222222223\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4255949235879457, Train acc: 0.7093850160256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.424963935887532, Train acc: 0.7102187028657617\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4251753735972492, Train acc: 0.7102623456790124\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4254025617162762, Train acc: 0.7099218398560504\n",
      "Val loss: 3.046170711517334, Val acc: 0.744\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4327415727142594, Train acc: 0.6992521367521367\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4214362517381325, Train acc: 0.7106036324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4230859204914494, Train acc: 0.7092236467236467\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4224063387283912, Train acc: 0.7111378205128205\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.424739933013916, Train acc: 0.7091346153846154\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4231151006160638, Train acc: 0.7112713675213675\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4242142341366908, Train acc: 0.7096688034188035\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4244816259950654, Train acc: 0.7095686431623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4243616423828762, Train acc: 0.7095500949667616\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4227860116551065, Train acc: 0.7114583333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.42386010180357, Train acc: 0.7107614607614607\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4239255252726737, Train acc: 0.7108484686609686\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4231754297648975, Train acc: 0.7121753780407627\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4226565275145684, Train acc: 0.712931166056166\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4224531395143254, Train acc: 0.7134437321937321\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4225235859043577, Train acc: 0.7132912660256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4223743179085389, Train acc: 0.7132667169431876\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4222292140451704, Train acc: 0.7135861823361823\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4221871795817318, Train acc: 0.7135908681961314\n",
      "Val loss: 3.0381433963775635, Val acc: 0.752\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4170547306028187, Train acc: 0.718482905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4174096268466396, Train acc: 0.71875\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4165001594782556, Train acc: 0.7189280626780626\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4174364051248274, Train acc: 0.7172809829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4176721625857882, Train acc: 0.7174145299145299\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.418230103631305, Train acc: 0.7170584045584045\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4180013632745243, Train acc: 0.717032967032967\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4174366710532424, Train acc: 0.7175814636752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4172437994794855, Train acc: 0.717622269705603\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4177860624769814, Train acc: 0.71696047008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.418207347346723, Train acc: 0.7165889665889665\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4182659350229465, Train acc: 0.7167022792022792\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4188520218650422, Train acc: 0.7162228796844181\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.419063627064883, Train acc: 0.7158692002442002\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4189898700116366, Train acc: 0.716025641025641\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4188995951006556, Train acc: 0.7160623664529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.41860183199906, Train acc: 0.716220462543992\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4185171611854719, Train acc: 0.7163906695156695\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4179447145543547, Train acc: 0.7169787449392713\n",
      "Val loss: 3.040781259536743, Val acc: 0.744\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4192134620796921, Train acc: 0.7174145299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4162472598573081, Train acc: 0.7224893162393162\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4145300218522379, Train acc: 0.7224893162393162\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4128431947822244, Train acc: 0.7238247863247863\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4145371567489755, Train acc: 0.722542735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4150978866805377, Train acc: 0.7224002849002849\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4139599005381267, Train acc: 0.7237103174603174\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4146458014973209, Train acc: 0.7228231837606838\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.414678225150475, Train acc: 0.7229047958214625\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4150084556677403, Train acc: 0.7223290598290598\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4151370977373574, Train acc: 0.7221736596736597\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4156883499900839, Train acc: 0.7211538461538461\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4159460959663366, Train acc: 0.7209278435239974\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4161193100087373, Train acc: 0.7209058302808303\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4162752174583935, Train acc: 0.7205128205128205\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4160306984797502, Train acc: 0.720703125\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4157165894860775, Train acc: 0.7212638260432378\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4156801555684262, Train acc: 0.721406101614435\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4162366621890048, Train acc: 0.7207321187584346\n",
      "Val loss: 3.038662910461426, Val acc: 0.756\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.412205384327815, Train acc: 0.7272970085470085\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4164657175031483, Train acc: 0.7200854700854701\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4148953973058283, Train acc: 0.7224002849002849\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.416658969516428, Train acc: 0.7210202991452992\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4161214107122178, Train acc: 0.7227029914529914\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4138964879886378, Train acc: 0.7252492877492878\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4137389540526748, Train acc: 0.7250457875457875\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4142424604322157, Train acc: 0.7244925213675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4152942992117103, Train acc: 0.7236467236467237\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.414879610395839, Train acc: 0.7239316239316239\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4149627466950199, Train acc: 0.7234120046620046\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4151336229764497, Train acc: 0.7230235042735043\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4146579363720735, Train acc: 0.7232905982905983\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4149275908976684, Train acc: 0.7229281135531136\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4145762370182917, Train acc: 0.7233084045584045\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4146714453768527, Train acc: 0.7231236645299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4144879116252058, Train acc: 0.7230549270990447\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4141959878001344, Train acc: 0.7233054368471035\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.414392864494993, Train acc: 0.7230937921727395\n",
      "Val loss: 3.0372824668884277, Val acc: 0.75\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.42275103340801, Train acc: 0.7094017094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4195180947964008, Train acc: 0.7147435897435898\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4172254253996064, Train acc: 0.7175925925925926\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4150394221656344, Train acc: 0.7205528846153846\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.414541456842015, Train acc: 0.7212606837606838\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4127773783485433, Train acc: 0.7227564102564102\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.412698953870743, Train acc: 0.7239392551892552\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4123832917111552, Train acc: 0.7244257478632479\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4122187099565469, Train acc: 0.7248041310541311\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4119082978647999, Train acc: 0.7251602564102564\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4112884598178463, Train acc: 0.7258644133644133\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.410573734686925, Train acc: 0.7263621794871795\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4106590039317817, Train acc: 0.7260437212360289\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4114638105562465, Train acc: 0.7252556471306472\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4117388768753096, Train acc: 0.7251602564102564\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.411340460818038, Train acc: 0.7256276709401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4117254775753567, Train acc: 0.7253173705379587\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.411553325589679, Train acc: 0.7253531576448243\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4110878455547722, Train acc: 0.7259615384615384\n",
      "Val loss: 3.031208038330078, Val acc: 0.76\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4044051639035218, Train acc: 0.7318376068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4091651083057761, Train acc: 0.7264957264957265\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4120516002687633, Train acc: 0.7231125356125356\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4124474729228222, Train acc: 0.7230235042735043\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4097399161412165, Train acc: 0.7256944444444444\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4101886457187838, Train acc: 0.7256944444444444\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4103246136899397, Train acc: 0.725503663003663\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4101860874738448, Train acc: 0.7261618589743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.409085439254529, Train acc: 0.7281873219373219\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.409440874442076, Train acc: 0.7272702991452992\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4092040386140672, Train acc: 0.7276369463869464\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4095496188881051, Train acc: 0.7272524928774928\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.409108957752125, Train acc: 0.7278517422748192\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4088973710825154, Train acc: 0.7279838217338217\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.408977708245954, Train acc: 0.7280626780626781\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4088352596403186, Train acc: 0.7284488514957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4083688791341191, Train acc: 0.7289467068878833\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4081856451596064, Train acc: 0.7290331196581197\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4084663716083274, Train acc: 0.7288855150697255\n",
      "Val loss: 3.029996156692505, Val acc: 0.764\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4010567746610723, Train acc: 0.7350427350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3970993512716048, Train acc: 0.7390491452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4003155788465103, Train acc: 0.7360220797720798\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4053748112458448, Train acc: 0.7312366452991453\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4034733519594893, Train acc: 0.7326923076923076\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.4074850163908086, Train acc: 0.728454415954416\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.406660691576854, Train acc: 0.7301205738705738\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4062010867473407, Train acc: 0.7303685897435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4059380092512168, Train acc: 0.7307989078822412\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.4057215415514432, Train acc: 0.7307425213675214\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4052674859275907, Train acc: 0.730963480963481\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4060337178387874, Train acc: 0.7306579415954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4061661821537155, Train acc: 0.7302966798159106\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4050529701805814, Train acc: 0.7312461843711844\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4048276762677054, Train acc: 0.7318198005698006\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4050454608140848, Train acc: 0.7318709935897436\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4054893312171335, Train acc: 0.731649069884364\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4047595265923742, Train acc: 0.7323717948717948\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.404411782995302, Train acc: 0.7328778677462888\n",
      "Val loss: 3.0269935131073, Val acc: 0.762\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4109313345362997, Train acc: 0.7238247863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4089051006186721, Train acc: 0.7267628205128205\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.4027816542872675, Train acc: 0.7359330484330484\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4053966189042115, Train acc: 0.7343082264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4058832629114135, Train acc: 0.7329594017094017\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.402967771233996, Train acc: 0.7355769230769231\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.4023575127779782, Train acc: 0.7355769230769231\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.4020801726569476, Train acc: 0.7358440170940171\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.4021136819580455, Train acc: 0.7358143399810066\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.401151910602537, Train acc: 0.7372596153846154\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.4013481293895398, Train acc: 0.7365724553224553\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.4010265503513848, Train acc: 0.7365562678062678\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.4000024439706997, Train acc: 0.7375904010519395\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4006560508148138, Train acc: 0.7365880647130647\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4011959437291506, Train acc: 0.735772792022792\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4015518052455707, Train acc: 0.7357605502136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4016327539120803, Train acc: 0.7357340372046255\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.401514302631389, Train acc: 0.7357549857549858\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4014923183219796, Train acc: 0.7359283625730995\n",
      "Val loss: 3.0284154415130615, Val acc: 0.768\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4040201851445386, Train acc: 0.7339743589743589\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.397062275144789, Train acc: 0.7410523504273504\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3993604203574679, Train acc: 0.7383368945868946\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.4012496109701629, Train acc: 0.7368456196581197\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.4016015818995289, Train acc: 0.7362179487179488\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3995006322181462, Train acc: 0.7379807692307693\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3993029224712479, Train acc: 0.7379044566544567\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3990896945325737, Train acc: 0.7380475427350427\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3991590934148428, Train acc: 0.7382181861348528\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3996915521784725, Train acc: 0.7371794871794872\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.399835497071415, Train acc: 0.7370823620823621\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3996728899811748, Train acc: 0.7373575498575499\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.400170863889535, Train acc: 0.7368918474687706\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.4005816764447279, Train acc: 0.7366834554334555\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.4012966251101588, Train acc: 0.735968660968661\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.4013232753062859, Train acc: 0.7357605502136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.4012418874488397, Train acc: 0.7357497486173957\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.4014831676102772, Train acc: 0.7357253086419753\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.4013432743202927, Train acc: 0.7361111111111112\n",
      "Val loss: 3.026796817779541, Val acc: 0.768\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3943405334766095, Train acc: 0.7473290598290598\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.394825171201657, Train acc: 0.7458600427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.396938969946315, Train acc: 0.7426103988603988\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3955831614315, Train acc: 0.7423210470085471\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.395976457840357, Train acc: 0.7421474358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3943896843836858, Train acc: 0.7438123219373219\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3956202258411636, Train acc: 0.7422542735042735\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3951097533234165, Train acc: 0.7423210470085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.396391918176939, Train acc: 0.741423314339981\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.396949260051434, Train acc: 0.7409722222222223\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3947410794682713, Train acc: 0.7432983682983683\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3949488236693575, Train acc: 0.7428329772079773\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3944226049577462, Train acc: 0.7430966469428008\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3942212556308007, Train acc: 0.7435706654456654\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3946688918306618, Train acc: 0.7430733618233618\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3952453704471262, Train acc: 0.7424379006410257\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3957006232471187, Train acc: 0.7419714680744093\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3956688744390113, Train acc: 0.7421652421652422\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.395420087011237, Train acc: 0.7424791947818263\n",
      "Val loss: 3.0199451446533203, Val acc: 0.768\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3946455156701243, Train acc: 0.7395833333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.395165199907417, Train acc: 0.7394497863247863\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.395899882004132, Train acc: 0.738960113960114\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3958426347145667, Train acc: 0.7397168803418803\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.396366628418621, Train acc: 0.7390491452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3967679885038284, Train acc: 0.7390491452991453\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3967822771223763, Train acc: 0.7398885836385837\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3969406065268395, Train acc: 0.7393162393162394\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3961292556208422, Train acc: 0.7399394586894587\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.394979768940526, Train acc: 0.7413728632478632\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3945739676809719, Train acc: 0.7422542735042735\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.393623036197108, Train acc: 0.7432781339031339\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3931296302800427, Train acc: 0.7439801117685733\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3920447682286357, Train acc: 0.7449442918192918\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3923916547726363, Train acc: 0.7446225071225071\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3929866623674703, Train acc: 0.7441239316239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3929141972639143, Train acc: 0.7441553544494721\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3926300966275498, Train acc: 0.7444800569800569\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3922069917859872, Train acc: 0.7448408681961314\n",
      "Val loss: 3.019090414047241, Val acc: 0.768\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.4084433877569997, Train acc: 0.7323717948717948\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.4017829141046247, Train acc: 0.7369123931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3982196261740139, Train acc: 0.7401175213675214\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3945757216877408, Train acc: 0.7436565170940171\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3932420176318567, Train acc: 0.7446581196581197\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3938364765243312, Train acc: 0.7434561965811965\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.39282640025147, Train acc: 0.7438186813186813\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3932233167509747, Train acc: 0.7432224893162394\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3922438496990874, Train acc: 0.7442426400759734\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.390680330227583, Train acc: 0.7461805555555555\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3901182301024086, Train acc: 0.7467463092463092\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3906762606737622, Train acc: 0.7462829415954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3903665161697116, Train acc: 0.7466305062458909\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3906528256897233, Train acc: 0.7459745115995116\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3909656146992306, Train acc: 0.7458511396011396\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3907001606928997, Train acc: 0.7458934294871795\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3904926209907666, Train acc: 0.7460407239819005\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3898421531615661, Train acc: 0.746809710351377\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3901878222655564, Train acc: 0.7464996626180836\n",
      "Val loss: 3.020855188369751, Val acc: 0.766\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3878644694629898, Train acc: 0.7537393162393162\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.384525692361033, Train acc: 0.7530715811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3830533115952104, Train acc: 0.7547186609686609\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3820236039976788, Train acc: 0.7560096153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.383496634165446, Train acc: 0.7541132478632478\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3816418039832699, Train acc: 0.7562321937321937\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3836355421860431, Train acc: 0.7540445665445665\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3850002645427346, Train acc: 0.7527377136752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3854068224473104, Train acc: 0.7523741690408358\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3862264995900995, Train acc: 0.7513087606837607\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3853821726667854, Train acc: 0.7518939393939394\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3849136246914877, Train acc: 0.7518696581196581\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3856166482833872, Train acc: 0.7512532873109796\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3856468571848048, Train acc: 0.7513354700854701\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3855930339237224, Train acc: 0.7518340455840455\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3858917435774436, Train acc: 0.7518863514957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.385868129926809, Train acc: 0.7520267722473605\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.386604684144117, Train acc: 0.7513651471984806\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3864910737705616, Train acc: 0.7514760458839406\n",
      "Val loss: 3.0147857666015625, Val acc: 0.768\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3764390965812228, Train acc: 0.7670940170940171\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3770684829125037, Train acc: 0.7628205128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.380250557875022, Train acc: 0.7587250712250713\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3834264573887882, Train acc: 0.7560763888888888\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3864196223071499, Train acc: 0.7523504273504273\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3855111986143975, Train acc: 0.7526264245014245\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.383782425497332, Train acc: 0.754769536019536\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.383911086453332, Train acc: 0.7547743055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.384299188371171, Train acc: 0.7542438271604939\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3839048218523335, Train acc: 0.7545138888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3835704304399468, Train acc: 0.7546620046620046\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3830764477069561, Train acc: 0.7550747863247863\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.382561749333539, Train acc: 0.7553829717291256\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3833952430403713, Train acc: 0.7543307387057387\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3835431066333739, Train acc: 0.753792735042735\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3831407374296434, Train acc: 0.7541232638888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.383360884188407, Train acc: 0.7538492961287079\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3838038009795708, Train acc: 0.7533831908831908\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3835265197788227, Train acc: 0.7537674313990104\n",
      "Val loss: 3.0137782096862793, Val acc: 0.77\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3903309320792174, Train acc: 0.7486645299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.391359728625697, Train acc: 0.7446581196581197\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3871770496042366, Train acc: 0.7495548433048433\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3842921618722444, Train acc: 0.7528044871794872\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3839908106714232, Train acc: 0.7527777777777778\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3849902482453915, Train acc: 0.7518251424501424\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3837438878558932, Train acc: 0.7518696581196581\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3844890757503672, Train acc: 0.7512019230769231\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3839012738759475, Train acc: 0.7520477207977208\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3838494463863535, Train acc: 0.7524038461538461\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.382455557932109, Train acc: 0.754079254079254\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3819562909949539, Train acc: 0.7544070512820513\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3820591687685875, Train acc: 0.7543145956607495\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3828535379798712, Train acc: 0.7536439255189256\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3825972095174328, Train acc: 0.7537393162393162\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3830301028031569, Train acc: 0.7531216613247863\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.383338507004513, Train acc: 0.752859477124183\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.382772889345573, Train acc: 0.7533535137701804\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3821923125717912, Train acc: 0.754203216374269\n",
      "Val loss: 2.995220184326172, Val acc: 0.784\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3813028559725509, Train acc: 0.7574786324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3812166187498305, Train acc: 0.7565438034188035\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3813805824671037, Train acc: 0.7570334757834758\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3801868827933939, Train acc: 0.7582131410256411\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3806507791209424, Train acc: 0.7576388888888889\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3803173908141264, Train acc: 0.7577457264957265\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3786322881305029, Train acc: 0.7590430402930403\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.378163177997638, Train acc: 0.7589476495726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3797306735970696, Train acc: 0.7573302469135802\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3793479204177856, Train acc: 0.7573985042735043\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3783700143818534, Train acc: 0.7584256021756022\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.377585699075987, Train acc: 0.7593037749287749\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3777493781592014, Train acc: 0.759060650887574\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.377327259498056, Train acc: 0.7596344627594628\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3778735295320168, Train acc: 0.7591702279202279\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3772841365928323, Train acc: 0.7599325587606838\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3772520274598135, Train acc: 0.7599453242835595\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3765971779483677, Train acc: 0.7604463437796771\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.376371594951113, Train acc: 0.760486954565902\n",
      "Val loss: 2.991664171218872, Val acc: 0.794\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3687845519465258, Train acc: 0.7700320512820513\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3743581751472929, Train acc: 0.7620192307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.372608026547989, Train acc: 0.7635327635327636\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3737004119106846, Train acc: 0.7628205128205128\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3724734037350386, Train acc: 0.7644764957264957\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3703925816761462, Train acc: 0.7677172364672364\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3701044345804478, Train acc: 0.7678571428571429\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3720352262513251, Train acc: 0.7659588675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3725229179078018, Train acc: 0.7649572649572649\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3714774009508965, Train acc: 0.7662126068376068\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3711060316134722, Train acc: 0.7664384226884227\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3707457125356735, Train acc: 0.7670940170940171\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3706081783512876, Train acc: 0.7670529257067719\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.370491223020868, Train acc: 0.7670558608058609\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3707119936277383, Train acc: 0.7667200854700855\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3706831567817264, Train acc: 0.7665765224358975\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3709692406019054, Train acc: 0.7660256410256411\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3710205970439928, Train acc: 0.7657437084520418\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3711382641322538, Train acc: 0.7654492802519118\n",
      "Val loss: 2.9848439693450928, Val acc: 0.798\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.373561600334624, Train acc: 0.7604166666666666\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3667249027480426, Train acc: 0.7681623931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3657160294361603, Train acc: 0.7698539886039886\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.367417163319058, Train acc: 0.7687633547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.36848859746232, Train acc: 0.7677884615384616\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3709241022751202, Train acc: 0.7653133903133903\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3722002354879228, Train acc: 0.763507326007326\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3719132430533059, Train acc: 0.7642561431623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3711982137910095, Train acc: 0.7645417853751187\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3720038316188714, Train acc: 0.76383547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.371594039247123, Train acc: 0.7638646076146076\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3719287833936533, Train acc: 0.7638888888888888\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.371814631694716, Train acc: 0.7640121630506246\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3715130686032175, Train acc: 0.7643467643467643\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3709998782883344, Train acc: 0.7649038461538461\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.371400535106659, Train acc: 0.7642895299145299\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3708534548784872, Train acc: 0.7647844394167923\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3706119121649327, Train acc: 0.7648830721747388\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3704829762982613, Train acc: 0.764971322537112\n",
      "Val loss: 2.9827919006347656, Val acc: 0.81\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3443566896976569, Train acc: 0.7876602564102564\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3594636030686207, Train acc: 0.7731036324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.358892207811361, Train acc: 0.7756410256410257\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3648134363003266, Train acc: 0.7694978632478633\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3660268710209773, Train acc: 0.7689636752136753\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3661769037572746, Train acc: 0.7690527065527065\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.365879515588502, Train acc: 0.7694978632478633\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.366593875691422, Train acc: 0.7691973824786325\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.367507789662534, Train acc: 0.768281101614435\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3685561557101389, Train acc: 0.7674412393162393\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3685070241988864, Train acc: 0.7675310800310801\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3693400696132259, Train acc: 0.7667378917378918\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3700278444873275, Train acc: 0.7660461867192636\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3702595613814972, Train acc: 0.7657967032967034\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3707339838359431, Train acc: 0.7651531339031339\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3698848293632524, Train acc: 0.7659254807692307\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3694371823511753, Train acc: 0.7664498491704375\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3687097768158654, Train acc: 0.7671236942070275\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3685533198160573, Train acc: 0.767332995951417\n",
      "Val loss: 2.9750850200653076, Val acc: 0.81\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3591870364979801, Train acc: 0.7743055555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3648026621239817, Train acc: 0.7698985042735043\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3672650410578802, Train acc: 0.7680733618233618\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3671122529567816, Train acc: 0.7688301282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3663491473238691, Train acc: 0.7697649572649573\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3663380461880283, Train acc: 0.7692307692307693\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.365534343707838, Train acc: 0.7699557387057387\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3644903358231244, Train acc: 0.7710336538461539\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3656363007236636, Train acc: 0.7691714150047484\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3661708067624996, Train acc: 0.7685897435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3657583955099348, Train acc: 0.7687937062937062\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.365678694513109, Train acc: 0.7687633547008547\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3660841354956994, Train acc: 0.7682856673241288\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3665741786851988, Train acc: 0.7678952991452992\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3671375820779392, Train acc: 0.7671830484330484\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3669708066771173, Train acc: 0.7675113514957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.366432745172607, Train acc: 0.768083836098542\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.367413307526852, Train acc: 0.7670791785375118\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3667532403459433, Train acc: 0.7677266081871345\n",
      "Val loss: 2.976318359375, Val acc: 0.81\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3600556076082408, Train acc: 0.7793803418803419\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3627703332493448, Train acc: 0.7751068376068376\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3617045811438493, Train acc: 0.7748397435897436\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3637800257430117, Train acc: 0.7721020299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3638162250192756, Train acc: 0.7724358974358975\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3649292338607657, Train acc: 0.771011396011396\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3648724197904705, Train acc: 0.7708714896214897\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3651153677039676, Train acc: 0.7704326923076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3635606783860543, Train acc: 0.7722578347578347\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.363793193580758, Train acc: 0.7719551282051282\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.362839751588159, Train acc: 0.7729943667443667\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3631048717050471, Train acc: 0.7726139601139601\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3629011338200716, Train acc: 0.7727029914529915\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3636310409131358, Train acc: 0.7717300061050061\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3636987608722133, Train acc: 0.7715277777777778\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3635988343729932, Train acc: 0.7717013888888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3636377351616062, Train acc: 0.7716817496229261\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.363873248199905, Train acc: 0.771545584045584\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3633274165456808, Train acc: 0.7722953216374269\n",
      "Val loss: 2.974576234817505, Val acc: 0.814\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3617176520518768, Train acc: 0.7729700854700855\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3605081749777508, Train acc: 0.7757745726495726\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3610533044548796, Train acc: 0.7742165242165242\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.361068553904183, Train acc: 0.7751736111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3621797308962569, Train acc: 0.7734508547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3626504372327755, Train acc: 0.7730591168091168\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3625258162199214, Train acc: 0.7731608669108669\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3618965059773536, Train acc: 0.7743723290598291\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.361035715367606, Train acc: 0.7749584520417854\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3610958415218908, Train acc: 0.7745459401709401\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3609430771422368, Train acc: 0.7744026806526807\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3616082522264572, Train acc: 0.7737936253561254\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3609743629295052, Train acc: 0.774552103879027\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3607113247825986, Train acc: 0.7746680402930403\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.360862433333003, Train acc: 0.7744480056980056\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3614407845287242, Train acc: 0.773704594017094\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.361170999724521, Train acc: 0.7741170186023127\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3611218769665796, Train acc: 0.7741126543209876\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3608569624828317, Train acc: 0.7744742465137202\n",
      "Val loss: 2.970400094985962, Val acc: 0.806\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3523698831215882, Train acc: 0.7780448717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.357062000494737, Train acc: 0.7739049145299145\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3584356960068402, Train acc: 0.7740384615384616\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3587825873978117, Train acc: 0.7740384615384616\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3592533714750894, Train acc: 0.7732905982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3596779203143214, Train acc: 0.7735042735042735\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.35966976920327, Train acc: 0.7735424297924298\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3598272904881046, Train acc: 0.7735376602564102\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3588781261715794, Train acc: 0.7746320037986705\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3594049048219992, Train acc: 0.7741720085470085\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3592164596107086, Train acc: 0.7743298368298368\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3594651600913783, Train acc: 0.7741274928774928\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3598131699283056, Train acc: 0.7737508218277449\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3603697820169731, Train acc: 0.7735233516483516\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3602549849072751, Train acc: 0.7736467236467236\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3597762346522422, Train acc: 0.7742888621794872\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.359644681499974, Train acc: 0.7744469582704877\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3596567272121072, Train acc: 0.774201685660019\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3598943002257216, Train acc: 0.7737713675213675\n",
      "Val loss: 2.9680442810058594, Val acc: 0.822\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3594262946365225, Train acc: 0.7743055555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3633453560690594, Train acc: 0.7712339743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3628318350539248, Train acc: 0.7727920227920227\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.363533004736289, Train acc: 0.7727697649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3623976988670154, Train acc: 0.7736111111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.362485716145942, Train acc: 0.7731926638176638\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3605287104153663, Train acc: 0.7754120879120879\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3601927584053104, Train acc: 0.7750400641025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3588362190684928, Train acc: 0.7765610161443495\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3583992652404002, Train acc: 0.7766025641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3584533858503032, Train acc: 0.7767579642579643\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3580781898946843, Train acc: 0.7769764957264957\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.358256693344066, Train acc: 0.776586127547666\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.358327689188304, Train acc: 0.7764613858363858\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.358166191856406, Train acc: 0.7766203703703703\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3587071322477782, Train acc: 0.7757912660256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3584664596751326, Train acc: 0.7759395424836601\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.35880853228646, Train acc: 0.7754481244064577\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3590184436743182, Train acc: 0.7753598740440846\n",
      "Val loss: 2.963244676589966, Val acc: 0.834\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3532514449877617, Train acc: 0.781784188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3534469441470938, Train acc: 0.7816506410256411\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3569359589166452, Train acc: 0.7759081196581197\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3574671617939942, Train acc: 0.7762419871794872\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3579449804420145, Train acc: 0.7754273504273504\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3578056273636994, Train acc: 0.7759526353276354\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3566844937856672, Train acc: 0.7775869963369964\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3567651668165484, Train acc: 0.7771768162393162\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3563933209476309, Train acc: 0.7772732668566001\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3555781598783965, Train acc: 0.7777777777777778\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.355214047450471, Train acc: 0.7780205905205905\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.355108393223537, Train acc: 0.7784232549857549\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3545789397287964, Train acc: 0.7788050624589086\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.354858479657016, Train acc: 0.7783119658119658\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3549573209550645, Train acc: 0.7784188034188034\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.354650853918149, Train acc: 0.7789296207264957\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.354306633116433, Train acc: 0.7791760935143288\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3547703054895428, Train acc: 0.7787571225071225\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3547912594224223, Train acc: 0.7788602114260009\n",
      "Val loss: 2.9635415077209473, Val acc: 0.822\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3537759067665818, Train acc: 0.7775106837606838\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3563044061008682, Train acc: 0.7771100427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3540550487333554, Train acc: 0.7785790598290598\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3560188122284718, Train acc: 0.7767761752136753\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.35699283404228, Train acc: 0.7759081196581197\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3565910246297506, Train acc: 0.7757300569800569\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3568306012904687, Train acc: 0.7748778998778999\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3555924375342507, Train acc: 0.7762419871794872\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3549548073937976, Train acc: 0.7770358499525166\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3550432323390602, Train acc: 0.7770566239316239\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3559037592398073, Train acc: 0.7764423076923077\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3550813918100124, Train acc: 0.7775774572649573\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.354103378368631, Train acc: 0.7787228796844181\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3540036167417253, Train acc: 0.7787126068376068\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3534500579888324, Train acc: 0.7794337606837607\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3534013365323727, Train acc: 0.7795138888888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3535991706339903, Train acc: 0.7791446706887883\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3535688148717708, Train acc: 0.7791726020892688\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3537120383653456, Train acc: 0.7790289023841656\n",
      "Val loss: 2.963498830795288, Val acc: 0.826\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3594720893436008, Train acc: 0.7745726495726496\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3545193998222678, Train acc: 0.7805822649572649\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.353703141552091, Train acc: 0.78125\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3545967253864322, Train acc: 0.7795138888888888\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3545343244177663, Train acc: 0.7793269230769231\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3545523246808608, Train acc: 0.7785790598290598\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3563421008351084, Train acc: 0.7764041514041514\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3547086998438225, Train acc: 0.7781116452991453\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3534127493076742, Train acc: 0.7791429249762583\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3529956840042374, Train acc: 0.7795940170940171\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3527943287696038, Train acc: 0.7796959984459985\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.352394082607367, Train acc: 0.7802038817663818\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3525756856942162, Train acc: 0.7798939842209073\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3528956528548355, Train acc: 0.7795138888888888\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3528173541751003, Train acc: 0.7794337606837607\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3530635977657433, Train acc: 0.7790798611111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3530226852978937, Train acc: 0.7790504022121669\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.353065969031534, Train acc: 0.7791132478632479\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.352597700171357, Train acc: 0.7794787449392713\n",
      "Val loss: 2.9632556438446045, Val acc: 0.822\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3491531865209596, Train acc: 0.7847222222222222\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3528500316489456, Train acc: 0.7796474358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3530855260343633, Train acc: 0.7792913105413105\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3533478104151213, Train acc: 0.7785790598290598\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3540623884934646, Train acc: 0.7775106837606838\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3532242452317154, Train acc: 0.7785345441595442\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3522494907635327, Train acc: 0.7793803418803419\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.351610246886555, Train acc: 0.7802817841880342\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3507466884533337, Train acc: 0.781517094017094\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3507788931202684, Train acc: 0.7814903846153847\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3502929960931098, Train acc: 0.7820755633255633\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3497277938062988, Train acc: 0.7828748219373219\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3498347691216803, Train acc: 0.7825238330046023\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3503186203475692, Train acc: 0.7820131257631258\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3506041592002933, Train acc: 0.781588319088319\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3503737007705574, Train acc: 0.7817007211538461\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3503258614278786, Train acc: 0.7817998994469583\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3505676718745363, Train acc: 0.7814429012345679\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3507541312433502, Train acc: 0.7812218848403059\n",
      "Val loss: 2.956073045730591, Val acc: 0.838\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3458681779030042, Train acc: 0.7852564102564102\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3469591670566134, Train acc: 0.7849893162393162\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.346325850554681, Train acc: 0.7855235042735043\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3479756590647576, Train acc: 0.7845219017094017\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3505522010672806, Train acc: 0.780982905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.349937074544423, Train acc: 0.7813835470085471\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.351044262139643, Train acc: 0.7807539682539683\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3503304202841897, Train acc: 0.7817508012820513\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3501742775963244, Train acc: 0.7815764482431149\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.350196314876915, Train acc: 0.7815438034188035\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3496847158267504, Train acc: 0.7821726884226884\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3489124131338548, Train acc: 0.7829861111111112\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.348775592154378, Train acc: 0.7829347468770546\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3487583381352408, Train acc: 0.7832722832722833\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3488215123146687, Train acc: 0.7832264957264957\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3490644978661823, Train acc: 0.7830361912393162\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3491037020196621, Train acc: 0.7828368526897939\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.348495734383238, Train acc: 0.7835202991452992\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3485598829057481, Train acc: 0.7833726945569051\n",
      "Val loss: 2.957514524459839, Val acc: 0.838\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.34843384506356, Train acc: 0.7796474358974359\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.347642788520226, Train acc: 0.7813835470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3458538062212475, Train acc: 0.7836538461538461\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.345057483921703, Train acc: 0.7855235042735043\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3471491940001137, Train acc: 0.784241452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3459252869641338, Train acc: 0.7850783475783476\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3467225190047378, Train acc: 0.7845314407814408\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.348313570786745, Train acc: 0.7831196581196581\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.349207871999496, Train acc: 0.781784188034188\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.350389289855957, Train acc: 0.780448717948718\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3506458941224757, Train acc: 0.7803515928515928\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3503979281482534, Train acc: 0.7806712962962963\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3495664170194346, Train acc: 0.7815787310979618\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.349530473151341, Train acc: 0.7817460317460317\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.348543911170416, Train acc: 0.7829059829059829\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.348297608968539, Train acc: 0.7832532051282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3479277864660433, Train acc: 0.7837952488687783\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.34783494280048, Train acc: 0.783980294396961\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.347464961722473, Train acc: 0.7842442645074224\n",
      "Val loss: 2.9544873237609863, Val acc: 0.84\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3457222351661096, Train acc: 0.7847222222222222\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3419569078673663, Train acc: 0.7885950854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.345107210327757, Train acc: 0.7857905982905983\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3440661639229865, Train acc: 0.7874599358974359\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3446310548700837, Train acc: 0.787232905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3448192787985516, Train acc: 0.7868144586894587\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3458523665883457, Train acc: 0.7859813797313797\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3464592353910463, Train acc: 0.7855235042735043\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3459515059888645, Train acc: 0.7856125356125356\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3464373892189092, Train acc: 0.7849626068376069\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3465602525742182, Train acc: 0.7846736596736597\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3465477297788333, Train acc: 0.7846999643874644\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3463922183345918, Train acc: 0.7854207758053912\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3461411591705676, Train acc: 0.7860004578754579\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.345904688549857, Train acc: 0.7859864672364673\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3461893877157798, Train acc: 0.7856570512820513\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.346743149994725, Train acc: 0.7851621417797888\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3461651128468237, Train acc: 0.78556801994302\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3461845737290823, Train acc: 0.7854110436347278\n",
      "Val loss: 2.9535491466522217, Val acc: 0.838\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3512593619843833, Train acc: 0.7841880341880342\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3527999678228655, Train acc: 0.7793803418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.352463976270453, Train acc: 0.7783119658119658\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.350504127323118, Train acc: 0.7799145299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3487352843977447, Train acc: 0.780982905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.348561918633616, Train acc: 0.7815616096866097\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3487474455501571, Train acc: 0.781517094017094\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.348387913316743, Train acc: 0.7817508012820513\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3476674855604471, Train acc: 0.7822293447293447\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.347144445598635, Train acc: 0.7829326923076924\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3467654750248954, Train acc: 0.7835810023310024\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3469147765398706, Train acc: 0.7834535256410257\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3468035247120553, Train acc: 0.7837360289283366\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3470414195741927, Train acc: 0.7835202991452992\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3470737690939183, Train acc: 0.7835648148148148\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3467770237953236, Train acc: 0.7841212606837606\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3461322061616372, Train acc: 0.7848164906988436\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3455906012798646, Train acc: 0.7854196343779677\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3458862917089698, Train acc: 0.7851720647773279\n",
      "Val loss: 2.9494779109954834, Val acc: 0.85\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.345564342971541, Train acc: 0.7868589743589743\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3444703297737317, Train acc: 0.7888621794871795\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3419584378897296, Train acc: 0.7904202279202279\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.342878706434853, Train acc: 0.7889957264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3432153787368384, Train acc: 0.788034188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3449015495104668, Train acc: 0.7862357549857549\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3466196589999728, Train acc: 0.7833104395604396\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.345799970575887, Train acc: 0.7844551282051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.344685720695503, Train acc: 0.7862951092117759\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3445998314099434, Train acc: 0.7864850427350427\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3436220914889605, Train acc: 0.7870775058275058\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3427717969288513, Train acc: 0.7879496082621082\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.342854224718534, Train acc: 0.7876191650230112\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3435223421334348, Train acc: 0.7868589743589743\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.343423917015054, Train acc: 0.7869301994301995\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3442468869889903, Train acc: 0.7861745459401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3441712699264903, Train acc: 0.7864504776269482\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3442050706513813, Train acc: 0.7864880104463438\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3443861851754406, Train acc: 0.7865215924426451\n",
      "Val loss: 2.952810525894165, Val acc: 0.832\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3425452016357682, Train acc: 0.7895299145299145\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3423775220528626, Train acc: 0.7903311965811965\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3420839336862591, Train acc: 0.7903311965811965\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3411272244575696, Train acc: 0.7918002136752137\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3419665890881138, Train acc: 0.7898504273504273\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.342417303992812, Train acc: 0.7892183048433048\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.341263144298642, Train acc: 0.7900259462759462\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.341955779454647, Train acc: 0.7890958867521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3415051278678554, Train acc: 0.789559591642925\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3419947049556635, Train acc: 0.7887286324786325\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3419459512646845, Train acc: 0.788243006993007\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3422223124069366, Train acc: 0.7880831552706553\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3428045576297476, Train acc: 0.7872904339250493\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3424540570367387, Train acc: 0.787736568986569\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3426481058115294, Train acc: 0.7876602564102564\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3425230251418219, Train acc: 0.7876936431623932\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3430868018626447, Train acc: 0.7871103569632981\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.34277393109212, Train acc: 0.7874673551756886\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3428813883489281, Train acc: 0.7876040260908682\n",
      "Val loss: 2.95119571685791, Val acc: 0.844\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3393148153256147, Train acc: 0.7924679487179487\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3415474188633454, Train acc: 0.7885950854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3441946350271545, Train acc: 0.7848112535612536\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3432532915702233, Train acc: 0.7863915598290598\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3423638042221722, Train acc: 0.7873397435897436\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3426570342137263, Train acc: 0.7875267094017094\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3417870820805848, Train acc: 0.7889194139194139\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3416679557572064, Train acc: 0.7895633012820513\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3422121863532723, Train acc: 0.788758309591643\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3418018826052673, Train acc: 0.7891826923076923\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3422576375218815, Train acc: 0.7887043512043512\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3420029712198807, Train acc: 0.7886396011396012\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3419581128922362, Train acc: 0.7888929980276134\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3415393101572262, Train acc: 0.7891674297924298\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3417632017380152, Train acc: 0.7890669515669516\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3411448400499475, Train acc: 0.7898804754273504\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3416164555460919, Train acc: 0.7894827802916038\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3412711073530366, Train acc: 0.7898415242165242\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3406763710634588, Train acc: 0.7904577147998201\n",
      "Val loss: 2.9477293491363525, Val acc: 0.844\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.343999275794396, Train acc: 0.7839209401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3398152080356565, Train acc: 0.7881944444444444\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3377120182385132, Train acc: 0.7903311965811965\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.337209754010551, Train acc: 0.7913995726495726\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3374143934657432, Train acc: 0.791559829059829\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3384357925154204, Train acc: 0.7902421652421653\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3381608138008723, Train acc: 0.7906364468864469\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3375546624008408, Train acc: 0.7914329594017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3372228401452162, Train acc: 0.7921415004748338\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3377303147927309, Train acc: 0.7917200854700854\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3379574046175704, Train acc: 0.7912296037296037\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3381709757014217, Train acc: 0.7908653846153846\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3382091410923442, Train acc: 0.7909270216962525\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3389547593136555, Train acc: 0.7900450244200244\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.338442618215186, Train acc: 0.7905448717948718\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.338915260683777, Train acc: 0.7898971688034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3390848570699485, Train acc: 0.7898441427853192\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3393388744319832, Train acc: 0.7895002374169041\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.339847287239387, Train acc: 0.7890238416554206\n",
      "Val loss: 2.948420524597168, Val acc: 0.84\n",
      "Epoch 64/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.338466140958998, Train acc: 0.7895299145299145\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.335462779061407, Train acc: 0.7936698717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3347863546463838, Train acc: 0.7954950142450142\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3377516264589424, Train acc: 0.7930021367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3378917355822701, Train acc: 0.7931089743589743\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3378897015525404, Train acc: 0.7931356837606838\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3374178223266415, Train acc: 0.7930021367521367\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3386263172341208, Train acc: 0.7917668269230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3396366208593617, Train acc: 0.7908357075023742\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3394847566245967, Train acc: 0.7907852564102564\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3398857761374166, Train acc: 0.790452602952603\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3391832514026565, Train acc: 0.7911547364672364\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3386588336447993, Train acc: 0.7913379355687048\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3393593221648126, Train acc: 0.790655525030525\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3388996367440944, Train acc: 0.7910078347578348\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.338158720706263, Train acc: 0.7915831997863247\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3378126871771643, Train acc: 0.7922008547008547\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3378248797749068, Train acc: 0.7921711775878443\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3377371729894956, Train acc: 0.7922289698605488\n",
      "Val loss: 2.945063352584839, Val acc: 0.846\n",
      "Epoch 65/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3406129739223382, Train acc: 0.7879273504273504\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3396777872346406, Train acc: 0.7899305555555556\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3374137939550939, Train acc: 0.7930911680911681\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3387516013577454, Train acc: 0.7920673076923077\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.339065453537509, Train acc: 0.7917735042735042\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3380080624523325, Train acc: 0.7927350427350427\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3378901502006075, Train acc: 0.793307387057387\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3383862108756335, Train acc: 0.7929019764957265\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3382728715681056, Train acc: 0.7927943969610636\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3386066351181423, Train acc: 0.7923878205128205\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3393135254199688, Train acc: 0.7913752913752914\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.339445301270553, Train acc: 0.79122150997151\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3395735474498078, Train acc: 0.7910708415516108\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3393452326165596, Train acc: 0.79132326007326\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3393423327693232, Train acc: 0.790954415954416\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3391277546811307, Train acc: 0.7909989316239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3384519145979001, Train acc: 0.7916352438411262\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.338164001108914, Train acc: 0.7921563390313391\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3376171406571307, Train acc: 0.7924960638776428\n",
      "Val loss: 2.9443166255950928, Val acc: 0.844\n",
      "Epoch 66/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3345767004877074, Train acc: 0.7932692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3356619461988792, Train acc: 0.7924679487179487\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3333860591605857, Train acc: 0.7940705128205128\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3352935660598624, Train acc: 0.7931356837606838\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3357012528639574, Train acc: 0.7931089743589743\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3357267108058657, Train acc: 0.7932692307692307\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.337084782138121, Train acc: 0.7912851037851037\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.337511115349256, Train acc: 0.7905315170940171\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3383837439056137, Train acc: 0.7896189458689459\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3388213502036201, Train acc: 0.7890758547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3381907304498515, Train acc: 0.7898698523698524\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3365093590187551, Train acc: 0.791755698005698\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3368290088272345, Train acc: 0.7910502958579881\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3368011106064905, Train acc: 0.791571275946276\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3363081426701995, Train acc: 0.791951566951567\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3360803680032747, Train acc: 0.7922676282051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.336440933116301, Train acc: 0.7916509552538964\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.336620259601959, Train acc: 0.7914589268755935\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3367417142887776, Train acc: 0.7915823211875843\n",
      "Val loss: 2.944762706756592, Val acc: 0.842\n",
      "Epoch 67/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.335702612868741, Train acc: 0.7938034188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3339622081854405, Train acc: 0.7938034188034188\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3354166898971949, Train acc: 0.7929131054131054\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3355137817880027, Train acc: 0.7927350427350427\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3371153419853277, Train acc: 0.791025641025641\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3357397151468826, Train acc: 0.7932247150997151\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3368772907280368, Train acc: 0.7916285103785103\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.336554853579937, Train acc: 0.7920005341880342\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.336337969400491, Train acc: 0.7919634377967711\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3369603093872722, Train acc: 0.7914797008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3360132617179794, Train acc: 0.7923222610722611\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3360090170830403, Train acc: 0.7923344017094017\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3360773510716606, Train acc: 0.7922419460881\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.335804197959993, Train acc: 0.7924488705738706\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3350312447615837, Train acc: 0.7931801994301995\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3347869956085825, Train acc: 0.7934528579059829\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3347539075838974, Train acc: 0.7936305932629462\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3341955548790898, Train acc: 0.7943079297245964\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3340075199420636, Train acc: 0.7945906432748538\n",
      "Val loss: 2.944775104522705, Val acc: 0.836\n",
      "Epoch 68/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3394630097935343, Train acc: 0.7887286324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3356882496776743, Train acc: 0.7926014957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3355952375634783, Train acc: 0.7930911680911681\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3360033030183907, Train acc: 0.7922676282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3343592492943137, Train acc: 0.7938034188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3334800317416504, Train acc: 0.7945601851851852\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3339484052343682, Train acc: 0.7938797313797313\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3338498496092284, Train acc: 0.7944043803418803\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3338478238720726, Train acc: 0.7941298670465338\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.334460550495702, Train acc: 0.7934294871794871\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3344162731274396, Train acc: 0.7938762626262627\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3351910565993046, Train acc: 0.7932914886039886\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3342492409555007, Train acc: 0.7939472386587771\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3344962205351198, Train acc: 0.7939560439560439\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3339305671191963, Train acc: 0.794551282051282\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3341833545356734, Train acc: 0.7940872061965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3342058833607962, Train acc: 0.7942747611865258\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3338631922476425, Train acc: 0.7948421177587844\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3338181489874958, Train acc: 0.7949420827710302\n",
      "Val loss: 2.94045352935791, Val acc: 0.85\n",
      "Epoch 69/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3325825206234925, Train acc: 0.7964743589743589\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.336028908053015, Train acc: 0.7923344017094017\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.337407504731094, Train acc: 0.7911324786324786\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3357777799296582, Train acc: 0.7926682692307693\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3350093242449639, Train acc: 0.7930021367521367\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.334548299808448, Train acc: 0.7939814814814815\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.334758688969781, Train acc: 0.7936126373626373\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3353698773261828, Train acc: 0.7931022970085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3336105686307294, Train acc: 0.7946937321937322\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3338268665167001, Train acc: 0.7943376068376068\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3339494054597203, Train acc: 0.7941919191919192\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3343752245617728, Train acc: 0.7935363247863247\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.333982588193042, Train acc: 0.7940910585141354\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3337972890471945, Train acc: 0.7944329975579976\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3335202204875456, Train acc: 0.7949252136752136\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3340615906521804, Train acc: 0.7945713141025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3341649142026062, Train acc: 0.7942747611865258\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3338985230281935, Train acc: 0.794485992402659\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3339923492059267, Train acc: 0.7945625281151597\n",
      "Val loss: 2.940591812133789, Val acc: 0.848\n",
      "Epoch 70/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3424404095380733, Train acc: 0.7873931623931624\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.333278430832757, Train acc: 0.797676282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3317623226730912, Train acc: 0.7991452991452992\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3324622990738633, Train acc: 0.7968082264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3311607458652595, Train acc: 0.7971688034188035\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3322843860017608, Train acc: 0.7958066239316239\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3318140451433604, Train acc: 0.7961309523809523\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3334948415430183, Train acc: 0.7944377670940171\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3327414753996296, Train acc: 0.7950201804368471\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3326423215050982, Train acc: 0.7949786324786324\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3324041135012872, Train acc: 0.7951874514374514\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3321946891624363, Train acc: 0.7958511396011396\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3314878098828318, Train acc: 0.7964332675871137\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3309763183814993, Train acc: 0.7968559218559218\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.331407635979503, Train acc: 0.7964387464387465\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.331717046407553, Train acc: 0.7960403311965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3315440599135624, Train acc: 0.7962072649572649\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3321242155852142, Train acc: 0.7955692070275404\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.332698003375954, Train acc: 0.794913967611336\n",
      "Val loss: 2.9365453720092773, Val acc: 0.85\n",
      "Epoch 71/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3308010345850236, Train acc: 0.7932692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3326317438712487, Train acc: 0.7944711538461539\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3299631746406229, Train acc: 0.7979878917378918\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3318129788097153, Train acc: 0.7962072649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3332370460542857, Train acc: 0.7943376068376068\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3327711368897701, Train acc: 0.7947382478632479\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3324877528771668, Train acc: 0.7955204517704517\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.332935749210863, Train acc: 0.7950387286324786\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3327244007349694, Train acc: 0.7947530864197531\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3321020984242105, Train acc: 0.7956196581196581\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3310931836715851, Train acc: 0.7966200466200466\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3305757842172585, Train acc: 0.7972978988603988\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.330382195685272, Train acc: 0.7973989151873767\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3296384970055977, Train acc: 0.7981723137973138\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3295882215527048, Train acc: 0.7980769230769231\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3300935990280576, Train acc: 0.7976929754273504\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3299762982109073, Train acc: 0.7978726747109101\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3301692366713245, Train acc: 0.7974833808167141\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3296535231752864, Train acc: 0.7980347503373819\n",
      "Val loss: 2.9387714862823486, Val acc: 0.844\n",
      "Epoch 72/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3346180059970953, Train acc: 0.7946047008547008\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3343718989282591, Train acc: 0.7954059829059829\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3329634075490837, Train acc: 0.7966524216524217\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3320230026530404, Train acc: 0.7968082264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3302540664998894, Train acc: 0.7979166666666667\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.329742637454954, Train acc: 0.7986556267806267\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3305164009369046, Train acc: 0.7980006105006106\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3304796628972404, Train acc: 0.7978098290598291\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3291329558645106, Train acc: 0.7988782051282052\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.329112234686175, Train acc: 0.7989583333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3303618631162843, Train acc: 0.7976155788655789\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3306422456037625, Train acc: 0.797409188034188\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3309639892164302, Train acc: 0.7971934582511505\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3310244959643764, Train acc: 0.7968940781440782\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3317031220493154, Train acc: 0.7962606837606837\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.332229011843347, Train acc: 0.7954727564102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3319942886473366, Train acc: 0.7957673453996983\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3319635493123634, Train acc: 0.7956879154795822\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3310413604055071, Train acc: 0.7966430499325237\n",
      "Val loss: 2.937842607498169, Val acc: 0.842\n",
      "Epoch 73/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.325770117278792, Train acc: 0.8020833333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3277434671027029, Train acc: 0.8003472222222222\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3265167661541888, Train acc: 0.8023504273504274\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3296125184776437, Train acc: 0.7990117521367521\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3287077708122057, Train acc: 0.8000534188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3274261194076973, Train acc: 0.8018162393162394\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3280709337111067, Train acc: 0.800976800976801\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3281542325121725, Train acc: 0.8011818910256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3281325016945515, Train acc: 0.8009556030389364\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3279213118756938, Train acc: 0.8009615384615385\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3281321603379328, Train acc: 0.8003350815850816\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3284878190766034, Train acc: 0.7999910968660968\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3280777708626672, Train acc: 0.8003985864562788\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3291574974199791, Train acc: 0.7991834554334555\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.327790181657188, Train acc: 0.8006232193732193\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3282320856029153, Train acc: 0.799979967948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3284278479336853, Train acc: 0.7996323529411765\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3281541282748, Train acc: 0.7998723884140551\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3283008919929096, Train acc: 0.7998341205578048\n",
      "Val loss: 2.937901020050049, Val acc: 0.848\n",
      "Epoch 74/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3251703620975852, Train acc: 0.8018162393162394\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3277858240991576, Train acc: 0.8002136752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3292569672619854, Train acc: 0.7986111111111112\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3297111774102235, Train acc: 0.7976095085470085\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3290584788363204, Train acc: 0.7979700854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.329692157585057, Train acc: 0.7978098290598291\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3298140046651838, Train acc: 0.7976953601953602\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3292896709381006, Train acc: 0.7982772435897436\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3289395017162007, Train acc: 0.7985517568850902\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3289757720425597, Train acc: 0.7981570512820513\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.32874290778272, Train acc: 0.7990724553224553\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3287545427297935, Train acc: 0.7991007834757835\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.328416655363024, Train acc: 0.7993096646942801\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3283758341028868, Train acc: 0.7991834554334555\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3281053121952588, Train acc: 0.7995014245014245\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3276346399743333, Train acc: 0.8000634348290598\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3271661357342628, Train acc: 0.800794997486174\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.327694066908964, Train acc: 0.8000949667616334\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3280673737039022, Train acc: 0.7995810841205578\n",
      "Val loss: 2.935765504837036, Val acc: 0.848\n",
      "Epoch 75/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3310945584223821, Train acc: 0.7946047008547008\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3290867988879864, Train acc: 0.7990117521367521\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3267214420514228, Train acc: 0.801460113960114\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3272453429352524, Train acc: 0.7999465811965812\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3276737376156016, Train acc: 0.7991452991452992\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.32787345956873, Train acc: 0.7981659544159544\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3275681563786097, Train acc: 0.7987637362637363\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3290390749262948, Train acc: 0.7976095085470085\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3287493842959064, Train acc: 0.7982549857549858\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3299448056098742, Train acc: 0.797142094017094\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3296267775110988, Train acc: 0.7974698912198912\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3301848945794281, Train acc: 0.7969417735042735\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3302234328945242, Train acc: 0.7966592702169625\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.330149588276324, Train acc: 0.7969131562881563\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3296424573642915, Train acc: 0.7973290598290599\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3295150292225373, Train acc: 0.7973758012820513\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3293775345824843, Train acc: 0.7974641779788839\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3293105816682633, Train acc: 0.7974833808167141\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.328752250323894, Train acc: 0.7978801169590644\n",
      "Val loss: 2.93231463432312, Val acc: 0.856\n",
      "Epoch 76/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3300965688167474, Train acc: 0.7975427350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3292642444626899, Train acc: 0.7954059829059829\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3297201839946953, Train acc: 0.7957621082621082\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.327694752277472, Train acc: 0.7988782051282052\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3276556218791211, Train acc: 0.79866452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3271713430046017, Train acc: 0.7989672364672364\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.32745729523264, Train acc: 0.7987637362637363\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3278373095214877, Train acc: 0.7986111111111112\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3272346198162575, Train acc: 0.7996498100664767\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3273977242983304, Train acc: 0.7995726495726496\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3270627036980167, Train acc: 0.7999222999222999\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3269700197412757, Train acc: 0.8001469017094017\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3268993070139687, Train acc: 0.8002136752136753\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3265173905352825, Train acc: 0.8005380036630036\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3262571681259026, Train acc: 0.8008012820512821\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3261468871536417, Train acc: 0.80078125\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3262991822503092, Train acc: 0.8005593262946205\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.326626697043974, Train acc: 0.8001394824311491\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3264964581393894, Train acc: 0.800171502474134\n",
      "Val loss: 2.9333274364471436, Val acc: 0.852\n",
      "Epoch 77/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3233389243101463, Train acc: 0.8034188034188035\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3199487680043929, Train acc: 0.8082264957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3213665410664006, Train acc: 0.8055555555555556\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3228300519478626, Train acc: 0.8042868589743589\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.324609632166023, Train acc: 0.8024038461538462\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3243657612053417, Train acc: 0.8025730056980057\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3246035008203416, Train acc: 0.8025412087912088\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3255177850906665, Train acc: 0.8011818910256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3257452771421399, Train acc: 0.8012820512820513\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3260178563941238, Train acc: 0.8010683760683761\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.325415938976854, Train acc: 0.8018162393162394\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3252086391476146, Train acc: 0.8020165598290598\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3246223337159668, Train acc: 0.8026997041420119\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3243331481248906, Train acc: 0.802960927960928\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3244168474463656, Train acc: 0.8026531339031339\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3246685463903296, Train acc: 0.8024839743589743\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3250415344343767, Train acc: 0.8019105077928608\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3256035982034144, Train acc: 0.8013265669515669\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3254186355257913, Train acc: 0.8015210301394512\n",
      "Val loss: 2.931520462036133, Val acc: 0.852\n",
      "Epoch 78/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3272926848158877, Train acc: 0.8026175213675214\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3200841062089317, Train acc: 0.8083600427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3255989150783616, Train acc: 0.8027065527065527\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3247483112873175, Train acc: 0.8026842948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3248489669245533, Train acc: 0.802190170940171\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3243228811823744, Train acc: 0.8030626780626781\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3256508791694128, Train acc: 0.8013583638583639\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3246359570413573, Train acc: 0.8025507478632479\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3249809717067853, Train acc: 0.8020833333333334\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.324360048872793, Train acc: 0.8025641025641026\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.324677189654311, Train acc: 0.8021318958818959\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3246736288749934, Train acc: 0.8022168803418803\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3244252822494758, Train acc: 0.8027202498356345\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3251229892865788, Train acc: 0.8019688644688645\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3251620332060376, Train acc: 0.8019052706552706\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3253684957058003, Train acc: 0.8016326121794872\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3252050684827725, Train acc: 0.8018633735545501\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3251570034117668, Train acc: 0.8019646248812915\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3250435626673045, Train acc: 0.8021957939721097\n",
      "Val loss: 2.9313035011291504, Val acc: 0.854\n",
      "Epoch 79/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3229854168036046, Train acc: 0.8031517094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3271997453819993, Train acc: 0.7975427350427351\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3259661116151729, Train acc: 0.8001246438746439\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3223127310092633, Train acc: 0.804954594017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.323384154963697, Train acc: 0.8038461538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.324016690593839, Train acc: 0.8027510683760684\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3241001622289674, Train acc: 0.8026556776556777\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3229603023610563, Train acc: 0.8039529914529915\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.323294282072743, Train acc: 0.8040716999050332\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3231789250659127, Train acc: 0.8042735042735043\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.323250440821318, Train acc: 0.8041715229215229\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3237225754648192, Train acc: 0.8035968660968661\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3234266609364949, Train acc: 0.8038913543721236\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3227894271249736, Train acc: 0.8045062576312576\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3232710763599798, Train acc: 0.8039529914529915\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3238172934860246, Train acc: 0.8033687232905983\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3236562212008576, Train acc: 0.8034188034188035\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.323452758992839, Train acc: 0.8035671889838556\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3234590404054039, Train acc: 0.8034890913180387\n",
      "Val loss: 2.9320669174194336, Val acc: 0.85\n",
      "Epoch 80/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3203638802226791, Train acc: 0.8036858974358975\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3184660999183981, Train acc: 0.8062232905982906\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3216799908553774, Train acc: 0.8039529914529915\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3226143116624947, Train acc: 0.8027510683760684\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3213972919007653, Train acc: 0.8047542735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3220360431915674, Train acc: 0.8039084757834758\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.32249709101387, Train acc: 0.8036095848595849\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.322768666805365, Train acc: 0.8033854166666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3230182162264021, Train acc: 0.803389126305793\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3241770654662042, Train acc: 0.8024038461538462\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3229530028066805, Train acc: 0.8038558663558664\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3238239651731616, Train acc: 0.8030849358974359\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3237810636491543, Train acc: 0.8029462524654832\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3238808839457958, Train acc: 0.8027892246642246\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.323326722848789, Train acc: 0.8035790598290599\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3236889333551765, Train acc: 0.8032184829059829\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3231229216324858, Train acc: 0.8037173202614379\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3233033094877078, Train acc: 0.8034781576448243\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3231188507697842, Train acc: 0.8037140125955915\n",
      "Val loss: 2.9306561946868896, Val acc: 0.852\n",
      "Epoch 81/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3247778334169307, Train acc: 0.8012820512820513\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3234399404281225, Train acc: 0.8026175213675214\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3265622545171667, Train acc: 0.7995014245014245\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3232030583243084, Train acc: 0.8029513888888888\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3224391183282576, Train acc: 0.8038995726495727\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3228663783467394, Train acc: 0.8028400997150997\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3246078575632656, Train acc: 0.8011675824175825\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.324817452675257, Train acc: 0.8009481837606838\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3248345854162378, Train acc: 0.8004510921177588\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3254153651050014, Train acc: 0.7996794871794872\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3255954511237866, Train acc: 0.7995580808080808\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3247653575364682, Train acc: 0.8006143162393162\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3252254526212293, Train acc: 0.8000287639710717\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3249500217018546, Train acc: 0.8003663003663004\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3245670504719445, Train acc: 0.8007478632478633\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3241826626989577, Train acc: 0.8011818910256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3239154839647542, Train acc: 0.8014548768225239\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.324226859967593, Train acc: 0.8014007597340931\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3241885086999945, Train acc: 0.8013382816014395\n",
      "Val loss: 2.9275431632995605, Val acc: 0.858\n",
      "Epoch 82/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3222907017438839, Train acc: 0.8015491452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.32638494275574, Train acc: 0.7988782051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.322655906025161, Train acc: 0.8027065527065527\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3229191170798407, Train acc: 0.8031517094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3211588896237887, Train acc: 0.8043269230769231\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.32165461388069, Train acc: 0.8037749287749287\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3200877000997355, Train acc: 0.8051739926739927\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3202040248956435, Train acc: 0.8050213675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3202602956143312, Train acc: 0.8050807217473884\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3194905226047222, Train acc: 0.8062232905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.320404564213549, Train acc: 0.8056283993783994\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3207340715957163, Train acc: 0.8052884615384616\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3206666559862668, Train acc: 0.8053295529257067\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3217100769608885, Train acc: 0.8041819291819292\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3222968352825892, Train acc: 0.8036324786324787\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3216345264361455, Train acc: 0.8044871794871795\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3221078907922024, Train acc: 0.8040315485168427\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.322428521267709, Train acc: 0.8035820275403609\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3222092343406318, Train acc: 0.8039389338731444\n",
      "Val loss: 2.924980878829956, Val acc: 0.858\n",
      "Epoch 83/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3309551080067952, Train acc: 0.7935363247863247\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3291820522047515, Train acc: 0.7939369658119658\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3261466318385893, Train acc: 0.7983440170940171\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.32685469969725, Train acc: 0.7980101495726496\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3238517431112435, Train acc: 0.8004273504273505\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.323092035418562, Train acc: 0.8011039886039886\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3233755499452025, Train acc: 0.800442612942613\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.321686756152373, Train acc: 0.8026509081196581\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3222943278346193, Train acc: 0.8024097815764483\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3219850800995134, Train acc: 0.8029647435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3223062860196935, Train acc: 0.802666083916084\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.322368472899467, Train acc: 0.8027733262108262\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3221865902599106, Train acc: 0.8026791584483892\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3231766615449583, Train acc: 0.8017971611721612\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3225938772543884, Train acc: 0.80252849002849\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3228938752769406, Train acc: 0.8022168803418803\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3223833884348997, Train acc: 0.8027589240824535\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3222125806020535, Train acc: 0.8028994539411206\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.321715867256614, Train acc: 0.8033625730994152\n",
      "Val loss: 2.9282150268554688, Val acc: 0.854\n",
      "Epoch 84/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.323845241823767, Train acc: 0.7994123931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.324439303487794, Train acc: 0.7994123931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3248204858894022, Train acc: 0.8000356125356125\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3242575694353154, Train acc: 0.8009481837606838\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3226642853174455, Train acc: 0.802991452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3230861061998243, Train acc: 0.8032407407407407\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3223016689985225, Train acc: 0.8039911477411478\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3227736544914734, Train acc: 0.8035857371794872\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3213908237609429, Train acc: 0.8048433048433048\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3218704299030142, Train acc: 0.8043803418803419\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3221723422026024, Train acc: 0.8039044289044289\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3214579430061189, Train acc: 0.8045984686609686\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3213675023692757, Train acc: 0.8046720907297831\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3207238203731066, Train acc: 0.8052884615384616\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3207009231263076, Train acc: 0.8052884615384616\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3201742230826974, Train acc: 0.8058560363247863\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.320274269119347, Train acc: 0.8056812468577175\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.320230115512837, Train acc: 0.8057781339031339\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3199978711312277, Train acc: 0.8060475708502024\n",
      "Val loss: 2.9294509887695312, Val acc: 0.856\n",
      "Epoch 85/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.32093062156286, Train acc: 0.8023504273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3143676977891188, Train acc: 0.8096955128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3168609223814092, Train acc: 0.8076032763532763\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3166880077785916, Train acc: 0.8081597222222222\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.32067658106486, Train acc: 0.8033653846153846\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3206512693665986, Train acc: 0.8035523504273504\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3201814172323225, Train acc: 0.8047161172161172\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3188296492792602, Train acc: 0.8060563568376068\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3195851412933437, Train acc: 0.8052884615384616\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3202270006522154, Train acc: 0.8044604700854701\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3206903812212822, Train acc: 0.8038558663558664\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.32124468149283, Train acc: 0.8032852564102564\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3209165188139165, Train acc: 0.8037886259040106\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3208963735636337, Train acc: 0.803895757020757\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3212176939700744, Train acc: 0.8034188034188035\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3216882928823814, Train acc: 0.802734375\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3216232398816123, Train acc: 0.8029788838612368\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3214049597411415, Train acc: 0.8034929962013295\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3214010471596247, Train acc: 0.8037140125955915\n",
      "Val loss: 2.927419424057007, Val acc: 0.856\n",
      "Epoch 86/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3146785100301106, Train acc: 0.8087606837606838\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3187339061345809, Train acc: 0.8070245726495726\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3181453749664829, Train acc: 0.8068019943019943\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3194975078615367, Train acc: 0.8059561965811965\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3196759464394332, Train acc: 0.8056623931623932\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3200657880543982, Train acc: 0.8061342592592593\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3189878347301367, Train acc: 0.8071581196581197\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3180803139495034, Train acc: 0.8078258547008547\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.317402635204826, Train acc: 0.8082858499525166\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3182004089029427, Train acc: 0.8069444444444445\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3174880078900388, Train acc: 0.8078379953379954\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3179030469340136, Train acc: 0.807380698005698\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.31851737817442, Train acc: 0.8069115713346483\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3184995435824178, Train acc: 0.8069101037851037\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3188254471857663, Train acc: 0.8066595441595441\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3192853933980322, Train acc: 0.8060897435897436\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3196883045769985, Train acc: 0.8058697838109603\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3192094485191426, Train acc: 0.8063271604938271\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3191465107750044, Train acc: 0.8064411830859199\n",
      "Val loss: 2.9279582500457764, Val acc: 0.856\n",
      "Epoch 87/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.322012157521696, Train acc: 0.8012820512820513\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3212484345476851, Train acc: 0.8031517094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3202579673538861, Train acc: 0.8051103988603988\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3188898329041963, Train acc: 0.8058894230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.320147467067099, Train acc: 0.8042735042735043\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.321282645236393, Train acc: 0.8030181623931624\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3208613765399826, Train acc: 0.8033806471306472\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.320166163210176, Train acc: 0.8043870192307693\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3200426357537367, Train acc: 0.8047839506172839\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3202691118941348, Train acc: 0.8049679487179487\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.320514256211335, Train acc: 0.8042929292929293\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.319561390285818, Train acc: 0.8052662037037037\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3204511871939815, Train acc: 0.8044871794871795\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3208369144735463, Train acc: 0.8043536324786325\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3201919732270417, Train acc: 0.8048611111111111\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3199182140012073, Train acc: 0.8052383814102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3200003702835919, Train acc: 0.8051627702362997\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3200032858201016, Train acc: 0.8052884615384616\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.319809568609357, Train acc: 0.8055696131354027\n",
      "Val loss: 2.9278335571289062, Val acc: 0.854\n",
      "Epoch 88/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3213106302114634, Train acc: 0.8031517094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.317611602636484, Train acc: 0.8072916666666666\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.320498356452355, Train acc: 0.8035078347578347\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3181276683114533, Train acc: 0.8058894230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3168101131406604, Train acc: 0.8080128205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3177358193954511, Train acc: 0.807113603988604\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3175345296824807, Train acc: 0.8073489010989011\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.317222708565557, Train acc: 0.8080261752136753\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3183754726692483, Train acc: 0.8070097340930674\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.319498596028385, Train acc: 0.8059294871794872\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3193336690963473, Train acc: 0.8059683372183373\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3188335664591557, Train acc: 0.8064458689458689\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3188110211426924, Train acc: 0.8065212031558185\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3190405026751415, Train acc: 0.8064522283272283\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.31958876185947, Train acc: 0.8058048433048433\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3196978846676328, Train acc: 0.8055889423076923\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3196455449420283, Train acc: 0.8055712669683258\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3195265781732253, Train acc: 0.8056149097815765\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3192488725690545, Train acc: 0.805935110211426\n",
      "Val loss: 2.9269275665283203, Val acc: 0.854\n",
      "Epoch 89/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3120198433215802, Train acc: 0.8151709401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3241811562807133, Train acc: 0.8015491452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3200729932540503, Train acc: 0.8050213675213675\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3214473637760196, Train acc: 0.804153311965812\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.320806942230616, Train acc: 0.8053952991452992\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3215562200274562, Train acc: 0.8040420227920227\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3224158036694276, Train acc: 0.803495115995116\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3211899687591782, Train acc: 0.8045873397435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3198347195827271, Train acc: 0.8056149097815765\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3193384820579463, Train acc: 0.8062232905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3197727247829483, Train acc: 0.8058469308469308\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3192610676132377, Train acc: 0.8062678062678063\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3190300936451247, Train acc: 0.8063157462195923\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3191491657997663, Train acc: 0.806280525030525\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3187490302952607, Train acc: 0.8064814814814815\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3190115374377651, Train acc: 0.8061732104700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3187782320921118, Train acc: 0.8064196832579186\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3187998053241885, Train acc: 0.8063123219373219\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3188305741969497, Train acc: 0.8062022042285201\n",
      "Val loss: 2.926055669784546, Val acc: 0.848\n",
      "Epoch 90/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3175842517461531, Train acc: 0.8063568376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3150453231273553, Train acc: 0.8088942307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3170991650334112, Train acc: 0.8068910256410257\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3187487247662666, Train acc: 0.805488782051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3177391317155627, Train acc: 0.806784188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3191112239136655, Train acc: 0.8052884615384616\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3193188432663205, Train acc: 0.804945054945055\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3187137668968265, Train acc: 0.8054553952991453\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3182855370151125, Train acc: 0.8059710351377019\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.317164004766024, Train acc: 0.8074252136752137\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3172641020925266, Train acc: 0.8073280885780886\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3170415650405776, Train acc: 0.8074474715099715\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3176047737703442, Train acc: 0.8068088428665352\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3179820487787435, Train acc: 0.806681166056166\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3169360307546762, Train acc: 0.8076923076923077\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3168640444930801, Train acc: 0.8077590811965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3172087277402345, Train acc: 0.8073466566113625\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3174595040360295, Train acc: 0.8069948955365622\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3170268462683454, Train acc: 0.8075236167341431\n",
      "Val loss: 2.9267077445983887, Val acc: 0.854\n",
      "Epoch 91/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3211230722248044, Train acc: 0.8020833333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3193431369259827, Train acc: 0.8048878205128205\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3210100319310811, Train acc: 0.80252849002849\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.320078149310544, Train acc: 0.8033520299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3188088906116975, Train acc: 0.8047008547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3188487967194995, Train acc: 0.8050658831908832\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3196981142437647, Train acc: 0.8040293040293041\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3188522165147667, Train acc: 0.8050213675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.318634827931722, Train acc: 0.8051697530864198\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3195175827058971, Train acc: 0.8039797008547008\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3191588338067946, Train acc: 0.8044871794871795\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3187653245749298, Train acc: 0.8050658831908832\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3182537005497859, Train acc: 0.805802103879027\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3184487372818738, Train acc: 0.8055364774114774\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3180523068137318, Train acc: 0.8060007122507122\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3181707032470622, Train acc: 0.8056557158119658\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3187821082819522, Train acc: 0.8048956762192057\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3186473262276066, Train acc: 0.8051697530864198\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3192029036866402, Train acc: 0.8045293522267206\n",
      "Val loss: 2.923342227935791, Val acc: 0.852\n",
      "Epoch 92/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3164500823387733, Train acc: 0.8066239316239316\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.310719999492678, Train acc: 0.813301282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3094165134973337, Train acc: 0.8146367521367521\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3125051693019705, Train acc: 0.811965811965812\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3131047379257332, Train acc: 0.812232905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3134645254184039, Train acc: 0.8112535612535613\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3134181330637762, Train acc: 0.8111645299145299\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.313566857168817, Train acc: 0.8111311431623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.31415894749271, Train acc: 0.8106303418803419\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.313914977790963, Train acc: 0.8107638888888888\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.314709017234156, Train acc: 0.8097562160062161\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3141721655160954, Train acc: 0.8102297008547008\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.314498994477402, Train acc: 0.8096852399737016\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3146091048970765, Train acc: 0.8095047313797313\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3146416632877795, Train acc: 0.8093839031339032\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3150090892345478, Train acc: 0.8091446314102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3151763970675692, Train acc: 0.808839240824535\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.315179591731355, Train acc: 0.8089535849952516\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3150509844156626, Train acc: 0.8091121232568601\n",
      "Val loss: 2.923953056335449, Val acc: 0.854\n",
      "Epoch 93/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3124541034046402, Train acc: 0.812767094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3137911558151245, Train acc: 0.8123664529914529\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3150879046176573, Train acc: 0.8108084045584045\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3146672712432013, Train acc: 0.8108306623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3134455179556823, Train acc: 0.8117521367521368\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3149799390396162, Train acc: 0.8098290598290598\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3154255720867487, Train acc: 0.8091804029304029\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3159903896670055, Train acc: 0.8086271367521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3159446487626005, Train acc: 0.8085826210826211\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3161405673393836, Train acc: 0.8084134615384615\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3159811072508865, Train acc: 0.8086392773892774\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.315763452963272, Train acc: 0.8088497150997151\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3163883676660602, Train acc: 0.8083908612754767\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3165151170467428, Train acc: 0.808264652014652\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.315985693619122, Train acc: 0.8087250712250712\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3161052126660306, Train acc: 0.8087106036324786\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3161071474795487, Train acc: 0.8084935897435898\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3163494253203853, Train acc: 0.8083303656220323\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.315766719671396, Train acc: 0.8087606837606838\n",
      "Val loss: 2.9228198528289795, Val acc: 0.852\n",
      "Epoch 94/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3150157174493513, Train acc: 0.811965811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3173764658789349, Train acc: 0.8087606837606838\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3161144976602321, Train acc: 0.8107193732193733\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3145954573256338, Train acc: 0.8110309829059829\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3139146784431914, Train acc: 0.8119123931623932\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3152942579356355, Train acc: 0.8098290598290598\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3155516739729995, Train acc: 0.8092567155067155\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3160352141429217, Train acc: 0.8085603632478633\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3151839356816393, Train acc: 0.8095619658119658\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3148970532621074, Train acc: 0.8096955128205128\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.314191022767702, Train acc: 0.8103632478632479\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3143297624044608, Train acc: 0.8101184116809117\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3149443918901553, Train acc: 0.8095208744247205\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3146583828733953, Train acc: 0.8099626068376068\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3146373718892068, Train acc: 0.8099358974358974\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3144878080258002, Train acc: 0.8102630876068376\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3150275215064537, Train acc: 0.8095619658119658\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3145509309578485, Train acc: 0.8101406695156695\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3144059335654563, Train acc: 0.8104616509221773\n",
      "Val loss: 2.921868085861206, Val acc: 0.852\n",
      "Epoch 95/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.32232378079341, Train acc: 0.8039529914529915\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.323413072488247, Train acc: 0.8011485042735043\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.320319201531913, Train acc: 0.8045762108262108\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3185789345676064, Train acc: 0.8058894230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.31829756427015, Train acc: 0.8057692307692308\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3165953125369514, Train acc: 0.807914886039886\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3163747461433084, Train acc: 0.8074252136752137\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3163965199238215, Train acc: 0.8074252136752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3158675218239808, Train acc: 0.8084342355175689\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3156922332242005, Train acc: 0.8086271367521367\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3155393496723071, Train acc: 0.8090034965034965\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.314977094318792, Train acc: 0.8095174501424501\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.315168307534493, Train acc: 0.8090894148586456\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.315274050384214, Train acc: 0.8086462148962149\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3149828882298917, Train acc: 0.8089387464387464\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3148548829759288, Train acc: 0.8090611645299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3150372542347244, Train acc: 0.8089020864756159\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3149050561337956, Train acc: 0.80917616334283\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.314776367182352, Train acc: 0.8093089293747189\n",
      "Val loss: 2.917419672012329, Val acc: 0.862\n",
      "Epoch 96/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.316789928664509, Train acc: 0.8076923076923077\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3228943246042626, Train acc: 0.8014155982905983\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3208309089356338, Train acc: 0.80252849002849\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3185538751447303, Train acc: 0.8050881410256411\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.317450742640047, Train acc: 0.8057692307692308\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3160238558070951, Train acc: 0.807113603988604\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3158312673533792, Train acc: 0.8071199633699634\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3165890925969832, Train acc: 0.8062900641025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3162544491397463, Train acc: 0.8067723171889839\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3152426943819746, Train acc: 0.8076388888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3147880419706688, Train acc: 0.8079594017094017\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3146419784961603, Train acc: 0.8080929487179487\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.314676439503476, Train acc: 0.8081443129520053\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3147774039607345, Train acc: 0.807997557997558\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3150085071552853, Train acc: 0.8078881766381767\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3149639371878061, Train acc: 0.8081096420940171\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3147765312319488, Train acc: 0.80850930115636\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3148958547162874, Train acc: 0.8084342355175689\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.314462425791354, Train acc: 0.8090699505173189\n",
      "Val loss: 2.919783592224121, Val acc: 0.854\n",
      "Epoch 97/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3198880619472928, Train acc: 0.8015491452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3198747461677616, Train acc: 0.8000801282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3172636072859805, Train acc: 0.8044871794871795\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3171659315753186, Train acc: 0.8042200854700855\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.315233441295787, Train acc: 0.8060363247863248\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3141815954463774, Train acc: 0.8073361823361823\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3138851045252202, Train acc: 0.8076923076923077\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.31556129302734, Train acc: 0.8061899038461539\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3158407564516421, Train acc: 0.8061490978157645\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.3151900038759932, Train acc: 0.8067574786324786\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3143619564770725, Train acc: 0.8075223387723388\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3147987332778779, Train acc: 0.8070913461538461\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3151211205632336, Train acc: 0.8068910256410257\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3151571463898133, Train acc: 0.8068528693528694\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3155980111526968, Train acc: 0.8065883190883191\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3149152438864748, Train acc: 0.807441907051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3150392204388235, Train acc: 0.8074252136752137\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3148560517194263, Train acc: 0.8075290835707503\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3145116722589134, Train acc: 0.8080718623481782\n",
      "Val loss: 2.9212820529937744, Val acc: 0.856\n",
      "Epoch 98/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3204032498547154, Train acc: 0.8087606837606838\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.313609231231559, Train acc: 0.813301282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.314733423059143, Train acc: 0.8108974358974359\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3128776937468438, Train acc: 0.8122996794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.3138099800827157, Train acc: 0.8112179487179487\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3130625982230206, Train acc: 0.8114316239316239\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3156275041810759, Train acc: 0.808264652014652\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3162439320331965, Train acc: 0.8073250534188035\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.315614109365349, Train acc: 0.8079890788224121\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.315467135926597, Train acc: 0.8083600427350427\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3151055829137819, Train acc: 0.8086149961149961\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3146421539138184, Train acc: 0.8089164886039886\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3145612944591054, Train acc: 0.80896614069691\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3135945628123114, Train acc: 0.8100198412698413\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3127990255328665, Train acc: 0.8106125356125357\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3127621608412163, Train acc: 0.8107638888888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.312702229659482, Train acc: 0.8108188788335847\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3125345662788108, Train acc: 0.8110161443494777\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3124233429034016, Train acc: 0.8111504723346828\n",
      "Val loss: 2.919613838195801, Val acc: 0.858\n",
      "Epoch 99/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.316438913345337, Train acc: 0.8047542735042735\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3122073636095748, Train acc: 0.8092948717948718\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.3125693499192892, Train acc: 0.8094729344729344\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3116360714292934, Train acc: 0.8110977564102564\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.311653486887614, Train acc: 0.811698717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3105804848874736, Train acc: 0.8129006410256411\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.310048211334098, Train acc: 0.8132249694749695\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3093371401485214, Train acc: 0.8138688568376068\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.3092474767625162, Train acc: 0.8139245014245015\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.309192518495087, Train acc: 0.814022435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3082112897340883, Train acc: 0.8149281274281275\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.308736763278983, Train acc: 0.8147035256410257\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3093307093520608, Train acc: 0.8141436554898094\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3101912356994965, Train acc: 0.8132249694749695\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3103006289555477, Train acc: 0.8131054131054131\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.310101910151987, Train acc: 0.813301282051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3102618005780373, Train acc: 0.8129556309703369\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3100867907659055, Train acc: 0.8133754748338081\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3103188399093086, Train acc: 0.8130623031938822\n",
      "Val loss: 2.9214813709259033, Val acc: 0.854\n",
      "Epoch 100/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.323695305066231, Train acc: 0.7983440170940171\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.3212789562013414, Train acc: 0.8015491452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.317917235556491, Train acc: 0.8059116809116809\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.3192328041435306, Train acc: 0.8040865384615384\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.317965307398739, Train acc: 0.8052350427350428\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.3168264885573646, Train acc: 0.8062678062678063\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.3152500229440767, Train acc: 0.8079594017094017\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.3148743719117255, Train acc: 0.8079927884615384\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.314486755831176, Train acc: 0.8081968186134852\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.314140582899762, Train acc: 0.8084935897435898\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.3135885840344077, Train acc: 0.8091491841491841\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.3133692316859535, Train acc: 0.8094284188034188\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.3140565294093949, Train acc: 0.8089866863905325\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.3134774710523631, Train acc: 0.8097336691086691\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.3130778111623562, Train acc: 0.8101139601139601\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.3127755638625886, Train acc: 0.8103131677350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.3123444440437957, Train acc: 0.8109288587229764\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.3116867610192366, Train acc: 0.8115651709401709\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.3112475224244826, Train acc: 0.8120782726045884\n",
      "Val loss: 2.921302556991577, Val acc: 0.856\n",
      "Tiempo total de entrenamiento: 2048.5336 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-6\n",
    "dropout_p = 0.6\n",
    "batch_size = 32\n",
    "criterion = perdida_regularizada_entropia\n",
    "epochs = 100\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    metadata_train,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    metadata_val,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    "    beta=0.5,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AGN       0.89      0.85      0.87       100\n",
      "          SN       0.85      0.71      0.77       100\n",
      "          VS       0.91      0.89      0.90       100\n",
      "    asteroid       0.84      0.96      0.90       100\n",
      "       bogus       0.83      0.90      0.87       100\n",
      "\n",
      "    accuracy                           0.86       500\n",
      "   macro avg       0.86      0.86      0.86       500\n",
      "weighted avg       0.86      0.86      0.86       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Asegúrate de tener tus datos de test preparados\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() missing 2 required positional arguments: 'batch_size' and 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\milan\\ing\\8_Semestre\\Inteligencia\\Proyecto\\The_big_proyect\\Algoritmo.ipynb Cell 37\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model \u001b[39m=\u001b[39m CNNModel(dropout_p\u001b[39m=\u001b[39mdropout_p)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m curves \u001b[39m=\u001b[39m train_model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     Train_images,  \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     Train_labels,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     Val_images,    \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     Val_labels,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     epochs,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     criterion,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     batch_size,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     lr,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     use_gpu\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     beta\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Mostrar las curvas de entrenamiento\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/milan/ing/8_Semestre/Inteligencia/Proyecto/The_big_proyect/Algoritmo.ipynb#X51sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m show_curves(curves)\n",
      "\u001b[1;31mTypeError\u001b[0m: train_model() missing 2 required positional arguments: 'batch_size' and 'lr'"
     ]
    }
   ],
   "source": [
    "# Instanciación del modelo\n",
    "lr = 5e-5\n",
    "dropout_p = 0.6\n",
    "batch_size = 32\n",
    "criterion = perdida_regularizada_entropia\n",
    "epochs = 100\n",
    "model = CNNModel(dropout_p=dropout_p)\n",
    "\n",
    "curves = train_model(\n",
    "    model,\n",
    "    Train_images,  \n",
    "    Train_labels,\n",
    "    Val_images,    \n",
    "    Val_labels,\n",
    "    epochs,\n",
    "    criterion,\n",
    "    batch_size,\n",
    "    lr,\n",
    "    use_gpu=True,\n",
    "    beta=0.4, \n",
    "    patience=30\n",
    ")\n",
    "\n",
    "\n",
    "# Mostrar las curvas de entrenamiento\n",
    "show_curves(curves)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de tener tus datos de test preparados\n",
    "Test_images = preparar_imagenes_para_modelo(data_procesada, key_principal='Test')\n",
    "Test_labels = extraer_etiquetas(data_procesada, key_principal='Test')\n",
    "metadata_test = extraer_metadata(data_procesada, key_principal='Test')\n",
    "\n",
    "# Definición de dataloader\n",
    "test_dataset = torch.utils.data.TensorDataset(Test_images, Test_labels, metadata_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "true_labels, predicted_labels = predict(model, test_loader, use_gpu=True)\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "report = classification_report(true_labels, predicted_labels, target_names=['AGN', 'SN', 'VS', 'asteroid', 'bogus'])\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combinación 1/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.2324507498843038, Train acc: 0.7677617521367521\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.17816481541874063, Train acc: 0.8001469017094017\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.15715718303310905, Train acc: 0.813301282051282\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.14360059664035454, Train acc: 0.8201789529914529\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13385503383783193, Train acc: 0.8259081196581196\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1259705827192024, Train acc: 0.8309517450142451\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1210579702560136, Train acc: 0.8339056776556777\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11615271544736674, Train acc: 0.8365718482905983\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.11160355251625619, Train acc: 0.8394171415004749\n",
      "Val loss: 0.3867376446723938, Val acc: 0.902\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07496460227884798, Train acc: 0.8597756410256411\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07627955206439026, Train acc: 0.8583733974358975\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07573863541298782, Train acc: 0.8575498575498576\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07650456946884465, Train acc: 0.8575387286324786\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07482585234519763, Train acc: 0.858840811965812\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07592626722619744, Train acc: 0.8585069444444444\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07508655294539436, Train acc: 0.8593368437118437\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07402009637946756, Train acc: 0.8597088675213675\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07338574421145411, Train acc: 0.8594640313390314\n",
      "Val loss: 0.3826371729373932, Val acc: 0.908\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08102924548662625, Train acc: 0.8556356837606838\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07412396778917721, Train acc: 0.8586404914529915\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07098647569998717, Train acc: 0.8620014245014245\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07164754081740339, Train acc: 0.8604767628205128\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06917272322198265, Train acc: 0.8616720085470085\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06855017515668842, Train acc: 0.8618678774928775\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06660586097010472, Train acc: 0.8632287851037851\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06595685119684945, Train acc: 0.8631810897435898\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06564696966639498, Train acc: 0.8631439933523267\n",
      "Val loss: 0.3853570520877838, Val acc: 0.902\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07530159547797635, Train acc: 0.8576388888888888\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.068906139741596, Train acc: 0.8605101495726496\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06505205098040763, Train acc: 0.8628027065527065\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06287558255796759, Train acc: 0.863815438034188\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06328110455447793, Train acc: 0.8633814102564102\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06225551889832543, Train acc: 0.8642494658119658\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.062491718047413346, Train acc: 0.8643353174603174\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06206259682265102, Train acc: 0.8643830128205128\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06061172870262849, Train acc: 0.8655330009496676\n",
      "Val loss: 0.3665387034416199, Val acc: 0.908\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06930291729095654, Train acc: 0.8592414529914529\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06068046683939094, Train acc: 0.8641159188034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06167852904042627, Train acc: 0.8638710826210826\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.061045702897075914, Train acc: 0.8635149572649573\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06091877904712644, Train acc: 0.8646634615384615\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0607249357676574, Train acc: 0.8650730056980057\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.059741247158784136, Train acc: 0.8662049755799756\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05849726217934209, Train acc: 0.8676883012820513\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05868088333015768, Train acc: 0.8677142687559354\n",
      "Val loss: 0.3815329074859619, Val acc: 0.904\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05506717394559811, Train acc: 0.8709935897435898\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05876107322864044, Train acc: 0.8675881410256411\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05725473581555902, Train acc: 0.8677439458689459\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.056614869807520486, Train acc: 0.867721688034188\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05737791565748362, Train acc: 0.866159188034188\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05691370359513155, Train acc: 0.8670762108262108\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05646341518750266, Train acc: 0.8668345543345544\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05493873290908642, Train acc: 0.8681724091880342\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.054794682085457465, Train acc: 0.8692426400759734\n",
      "Val loss: 0.3798660635948181, Val acc: 0.896\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.051452804324973345, Train acc: 0.8720619658119658\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05187341787366786, Train acc: 0.8712606837606838\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05336520101270105, Train acc: 0.8705484330484331\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.054661810270741455, Train acc: 0.8701255341880342\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.055418512148734854, Train acc: 0.869017094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05536103278313607, Train acc: 0.8686120014245015\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05584719566052941, Train acc: 0.8682272588522588\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.055840234311982095, Train acc: 0.8680555555555556\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05581803510897746, Train acc: 0.8680407169990503\n",
      "Val loss: 0.36062565445899963, Val acc: 0.908\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04758519469163357, Train acc: 0.8708600427350427\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05482495811759916, Train acc: 0.866920405982906\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05987530895787427, Train acc: 0.8638265669515669\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05851123615717276, Train acc: 0.8649172008547008\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05738252197575365, Train acc: 0.865357905982906\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.056745905439738197, Train acc: 0.8664752492877493\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05530054450617314, Train acc: 0.8670825702075702\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05491198637546637, Train acc: 0.8674712873931624\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.054618919422144224, Train acc: 0.8678478157644824\n",
      "Val loss: 0.3594631850719452, Val acc: 0.904\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05261051476511181, Train acc: 0.8693910256410257\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05160503789909884, Train acc: 0.8678552350427351\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05303146051205801, Train acc: 0.8662749287749287\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.052235213100400746, Train acc: 0.8672876602564102\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.054308748703736524, Train acc: 0.8669070512820513\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05424632216960277, Train acc: 0.8675213675213675\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05287186559448894, Train acc: 0.8689713064713065\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05313744281346981, Train acc: 0.8688067574786325\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05339327972499054, Train acc: 0.8690349002849003\n",
      "Val loss: 0.3516231179237366, Val acc: 0.912\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.052289933475673706, Train acc: 0.8689903846153846\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05406317993616446, Train acc: 0.8696581196581197\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05712371444770074, Train acc: 0.8681891025641025\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05680861196711532, Train acc: 0.8684895833333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0567815157592806, Train acc: 0.8681623931623932\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05638474149581713, Train acc: 0.8685229700854701\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05495754208174672, Train acc: 0.8690666971916972\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.053802869736384124, Train acc: 0.8698417467948718\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.054043597164090657, Train acc: 0.87034069325736\n",
      "Val loss: 0.352997362613678, Val acc: 0.914\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0562024771148323, Train acc: 0.8700587606837606\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.058132369803567216, Train acc: 0.8667868589743589\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05592548592477782, Train acc: 0.8680555555555556\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05542834491556526, Train acc: 0.8665865384615384\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05426851704589322, Train acc: 0.8669871794871795\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05456823963894803, Train acc: 0.8671875\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.054757258940092375, Train acc: 0.8676358363858364\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.054395725870998494, Train acc: 0.8682224893162394\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05478524646641296, Train acc: 0.8676400759734093\n",
      "Val loss: 0.34193798899650574, Val acc: 0.91\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.055084027540989414, Train acc: 0.8703258547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05282166778531849, Train acc: 0.8713274572649573\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05350698263217241, Train acc: 0.8710826210826211\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.053082483446496166, Train acc: 0.8704927884615384\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05281871878183805, Train acc: 0.871207264957265\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05234124781059743, Train acc: 0.8711271367521367\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.052924818026131616, Train acc: 0.8707837301587301\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.051972823997593336, Train acc: 0.8711104433760684\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.050962853041129914, Train acc: 0.8714239078822412\n",
      "Val loss: 0.3553001880645752, Val acc: 0.906\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0585848528605241, Train acc: 0.8636485042735043\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.056279885463225536, Train acc: 0.8665865384615384\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05327698571389897, Train acc: 0.8682336182336182\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.053256929302826904, Train acc: 0.8689236111111112\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05260026449831123, Train acc: 0.8699786324786325\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05238865608842964, Train acc: 0.8697916666666666\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0528662279703096, Train acc: 0.869429181929182\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05331027011076609, Train acc: 0.869140625\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0532651268235865, Train acc: 0.8691981244064577\n",
      "Val loss: 0.3498746156692505, Val acc: 0.906\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04785362981323503, Train acc: 0.874198717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05047987503373725, Train acc: 0.8704594017094017\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04953175494473884, Train acc: 0.8710826210826211\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05120203128227821, Train acc: 0.8703258547008547\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05358298313923371, Train acc: 0.8683226495726496\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.054403354809155154, Train acc: 0.8673210470085471\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05337712467080653, Train acc: 0.868379884004884\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05364080091826936, Train acc: 0.8682224893162394\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05232658175661353, Train acc: 0.8692723171889839\n",
      "Val loss: 0.352101594209671, Val acc: 0.914\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05369838206177084, Train acc: 0.8705929487179487\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05229830729146289, Train acc: 0.8719951923076923\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05069170887993272, Train acc: 0.8715722934472935\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.048410244604461215, Train acc: 0.8733306623931624\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.048456909462936924, Train acc: 0.8738247863247863\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04932422583599036, Train acc: 0.8733751780626781\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.050401766439933916, Train acc: 0.8729586385836385\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05066564386217003, Train acc: 0.8727297008547008\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.050899999858307364, Train acc: 0.872536799620133\n",
      "Val loss: 0.3600196838378906, Val acc: 0.91\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0459351761218829, Train acc: 0.8771367521367521\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04561621014379028, Train acc: 0.8742654914529915\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04842080273519554, Train acc: 0.8714832621082621\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04878665685144245, Train acc: 0.871360844017094\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.047868398264942005, Train acc: 0.872008547008547\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.049730666342623894, Train acc: 0.8708377849002849\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.050212968509275833, Train acc: 0.8705166361416361\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04961055389836303, Train acc: 0.8710603632478633\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.049613576357634095, Train acc: 0.871275522317189\n",
      "Val loss: 0.36361363530158997, Val acc: 0.904\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.053334554036458336, Train acc: 0.8668536324786325\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05289257948215191, Train acc: 0.8685229700854701\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04941125948544581, Train acc: 0.8719284188034188\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.049400486115716465, Train acc: 0.8714610042735043\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05031626576032394, Train acc: 0.8704059829059829\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05055754256044698, Train acc: 0.8706152065527065\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.050584659216895936, Train acc: 0.8710889804639804\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05095932502140347, Train acc: 0.8706096420940171\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.050900618546595486, Train acc: 0.8708155270655271\n",
      "Val loss: 0.36746951937675476, Val acc: 0.91\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05703372527391483, Train acc: 0.8659188034188035\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05248737691814064, Train acc: 0.867721688034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05264174377816355, Train acc: 0.8688568376068376\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.050941681568948634, Train acc: 0.870826655982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.052856248362451536, Train acc: 0.8693910256410257\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.052291338769798607, Train acc: 0.8699919871794872\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05147183723793216, Train acc: 0.8709363553113553\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.051443186103024036, Train acc: 0.8706931089743589\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.050954097837690844, Train acc: 0.8708600427350427\n",
      "Val loss: 0.3672282099723816, Val acc: 0.908\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.053772000930248164, Train acc: 0.8719284188034188\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.051730062080244735, Train acc: 0.8723290598290598\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0519071618715922, Train acc: 0.8714387464387464\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05018230877880357, Train acc: 0.8720953525641025\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04876729362031333, Train acc: 0.8729967948717948\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04845971217182627, Train acc: 0.8730413105413105\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04950901549377721, Train acc: 0.8726533882783882\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.0496994277350923, Train acc: 0.872512686965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05039799496432428, Train acc: 0.8720471272554606\n",
      "Val loss: 0.3545945882797241, Val acc: 0.904\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05416193451636877, Train acc: 0.8703258547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0531045428962789, Train acc: 0.8711271367521367\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.047777834186526785, Train acc: 0.874465811965812\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.045150717163187824, Train acc: 0.8755008012820513\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04540297190348307, Train acc: 0.8750534188034188\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04429518272224654, Train acc: 0.8754006410256411\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.045686267598353605, Train acc: 0.874198717948718\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04775519423887261, Train acc: 0.873046875\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.047835738372485746, Train acc: 0.8723735754985755\n",
      "Val loss: 0.3466772437095642, Val acc: 0.898\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.049330969906260826, Train acc: 0.8723290598290598\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04764819540019728, Train acc: 0.8745325854700855\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04995780903050023, Train acc: 0.8717503561253561\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0505755220213507, Train acc: 0.8712940705128205\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.049970643846397725, Train acc: 0.8717681623931623\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.050812418304617246, Train acc: 0.8706597222222222\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05050749590982011, Train acc: 0.8711271367521367\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04970404244640954, Train acc: 0.8712773771367521\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.049530391509716325, Train acc: 0.8713348765432098\n",
      "Val loss: 0.3622587025165558, Val acc: 0.906\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.051936852881032176, Train acc: 0.8712606837606838\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.051389162993838645, Train acc: 0.8695913461538461\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04962317293186133, Train acc: 0.8716613247863247\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04946535844833423, Train acc: 0.8723958333333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05041102828123631, Train acc: 0.8712606837606838\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05192640737930254, Train acc: 0.8697694088319088\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.052274146213927404, Train acc: 0.8690666971916972\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05224684084582533, Train acc: 0.8690905448717948\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.051803354609046566, Train acc: 0.8693316714150048\n",
      "Val loss: 0.35435646772384644, Val acc: 0.904\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05326921639279423, Train acc: 0.8695245726495726\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05280179180141188, Train acc: 0.8695913461538461\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.048694870920262784, Train acc: 0.8739316239316239\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0518373438817823, Train acc: 0.870292467948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05095856006328876, Train acc: 0.870940170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.051573095795435786, Train acc: 0.8708600427350427\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05138386263806596, Train acc: 0.8708791208791209\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05043547148378486, Train acc: 0.8710436698717948\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.049715180965343886, Train acc: 0.8714832621082621\n",
      "Val loss: 0.3560407757759094, Val acc: 0.92\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04599998216343741, Train acc: 0.8721955128205128\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04825148585005703, Train acc: 0.8700587606837606\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.050563804444424446, Train acc: 0.8700142450142451\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04979530785583023, Train acc: 0.870559561965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0495474097565708, Train acc: 0.871207264957265\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05080281202270095, Train acc: 0.870147792022792\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.050794631606638795, Train acc: 0.8702495421245421\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.050684179664931744, Train acc: 0.8704760950854701\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04982674577523727, Train acc: 0.8713793922127255\n",
      "Val loss: 0.3619154095649719, Val acc: 0.916\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05285323022777199, Train acc: 0.8707264957264957\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05270554774846786, Train acc: 0.8700587606837606\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04913491978604569, Train acc: 0.8721509971509972\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.050937830765023194, Train acc: 0.8711605235042735\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04963760956739768, Train acc: 0.8718482905982906\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.048451916020140685, Train acc: 0.8728855056980057\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04849601847202641, Train acc: 0.8726152319902319\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04802374630911738, Train acc: 0.8724626068376068\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04822918537788355, Train acc: 0.8721064814814815\n",
      "Val loss: 0.36929771304130554, Val acc: 0.91\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06201673216289944, Train acc: 0.8644497863247863\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.057473522348281667, Train acc: 0.8681891025641025\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05732772127855198, Train acc: 0.8669871794871795\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05593148052182972, Train acc: 0.8690237713675214\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05376132135717278, Train acc: 0.8700587606837606\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05336877903197905, Train acc: 0.869880698005698\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05253632925223373, Train acc: 0.8701159951159951\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.051223982125520706, Train acc: 0.8714443108974359\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05127296488509219, Train acc: 0.8709639126305793\n",
      "Val loss: 0.3555767238140106, Val acc: 0.912\n",
      "Early stopping at epoch 26 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 626.7444 [s]\n",
      "Combinación 2/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8951249076769903, Train acc: 0.4935897435897436\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8245747751659818, Train acc: 0.5190972222222222\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8103920664542761, Train acc: 0.5223468660968661\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7970517051652966, Train acc: 0.5287126068376068\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.7886344535737975, Train acc: 0.5324252136752137\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7806844585641497, Train acc: 0.5351228632478633\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7758142218193874, Train acc: 0.537469474969475\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.7722266585462623, Train acc: 0.5386952457264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.7669693512624485, Train acc: 0.5404499050332384\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7622703102664051, Train acc: 0.5422008547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7595393204976165, Train acc: 0.5442162004662005\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.756260932638095, Train acc: 0.5469417735042735\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7530816906694516, Train acc: 0.548939842209073\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7512019475664994, Train acc: 0.5496413308913309\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.7488984058555375, Train acc: 0.5510861823361823\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.7462019291666583, Train acc: 0.5527176816239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.7443136568043087, Train acc: 0.5536858974358975\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.7421072067839694, Train acc: 0.5545316951566952\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7407241563538522, Train acc: 0.5557242465137202\n",
      "Val loss: 0.6382557153701782, Val acc: 0.816\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6979003125785762, Train acc: 0.5833333333333334\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.7005482751589555, Train acc: 0.5769230769230769\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7056472039969898, Train acc: 0.5736289173789174\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7066218011781701, Train acc: 0.5715144230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.7058118159444923, Train acc: 0.5726495726495726\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7062435681663687, Train acc: 0.5734508547008547\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7175667735319289, Train acc: 0.5719627594627594\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.719183597777389, Train acc: 0.5712473290598291\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.7168363504826399, Train acc: 0.5712547483380817\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7143698033104595, Train acc: 0.5717147435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7156966735016216, Train acc: 0.5713869463869464\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.7147221458051619, Train acc: 0.5719150641025641\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7143774303968293, Train acc: 0.5720126561472715\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7139280559597435, Train acc: 0.5723634004884005\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.712425056583861, Train acc: 0.5729700854700854\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.7122882280148503, Train acc: 0.5724826388888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.7119395121549948, Train acc: 0.5722882101558572\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.7120810176430717, Train acc: 0.5722637701804368\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7120486692003017, Train acc: 0.5722559604138552\n",
      "Val loss: 0.6202919483184814, Val acc: 0.844\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7165253852677141, Train acc: 0.5635683760683761\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.7123628732485648, Train acc: 0.5699786324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7069231095986489, Train acc: 0.5736289173789174\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7015362816870722, Train acc: 0.5771901709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.7070787433885102, Train acc: 0.5734508547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7062255468463626, Train acc: 0.5750979344729344\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7048304941919115, Train acc: 0.5764270451770451\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.7049376869048828, Train acc: 0.5764222756410257\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.7025282335700246, Train acc: 0.5783772554605888\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7027396849332712, Train acc: 0.5777243589743589\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7034698331087434, Train acc: 0.5779671717171717\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.7025467363263127, Train acc: 0.5791488603988604\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7034679214708277, Train acc: 0.5787105522682445\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7044137419907601, Train acc: 0.5774763431013431\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.7041199470347489, Train acc: 0.5780804843304843\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.704205606244186, Train acc: 0.5777076655982906\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.704584055671505, Train acc: 0.5777714932126696\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.705049029216241, Train acc: 0.5771901709401709\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7040010418674187, Train acc: 0.5779211650922177\n",
      "Val loss: 0.5833163261413574, Val acc: 0.834\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7068107912683079, Train acc: 0.5750534188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6955640743940305, Train acc: 0.5837339743589743\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7026381823751662, Train acc: 0.5774572649572649\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6972522982674786, Train acc: 0.5812633547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.696033943667371, Train acc: 0.58125\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6955727206760662, Train acc: 0.5811965811965812\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6988430622878674, Train acc: 0.5786401098901099\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6970508849073169, Train acc: 0.5807291666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6983399626366433, Train acc: 0.5793269230769231\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.699596732065209, Train acc: 0.5774305555555556\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6993728603794303, Train acc: 0.5780157342657343\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6995505519911774, Train acc: 0.5781027421652422\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6973049577563158, Train acc: 0.5798200197238659\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6984897052666789, Train acc: 0.578926282051282\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6976604419216471, Train acc: 0.5792913105413106\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6989197134016416, Train acc: 0.5781750801282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6993254038213544, Train acc: 0.5777872046254399\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6989727446920851, Train acc: 0.5777837132003799\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6998258584584945, Train acc: 0.5769090193432299\n",
      "Val loss: 0.6339465975761414, Val acc: 0.83\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7043038759476099, Train acc: 0.5723824786324786\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.7013471523920695, Train acc: 0.5703792735042735\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6987998882929484, Train acc: 0.5771901709401709\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7010658654010195, Train acc: 0.5755876068376068\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.7014499857894376, Train acc: 0.5767094017094017\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6966017746178174, Train acc: 0.5791043447293447\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6976390685499515, Train acc: 0.5782203907203908\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6962493354311357, Train acc: 0.5787927350427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.69599434280554, Train acc: 0.5792972459639126\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.694390753038928, Train acc: 0.5798878205128205\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6941322221159472, Train acc: 0.5801524864024864\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6950581321476871, Train acc: 0.5787259615384616\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6950185240725794, Train acc: 0.5792447403024326\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6954292984126689, Train acc: 0.5792315323565324\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6966002822092116, Train acc: 0.5786858974358975\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6975151000178268, Train acc: 0.5787259615384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.697293394980807, Train acc: 0.5788084464555053\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6974570199436838, Train acc: 0.5789707977207977\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6981183153870831, Train acc: 0.5784694107062528\n",
      "Val loss: 0.6342757344245911, Val acc: 0.82\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6981458712337364, Train acc: 0.5806623931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6906963621194546, Train acc: 0.5849358974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.699666122765283, Train acc: 0.5769230769230769\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7026170333608602, Train acc: 0.5752537393162394\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6991997668376335, Train acc: 0.5768162393162393\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.697808265388861, Train acc: 0.5775017806267806\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6979044118352571, Train acc: 0.5769993894993894\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6978774004830763, Train acc: 0.5770566239316239\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6973806474453363, Train acc: 0.5781991927825261\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6968046934686155, Train acc: 0.5799412393162393\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6987590247702951, Train acc: 0.578962703962704\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6984371031366522, Train acc: 0.5785924145299145\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6987241713292186, Train acc: 0.5790803747534516\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6986125094477219, Train acc: 0.5786210317460317\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6993397494157155, Train acc: 0.5781873219373219\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6985577677464129, Train acc: 0.5783420138888888\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6978713624617633, Train acc: 0.5786984665661137\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.698479024489947, Train acc: 0.5785108024691358\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6973978325128019, Train acc: 0.5790879442195231\n",
      "Val loss: 0.6048510074615479, Val acc: 0.842\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6948463827626318, Train acc: 0.5771901709401709\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6969978313924919, Train acc: 0.5777243589743589\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6949556047929998, Train acc: 0.5808404558404558\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7013281426495976, Train acc: 0.5762553418803419\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6955905860305851, Train acc: 0.5780448717948717\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6977695341293628, Train acc: 0.5782140313390314\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6959056770190214, Train acc: 0.5806623931623932\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6972108443832805, Train acc: 0.5797943376068376\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6970918006480363, Train acc: 0.5803062678062678\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6956302893467439, Train acc: 0.581383547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6967875012243637, Train acc: 0.5803952991452992\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6970299170737253, Train acc: 0.5802617521367521\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6977791237564639, Train acc: 0.5795940170940171\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6970536193932078, Train acc: 0.5803952991452992\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6962977678347857, Train acc: 0.5814814814814815\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6960089148746597, Train acc: 0.5815972222222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6954665074640929, Train acc: 0.5819193061840121\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6961201453424021, Train acc: 0.5809740028490028\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6960700344272525, Train acc: 0.5810841205578048\n",
      "Val loss: 0.6248034834861755, Val acc: 0.836\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7004922462834252, Train acc: 0.5803952991452992\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.700912492779585, Train acc: 0.5809294871794872\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7000479666765599, Train acc: 0.5788817663817664\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6949677324702597, Train acc: 0.5813301282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6997960311734778, Train acc: 0.576602564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7007587745233819, Train acc: 0.5763888888888888\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6999811672756815, Train acc: 0.5756639194139194\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6988423375619782, Train acc: 0.5772569444444444\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6986766617182653, Train acc: 0.5777540360873694\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6975374587071248, Train acc: 0.5778044871794872\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6980104158345173, Train acc: 0.5777486402486403\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6979461769305403, Train acc: 0.5777021011396012\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6970456619939861, Train acc: 0.5787721893491125\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6968951437062832, Train acc: 0.5789453601953602\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.69659779774837, Train acc: 0.5788283475783476\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6974202077835798, Train acc: 0.5782919337606838\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6979807314559885, Train acc: 0.5773629964806435\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6984635188340688, Train acc: 0.577397910731244\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6977869970703039, Train acc: 0.5776118983355826\n",
      "Val loss: 0.6410772204399109, Val acc: 0.83\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6974490124445695, Train acc: 0.5787927350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6938797110675746, Train acc: 0.5793269230769231\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6962640299077048, Train acc: 0.5820868945868946\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.699885999927154, Train acc: 0.5793936965811965\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.7009660755467211, Train acc: 0.5793269230769231\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.701480185416689, Train acc: 0.5787037037037037\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7017020940853417, Train acc: 0.5787164224664225\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6985722090571355, Train acc: 0.5789596688034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.69768924494641, Train acc: 0.5797424026590693\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6980573766506635, Train acc: 0.5792735042735043\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6973010712153428, Train acc: 0.57869560994561\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6973597928244843, Train acc: 0.5794604700854701\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6981841198987353, Train acc: 0.5786283694937541\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6983780003947653, Train acc: 0.5795558608058609\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6986261678557111, Train acc: 0.5794337606837607\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6979936345074421, Train acc: 0.5795105502136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6973558681852677, Train acc: 0.5800182252388135\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6979973969692744, Train acc: 0.5797275641025641\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6985280404635715, Train acc: 0.5794534412955465\n",
      "Val loss: 0.6598469614982605, Val acc: 0.82\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6844582825134962, Train acc: 0.5865384615384616\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6878843459053936, Train acc: 0.5840010683760684\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.689134655769734, Train acc: 0.5833333333333334\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.687436477941835, Train acc: 0.5847355769230769\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6905699502708565, Train acc: 0.5820512820512821\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6863584079463937, Train acc: 0.5860487891737892\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6920393925519508, Train acc: 0.5832570207570208\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6908904877610695, Train acc: 0.5836338141025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6914276694923158, Train acc: 0.5831849477682811\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6943010703620748, Train acc: 0.5803952991452992\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6958728865217403, Train acc: 0.5793997668997669\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6972171242462943, Train acc: 0.5787482193732194\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6966690328621221, Train acc: 0.5787516436554898\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6974093700976308, Train acc: 0.5779342185592186\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6979532023610553, Train acc: 0.5775997150997151\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6973165016557671, Train acc: 0.5781750801282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6967156409707844, Train acc: 0.5782428355957768\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6966069603917265, Train acc: 0.5781695156695157\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6964693754790765, Train acc: 0.5783850652271705\n",
      "Val loss: 0.6267738342285156, Val acc: 0.834\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6880908697588831, Train acc: 0.5876068376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6929381130597531, Train acc: 0.5790598290598291\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6966010437392102, Train acc: 0.5797720797720798\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6957431138835402, Train acc: 0.5801282051282052\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6945121699418777, Train acc: 0.5800213675213676\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6949474362396447, Train acc: 0.5800391737891738\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6952095481138381, Train acc: 0.5793269230769231\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6951394902271593, Train acc: 0.5787593482905983\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6955367441360767, Train acc: 0.5789707977207977\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6970795080957249, Train acc: 0.5784722222222223\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6962497568954862, Train acc: 0.5792055167055167\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6955516690712029, Train acc: 0.5797053062678063\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6955702081861502, Train acc: 0.5799843852728468\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6953946798312067, Train acc: 0.5796130952380952\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6954427586831259, Train acc: 0.5795940170940171\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6946819128675593, Train acc: 0.5792100694444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6941717688674122, Train acc: 0.5798925339366516\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6941032792881117, Train acc: 0.5799353038936372\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.693326475515164, Train acc: 0.5808029689608637\n",
      "Val loss: 0.6094382405281067, Val acc: 0.818\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6866429122085245, Train acc: 0.5884081196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6851336687293827, Train acc: 0.5860042735042735\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6875434493237411, Train acc: 0.5878739316239316\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6846538626740122, Train acc: 0.5878739316239316\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6870014687888643, Train acc: 0.5850961538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6896955145050657, Train acc: 0.5849804131054132\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6919945922745017, Train acc: 0.5845543345543346\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6928189221419331, Train acc: 0.5843015491452992\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6921424932233062, Train acc: 0.584639126305793\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6936194053063026, Train acc: 0.5834935897435898\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6935568746662732, Train acc: 0.5834547397047397\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6942186394860262, Train acc: 0.5833778490028491\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6942357773023714, Train acc: 0.5830867850098619\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6950443097013839, Train acc: 0.5835813492063492\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6943125677414429, Train acc: 0.584241452991453\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6944365163898876, Train acc: 0.5835002670940171\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6937003715095836, Train acc: 0.5835375816993464\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6940251715757229, Train acc: 0.5833184947768281\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6939395520931635, Train acc: 0.5835863697705803\n",
      "Val loss: 0.6524825692176819, Val acc: 0.804\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7014054941315936, Train acc: 0.5734508547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.702546811893455, Train acc: 0.5702457264957265\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7034312377967725, Train acc: 0.5721153846153846\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7007528568307558, Train acc: 0.5765892094017094\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.700358749811466, Train acc: 0.5762820512820512\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7015434817710833, Train acc: 0.5757211538461539\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6990002790359059, Train acc: 0.5774191086691086\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6995867552856604, Train acc: 0.577590811965812\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.698874675310575, Train acc: 0.5794159544159544\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6980822198411338, Train acc: 0.5798878205128205\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6972797938160129, Train acc: 0.5804438616938616\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6960672658265825, Train acc: 0.5806846509971509\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.696370406224491, Train acc: 0.5803131163708086\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6956670060662851, Train acc: 0.5800900488400489\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6946530145457667, Train acc: 0.5806445868945869\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6960397070417037, Train acc: 0.5797275641025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6954724939100823, Train acc: 0.5800653594771242\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6944678275503664, Train acc: 0.5802172364672364\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6948114052260721, Train acc: 0.5805358749437697\n",
      "Val loss: 0.6146256327629089, Val acc: 0.828\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7074913739139198, Train acc: 0.5793269230769231\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6981145799414724, Train acc: 0.5779914529914529\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6978635665697929, Train acc: 0.5791488603988604\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6957251716755394, Train acc: 0.5811298076923077\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6946298595677074, Train acc: 0.5823717948717949\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6943176797991805, Train acc: 0.5826210826210826\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6922638035300888, Train acc: 0.5844017094017094\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.693227670641027, Train acc: 0.5834001068376068\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6925560219278816, Train acc: 0.5841346153846154\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6946175351356849, Train acc: 0.5826388888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6947462932027951, Train acc: 0.5827020202020202\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6943545928622923, Train acc: 0.5826210826210826\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6952513385806564, Train acc: 0.5814842209072978\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.695931151673034, Train acc: 0.5804143772893773\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6955138631695695, Train acc: 0.5805555555555556\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6954624831644644, Train acc: 0.580612313034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6962765500346519, Train acc: 0.5802067621920564\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6962945395470345, Train acc: 0.579786918328585\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6957982889419733, Train acc: 0.5803390688259109\n",
      "Val loss: 0.6297757625579834, Val acc: 0.84\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7069852250254053, Train acc: 0.5739850427350427\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6966536708621898, Train acc: 0.5765224358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6943159857366838, Train acc: 0.5814636752136753\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6929427716467116, Train acc: 0.5815972222222222\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6918351085267516, Train acc: 0.5808760683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6957334912145919, Train acc: 0.5795495014245015\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6931654259617075, Train acc: 0.5811965811965812\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6908901131942741, Train acc: 0.5824986645299145\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6929069762272939, Train acc: 0.5804546533713201\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.692634187396775, Train acc: 0.580982905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6923690101807675, Train acc: 0.5812451437451438\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6926121041603238, Train acc: 0.5812856125356125\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.693823783884387, Train acc: 0.5807651216305062\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.693142287787937, Train acc: 0.5815399877899878\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6954009965444222, Train acc: 0.5803774928774929\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6944459033891176, Train acc: 0.5805956196581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6936816177127229, Train acc: 0.5808823529411765\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6921071744995353, Train acc: 0.5820868945868946\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6928603442145316, Train acc: 0.581674538911381\n",
      "Val loss: 0.6692801713943481, Val acc: 0.84\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6885118369872754, Train acc: 0.5745192307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6921901114476032, Train acc: 0.5779914529914529\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.6833086530027905, Train acc: 0.5854700854700855\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.686567366696321, Train acc: 0.5828659188034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.68995570194008, Train acc: 0.5826923076923077\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6929151186576257, Train acc: 0.5813746438746439\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6941439185168717, Train acc: 0.5810057997557998\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6945928479425418, Train acc: 0.5807291666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6954177588875364, Train acc: 0.5802469135802469\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6958155031132902, Train acc: 0.5801014957264957\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6952467688823173, Train acc: 0.5806866744366744\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6946157588705718, Train acc: 0.5803507834757835\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6930308709365611, Train acc: 0.5812376725838264\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6932896014316615, Train acc: 0.5803380647130647\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6943385100092982, Train acc: 0.5793447293447294\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.694255684558143, Train acc: 0.5794103899572649\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6949589114623073, Train acc: 0.5791383861236803\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.695361831447451, Train acc: 0.5787927350427351\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6947435202278964, Train acc: 0.5788489653621233\n",
      "Val loss: 0.618888258934021, Val acc: 0.832\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6926867893108954, Train acc: 0.5705128205128205\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6901096607375349, Train acc: 0.5803952991452992\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.686506212521822, Train acc: 0.5836894586894587\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6905400688704263, Train acc: 0.5801949786324786\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6899131459048671, Train acc: 0.5811965811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6910392417017891, Train acc: 0.5803062678062678\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6906252580175178, Train acc: 0.581425518925519\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.692832242219876, Train acc: 0.5804620726495726\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6934307730152622, Train acc: 0.5798017568850902\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6914315492933631, Train acc: 0.580181623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6920849415527913, Train acc: 0.5801282051282052\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6914879905107694, Train acc: 0.5808404558404558\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6913671039735542, Train acc: 0.5806213017751479\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6925327164895368, Train acc: 0.5800709706959707\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6932273092263105, Train acc: 0.5802706552706552\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6930491819531999, Train acc: 0.5809628739316239\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6936163401531777, Train acc: 0.5806623931623932\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6940218065737093, Train acc: 0.5804843304843305\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6945684227490821, Train acc: 0.5801282051282052\n",
      "Val loss: 0.6226277947425842, Val acc: 0.838\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7058095669644511, Train acc: 0.5731837606837606\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6984472189448837, Train acc: 0.5815972222222222\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.7030394759103443, Train acc: 0.5797720797720798\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6986085686546105, Train acc: 0.5790598290598291\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6977896838106661, Train acc: 0.5798611111111112\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6942084961042785, Train acc: 0.5802172364672364\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6967661227571513, Train acc: 0.5784874847374848\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6964015812318549, Train acc: 0.5784588675213675\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6971135630510245, Train acc: 0.5778430674264008\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6957781779969859, Train acc: 0.578926282051282\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6955556229463176, Train acc: 0.5790112665112666\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6952411669951218, Train acc: 0.5787259615384616\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6946408705128251, Train acc: 0.5800049309664694\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6939161962969399, Train acc: 0.5802235958485958\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6933482115255122, Train acc: 0.5806980056980057\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6936832930829026, Train acc: 0.5805121527777778\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6924736477311021, Train acc: 0.581510809451986\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.6920025227670996, Train acc: 0.5823094729344729\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6930853485980658, Train acc: 0.5812949842555105\n",
      "Val loss: 0.612374484539032, Val acc: 0.848\n",
      "Early stopping at epoch 18 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 504.1421 [s]\n",
      "Combinación 3/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.9369669189819922, Train acc: 0.437366452991453\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.7815092774028451, Train acc: 0.5444711538461539\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.6833843221182158, Train acc: 0.6072382478632479\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.6185545595283182, Train acc: 0.6468349358974359\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.5729559090911832, Train acc: 0.6734775641025641\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.5367447550904717, Train acc: 0.6946225071225072\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.506713921497593, Train acc: 0.7118055555555556\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48441472670270336, Train acc: 0.7250600961538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4641960759533097, Train acc: 0.7371646486229819\n",
      "Val loss: 0.428227961063385, Val acc: 0.858\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.29867754239811856, Train acc: 0.8357371794871795\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.2872471897902652, Train acc: 0.8414797008547008\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.2786662350183199, Train acc: 0.8456641737891738\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2729258829902889, Train acc: 0.8485243055555556\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.2684014176711058, Train acc: 0.8502403846153846\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.2647493644020496, Train acc: 0.8521412037037037\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.26062272101531536, Train acc: 0.8541285103785103\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.2572093469560401, Train acc: 0.8562199519230769\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.2538177344986969, Train acc: 0.8581879154795822\n",
      "Val loss: 0.30206775665283203, Val acc: 0.886\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.23245392956285396, Train acc: 0.8723290598290598\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.22426151738971725, Train acc: 0.8756677350427351\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.22040602025652883, Train acc: 0.8768696581196581\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.21678496999108893, Train acc: 0.8781383547008547\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.21432065940820255, Train acc: 0.8796207264957265\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.2122201587994214, Train acc: 0.8810986467236467\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.2109247745902838, Train acc: 0.8812957875457875\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.20960123517001286, Train acc: 0.8819277510683761\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.20894284902984261, Train acc: 0.882508309591643\n",
      "Val loss: 0.2806210517883301, Val acc: 0.892\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.19367669497290227, Train acc: 0.8883547008547008\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.19046859303091326, Train acc: 0.8893563034188035\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.18966262484038318, Train acc: 0.8906695156695157\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.18829146654814735, Train acc: 0.8913928952991453\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.18816425945514287, Train acc: 0.8909722222222223\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.18776190026193604, Train acc: 0.8915375712250713\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.18673487620767074, Train acc: 0.8929143772893773\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.18604801459890655, Train acc: 0.8934628739316239\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.18563199956023116, Train acc: 0.8938894824311491\n",
      "Val loss: 0.2826089560985565, Val acc: 0.884\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.18492293026712206, Train acc: 0.8928952991452992\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.18362073565268108, Train acc: 0.8954326923076923\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.17839324578345672, Train acc: 0.8989049145299145\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.17533504362735483, Train acc: 0.8999732905982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.17282470557679477, Train acc: 0.901201923076923\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1717899513996055, Train acc: 0.9015758547008547\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1709663677546713, Train acc: 0.9020909645909646\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.17165440956974387, Train acc: 0.9019097222222222\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1709498486075646, Train acc: 0.9023474596391263\n",
      "Val loss: 0.2638229429721832, Val acc: 0.884\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1737498178696021, Train acc: 0.9022435897435898\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.16874802068003222, Train acc: 0.9047142094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.16226134841937964, Train acc: 0.9072293447293447\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.16188017796311113, Train acc: 0.9066172542735043\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.16049482100285017, Train acc: 0.9065705128205128\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1612612032079459, Train acc: 0.9066506410256411\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1606075175843396, Train acc: 0.9066124847374848\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.15971217260688034, Train acc: 0.9071681356837606\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.15870229828419963, Train acc: 0.9078377255460589\n",
      "Val loss: 0.25229737162590027, Val acc: 0.896\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.14717544589796636, Train acc: 0.9158653846153846\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1502346109248634, Train acc: 0.9129941239316239\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.14830565261535156, Train acc: 0.9137286324786325\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.14719232138341817, Train acc: 0.9147302350427351\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.14796534895132749, Train acc: 0.9143696581196581\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.14776825034550453, Train acc: 0.9147302350427351\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1473381815612389, Train acc: 0.9150259462759462\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.14630946919767776, Train acc: 0.9154313568376068\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1450574410989074, Train acc: 0.9160582858499525\n",
      "Val loss: 0.24071082472801208, Val acc: 0.898\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1249001029974375, Train acc: 0.9260149572649573\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1319889182973112, Train acc: 0.921340811965812\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13432194137590223, Train acc: 0.9205840455840456\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13586478381075412, Train acc: 0.9204393696581197\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13629694263904524, Train acc: 0.9199252136752136\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13574284707803672, Train acc: 0.9198717948717948\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.13523814402366005, Train acc: 0.9200816544566545\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1346002323154965, Train acc: 0.9203725961538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.13409991554583353, Train acc: 0.9210143637226971\n",
      "Val loss: 0.2932320237159729, Val acc: 0.896\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12169952569609015, Train acc: 0.9313568376068376\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.12389337538908689, Train acc: 0.9284188034188035\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.12301451736196153, Train acc: 0.9278400997150997\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.12302264688998206, Train acc: 0.9282852564102564\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.12426738751749707, Train acc: 0.927457264957265\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12339925816935352, Train acc: 0.9278178418803419\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.12307163605395982, Train acc: 0.9277510683760684\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12331725580570026, Train acc: 0.9274839743589743\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12350864549930732, Train acc: 0.9275878442545109\n",
      "Val loss: 0.23964285850524902, Val acc: 0.898\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12143238984112047, Train acc: 0.9282852564102564\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.12014355800218052, Train acc: 0.9280181623931624\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11915572569241212, Train acc: 0.9293091168091168\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1184748239401314, Train acc: 0.9292868589743589\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.11826642754240933, Train acc: 0.9295940170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.11711411363845876, Train acc: 0.9296652421652422\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.11653531650229106, Train acc: 0.9301167582417582\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.11532710442462793, Train acc: 0.9304720886752137\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.11484020028585269, Train acc: 0.9303923314339981\n",
      "Val loss: 0.2562607228755951, Val acc: 0.888\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09875759087566637, Train acc: 0.9384348290598291\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1051514765773064, Train acc: 0.9354300213675214\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10849736376195891, Train acc: 0.9334490740740741\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10750179398709382, Train acc: 0.9341947115384616\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.10624390454628528, Train acc: 0.9355769230769231\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.10526590524745463, Train acc: 0.9362313034188035\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10441503214384901, Train acc: 0.9364316239316239\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10458291144484383, Train acc: 0.9362479967948718\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.10463438909023236, Train acc: 0.9362535612535613\n",
      "Val loss: 0.2408732771873474, Val acc: 0.906\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.10533606859608594, Train acc: 0.9336271367521367\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10040422261525424, Train acc: 0.9374332264957265\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.102615300916199, Train acc: 0.9359864672364673\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.09906499407803401, Train acc: 0.9371327457264957\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09942556971158736, Train acc: 0.9373664529914529\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.099606081546202, Train acc: 0.9373664529914529\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.09919149000129421, Train acc: 0.9379769536019537\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.09902340836791146, Train acc: 0.9381343482905983\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09915773030671186, Train acc: 0.9379896723646723\n",
      "Val loss: 0.2879694700241089, Val acc: 0.892\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.09033376475175221, Train acc: 0.9461805555555556\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0919575804255457, Train acc: 0.9433092948717948\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.09239475866328617, Train acc: 0.9426638176638177\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.09379170386072917, Train acc: 0.9425413995726496\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09401778878055067, Train acc: 0.942815170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09341024677468161, Train acc: 0.9429086538461539\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0944664818354142, Train acc: 0.9421932234432234\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.09293207536554998, Train acc: 0.9431590544871795\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09233267748938893, Train acc: 0.9430941358024691\n",
      "Val loss: 0.2629893720149994, Val acc: 0.904\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0937625654360168, Train acc: 0.9407051282051282\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0908977307825007, Train acc: 0.9426415598290598\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08882013674390282, Train acc: 0.9439102564102564\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08695789163884445, Train acc: 0.9453792735042735\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0866059216678652, Train acc: 0.9452991452991453\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0861349756115692, Train acc: 0.9457799145299145\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0873838689937842, Train acc: 0.9448641636141636\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08802095180552484, Train acc: 0.9446447649572649\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08739050617131978, Train acc: 0.9450676638176638\n",
      "Val loss: 0.2910005748271942, Val acc: 0.882\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08794461808398239, Train acc: 0.9457799145299145\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0873473520971771, Train acc: 0.9483173076923077\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0848986869863635, Train acc: 0.9479166666666666\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0821566600830127, Train acc: 0.9488848824786325\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08227200040705183, Train acc: 0.947809829059829\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08249056535611125, Train acc: 0.9478498931623932\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08225806224841047, Train acc: 0.947821275946276\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08238621426220888, Train acc: 0.9478665865384616\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08271728618758582, Train acc: 0.9475308641975309\n",
      "Val loss: 0.28470855951309204, Val acc: 0.904\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08435248749123679, Train acc: 0.9501869658119658\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08035208058790264, Train acc: 0.9509882478632479\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07959914576803517, Train acc: 0.9512108262108262\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08055131538556172, Train acc: 0.9500534188034188\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07983634734255636, Train acc: 0.9502938034188034\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07917063095291116, Train acc: 0.9504540598290598\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07934212173370506, Train acc: 0.9499961843711844\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07915662194833033, Train acc: 0.9501034989316239\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07782132628551576, Train acc: 0.9507508309591642\n",
      "Val loss: 0.2956792414188385, Val acc: 0.894\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07084845426755074, Train acc: 0.953392094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07245821531257059, Train acc: 0.9515892094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07200870864581518, Train acc: 0.9517895299145299\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07373214072078212, Train acc: 0.9514890491452992\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07386927867037618, Train acc: 0.9514690170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07282587407193972, Train acc: 0.9522346866096866\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.07316845351595873, Train acc: 0.9523618742368742\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.07307328472439295, Train acc: 0.9527243589743589\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.07238526589963284, Train acc: 0.9531101614434948\n",
      "Val loss: 0.33367982506752014, Val acc: 0.89\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07000796223043376, Train acc: 0.9565972222222222\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07044259134011391, Train acc: 0.9562633547008547\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07004090672374791, Train acc: 0.9547720797720798\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07034895750574577, Train acc: 0.9545606303418803\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0699370416565838, Train acc: 0.9549679487179488\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06958873964782454, Train acc: 0.955016915954416\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06962941014759415, Train acc: 0.9546512515262515\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.0694864745466755, Train acc: 0.9547943376068376\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06988242447574368, Train acc: 0.9544901471984806\n",
      "Val loss: 0.3149433135986328, Val acc: 0.89\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0642236336810976, Train acc: 0.9594017094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0615571771039922, Train acc: 0.9584001068376068\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0626866015953216, Train acc: 0.9586894586894587\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06440590836235091, Train acc: 0.9569310897435898\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06581816522993593, Train acc: 0.9563301282051282\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06668262560440605, Train acc: 0.9558404558404558\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06732351807775078, Train acc: 0.955376221001221\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06714226950246555, Train acc: 0.9555288461538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06716757430451774, Train acc: 0.9554991690408358\n",
      "Val loss: 0.3024113178253174, Val acc: 0.888\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.06922828374255417, Train acc: 0.9545940170940171\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06727830737701848, Train acc: 0.9552617521367521\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.06586223131740875, Train acc: 0.9563301282051282\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06523358551228148, Train acc: 0.9565304487179487\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06414737552404404, Train acc: 0.9574252136752137\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06434205553343153, Train acc: 0.9571981837606838\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06444399887570823, Train acc: 0.9569406288156288\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06433395156238833, Train acc: 0.9569644764957265\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06368940758739101, Train acc: 0.9574578584995251\n",
      "Val loss: 0.31756892800331116, Val acc: 0.902\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05216579884290695, Train acc: 0.9631410256410257\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05867650184748519, Train acc: 0.9597355769230769\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05838890541397948, Train acc: 0.9606481481481481\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.058187647842061825, Train acc: 0.960403311965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.058715360032187566, Train acc: 0.9595619658119658\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05993435382206216, Train acc: 0.9588675213675214\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.060925263952422926, Train acc: 0.9581043956043956\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06075281123670503, Train acc: 0.9581497061965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.061106354210451456, Train acc: 0.9577991452991453\n",
      "Val loss: 0.3008018732070923, Val acc: 0.902\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05499194466914886, Train acc: 0.9639423076923077\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.053406434499809884, Train acc: 0.9630742521367521\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05452141248517566, Train acc: 0.9622507122507122\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.055802742815297894, Train acc: 0.9615718482905983\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05585580112842413, Train acc: 0.9612446581196581\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05618878828621658, Train acc: 0.9611823361823362\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.056052822888043044, Train acc: 0.9613286019536019\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.056477914444911174, Train acc: 0.9613214476495726\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05645718953350218, Train acc: 0.9614494301994302\n",
      "Val loss: 0.3333991467952728, Val acc: 0.88\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0498601937523255, Train acc: 0.9640758547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05415007647158753, Train acc: 0.9623397435897436\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.053882545406831975, Train acc: 0.9627849002849003\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0530687754926009, Train acc: 0.9629407051282052\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05312088642619614, Train acc: 0.9627938034188034\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.054643047363245247, Train acc: 0.9618945868945868\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.055153352819082936, Train acc: 0.9615193833943834\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05516443345058932, Train acc: 0.9615384615384616\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05507684541478795, Train acc: 0.9614642687559354\n",
      "Val loss: 0.3439315855503082, Val acc: 0.884\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.053215732177098594, Train acc: 0.9619391025641025\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05039283314831237, Train acc: 0.9634081196581197\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05054352415508015, Train acc: 0.9630074786324786\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04996957623551034, Train acc: 0.9634081196581197\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05096729776033988, Train acc: 0.9633012820512821\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.052522067808442645, Train acc: 0.9624510327635327\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05327846417351374, Train acc: 0.9619200244200244\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.053110252086741805, Train acc: 0.9622061965811965\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.052555234860434265, Train acc: 0.9627107075023742\n",
      "Val loss: 0.30095720291137695, Val acc: 0.896\n",
      "Early stopping at epoch 24 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 589.7741 [s]\n",
      "Combinación 4/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.48482036106606835, Train acc: 0.6784188034188035\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3577729999764353, Train acc: 0.7514022435897436\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.31151201896518044, Train acc: 0.7781339031339032\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2827574263653185, Train acc: 0.7953725961538461\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.2635840722638318, Train acc: 0.8057692307692308\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.24648818458587016, Train acc: 0.8156606125356125\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.23591124614807565, Train acc: 0.8220009157509157\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.22572548986754865, Train acc: 0.8285757211538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.21764394623941166, Train acc: 0.8330217236467237\n",
      "Val loss: 0.3332863450050354, Val acc: 0.888\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.13906536779851994, Train acc: 0.8728632478632479\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13514005934071338, Train acc: 0.8788060897435898\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13404527009382547, Train acc: 0.8792735042735043\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1341177910948411, Train acc: 0.8790397970085471\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13176244288428218, Train acc: 0.8805021367521367\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13118998264824902, Train acc: 0.8811431623931624\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.12858919863008025, Train acc: 0.8828411172161172\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12650907775148368, Train acc: 0.8840645032051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12480806209083296, Train acc: 0.8852682811016145\n",
      "Val loss: 0.3137507140636444, Val acc: 0.906\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.10650083002371666, Train acc: 0.8969017094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10657197714616091, Train acc: 0.8945646367521367\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.10289985353281016, Train acc: 0.8969017094017094\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.10407083331901804, Train acc: 0.8967681623931624\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.10412855645020803, Train acc: 0.8972756410256411\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.10210092771172184, Train acc: 0.8992610398860399\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10083260792006211, Train acc: 0.9001068376068376\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.09975990278916991, Train acc: 0.9002236912393162\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.09937409722623888, Train acc: 0.9005371557454891\n",
      "Val loss: 0.26615336537361145, Val acc: 0.918\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.08633483436881986, Train acc: 0.907051282051282\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.08283112179010342, Train acc: 0.9100560897435898\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08513099821204813, Train acc: 0.9094996438746439\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.08187213782061878, Train acc: 0.9116252670940171\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.08230851155060988, Train acc: 0.9114850427350427\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.08222877771256656, Train acc: 0.9115251068376068\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.08175363738239903, Train acc: 0.9119352869352869\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08147337317912497, Train acc: 0.9118589743589743\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08103040385733076, Train acc: 0.9118886514719848\n",
      "Val loss: 0.27356013655662537, Val acc: 0.918\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07702450874524239, Train acc: 0.9154647435897436\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.06772414384744106, Train acc: 0.9187366452991453\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07012716462129881, Train acc: 0.9183137464387464\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07032318063016631, Train acc: 0.9174679487179487\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.07069119620017517, Train acc: 0.9164262820512821\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0710666965682622, Train acc: 0.9159099002849003\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06963697206843031, Train acc: 0.9169909951159951\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06914160585301554, Train acc: 0.9169671474358975\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06854233181091682, Train acc: 0.9173492402659069\n",
      "Val loss: 0.2664733827114105, Val acc: 0.918\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05712742377550174, Train acc: 0.9296207264957265\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.055687926454931244, Train acc: 0.9282184829059829\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05824172101978563, Train acc: 0.9258368945868946\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05963854431214496, Train acc: 0.9241786858974359\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.059788545533123176, Train acc: 0.9236378205128205\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.06052776759336477, Train acc: 0.9231659544159544\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05934921063491858, Train acc: 0.9236492673992674\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05934538646067819, Train acc: 0.9237947382478633\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05959666518234459, Train acc: 0.9233885327635327\n",
      "Val loss: 0.2808081805706024, Val acc: 0.92\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.051486153887887284, Train acc: 0.9273504273504274\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04964254325271672, Train acc: 0.9292200854700855\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04962887056702562, Train acc: 0.9292646011396012\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.05081563981042968, Train acc: 0.9289196047008547\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.050449394517474704, Train acc: 0.9287927350427351\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05014125056076593, Train acc: 0.9287304131054132\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0497471247873609, Train acc: 0.9288194444444444\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.050074301978461765, Train acc: 0.9284354967948718\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.05017684531687332, Train acc: 0.9286265432098766\n",
      "Val loss: 0.2680559456348419, Val acc: 0.928\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.042459248732297845, Train acc: 0.9337606837606838\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.03898619841306637, Train acc: 0.9352964743589743\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.03950268815555464, Train acc: 0.9350516381766382\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.039383901299064994, Train acc: 0.9350293803418803\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.03875916197768643, Train acc: 0.935443376068376\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.039201043100438565, Train acc: 0.9349403490028491\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.039703091458668786, Train acc: 0.9345810439560439\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04086187827345143, Train acc: 0.9337606837606838\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04008027636653904, Train acc: 0.9345322886989553\n",
      "Val loss: 0.2979775071144104, Val acc: 0.912\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.02732431875844287, Train acc: 0.9431089743589743\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.029796341800282143, Train acc: 0.9413728632478633\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.029718595182793774, Train acc: 0.9403044871794872\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.029338203370571136, Train acc: 0.9404046474358975\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.029771012500819996, Train acc: 0.939957264957265\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.030396973220711082, Train acc: 0.9391025641025641\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.03139573903310867, Train acc: 0.938301282051282\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.03212568061983483, Train acc: 0.9382512019230769\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.03251771583572978, Train acc: 0.9379748338081672\n",
      "Val loss: 0.29312407970428467, Val acc: 0.908\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.03259910069979154, Train acc: 0.9379006410256411\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.03198278236847658, Train acc: 0.9385016025641025\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.030924691136745987, Train acc: 0.9392806267806267\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.027678374360259782, Train acc: 0.9409722222222222\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.02813979132562621, Train acc: 0.9411057692307693\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.02763170744703706, Train acc: 0.9414841524216524\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.026788190243736146, Train acc: 0.9416971916971917\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.026555130815404095, Train acc: 0.9414897168803419\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.026936395046038505, Train acc: 0.9412838319088319\n",
      "Val loss: 0.27847200632095337, Val acc: 0.924\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0200015224962153, Train acc: 0.9477831196581197\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.018596624907774802, Train acc: 0.9463808760683761\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.025255571191127483, Train acc: 0.9426638176638177\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.023141644512995694, Train acc: 0.9442775106837606\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.023573590165529495, Train acc: 0.9436698717948718\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.023804647524302502, Train acc: 0.9430422008547008\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.02310841758900364, Train acc: 0.9432234432234432\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.02276667882489343, Train acc: 0.9432091346153846\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.022789433768332173, Train acc: 0.9432425213675214\n",
      "Val loss: 0.2875484824180603, Val acc: 0.928\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.013278365135192871, Train acc: 0.9477831196581197\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.015418940120273165, Train acc: 0.9473157051282052\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.014984670739907485, Train acc: 0.948272792022792\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.016801532262410872, Train acc: 0.9466145833333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.016204777207130042, Train acc: 0.9473290598290598\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.015536260974203419, Train acc: 0.9474492521367521\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.015668340727814242, Train acc: 0.9474206349206349\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.016943796227375667, Train acc: 0.9467648237179487\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.016931798949427302, Train acc: 0.9468037749287749\n",
      "Val loss: 0.27027249336242676, Val acc: 0.934\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.009741783906251956, Train acc: 0.9503205128205128\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.010254862216802744, Train acc: 0.9499866452991453\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.011922574196106348, Train acc: 0.9493411680911681\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.011936523657069247, Train acc: 0.9493189102564102\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.011758612567542964, Train acc: 0.9493589743589743\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.011509765374694454, Train acc: 0.9497640669515669\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.012542698300365127, Train acc: 0.9486797924297924\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.012359736805670282, Train acc: 0.9488348023504274\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.012692991753815473, Train acc: 0.9488960113960114\n",
      "Val loss: 0.3121727705001831, Val acc: 0.908\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.00591704249382019, Train acc: 0.9537927350427351\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.00418929812999872, Train acc: 0.9535924145299145\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.006076286363805461, Train acc: 0.9521456552706553\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0063420129446392385, Train acc: 0.9517561431623932\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.007636966358902108, Train acc: 0.9506143162393162\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.007896100438897767, Train acc: 0.9505653490028491\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.008405180222676641, Train acc: 0.9501106532356532\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.007685274140447633, Train acc: 0.9507545405982906\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.008410854896588882, Train acc: 0.9501424501424501\n",
      "Val loss: 0.2924703061580658, Val acc: 0.916\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.015117003112776667, Train acc: 0.9465811965811965\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0073971743894438455, Train acc: 0.9517895299145299\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.008700415322243997, Train acc: 0.9507656695156695\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.006891302223134245, Train acc: 0.9523904914529915\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0062162400820316415, Train acc: 0.9525106837606837\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.005892805436737517, Train acc: 0.9525017806267806\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.00502219741597717, Train acc: 0.9527434371184371\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.004292339930294925, Train acc: 0.9529914529914529\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0047138312093892324, Train acc: 0.9525462962962963\n",
      "Val loss: 0.2881275713443756, Val acc: 0.92\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.006301163226111323, Train acc: 0.9509882478632479\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0034221126737757628, Train acc: 0.9529914529914529\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.003983487345893838, Train acc: 0.9537927350427351\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0019460944856843378, Train acc: 0.9545272435897436\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.0021625817586214116, Train acc: 0.9541666666666667\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0016526538889292638, Train acc: 0.9547275641025641\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0019666136621118904, Train acc: 0.9541552197802198\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.0018445577185887557, Train acc: 0.9543770032051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0013887438735617759, Train acc: 0.9547127255460589\n",
      "Val loss: 0.307739794254303, Val acc: 0.916\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.001279076704612145, Train acc: 0.9543269230769231\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.0010940687906028878, Train acc: 0.953926282051282\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0011281822821353575, Train acc: 0.9545495014245015\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0014846259648473854, Train acc: 0.9547275641025641\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0020233285223316944, Train acc: 0.9553685897435897\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.0014951725800832112, Train acc: 0.9554620726495726\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.002046981883281899, Train acc: 0.9557387057387058\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.0020613055039420086, Train acc: 0.9557458600427351\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.0018685511317801046, Train acc: 0.9557217473884141\n",
      "Val loss: 0.27244144678115845, Val acc: 0.924\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0067241803193703676, Train acc: 0.9588675213675214\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.0018380978423306066, Train acc: 0.9547943376068376\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0027626211996431703, Train acc: 0.9559294871794872\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0035376037733677104, Train acc: 0.9566306089743589\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.004131902155713138, Train acc: 0.9569444444444445\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.00445194330140736, Train acc: 0.9571759259259259\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.0038630801715839186, Train acc: 0.956959706959707\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.003974675097399288, Train acc: 0.9571481036324786\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.003640723902049341, Train acc: 0.9567752849002849\n",
      "Val loss: 0.3073939085006714, Val acc: 0.906\n",
      "Early stopping at epoch 18 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 438.5479 [s]\n",
      "Combinación 5/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.6366916237733303, Train acc: 0.23357371794871795\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.6053823632562262, Train acc: 0.2863247863247863\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.5796326296961206, Train acc: 0.33275462962962965\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.5528801792961919, Train acc: 0.3730969551282051\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.5261363565412343, Train acc: 0.40550213675213675\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.501793619489398, Train acc: 0.4315126424501424\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.47982356698230655, Train acc: 0.4534302503052503\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.46080555881445223, Train acc: 0.4718215811965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.44267841938667263, Train acc: 0.48921236942070273\n",
      "Val loss: 0.9009128212928772, Val acc: 0.688\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.26144516926545364, Train acc: 0.6526442307692307\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.253402740527422, Train acc: 0.6598557692307693\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.24748829576024983, Train acc: 0.6647970085470085\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2415791716840532, Train acc: 0.6700053418803419\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.23310605746049148, Train acc: 0.6760950854700855\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.2268971366929872, Train acc: 0.6799991096866097\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.22128488176180475, Train acc: 0.6844284188034188\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.21621541551545134, Train acc: 0.6878505608974359\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.21057317988258933, Train acc: 0.691625118708452\n",
      "Val loss: 0.7251712083816528, Val acc: 0.758\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1551689851997245, Train acc: 0.7330395299145299\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.15364708554031503, Train acc: 0.7328392094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.15084883782938335, Train acc: 0.7350872507122507\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.14833335298248845, Train acc: 0.7369123931623932\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.14440756649033637, Train acc: 0.7395299145299146\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1414671417994377, Train acc: 0.7418091168091168\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.13923520497495584, Train acc: 0.743646978021978\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.13601195500192478, Train acc: 0.7458600427350427\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.13309161734377217, Train acc: 0.7481303418803419\n",
      "Val loss: 0.6837340593338013, Val acc: 0.76\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.10082477280217358, Train acc: 0.7708333333333334\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.09950327363788572, Train acc: 0.7717681623931624\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.09713236524848178, Train acc: 0.7738158831908832\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.09515136760524195, Train acc: 0.7757077991452992\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.09495233969810682, Train acc: 0.7753205128205128\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.09258119214294303, Train acc: 0.7773326210826211\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.09070577122207381, Train acc: 0.7785218253968254\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.08903811782853216, Train acc: 0.7802317040598291\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.08814268472187879, Train acc: 0.7813687084520418\n",
      "Val loss: 0.6134850978851318, Val acc: 0.8\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07356635423806998, Train acc: 0.7887286324786325\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07421884704858829, Train acc: 0.7885950854700855\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.0707451478708164, Train acc: 0.7930021367521367\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.06866523992811513, Train acc: 0.7947716346153846\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06879344842372796, Train acc: 0.7942574786324786\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0664341158506877, Train acc: 0.7961850071225072\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06533527985597268, Train acc: 0.7968559218559218\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.0638649313368349, Train acc: 0.798293936965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06273649713592312, Train acc: 0.7988336894586895\n",
      "Val loss: 0.5645519495010376, Val acc: 0.81\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.04438471692240136, Train acc: 0.8143696581196581\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.043473562114259116, Train acc: 0.8114316239316239\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.04268396783758093, Train acc: 0.8124554843304843\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.04338542773173405, Train acc: 0.8123664529914529\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.04314626893426618, Train acc: 0.8130608974358975\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04286468164873259, Train acc: 0.8136128917378918\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04225646859996921, Train acc: 0.8145795177045178\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04077663654700304, Train acc: 0.8159054487179487\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.03983047652674763, Train acc: 0.8168476970560304\n",
      "Val loss: 0.5401068329811096, Val acc: 0.834\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.032824841829446644, Train acc: 0.8202457264957265\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.03148352972462646, Train acc: 0.8236511752136753\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.02982448988151007, Train acc: 0.8248753561253561\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.030563530886275135, Train acc: 0.8247863247863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.029212607685317343, Train acc: 0.8258814102564103\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.029051276268782438, Train acc: 0.8270566239316239\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.027476626656430983, Train acc: 0.8278006715506715\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.02758462268572587, Train acc: 0.827857905982906\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.027589333929114418, Train acc: 0.8276353276353277\n",
      "Val loss: 0.5324323773384094, Val acc: 0.82\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.015699040685963426, Train acc: 0.8333333333333334\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.016793513145202246, Train acc: 0.8329994658119658\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.01564298667799034, Train acc: 0.8339120370370371\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.014736390011942284, Train acc: 0.8357037927350427\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.015331185373485597, Train acc: 0.8357104700854701\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.014172468684677385, Train acc: 0.8363826566951567\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.013879915369590415, Train acc: 0.8370154151404151\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.013622179563738342, Train acc: 0.8370559561965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.01275745676680055, Train acc: 0.8372803893637227\n",
      "Val loss: 0.5007747411727905, Val acc: 0.846\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.0001689012234027569, Train acc: 0.8476228632478633\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.003333726245113927, Train acc: 0.8443509615384616\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.002908754722345249, Train acc: 0.8423700142450142\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.004959878885847891, Train acc: 0.8412793803418803\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.005444940440675132, Train acc: 0.8408653846153846\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.005688700156334119, Train acc: 0.840789707977208\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.0043738065912901115, Train acc: 0.8422428266178266\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.004015271289226336, Train acc: 0.8424979967948718\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.002610120848033503, Train acc: 0.8437648385565052\n",
      "Val loss: 0.5062985420227051, Val acc: 0.844\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0007895643894488995, Train acc: 0.8510950854700855\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.0035346313419505064, Train acc: 0.851028311965812\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0028987031037311607, Train acc: 0.8491809116809117\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.003676077366894127, Train acc: 0.8494591346153846\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0026676383792844593, Train acc: 0.8478899572649573\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.0032482483448126377, Train acc: 0.8486467236467237\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.003529581685933729, Train acc: 0.8491491147741148\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.00421890973025917, Train acc: 0.849609375\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.005540732984189634, Train acc: 0.8509170227920227\n",
      "Val loss: 0.47190436720848083, Val acc: 0.866\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0034513952385666026, Train acc: 0.8508279914529915\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.007122774664153401, Train acc: 0.8547008547008547\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.009083702693297993, Train acc: 0.8557247150997151\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.010688181998383285, Train acc: 0.8561030982905983\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.011764978853046384, Train acc: 0.8562767094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01191762633133478, Train acc: 0.8558137464387464\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.012897184261908898, Train acc: 0.8569711538461539\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.012434558926993964, Train acc: 0.8566038995726496\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.012873371272345214, Train acc: 0.857178893637227\n",
      "Val loss: 0.4787529706954956, Val acc: 0.852\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.023910591235527627, Train acc: 0.8677884615384616\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.02011224678438953, Train acc: 0.8617120726495726\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.017813749292976837, Train acc: 0.8592859686609686\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.01687847434455513, Train acc: 0.8593416132478633\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01771800905211359, Train acc: 0.8597756410256411\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01718185604297877, Train acc: 0.8592191951566952\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.01847453992154281, Train acc: 0.8602716727716728\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.018321025766368605, Train acc: 0.8607271634615384\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.018760825607285314, Train acc: 0.8609182098765432\n",
      "Val loss: 0.46916353702545166, Val acc: 0.868\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0168339824065184, Train acc: 0.8573717948717948\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.020564587579833135, Train acc: 0.8629139957264957\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.019628264370806878, Train acc: 0.8618678774928775\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.021598375633231595, Train acc: 0.8625801282051282\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.022291168697878845, Train acc: 0.8633279914529914\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.021447311694126184, Train acc: 0.8623575498575499\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.0218154634457077, Train acc: 0.8628281440781441\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.023028674225012463, Train acc: 0.8636818910256411\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.024244500021649223, Train acc: 0.8643607549857549\n",
      "Val loss: 0.4481351971626282, Val acc: 0.854\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.03236346825575217, Train acc: 0.8708600427350427\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.03133642482451904, Train acc: 0.8711939102564102\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.031602243072966225, Train acc: 0.8711271367521367\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.03072866275269761, Train acc: 0.870292467948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.02919773627550174, Train acc: 0.8692040598290598\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.029029686863605794, Train acc: 0.8694577991452992\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.029540657051025876, Train acc: 0.8696581196581197\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.029006088327648293, Train acc: 0.8693910256410257\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.028628612572198127, Train acc: 0.8694355413105413\n",
      "Val loss: 0.4446854293346405, Val acc: 0.872\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.03274865308378497, Train acc: 0.8719284188034188\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.034445017703578, Train acc: 0.8733306623931624\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.03525919690091386, Train acc: 0.8733084045584045\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.03533757051341554, Train acc: 0.8733640491452992\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.03386624389224582, Train acc: 0.8729700854700855\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.03426624473683175, Train acc: 0.8727297008547008\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.03397093718741839, Train acc: 0.8723099816849816\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.03390130884626992, Train acc: 0.8724292200854701\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.034205990158028526, Train acc: 0.8727445394112061\n",
      "Val loss: 0.4484219253063202, Val acc: 0.882\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.03777257677836296, Train acc: 0.8779380341880342\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.03507484941401033, Train acc: 0.8757345085470085\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.03692458939348531, Train acc: 0.8765135327635327\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.03706526940959132, Train acc: 0.8767027243589743\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.037541757027308144, Train acc: 0.8764690170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.03738020139711874, Train acc: 0.8763132122507122\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.037603679193827406, Train acc: 0.8758585164835165\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.03825862848987946, Train acc: 0.8763354700854701\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.038411142706078566, Train acc: 0.8763799857549858\n",
      "Val loss: 0.45864129066467285, Val acc: 0.876\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.03905556650243254, Train acc: 0.8808760683760684\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.04029143608023978, Train acc: 0.8795405982905983\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.039930999109208415, Train acc: 0.8782941595441596\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.03975153466065725, Train acc: 0.8786057692307693\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.04035221322988852, Train acc: 0.8785523504273505\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.04050434752237423, Train acc: 0.8778490028490028\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.040280905835357775, Train acc: 0.877518315018315\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.040674247866512366, Train acc: 0.8775373931623932\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.04098258282837139, Train acc: 0.8776857787274454\n",
      "Val loss: 0.4393830895423889, Val acc: 0.884\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.046803688646381736, Train acc: 0.8838141025641025\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.04621219163776463, Train acc: 0.8804086538461539\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.04294127208894474, Train acc: 0.8779825498575499\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.044290592973558314, Train acc: 0.8796073717948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.04363893413136148, Train acc: 0.8786591880341881\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.04288376320121635, Train acc: 0.8781160968660968\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.042546193205247725, Train acc: 0.8785485347985348\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.043050877590719454, Train acc: 0.8790064102564102\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.04426026743361753, Train acc: 0.8802973646723646\n",
      "Val loss: 0.4228484332561493, Val acc: 0.878\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.04907993781260955, Train acc: 0.8838141025641025\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.04395939116804009, Train acc: 0.8802083333333334\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.04618029623289733, Train acc: 0.88065349002849\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.047273209971240446, Train acc: 0.8818776709401709\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.04811987810664707, Train acc: 0.8826655982905983\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.04738049357704967, Train acc: 0.8823005698005698\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.04722607496020558, Train acc: 0.8822115384615384\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.04764958986869225, Train acc: 0.8827457264957265\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.04749086762425567, Train acc: 0.8825973409306742\n",
      "Val loss: 0.42085546255111694, Val acc: 0.882\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.04713482225043142, Train acc: 0.8842147435897436\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.04770958856639699, Train acc: 0.8853498931623932\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.04884196762685423, Train acc: 0.8847489316239316\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.049649871312654935, Train acc: 0.8852163461538461\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.05114110015396379, Train acc: 0.8868856837606838\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.050222436884189946, Train acc: 0.8855724715099715\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.05010636651181185, Train acc: 0.8849778693528694\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.05053835766565087, Train acc: 0.8849659455128205\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.05111213802498856, Train acc: 0.885238603988604\n",
      "Val loss: 0.4278515875339508, Val acc: 0.88\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.047972726006793164, Train acc: 0.8800747863247863\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.05005318461320339, Train acc: 0.8804754273504274\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0515704597333218, Train acc: 0.8818554131054132\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.05257633735990932, Train acc: 0.8834802350427351\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.054379354990445654, Train acc: 0.8856303418803418\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.05387170362336683, Train acc: 0.8853498931623932\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.05327512121899224, Train acc: 0.8846726190476191\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.05294076311919424, Train acc: 0.8851829594017094\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.05350764705697809, Train acc: 0.8854760208926875\n",
      "Val loss: 0.42709535360336304, Val acc: 0.898\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.05383965729648232, Train acc: 0.8892895299145299\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.05674353089088049, Train acc: 0.890090811965812\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.05776618968727242, Train acc: 0.8891559829059829\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0588801839412787, Train acc: 0.8901241987179487\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.05919548996493348, Train acc: 0.8902510683760684\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.057003178700083, Train acc: 0.8886885683760684\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.05695640545916062, Train acc: 0.8891369047619048\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.05750762572527951, Train acc: 0.8895566239316239\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.0577228501764571, Train acc: 0.8893043684710351\n",
      "Val loss: 0.41798508167266846, Val acc: 0.882\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.06942800018522474, Train acc: 0.8948985042735043\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.06438211001392104, Train acc: 0.8910256410256411\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.05987138362691613, Train acc: 0.8895121082621082\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.06082359949747721, Train acc: 0.8892895299145299\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.06127498827428899, Train acc: 0.8896901709401709\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.060977850035045224, Train acc: 0.8894230769230769\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.060777453057495226, Train acc: 0.8892895299145299\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.06086126628976602, Train acc: 0.8894063835470085\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.06001691861256801, Train acc: 0.8890817901234568\n",
      "Val loss: 0.4217792749404907, Val acc: 0.888\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.05952732583396455, Train acc: 0.8932959401709402\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.05962183027185945, Train acc: 0.8904914529914529\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.05976828618606611, Train acc: 0.8908475783475783\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.06099006189749791, Train acc: 0.8911925747863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.060660365809742205, Train acc: 0.8906517094017095\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.06183388990554375, Train acc: 0.8908030626780626\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.062134703509827964, Train acc: 0.8912545787545788\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.06128862452430603, Train acc: 0.8907919337606838\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.061677229653509705, Train acc: 0.891159188034188\n",
      "Val loss: 0.42963847517967224, Val acc: 0.89\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0681306763082488, Train acc: 0.9001068376068376\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.06292974006416452, Train acc: 0.8942307692307693\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.06393572891879286, Train acc: 0.8942307692307693\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0637216225393817, Train acc: 0.8929620726495726\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.06370435759552523, Train acc: 0.892681623931624\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.06446264998355822, Train acc: 0.8926504629629629\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.0638553474339519, Train acc: 0.8925328144078144\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.06422526991138092, Train acc: 0.8927784455128205\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.06442875637967363, Train acc: 0.8928952991452992\n",
      "Val loss: 0.4182393550872803, Val acc: 0.888\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.06376332070073511, Train acc: 0.8943643162393162\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.06506490121539842, Train acc: 0.8943643162393162\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.06569381818132862, Train acc: 0.8950320512820513\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.06569967807358147, Train acc: 0.8948317307692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.06631854920305758, Train acc: 0.8945245726495726\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.06592640887807917, Train acc: 0.8942085113960114\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.06698447163870629, Train acc: 0.8951655982905983\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.06673714786003797, Train acc: 0.8949819711538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.06712573789350214, Train acc: 0.8948539886039886\n",
      "Val loss: 0.42266684770584106, Val acc: 0.888\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.06567361314072569, Train acc: 0.8918269230769231\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.06982368128931421, Train acc: 0.8942975427350427\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.07051416238149007, Train acc: 0.8951655982905983\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.07052787781780602, Train acc: 0.8957331730769231\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.06999219248437474, Train acc: 0.8955662393162394\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.06995396074066815, Train acc: 0.8953436609686609\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.07030752552297963, Train acc: 0.8954899267399268\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.07074851291174562, Train acc: 0.895950186965812\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.07035764975425525, Train acc: 0.8959075261158594\n",
      "Val loss: 0.40939316153526306, Val acc: 0.884\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.07342880327477415, Train acc: 0.8978365384615384\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.07166082469316629, Train acc: 0.8969017094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.07059068251878788, Train acc: 0.8973023504273504\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.07138942812497799, Train acc: 0.8973357371794872\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.07235127926891685, Train acc: 0.8975160256410256\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.07268397187405502, Train acc: 0.8977252492877493\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.07202733581901616, Train acc: 0.8973023504273504\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.07210488649260284, Train acc: 0.8972522702991453\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.07239028528431768, Train acc: 0.8975100902184235\n",
      "Val loss: 0.40696632862091064, Val acc: 0.886\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.07606762010827024, Train acc: 0.9019764957264957\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.07623229143965958, Train acc: 0.9025106837606838\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.07555754554916991, Train acc: 0.9024661680911681\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.07413120943511653, Train acc: 0.9009081196581197\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.07398067488629594, Train acc: 0.9010683760683761\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.07418214374797637, Train acc: 0.900863603988604\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.07412311230969225, Train acc: 0.9003930097680097\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.07405062570658505, Train acc: 0.8997896634615384\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.07445213108094442, Train acc: 0.9000771604938271\n",
      "Val loss: 0.41446372866630554, Val acc: 0.894\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0768160687552558, Train acc: 0.9001068376068376\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.07786176334588955, Train acc: 0.9015758547008547\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.07727216791223597, Train acc: 0.9007300569800569\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.07801480533984992, Train acc: 0.9012753739316239\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0770695403090909, Train acc: 0.9005876068376069\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.07807459217360896, Train acc: 0.90193198005698\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.07837721516215612, Train acc: 0.9021672771672772\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.07795542258864795, Train acc: 0.9019931891025641\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.07755593379565001, Train acc: 0.9020061728395061\n",
      "Val loss: 0.42542898654937744, Val acc: 0.886\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0790536166765751, Train acc: 0.905715811965812\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.07965689018750802, Train acc: 0.9041800213675214\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.07867454675867347, Train acc: 0.9030448717948718\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.080099909988224, Train acc: 0.9036458333333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.07972395674795167, Train acc: 0.9030715811965812\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.07971342761292417, Train acc: 0.9035345441595442\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.07951960251444862, Train acc: 0.9031784188034188\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.07946481504756162, Train acc: 0.9028612446581197\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.07915767025404166, Train acc: 0.9028519705603039\n",
      "Val loss: 0.4186399579048157, Val acc: 0.894\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.07593970395560957, Train acc: 0.9015758547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.07997719459554069, Train acc: 0.9019764957264957\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.07911512739637978, Train acc: 0.9023326210826211\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.07819447227013417, Train acc: 0.9005742521367521\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.08015031850236094, Train acc: 0.9026442307692307\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.08025242371267063, Train acc: 0.9034232549857549\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.07997364271545876, Train acc: 0.9031402625152625\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.08050980078231575, Train acc: 0.9038461538461539\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.0808846165991237, Train acc: 0.9041429249762583\n",
      "Val loss: 0.4114806354045868, Val acc: 0.886\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.07870717512236701, Train acc: 0.905982905982906\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.08165705687979348, Train acc: 0.9051816239316239\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.08139821241723846, Train acc: 0.9044693732193733\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.08284495452530363, Train acc: 0.9050814636752137\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.08371775461058331, Train acc: 0.9056623931623932\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.08335946886627763, Train acc: 0.9059383903133903\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.08225869553866404, Train acc: 0.9048572954822954\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.08213415175166905, Train acc: 0.9048143696581197\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.08335382991593335, Train acc: 0.9055080721747388\n",
      "Val loss: 0.4183579981327057, Val acc: 0.898\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.08801558180751963, Train acc: 0.9079861111111112\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.08875864833338648, Train acc: 0.9085870726495726\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.08798444695622154, Train acc: 0.9076745014245015\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.08567579835653305, Train acc: 0.905982905982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.08529616100156409, Train acc: 0.905982905982906\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.08477011560714483, Train acc: 0.9053819444444444\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.08470038784001482, Train acc: 0.9058875152625152\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.08424854112995996, Train acc: 0.9052483974358975\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.0841326483181286, Train acc: 0.9053893637226971\n",
      "Val loss: 0.41437721252441406, Val acc: 0.884\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.09063737642051828, Train acc: 0.9122596153846154\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.09043396741915972, Train acc: 0.9121928418803419\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.09102429530219815, Train acc: 0.9119925213675214\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.08983604291565397, Train acc: 0.9101896367521367\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.08954204453362359, Train acc: 0.9100160256410257\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.08879469067622454, Train acc: 0.9098335113960114\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.08856680650850791, Train acc: 0.9094551282051282\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.08854863786289835, Train acc: 0.9094384348290598\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.08812360699020559, Train acc: 0.9088022317188984\n",
      "Val loss: 0.4138558506965637, Val acc: 0.884\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.09175390285304469, Train acc: 0.9099893162393162\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.08966327936221392, Train acc: 0.9092548076923077\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.08964469434528949, Train acc: 0.9082977207977208\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.08939645171929629, Train acc: 0.9076856303418803\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.08862177897722293, Train acc: 0.9072649572649573\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.08922550927030395, Train acc: 0.9082977207977208\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.08903578855324723, Train acc: 0.908234126984127\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.0893227392091201, Train acc: 0.9084368322649573\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.08903807350712964, Train acc: 0.9083422364672364\n",
      "Val loss: 0.4162333011627197, Val acc: 0.902\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.08767263502137274, Train acc: 0.9073183760683761\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.08756466248096564, Train acc: 0.9077190170940171\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.08930869200970033, Train acc: 0.9099448005698005\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.08989761954444087, Train acc: 0.9106236645299145\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.09010386920382833, Train acc: 0.9103899572649573\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.08978285430333553, Train acc: 0.9096554487179487\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.09097153017517993, Train acc: 0.9104281135531136\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.09150375683720295, Train acc: 0.9109408386752137\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.0915742996241632, Train acc: 0.9110280151946819\n",
      "Val loss: 0.412837415933609, Val acc: 0.88\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.09106918761872837, Train acc: 0.906517094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.09415113212715866, Train acc: 0.9113247863247863\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0919748103177106, Train acc: 0.9093215811965812\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.09302692554700069, Train acc: 0.9104567307692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.09389048162688557, Train acc: 0.9114049145299146\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.09375009985051604, Train acc: 0.9110576923076923\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.09340678077186566, Train acc: 0.9108096764346765\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.09378526862869915, Train acc: 0.9112913995726496\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.09366101108611706, Train acc: 0.9113099477682811\n",
      "Val loss: 0.4107900559902191, Val acc: 0.886\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.09055898051995498, Train acc: 0.9113247863247863\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.09203381785470197, Train acc: 0.9106570512820513\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.09171606307355766, Train acc: 0.9113247863247863\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.09141239256430896, Train acc: 0.9104567307692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.09256581806728982, Train acc: 0.9112713675213675\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.09317347318188757, Train acc: 0.9117254273504274\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.09362932151781625, Train acc: 0.9119543650793651\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.09347784557403663, Train acc: 0.9118255876068376\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.09347347115632589, Train acc: 0.9117847815764483\n",
      "Val loss: 0.40773993730545044, Val acc: 0.89\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.08954600645945622, Train acc: 0.905715811965812\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.08912173040911682, Train acc: 0.9082532051282052\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.09093975613259861, Train acc: 0.9100783475783476\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.09349608013772556, Train acc: 0.9117922008547008\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.09366456876453172, Train acc: 0.9118589743589743\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.09409228548874543, Train acc: 0.9127270299145299\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.09423155778784746, Train acc: 0.9126602564102564\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.09493764575857383, Train acc: 0.9134281517094017\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.09488650488491185, Train acc: 0.9130608974358975\n",
      "Val loss: 0.40880000591278076, Val acc: 0.882\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.1003686599751823, Train acc: 0.9186698717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.098466920801717, Train acc: 0.9141960470085471\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.09642943688946912, Train acc: 0.912170584045584\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.09582299630866091, Train acc: 0.9111912393162394\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.09663708520750715, Train acc: 0.912232905982906\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.09720543234946041, Train acc: 0.9130163817663818\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.09715998027205322, Train acc: 0.9135569291819292\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.09736223827697273, Train acc: 0.9139289529914529\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.09766487025807047, Train acc: 0.9139660493827161\n",
      "Val loss: 0.42217546701431274, Val acc: 0.89\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.09873750118108895, Train acc: 0.9158653846153846\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.10188554902362008, Train acc: 0.9174679487179487\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.1014890542566946, Train acc: 0.9167111823361823\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.09987381304431166, Train acc: 0.9157986111111112\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.09905686205268925, Train acc: 0.9153044871794872\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.09911736693137731, Train acc: 0.9155092592592593\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.09899297658631508, Train acc: 0.915254884004884\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.09930679775201358, Train acc: 0.9153979700854701\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.09943301926198056, Train acc: 0.9158208689458689\n",
      "Val loss: 0.4223054051399231, Val acc: 0.882\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.10497280547761509, Train acc: 0.9236111111111112\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.10416239716558374, Train acc: 0.9203392094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.10149353708636727, Train acc: 0.9176014957264957\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.10255468585807034, Train acc: 0.9182024572649573\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.10189115038284889, Train acc: 0.9177083333333333\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.10280392622506176, Train acc: 0.9183360042735043\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.10249071905755588, Train acc: 0.9180021367521367\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.10252983081671926, Train acc: 0.9181857638888888\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.1019516148005682, Train acc: 0.9176608499525166\n",
      "Val loss: 0.4057188332080841, Val acc: 0.892\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.10275582981924726, Train acc: 0.9150641025641025\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.10500112188677503, Train acc: 0.9179353632478633\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.10410864341292965, Train acc: 0.9185363247863247\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.10365934434354815, Train acc: 0.9186030982905983\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.10444291060806339, Train acc: 0.9191239316239316\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.1044950173621164, Train acc: 0.9193598646723646\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.10418263787315005, Train acc: 0.9191849816849816\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.10402653203942837, Train acc: 0.9185864049145299\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.10358170102238089, Train acc: 0.918417616334283\n",
      "Val loss: 0.42590710520744324, Val acc: 0.886\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.09879068227914664, Train acc: 0.9139957264957265\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.0998343006413207, Train acc: 0.9147970085470085\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.10091305246040692, Train acc: 0.9159989316239316\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.1013062928094823, Train acc: 0.9160657051282052\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.10172660641181164, Train acc: 0.916025641025641\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.10217091700120529, Train acc: 0.9162437678062678\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.1029529439078437, Train acc: 0.9172771672771672\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.10373139792145827, Train acc: 0.9182859241452992\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.1042030960010101, Train acc: 0.9189221272554606\n",
      "Val loss: 0.42475783824920654, Val acc: 0.888\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.10642637248732086, Train acc: 0.9172008547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.10875419457244058, Train acc: 0.9210069444444444\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.10781580549359661, Train acc: 0.9205840455840456\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.1071626379703864, Train acc: 0.9201055021367521\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.10673857336370354, Train acc: 0.9199786324786324\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.10690977103859611, Train acc: 0.9206730769230769\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.10624682892373194, Train acc: 0.9203487484737485\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.10657452155127485, Train acc: 0.9207565438034188\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.10652011066873303, Train acc: 0.9206730769230769\n",
      "Val loss: 0.39955684542655945, Val acc: 0.892\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.11034084283388577, Train acc: 0.9202724358974359\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.1097065224352046, Train acc: 0.9214743589743589\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.108377086470949, Train acc: 0.9209401709401709\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.10815278135049038, Train acc: 0.9207732371794872\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.10812148023874332, Train acc: 0.9207799145299145\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.10740364510619063, Train acc: 0.9204059829059829\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.10805325761382834, Train acc: 0.9208829365079365\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.10809744343671024, Train acc: 0.9209568643162394\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.1085946196504468, Train acc: 0.9210440408357075\n",
      "Val loss: 0.4111894369125366, Val acc: 0.878\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.11574654701428536, Train acc: 0.9264155982905983\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.11176186685378735, Train acc: 0.9218082264957265\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.1103634543907948, Train acc: 0.9212517806267806\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.11073336782109024, Train acc: 0.9215077457264957\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.11075028395041442, Train acc: 0.921875\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.10988138352873658, Train acc: 0.921607905982906\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.1108592144953899, Train acc: 0.922657203907204\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.11118918237013695, Train acc: 0.9232271634615384\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.11088182033523422, Train acc: 0.9233588556505223\n",
      "Val loss: 0.3985605835914612, Val acc: 0.886\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.10479299329284929, Train acc: 0.9184027777777778\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.10723618004057142, Train acc: 0.921073717948718\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.10992914675987005, Train acc: 0.9232549857549858\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.1104073988703581, Train acc: 0.9233106303418803\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.11058489614062839, Train acc: 0.9228365384615385\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.11134983866642682, Train acc: 0.9237891737891738\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.11216453867663102, Train acc: 0.9246794871794872\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.11227745155238697, Train acc: 0.9245960202991453\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.11206264022635146, Train acc: 0.9245756172839507\n",
      "Val loss: 0.40478789806365967, Val acc: 0.898\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.10293860848133381, Train acc: 0.9129273504273504\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.10994805179090582, Train acc: 0.9208733974358975\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.11360126079996767, Train acc: 0.9230324074074074\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.11347261045733069, Train acc: 0.9230101495726496\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.11391246028435537, Train acc: 0.9237446581196581\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.11380343631631629, Train acc: 0.9241452991452992\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.11314066076453352, Train acc: 0.9239354395604396\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.11312446463057119, Train acc: 0.9239783653846154\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.11334345896585941, Train acc: 0.9241898148148148\n",
      "Val loss: 0.4117439389228821, Val acc: 0.892\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.11347203249605294, Train acc: 0.9266826923076923\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.11447083988250831, Train acc: 0.9258814102564102\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.1140457168433741, Train acc: 0.9248130341880342\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.11478784323757531, Train acc: 0.9254139957264957\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.11434920318106301, Train acc: 0.9244925213675214\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.11560838022123375, Train acc: 0.9252359330484331\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.1158321982338315, Train acc: 0.9253281440781441\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.11591235290353115, Train acc: 0.9255809294871795\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.1158489248974484, Train acc: 0.9254214150047484\n",
      "Val loss: 0.44241881370544434, Val acc: 0.886\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.11801053468997662, Train acc: 0.9296207264957265\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.11716031404132517, Train acc: 0.9263488247863247\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.11769779710008887, Train acc: 0.9271278490028491\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.11757296049951488, Train acc: 0.9272502670940171\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.11700247648434761, Train acc: 0.9265224358974359\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.11600182211806631, Train acc: 0.9258814102564102\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.11551116197682708, Train acc: 0.9255570818070818\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.11615736534198125, Train acc: 0.9262319711538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.11623020100797343, Train acc: 0.9265639838556505\n",
      "Val loss: 0.4142303764820099, Val acc: 0.89\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.1154722950906835, Train acc: 0.9225427350427351\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.11407556391169882, Train acc: 0.9229433760683761\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.11535869976054569, Train acc: 0.9241452991452992\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.11584710801004344, Train acc: 0.9243790064102564\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.1158227367278857, Train acc: 0.9255074786324786\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.11604050434382893, Train acc: 0.9253249643874644\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.11680721618025876, Train acc: 0.9261866605616605\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.11736484180785652, Train acc: 0.9266159188034188\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.11761343388589132, Train acc: 0.9266233380816714\n",
      "Val loss: 0.4159952402114868, Val acc: 0.888\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.11854517332509033, Train acc: 0.9290865384615384\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.11917306788456745, Train acc: 0.9272168803418803\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.1189106127984843, Train acc: 0.9266381766381766\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.11810353360114953, Train acc: 0.9263488247863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.11923891754231901, Train acc: 0.927857905982906\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.11897458102118935, Train acc: 0.9274394586894587\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.11999885372189811, Train acc: 0.9279990842490843\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.11958369294292906, Train acc: 0.9276342147435898\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.11959610130825386, Train acc: 0.9277510683760684\n",
      "Val loss: 0.4199778437614441, Val acc: 0.892\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.12147452179183307, Train acc: 0.9266826923076923\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.12060965890558357, Train acc: 0.9269497863247863\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.12103373924551526, Train acc: 0.9274839743589743\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.12127530632110742, Train acc: 0.9279180021367521\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.12009621125001174, Train acc: 0.9265491452991453\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.12036372763648671, Train acc: 0.9272391381766382\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.12028122181129688, Train acc: 0.9277129120879121\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.120385939334957, Train acc: 0.9279180021367521\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.12017239512767773, Train acc: 0.9279439696106363\n",
      "Val loss: 0.4134902358055115, Val acc: 0.888\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.11467546937812088, Train acc: 0.9237446581196581\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.11944786491047622, Train acc: 0.9281517094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.12028792084452093, Train acc: 0.9294426638176638\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.12049999947731312, Train acc: 0.9294537927350427\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.12189917931189904, Train acc: 0.9303952991452992\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.12264132385070507, Train acc: 0.9308003917378918\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.12236151175621228, Train acc: 0.9301930708180708\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.1227414103017913, Train acc: 0.9300714476495726\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.12288297323646255, Train acc: 0.9301549145299145\n",
      "Val loss: 0.4164220094680786, Val acc: 0.886\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.12211915825167273, Train acc: 0.9304220085470085\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.12395831216604282, Train acc: 0.930221688034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.12517715009868655, Train acc: 0.9313123219373219\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.12412961890809557, Train acc: 0.9301883012820513\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.12465794677408333, Train acc: 0.9307425213675213\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.12461782380556449, Train acc: 0.9308226495726496\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.12426526249546707, Train acc: 0.9310515873015873\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.1238712156111868, Train acc: 0.9302550747863247\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.12381048926487494, Train acc: 0.9300807217473884\n",
      "Val loss: 0.4036661386489868, Val acc: 0.888\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.12358833327252641, Train acc: 0.9298878205128205\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.12410347252829462, Train acc: 0.9301549145299145\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.12359210483708613, Train acc: 0.9309561965811965\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.12356747241101713, Train acc: 0.9302884615384616\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.12491391112661769, Train acc: 0.9315705128205128\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.12589331056147898, Train acc: 0.932380698005698\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.12583774965033573, Train acc: 0.9319673382173382\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.12542974022336495, Train acc: 0.931907719017094\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.12553373584833352, Train acc: 0.9318316714150048\n",
      "Val loss: 0.41461676359176636, Val acc: 0.884\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.1280876650260045, Train acc: 0.9329594017094017\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.12270483667524452, Train acc: 0.9283520299145299\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.12351113201206566, Train acc: 0.9289975071225072\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.12500805356818387, Train acc: 0.9299879807692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.1249903065017146, Train acc: 0.9300747863247864\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.12446885702446994, Train acc: 0.9296652421652422\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.12441113172724425, Train acc: 0.929945054945055\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.12508760590074408, Train acc: 0.9307892628205128\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.12574347233840202, Train acc: 0.931386514719848\n",
      "Val loss: 0.4341669976711273, Val acc: 0.886\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.12361137352438055, Train acc: 0.9273504273504274\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.12256813125732617, Train acc: 0.9284855769230769\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.12391689963150568, Train acc: 0.9309116809116809\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.12447839453179613, Train acc: 0.9314236111111112\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.12533793658272832, Train acc: 0.9324252136752137\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.12573910345379105, Train acc: 0.9328035968660968\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.12659340270855196, Train acc: 0.9333028083028083\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.1272426788878237, Train acc: 0.9339610042735043\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.12685657707940706, Train acc: 0.9334193969610636\n",
      "Val loss: 0.4273470640182495, Val acc: 0.89\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.12255386766205487, Train acc: 0.9280181623931624\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.128553732083394, Train acc: 0.9326923076923077\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.12852778360035344, Train acc: 0.932647792022792\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.12864217448693055, Train acc: 0.933326655982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.12918473777607975, Train acc: 0.9337606837606838\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.12947463093490003, Train acc: 0.9339610042735043\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.12967372395471194, Train acc: 0.9341804029304029\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.12915491761687475, Train acc: 0.93359375\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.12901927175571662, Train acc: 0.9332264957264957\n",
      "Val loss: 0.4242446720600128, Val acc: 0.888\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.13162597911989588, Train acc: 0.9353632478632479\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.13115236532484364, Train acc: 0.9363648504273504\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.13070142328569354, Train acc: 0.9349626068376068\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.13078237617881888, Train acc: 0.9350293803418803\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.13070900623614973, Train acc: 0.9346153846153846\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.13087679615557363, Train acc: 0.9347845441595442\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.13019544439292507, Train acc: 0.9342757936507936\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.13068090840919405, Train acc: 0.9343449519230769\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.13079613251903457, Train acc: 0.9342800332383666\n",
      "Val loss: 0.4240145981311798, Val acc: 0.892\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.13371773917450863, Train acc: 0.9365651709401709\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.1333675246972304, Train acc: 0.936698717948718\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.13230300628901207, Train acc: 0.9356303418803419\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.1313807791751674, Train acc: 0.9351295405982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.1312650626541203, Train acc: 0.9350694444444444\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.1300268788368274, Train acc: 0.9343171296296297\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.13024936672823187, Train acc: 0.9345047313797313\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.13036464272528633, Train acc: 0.9342114049145299\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.1304646441173463, Train acc: 0.9339535849952516\n",
      "Val loss: 0.4151756465435028, Val acc: 0.886\n",
      "Early stopping at epoch 63 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 1561.4721 [s]\n",
      "Combinación 6/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5767718950907389, Train acc: 0.6907051282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5111346379177183, Train acc: 0.718215811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.48663763544837974, Train acc: 0.7315705128205128\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4686443903125249, Train acc: 0.7376469017094017\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4598951615074761, Train acc: 0.7413461538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4515800285245958, Train acc: 0.7451923076923077\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.44371980032059155, Train acc: 0.7492368742368742\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4378592423203155, Train acc: 0.7529714209401709\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4342077902437728, Train acc: 0.7553122032288699\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4313303861480493, Train acc: 0.7563568376068376\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.42847924219283745, Train acc: 0.7583041958041958\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.425432080408277, Train acc: 0.7594818376068376\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4219618024969477, Train acc: 0.7613412228796844\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.42015040486650734, Train acc: 0.7628205128205128\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4181274236607076, Train acc: 0.7639779202279202\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.4154702913589202, Train acc: 0.7650574252136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4134858108798483, Train acc: 0.7665755404725993\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4112857074988873, Train acc: 0.7678062678062678\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.40964437710825946, Train acc: 0.7683310841205578\n",
      "Val loss: 0.34112420678138733, Val acc: 0.882\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3803050220012665, Train acc: 0.7895299145299145\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.388167216291285, Train acc: 0.7828525641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.38541535171348484, Train acc: 0.7849893162393162\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.383814223174356, Train acc: 0.7827190170940171\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.38399574715867, Train acc: 0.7825320512820513\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3815798605991225, Train acc: 0.7839654558404558\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3796785679237601, Train acc: 0.7844169719169719\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.37883805574323887, Train acc: 0.784354967948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3808614363839031, Train acc: 0.7837725546058879\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3791787336142654, Train acc: 0.785042735042735\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.378113066631412, Train acc: 0.7861305361305362\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.37838945601485735, Train acc: 0.7856347934472935\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.37757665931904183, Train acc: 0.7856673241288626\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.37896889271680834, Train acc: 0.7844360500610501\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3795182701529261, Train acc: 0.7847222222222222\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.37875965125381184, Train acc: 0.7854400373931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3787059535732696, Train acc: 0.7852564102564102\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.37856435741794076, Train acc: 0.7852564102564102\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3781136846185749, Train acc: 0.7852142375168691\n",
      "Val loss: 0.3461516499519348, Val acc: 0.892\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.37875744700431824, Train acc: 0.7839209401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.37188157330975574, Train acc: 0.7897970085470085\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.37108124733141007, Train acc: 0.7907763532763533\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3742603603909668, Train acc: 0.7885950854700855\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.37384758744484337, Train acc: 0.7887286324786325\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.372521676759944, Train acc: 0.7889512108262108\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.37379893056493396, Train acc: 0.7870497557997558\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.37328346751821345, Train acc: 0.7870926816239316\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.37333384573686046, Train acc: 0.7873931623931624\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3734921445321833, Train acc: 0.7874465811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3737594698900943, Train acc: 0.7874902874902875\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.373656075893559, Train acc: 0.7876157407407407\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.37425795463963923, Train acc: 0.786345332018409\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.37473453340258206, Train acc: 0.7862484737484737\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.37445482270330444, Train acc: 0.7860754985754985\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3743915915584717, Train acc: 0.7856403579059829\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3746432976515345, Train acc: 0.7856491955756661\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.37471074699393253, Train acc: 0.7857164055080722\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3738541833141394, Train acc: 0.7864091318038686\n",
      "Val loss: 0.3316987454891205, Val acc: 0.892\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3962306677021532, Train acc: 0.7777777777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3829337891477805, Train acc: 0.7852564102564102\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.37658168770309186, Train acc: 0.7884615384615384\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.37464356798137355, Train acc: 0.7891960470085471\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.37297404176659055, Train acc: 0.7879807692307692\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3740389031647277, Train acc: 0.7880608974358975\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3727798991187297, Train acc: 0.7872786935286935\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.37139084766435826, Train acc: 0.7880942841880342\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.37282737563138674, Train acc: 0.7874228395061729\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.37072187746182467, Train acc: 0.7883547008547008\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3704203992572605, Train acc: 0.7889714452214452\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3697793518015292, Train acc: 0.7901531339031339\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.370423182590395, Train acc: 0.789447731755424\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.37054205104544924, Train acc: 0.7892818986568987\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.37067685517830046, Train acc: 0.7889423076923077\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36967710989853764, Train acc: 0.7893462873931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3700470922204824, Train acc: 0.7888229009552539\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36933597372552945, Train acc: 0.7889512108262108\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3695231367353378, Train acc: 0.7891784750337382\n",
      "Val loss: 0.368228942155838, Val acc: 0.862\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.37291623728397566, Train acc: 0.7865918803418803\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36529388132258356, Train acc: 0.7905982905982906\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36639995070604175, Train acc: 0.7875712250712251\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.36681303849969155, Train acc: 0.7903979700854701\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36526662060338205, Train acc: 0.791292735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3644736416404403, Train acc: 0.7901531339031339\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.36238548619916006, Train acc: 0.7911324786324786\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36319394177223885, Train acc: 0.7907318376068376\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36391140383306503, Train acc: 0.7904795821462488\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36417315588292914, Train acc: 0.7901976495726496\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36436415180598664, Train acc: 0.7895541958041958\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36470801764276634, Train acc: 0.7898415242165242\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3648379365622723, Train acc: 0.789612097304405\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3653967603586241, Train acc: 0.7896062271062271\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36562894037646104, Train acc: 0.7898148148148149\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3664442901937371, Train acc: 0.7898804754273504\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3660186420528657, Train acc: 0.7899226998491704\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3662968834071417, Train acc: 0.7900344254510921\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3657659391830271, Train acc: 0.790710751237067\n",
      "Val loss: 0.3187335133552551, Val acc: 0.888\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3831078510763299, Train acc: 0.7702991452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.37928977182023543, Train acc: 0.7776442307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3773293515980414, Train acc: 0.78125\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3722081827556985, Train acc: 0.7850560897435898\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3709637401705114, Train acc: 0.7878205128205128\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.37171123386957705, Train acc: 0.7873041310541311\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.372284438256379, Train acc: 0.7870115995115995\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36997258220600265, Train acc: 0.7879273504273504\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3697883285093851, Train acc: 0.7879570275403609\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36754124112363556, Train acc: 0.7897702991452992\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3668559086281103, Train acc: 0.7897241647241647\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3665951666584042, Train acc: 0.7898192663817664\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3664826590183297, Train acc: 0.7898586456278764\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36538163745396013, Train acc: 0.7901022588522588\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36549929808347653, Train acc: 0.7900284900284901\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36562394595935815, Train acc: 0.790214342948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3649021688944254, Train acc: 0.7902683509301156\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3639775350307807, Train acc: 0.7907021604938271\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36300418714628635, Train acc: 0.7908513270355375\n",
      "Val loss: 0.3387678861618042, Val acc: 0.892\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.37046322697757655, Train acc: 0.7836538461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36911511631348193, Train acc: 0.7879273504273504\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3633657305413841, Train acc: 0.7940705128205128\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3634744194519316, Train acc: 0.7938034188034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36419950025713343, Train acc: 0.7923611111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36606367488532326, Train acc: 0.7926014957264957\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.36461349024513556, Train acc: 0.7940705128205128\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36426208628357476, Train acc: 0.7934027777777778\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36284244112875613, Train acc: 0.7941595441595442\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3623993725476102, Train acc: 0.7943643162393162\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3616716605730546, Train acc: 0.7949203574203574\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.361849723771172, Train acc: 0.7945601851851852\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3621133841686857, Train acc: 0.7940499671268902\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36319227805941096, Train acc: 0.7933264652014652\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36261056206504844, Train acc: 0.7936253561253561\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3619366049225259, Train acc: 0.7942207532051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3618055172021093, Train acc: 0.7944004524886877\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36217111775763017, Train acc: 0.7939963200379867\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3624890111973304, Train acc: 0.7936769005847953\n",
      "Val loss: 0.33982783555984497, Val acc: 0.876\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3943130650326737, Train acc: 0.7777777777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3745894092015731, Train acc: 0.7837873931623932\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3710825095574061, Train acc: 0.7861467236467237\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3668916997555484, Train acc: 0.788261217948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36935382184819276, Train acc: 0.7878739316239316\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36716460752860774, Train acc: 0.7905537749287749\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3685510662703869, Train acc: 0.7893772893772893\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3691640753681079, Train acc: 0.789596688034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36756343430603333, Train acc: 0.7893518518518519\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36798402348644715, Train acc: 0.7889690170940171\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3669974347048243, Train acc: 0.7897970085470085\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36709288317720773, Train acc: 0.7894631410256411\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36625451846862606, Train acc: 0.7895710059171598\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3674907102289363, Train acc: 0.789052960927961\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3657236343095785, Train acc: 0.7897792022792023\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36510046410700703, Train acc: 0.7899305555555556\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3657981803619184, Train acc: 0.7894513574660633\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3655719125868022, Train acc: 0.7897079772079773\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3658449938650663, Train acc: 0.7892628205128205\n",
      "Val loss: 0.3455091714859009, Val acc: 0.894\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3574994973137847, Train acc: 0.7938034188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35524950603134614, Train acc: 0.7939369658119658\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3590926302124632, Train acc: 0.7916666666666666\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3602839995525841, Train acc: 0.7912660256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36112127729460725, Train acc: 0.7912393162393162\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3611830644458108, Train acc: 0.7916666666666666\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3626224236865329, Train acc: 0.7921626984126984\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3613118852178256, Train acc: 0.7930021367521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3611607720397929, Train acc: 0.7930911680911681\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3597583390963383, Train acc: 0.7939903846153846\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3599163821573487, Train acc: 0.7935120435120435\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36095891987368933, Train acc: 0.7929576210826211\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36229189614141405, Train acc: 0.7921392176199868\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3612869246538742, Train acc: 0.792506105006105\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3608447285544159, Train acc: 0.7928240740740741\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3614459107669755, Train acc: 0.7925180288461539\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3620151936138857, Train acc: 0.7922479889391654\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3628135033213628, Train acc: 0.7916815052231719\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36209172868428335, Train acc: 0.7919337606837606\n",
      "Val loss: 0.3619632422924042, Val acc: 0.888\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.34975372344000727, Train acc: 0.7946047008547008\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.354561159434991, Train acc: 0.7930021367521367\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3553721136771716, Train acc: 0.792022792022792\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.35520340158389163, Train acc: 0.7944043803418803\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3613890285930063, Train acc: 0.7905448717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3614473883327935, Train acc: 0.7908653846153846\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3599458025888937, Train acc: 0.792239010989011\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.35957379132891315, Train acc: 0.7928352029914529\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3600234277794051, Train acc: 0.7929131054131054\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3616099435294795, Train acc: 0.7921207264957265\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3616807094681791, Train acc: 0.7921280108780109\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36180315271784097, Train acc: 0.7925347222222222\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3626461653608307, Train acc: 0.7921186719263642\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3632421499170518, Train acc: 0.7916285103785103\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36313256436263736, Train acc: 0.7913461538461538\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3626143718138337, Train acc: 0.7913828792735043\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3616518738088205, Train acc: 0.792090874811463\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36110051035711227, Train acc: 0.7921860161443495\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36048387997841114, Train acc: 0.7923414304993253\n",
      "Val loss: 0.36421525478363037, Val acc: 0.874\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3721681816710366, Train acc: 0.7889957264957265\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3556046491632095, Train acc: 0.7982104700854701\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.354510013449226, Train acc: 0.7980769230769231\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3599166161356828, Train acc: 0.7955395299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35812927290924595, Train acc: 0.79508547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.35679754903513483, Train acc: 0.7953614672364673\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3553502557478545, Train acc: 0.7954059829059829\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3574905886163569, Train acc: 0.7938368055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.357864872102271, Train acc: 0.7935956790123457\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.35930628235268797, Train acc: 0.7925480769230769\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.35922925022561697, Train acc: 0.7926379176379177\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3574200153796591, Train acc: 0.7941372863247863\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.35824827755814237, Train acc: 0.7938856015779092\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.35919926557530707, Train acc: 0.7933455433455433\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36008639007042614, Train acc: 0.7929309116809117\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3605590925559911, Train acc: 0.7926014957264957\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3618337978230103, Train acc: 0.7918709150326797\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36233701244180583, Train acc: 0.7914737654320988\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3624066873292262, Train acc: 0.7918494152046783\n",
      "Val loss: 0.3450604975223541, Val acc: 0.882\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3642192751678646, Train acc: 0.7919337606837606\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.37019661985910857, Train acc: 0.7855235042735043\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36839632694198193, Train acc: 0.7877492877492878\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.36939738015843254, Train acc: 0.7865251068376068\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3678586441227514, Train acc: 0.7866987179487179\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36929070344592774, Train acc: 0.7872596153846154\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3676887886355357, Train acc: 0.7881944444444444\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36623344452590006, Train acc: 0.7888955662393162\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3669469789703574, Train acc: 0.7887286324786325\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36461252571425884, Train acc: 0.7900106837606837\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36463624608312917, Train acc: 0.7903554778554779\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36536225068390876, Train acc: 0.7898860398860399\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36595886233486213, Train acc: 0.7897970085470085\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3648954691485899, Train acc: 0.7904456654456654\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36457077541582267, Train acc: 0.790954415954416\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.362747222949297, Train acc: 0.7923677884615384\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.36345004736478154, Train acc: 0.792090874811463\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3640462353910476, Train acc: 0.7912511870845205\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36422968910210207, Train acc: 0.7912168241115609\n",
      "Val loss: 0.3530772924423218, Val acc: 0.884\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3665633348063526, Train acc: 0.7900641025641025\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35935057273023147, Train acc: 0.7926014957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3615173344258909, Train acc: 0.791488603988604\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3592866135394981, Train acc: 0.7934027777777778\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3575613578415325, Train acc: 0.7943910256410256\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36038354313016957, Train acc: 0.7942930911680912\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3574683318535487, Train acc: 0.7948336385836385\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.35884783991700053, Train acc: 0.7949719551282052\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.35916182674743513, Train acc: 0.7950201804368471\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3593715833802508, Train acc: 0.7946314102564103\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.35889555423548725, Train acc: 0.7951388888888888\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3584240432138796, Train acc: 0.7953169515669516\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3588304386610737, Train acc: 0.7947279750164365\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.35908317191114647, Train acc: 0.7945283882783882\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.35921368713562307, Train acc: 0.7944088319088319\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3600545985719714, Train acc: 0.7940037393162394\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.35934726762046404, Train acc: 0.7943061840120663\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3601759743982004, Train acc: 0.7934472934472935\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36035502241824696, Train acc: 0.7929880791722898\n",
      "Val loss: 0.36313194036483765, Val acc: 0.864\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.36256005264754987, Train acc: 0.7922008547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35166264968550104, Train acc: 0.7988782051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3553884472602453, Train acc: 0.7958511396011396\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.36194793246368057, Train acc: 0.7926014957264957\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36184904496384485, Train acc: 0.7933760683760683\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36230853369772603, Train acc: 0.7942040598290598\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3625000523305492, Train acc: 0.7945665445665445\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.35975973300126374, Train acc: 0.7960069444444444\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3617478957203379, Train acc: 0.7950498575498576\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3614339533779356, Train acc: 0.7950053418803419\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3633047919011246, Train acc: 0.7938034188034188\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3625997022813202, Train acc: 0.7937811609686609\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3613890393990423, Train acc: 0.7947074293228139\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3611892546653311, Train acc: 0.7948145604395604\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3612556168496439, Train acc: 0.7945156695156695\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36060539543883413, Train acc: 0.7948551014957265\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.36054442494043815, Train acc: 0.7947146807440925\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3598257938766072, Train acc: 0.7952427587844254\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.360233232050504, Train acc: 0.794899910031489\n",
      "Val loss: 0.3297678232192993, Val acc: 0.902\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.35351476366193885, Train acc: 0.7913995726495726\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35408079248462987, Train acc: 0.7946047008547008\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36116298124661134, Train acc: 0.7900641025641025\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3619072048518902, Train acc: 0.7912660256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.361380040874848, Train acc: 0.7915064102564102\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.35978115137740757, Train acc: 0.7931801994301995\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3568987459926815, Train acc: 0.7947954822954822\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.35700030986251485, Train acc: 0.7947716346153846\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.35678590493098056, Train acc: 0.7951092117758785\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.355819752608609, Train acc: 0.7951655982905983\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.35527131927041183, Train acc: 0.7961587024087025\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3562742456093303, Train acc: 0.7956285612535613\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.35707620292764836, Train acc: 0.7953854372123603\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3581724580336403, Train acc: 0.7946619352869353\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.35807591736995936, Train acc: 0.794818376068376\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3576383484113548, Train acc: 0.795188969017094\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.357752091145024, Train acc: 0.7949032176973353\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3571264092568998, Train acc: 0.7961033950617284\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3576058991122664, Train acc: 0.7956449617633828\n",
      "Val loss: 0.3471052050590515, Val acc: 0.894\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.37519248695964486, Train acc: 0.781784188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3686931368250113, Train acc: 0.7841880341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3692183266168306, Train acc: 0.7856125356125356\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3687536367812218, Train acc: 0.7851228632478633\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36768949332400264, Train acc: 0.7871260683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36757213100154174, Train acc: 0.7875712250712251\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3685966055107932, Train acc: 0.7864392551892552\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36705409764097285, Train acc: 0.7878939636752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3640549007417583, Train acc: 0.7901234567901234\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3637208873899574, Train acc: 0.7908119658119658\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3645007705262398, Train acc: 0.790452602952603\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36428765762607934, Train acc: 0.7905315170940171\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3621400456280398, Train acc: 0.7917077580539119\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36256569201838545, Train acc: 0.7917429792429792\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3625743274335508, Train acc: 0.7920762108262108\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36241583194997573, Train acc: 0.7921340811965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.36314647045040804, Train acc: 0.7916038210155857\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36309253405302, Train acc: 0.7917853751187085\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36235221539378115, Train acc: 0.792285200179937\n",
      "Val loss: 0.3515954613685608, Val acc: 0.886\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.359421805311472, Train acc: 0.7897970085470085\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3597203517953555, Train acc: 0.7920673076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36049370060109687, Train acc: 0.791488603988604\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.35572242469359666, Train acc: 0.7942708333333334\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35453071823486915, Train acc: 0.795352564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3534841654222575, Train acc: 0.7933582621082621\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.35618754717383777, Train acc: 0.7921626984126984\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.35599233135262615, Train acc: 0.7917668269230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3578366473218088, Train acc: 0.7911324786324786\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.35954254208466946, Train acc: 0.790758547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.35883154682438784, Train acc: 0.7919823232323232\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.35960349279251536, Train acc: 0.7915553774928775\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3604289295346544, Train acc: 0.7914406640368179\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3612508670368911, Train acc: 0.7917429792429792\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36149568765761164, Train acc: 0.7917022792022792\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3607848481490062, Train acc: 0.7922342414529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3606670417262418, Train acc: 0.7923265460030166\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36011273395500065, Train acc: 0.7925718186134852\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.35991924251729285, Train acc: 0.792805330634278\n",
      "Val loss: 0.34118375182151794, Val acc: 0.878\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.35899544742881745, Train acc: 0.7932692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36248790638314354, Train acc: 0.7918002136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3605493500276848, Train acc: 0.7921118233618234\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3623638355260731, Train acc: 0.7922008547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35971131393542655, Train acc: 0.7925747863247863\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3588726864835815, Train acc: 0.7934918091168092\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.35735028683775366, Train acc: 0.7939560439560439\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3590100497707852, Train acc: 0.7939369658119658\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3605664900989614, Train acc: 0.7927350427350427\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3620814810960721, Train acc: 0.7922809829059829\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3621956984039972, Train acc: 0.7924679487179487\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3610581550481822, Train acc: 0.7934918091168092\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36133363495172444, Train acc: 0.7931048652202498\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36104175288307505, Train acc: 0.7929449023199023\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36028825365580047, Train acc: 0.7929309116809117\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.35959900692742097, Train acc: 0.7935697115384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.35976653506013845, Train acc: 0.7933320764203117\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3601245388515994, Train acc: 0.7934324548907882\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36043502839497493, Train acc: 0.7936909581646424\n",
      "Val loss: 0.3332611620426178, Val acc: 0.896\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3488660475127717, Train acc: 0.7972756410256411\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3566085406475597, Train acc: 0.7940705128205128\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.35979207738851887, Train acc: 0.7923789173789174\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3563406901418144, Train acc: 0.7950721153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35801307019005474, Train acc: 0.7946047008547008\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.35935394731257375, Train acc: 0.7946047008547008\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3600904835485859, Train acc: 0.7945665445665445\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36120418233112395, Train acc: 0.7942374465811965\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3621285394301102, Train acc: 0.7934472934472935\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3615306330542279, Train acc: 0.7937767094017094\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.360484556984605, Train acc: 0.7940219502719502\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36029221781892995, Train acc: 0.7940037393162394\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3599018271642637, Train acc: 0.794008875739645\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3605053216419086, Train acc: 0.7933646214896215\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36064032885084124, Train acc: 0.7935185185185185\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3609187923069311, Train acc: 0.7930355235042735\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.36066079960059977, Train acc: 0.7937091503267973\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3602332036945781, Train acc: 0.7941743827160493\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3604319825922644, Train acc: 0.79384559154296\n",
      "Val loss: 0.31780046224594116, Val acc: 0.892\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.35601871428836107, Train acc: 0.7930021367521367\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3575679518473454, Train acc: 0.7905982905982906\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3606159971055821, Train acc: 0.790954415954416\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3597449609357068, Train acc: 0.7903311965811965\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3626090388522189, Train acc: 0.7877136752136752\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36315587364625385, Train acc: 0.7890847578347578\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3614112717691941, Train acc: 0.7896443833943834\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36096145489659065, Train acc: 0.7906650641025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36100917901748264, Train acc: 0.7900047483380817\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3602028654426591, Train acc: 0.7901709401709401\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3602534711661965, Train acc: 0.7901126651126651\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3607072277986935, Train acc: 0.7902866809116809\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36210382463781554, Train acc: 0.7893655489809336\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3622332541260673, Train acc: 0.7894726800976801\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36094985524473705, Train acc: 0.7901887464387465\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36151951210748434, Train acc: 0.7899806356837606\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3604800837491852, Train acc: 0.7905825791855203\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36107361874236227, Train acc: 0.7904647435897436\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3612412361480929, Train acc: 0.7902609086819613\n",
      "Val loss: 0.3327283561229706, Val acc: 0.882\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3659211951188552, Train acc: 0.7865918803418803\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35471120419410557, Train acc: 0.7915331196581197\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36050297281680965, Train acc: 0.7907763532763533\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3549590503048693, Train acc: 0.7941372863247863\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35641792979505327, Train acc: 0.7935897435897435\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.35750157046436926, Train acc: 0.7939369658119658\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3599716118003568, Train acc: 0.7920863858363858\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3588557322947388, Train acc: 0.7935697115384616\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.35808219266073654, Train acc: 0.7938330959164293\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.35959944085687656, Train acc: 0.7927617521367522\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3602919223216864, Train acc: 0.7928078865578866\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3605771004368267, Train acc: 0.7924902065527065\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3604304356687873, Train acc: 0.7926323142669297\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3609521912003757, Train acc: 0.7920673076923077\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36044630306398767, Train acc: 0.7925391737891738\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36035027416247845, Train acc: 0.7930188301282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3599452468739376, Train acc: 0.7931278280542986\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36113556408644404, Train acc: 0.7923492402659069\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36004616829574615, Train acc: 0.7930021367521367\n",
      "Val loss: 0.3285607695579529, Val acc: 0.898\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.372673170052023, Train acc: 0.7863247863247863\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36307600904733706, Train acc: 0.7901976495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3605949348873562, Train acc: 0.792289886039886\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.35930915453877205, Train acc: 0.7940705128205128\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36262447683729676, Train acc: 0.7923611111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36343920383697903, Train acc: 0.7907318376068376\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.36201108539060795, Train acc: 0.7925442612942613\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36279487777023744, Train acc: 0.7916666666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36376972680474506, Train acc: 0.7905982905982906\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36418993685744766, Train acc: 0.7909722222222222\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36345269085829257, Train acc: 0.7916181041181041\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3639634229624883, Train acc: 0.7915998931623932\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36334409780756255, Train acc: 0.7917693951347797\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36343743770368514, Train acc: 0.79183836996337\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3631749910931302, Train acc: 0.7914351851851852\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3622560470691349, Train acc: 0.7925848023504274\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3616634088733677, Train acc: 0.7929235796882855\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3619070446290295, Train acc: 0.7931060066476733\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36118823600228284, Train acc: 0.7937331309041835\n",
      "Val loss: 0.3482765853404999, Val acc: 0.886\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.35178780542989063, Train acc: 0.7970085470085471\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3542667138907645, Train acc: 0.7908653846153846\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.35599879788876937, Train acc: 0.7922008547008547\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3589415633652964, Train acc: 0.7907318376068376\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35838046555335706, Train acc: 0.7919337606837606\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3581439095132711, Train acc: 0.7915776353276354\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3562957310414576, Train acc: 0.7922771672771672\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3579730704649646, Train acc: 0.7919337606837606\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.35981505553097465, Train acc: 0.7916963437796771\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.35902409966175375, Train acc: 0.7915331196581197\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3591072646887986, Train acc: 0.7918123543123543\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.359355646925859, Train acc: 0.7918892450142451\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3591930770133374, Train acc: 0.7924268573307035\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3596942678039327, Train acc: 0.7915140415140415\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36030449926683367, Train acc: 0.7915776353276354\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3597986761910411, Train acc: 0.7918836805555556\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.35972381968663647, Train acc: 0.7920123177476118\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3598518198513464, Train acc: 0.7919634377967711\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3603849498889385, Train acc: 0.7916666666666666\n",
      "Val loss: 0.35162556171417236, Val acc: 0.892\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3496871781654847, Train acc: 0.7988782051282052\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35351538403421384, Train acc: 0.7959401709401709\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.35884682664334605, Train acc: 0.7954059829059829\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3551275303794278, Train acc: 0.7987446581196581\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3586093679961995, Train acc: 0.7970085470085471\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3585623884387845, Train acc: 0.7953614672364673\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3552366409982954, Train acc: 0.7973137973137974\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.35621298734958357, Train acc: 0.7965077457264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.35453388642551553, Train acc: 0.7979878917378918\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3564089683003915, Train acc: 0.7968215811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3554233849951715, Train acc: 0.7971299533799534\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3563876801470236, Train acc: 0.7965188746438746\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3576200620701719, Train acc: 0.7954059829059829\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.35711174974074733, Train acc: 0.7957493894993894\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.35724103251243927, Train acc: 0.7954237891737892\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3572883550953279, Train acc: 0.7956229967948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3575299295005396, Train acc: 0.7955002513826043\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.35839504283717555, Train acc: 0.7953317901234568\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.35783602779934764, Train acc: 0.7954200404858299\n",
      "Val loss: 0.3596241772174835, Val acc: 0.884\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3772883799850431, Train acc: 0.7911324786324786\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35678364355594683, Train acc: 0.8008814102564102\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36234679143143517, Train acc: 0.7963853276353277\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.36729317352685154, Train acc: 0.7934695512820513\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3656312144974358, Train acc: 0.7933226495726495\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36454788805582583, Train acc: 0.7933137464387464\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.36397554407917393, Train acc: 0.7931166056166056\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3636864587887485, Train acc: 0.7929019764957265\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3636379768932194, Train acc: 0.7928537511870846\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36275960078351516, Train acc: 0.7933760683760683\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36515117836674704, Train acc: 0.7919337606837606\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3629106297197505, Train acc: 0.7929353632478633\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.362879082609524, Train acc: 0.7933719592373438\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3637426158411017, Train acc: 0.7931738400488401\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36329533361811245, Train acc: 0.7929487179487179\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3635787792089913, Train acc: 0.7928685897435898\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.36393817402928125, Train acc: 0.7922794117647058\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3641985197536513, Train acc: 0.7920079534662868\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36429038973633254, Train acc: 0.7915260908681961\n",
      "Val loss: 0.3404707610607147, Val acc: 0.882\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.35773692935959905, Train acc: 0.7927350427350427\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36276471799510157, Train acc: 0.7935363247863247\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36361030274816386, Train acc: 0.791755698005698\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.35913013326179266, Train acc: 0.7937366452991453\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35994793018724164, Train acc: 0.7934829059829059\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3631965710948675, Train acc: 0.7917111823361823\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3629624385645975, Train acc: 0.7910180097680097\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36338935415141094, Train acc: 0.7908319978632479\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3645291296731599, Train acc: 0.7898860398860399\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3635576339868399, Train acc: 0.790090811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3631918390151967, Train acc: 0.7900641025641025\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.362454055593564, Train acc: 0.7901086182336182\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3612126036743676, Train acc: 0.7910502958579881\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.35994423029805567, Train acc: 0.7920673076923077\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3594793367165106, Train acc: 0.7924145299145299\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3603177572815464, Train acc: 0.7922676282051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3611488129743204, Train acc: 0.7914781297134238\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36032773719186006, Train acc: 0.7921415004748338\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3605542347552376, Train acc: 0.7923695456590193\n",
      "Val loss: 0.32469359040260315, Val acc: 0.896\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.370023334510306, Train acc: 0.7938034188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.37084005461034614, Train acc: 0.7876602564102564\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3704583639772529, Train acc: 0.7880163817663818\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.36605312780309945, Train acc: 0.7893963675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.364213041566376, Train acc: 0.7910790598290598\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.363297693771005, Train acc: 0.7908208689458689\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3630013872160871, Train acc: 0.7909798534798534\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36131815832012737, Train acc: 0.7926014957264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36478278970616496, Train acc: 0.7905389363722697\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3649017238336751, Train acc: 0.7896901709401709\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3635460099302342, Train acc: 0.7910353535353535\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36388237956921937, Train acc: 0.7908431267806267\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36320657872354256, Train acc: 0.7914201183431953\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3633553399016423, Train acc: 0.7913423382173382\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36248375916073466, Train acc: 0.7918625356125356\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36340854983203685, Train acc: 0.7915998931623932\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.362150882081868, Train acc: 0.7923893916540975\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36196518940576916, Train acc: 0.7925421415004749\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36185624956199836, Train acc: 0.7922430274403959\n",
      "Val loss: 0.3469785749912262, Val acc: 0.88\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3652299655298901, Train acc: 0.7868589743589743\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3596427925886252, Train acc: 0.7919337606837606\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.35805680150659674, Train acc: 0.7904202279202279\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.36266306698577017, Train acc: 0.7889957264957265\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.36152894412859893, Train acc: 0.7895833333333333\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3628450583486136, Train acc: 0.7901531339031339\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3604653720199261, Train acc: 0.7902930402930403\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.36061510984968936, Train acc: 0.7903311965811965\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36059740995183404, Train acc: 0.7907763532763533\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36112860253988166, Train acc: 0.7911324786324786\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36235450080826753, Train acc: 0.7905254467754468\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36431434396070633, Train acc: 0.7898860398860399\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36370934485252715, Train acc: 0.7902490138067061\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3637242425910865, Train acc: 0.7901785714285714\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36293000187459495, Train acc: 0.7906339031339031\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3619771251200229, Train acc: 0.7914496527777778\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.36162005274244263, Train acc: 0.791823780794369\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3620620952018759, Train acc: 0.792022792022792\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36152417591118996, Train acc: 0.7921446243814665\n",
      "Val loss: 0.3703486919403076, Val acc: 0.87\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3601260659022209, Train acc: 0.7972756410256411\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3604249093267653, Train acc: 0.7946047008547008\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36408925472501336, Train acc: 0.792022792022792\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3627147858277855, Train acc: 0.7930021367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3612677813340456, Train acc: 0.7930021367521367\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.35936320505165986, Train acc: 0.7937143874643875\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.36003793544967244, Train acc: 0.7929639804639804\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3580938564279141, Train acc: 0.7936364850427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.35563200773053927, Train acc: 0.7943079297245964\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3567674306722788, Train acc: 0.7943376068376068\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3567412558598581, Train acc: 0.7941919191919192\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3572941268780972, Train acc: 0.7939369658119658\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.35837060882220684, Train acc: 0.7931870479947403\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3599827717512082, Train acc: 0.7923344017094017\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.35970755891073125, Train acc: 0.7923076923076923\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3597208177830037, Train acc: 0.7925347222222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3592903082502014, Train acc: 0.7932063851181498\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3591614351002013, Train acc: 0.7932395536562203\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3590539157109061, Train acc: 0.7933957489878543\n",
      "Val loss: 0.320330411195755, Val acc: 0.892\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3729766223929886, Train acc: 0.7836538461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3683364200923178, Train acc: 0.7905982905982906\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3630195079196213, Train acc: 0.7915776353276354\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3558934420410894, Train acc: 0.7972088675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3621693507473693, Train acc: 0.7941773504273504\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3591121265776137, Train acc: 0.7962962962962963\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3601075176135961, Train acc: 0.7960164835164835\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3588560132516755, Train acc: 0.7959067841880342\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3586235096057256, Train acc: 0.7959104938271605\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3595243634449111, Train acc: 0.7957532051282051\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.360564020662782, Train acc: 0.7946289821289821\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3598875922397671, Train acc: 0.7952724358974359\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.362082556732621, Train acc: 0.7942348783694938\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36317040896786873, Train acc: 0.7935554029304029\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36384275419759615, Train acc: 0.7931267806267807\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3637236058950806, Train acc: 0.7928685897435898\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3634224151953673, Train acc: 0.792955002513826\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3619717132133862, Train acc: 0.7935808404558404\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36198489729263095, Train acc: 0.7933114035087719\n",
      "Val loss: 0.35845357179641724, Val acc: 0.878\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.34277102312980556, Train acc: 0.8050213675213675\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3449959754307046, Train acc: 0.8032852564102564\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3503169541769897, Train acc: 0.7977207977207977\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3476998858854302, Train acc: 0.8014155982905983\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.34893789097794103, Train acc: 0.8003739316239317\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.34962127879898774, Train acc: 0.7986556267806267\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3507964690926602, Train acc: 0.7985729548229549\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.352333632703775, Train acc: 0.7980101495726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3529480240198961, Train acc: 0.7973943494776828\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.35604477071354534, Train acc: 0.7954594017094017\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.355931621007245, Train acc: 0.795770202020202\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.35495762478167514, Train acc: 0.7964521011396012\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.35557770468466066, Train acc: 0.7958990795529257\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3545914857997417, Train acc: 0.7962072649572649\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3558234493532072, Train acc: 0.795548433048433\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3561516416967552, Train acc: 0.7959067841880342\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3569688487061908, Train acc: 0.7952331573655103\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3577239800594811, Train acc: 0.7949163105413105\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.35819433049688026, Train acc: 0.794632816014395\n",
      "Val loss: 0.3630005419254303, Val acc: 0.892\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3571072551939223, Train acc: 0.7905982905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36468618859847385, Train acc: 0.7918002136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.36256162894417415, Train acc: 0.7932692307692307\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3641978358674763, Train acc: 0.7922008547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3652218114107083, Train acc: 0.7909188034188034\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3629284807801926, Train acc: 0.7923344017094017\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3634465162315939, Train acc: 0.7924297924297924\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3632257250416228, Train acc: 0.7920673076923077\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.362113070142688, Train acc: 0.7919337606837606\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3629342115078217, Train acc: 0.792094017094017\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36357783964417567, Train acc: 0.7911324786324786\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36191037731335374, Train acc: 0.792022792022792\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36099387950249834, Train acc: 0.7919748520710059\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36189509870004594, Train acc: 0.7917811355311355\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.36232335307659247, Train acc: 0.7914529914529914\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3617806727560157, Train acc: 0.7916666666666666\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3610657628320341, Train acc: 0.7915095525389643\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.361163999563382, Train acc: 0.7911176400759734\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36080998932382197, Train acc: 0.7913855150697255\n",
      "Val loss: 0.3440464735031128, Val acc: 0.874\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.35877111605089956, Train acc: 0.7940705128205128\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35670189298370963, Train acc: 0.7950053418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3546075480870711, Train acc: 0.7945156695156695\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3534527962597517, Train acc: 0.7948050213675214\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35738718909585576, Train acc: 0.7944978632478632\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3582940988263853, Train acc: 0.7943821225071225\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3589165002580673, Train acc: 0.7941849816849816\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.35862014848643387, Train acc: 0.7946047008547008\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3597298751020024, Train acc: 0.7935956790123457\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3592409423401213, Train acc: 0.7938568376068376\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36064312399221005, Train acc: 0.7926621989121989\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.35955066604279723, Train acc: 0.7932247150997151\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3581724487230387, Train acc: 0.7942143326758712\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.35679916266047185, Train acc: 0.7943376068376068\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3573760074088376, Train acc: 0.794480056980057\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3585166260122489, Train acc: 0.7938368055555556\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.35913931323302456, Train acc: 0.793552036199095\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3581723350773283, Train acc: 0.7943079297245964\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.35942790435741895, Train acc: 0.7937050157444895\n",
      "Val loss: 0.33146047592163086, Val acc: 0.894\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3509620024353011, Train acc: 0.7922008547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.353358011215161, Train acc: 0.7912660256410257\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.35997633283634134, Train acc: 0.7897079772079773\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.35710723685403156, Train acc: 0.789863782051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35472152640676907, Train acc: 0.7926816239316239\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3557985845284584, Train acc: 0.7913995726495726\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3588519704793108, Train acc: 0.789072039072039\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3571780826100427, Train acc: 0.7896968482905983\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.35715861503894514, Train acc: 0.7897970085470085\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.35553742805097854, Train acc: 0.7918002136752137\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3570445645962376, Train acc: 0.7914481351981352\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.35772622712062635, Train acc: 0.7914440883190883\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.35754180499239235, Train acc: 0.7926117685733071\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.35749468085457264, Train acc: 0.7929449023199023\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.35894154727119326, Train acc: 0.7918447293447294\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.35927775834137815, Train acc: 0.7920005341880342\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.36048519056217343, Train acc: 0.7914624183006536\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3601993444992153, Train acc: 0.7913995726495726\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.36012698140194754, Train acc: 0.7915823211875843\n",
      "Val loss: 0.33830851316452026, Val acc: 0.886\n",
      "Early stopping at epoch 34 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 951.6043 [s]\n",
      "Combinación 7/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.14711862496840647, Train acc: 0.7542735042735043\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.10376525663921976, Train acc: 0.7746394230769231\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.08274763499909317, Train acc: 0.7885950854700855\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.07396645563789922, Train acc: 0.7925347222222222\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.06443568050351917, Train acc: 0.7991720085470085\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.05808999293889755, Train acc: 0.8032184829059829\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.05428193143872551, Train acc: 0.8051930708180708\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.05030628296936679, Train acc: 0.8075754540598291\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04631879433267816, Train acc: 0.8098587369420702\n",
      "Val loss: 0.4892391860485077, Val acc: 0.898\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.009873850223345634, Train acc: 0.8331997863247863\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.008969711951720409, Train acc: 0.8328659188034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.009265906280941434, Train acc: 0.8301727207977208\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.010280442161437793, Train acc: 0.8288261217948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.009855662146185199, Train acc: 0.8284989316239316\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.0076229695411149595, Train acc: 0.8304398148148148\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.006696094960083455, Train acc: 0.8304716117216118\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.00536072757254299, Train acc: 0.8312633547008547\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.0036703906281155174, Train acc: 0.8325023741690408\n",
      "Val loss: 0.4418601393699646, Val acc: 0.906\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.006581960325567131, Train acc: 0.8380074786324786\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.007724832393165328, Train acc: 0.8378739316239316\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.006115781700509226, Train acc: 0.8369391025641025\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0051492459626279324, Train acc: 0.8373063568376068\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.004724229451937553, Train acc: 0.8369925213675213\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.004866295348205458, Train acc: 0.8370949074074074\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.004687830969527527, Train acc: 0.8371871184371185\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.004649938394625981, Train acc: 0.8368890224358975\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.004794164345814631, Train acc: 0.837369420702754\n",
      "Val loss: 0.4242742657661438, Val acc: 0.898\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.004466995214804625, Train acc: 0.8329326923076923\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.004829366492409991, Train acc: 0.8336004273504274\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.007389849747008408, Train acc: 0.8372061965811965\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.00774465857917427, Train acc: 0.8365050747863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.008153113901105702, Train acc: 0.8377403846153846\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.008260997993993623, Train acc: 0.8380297364672364\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.00839141116765247, Train acc: 0.8383699633699634\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.008285005887349447, Train acc: 0.8375400641025641\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.008358711389168489, Train acc: 0.8372803893637227\n",
      "Val loss: 0.44781047105789185, Val acc: 0.9\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.011785461861863095, Train acc: 0.8401442307692307\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01022838374488374, Train acc: 0.8390090811965812\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.012191245528707477, Train acc: 0.8397881054131054\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.011151088035514211, Train acc: 0.8393763354700855\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.011825991339153713, Train acc: 0.8396100427350427\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.011110676956652236, Train acc: 0.8391426282051282\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.011258835284555643, Train acc: 0.8398580586080586\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.011477109999992909, Train acc: 0.8397936698717948\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.012312540952523097, Train acc: 0.8409603513770181\n",
      "Val loss: 0.44030267000198364, Val acc: 0.902\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0030684476224785177, Train acc: 0.8318643162393162\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.005559180918921772, Train acc: 0.8350026709401709\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.005991852606124008, Train acc: 0.8364939458689459\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.007836845249701768, Train acc: 0.8372061965811965\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.008971092130383874, Train acc: 0.8374198717948718\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.00900242403361872, Train acc: 0.8366052350427351\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.010029630928831369, Train acc: 0.8372061965811965\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.011290176015379082, Train acc: 0.8386752136752137\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01102444274812682, Train acc: 0.838170702754036\n",
      "Val loss: 0.44899964332580566, Val acc: 0.9\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.011467314683474027, Train acc: 0.8400106837606838\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01400012287319216, Train acc: 0.8402110042735043\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.013226890920573829, Train acc: 0.8386752136752137\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.012968315209588435, Train acc: 0.8388755341880342\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.014334596324170756, Train acc: 0.8398237179487179\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01333579425288741, Train acc: 0.8392761752136753\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.013458767071166172, Train acc: 0.8396291208791209\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01378118870859472, Train acc: 0.8398270566239316\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01430289895899049, Train acc: 0.8401739078822412\n",
      "Val loss: 0.4157089591026306, Val acc: 0.904\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.015546786988902295, Train acc: 0.8404113247863247\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.018400559950078655, Train acc: 0.8436832264957265\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01577056362757995, Train acc: 0.8428151709401709\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.014793013978717674, Train acc: 0.8427150106837606\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.013473962667660835, Train acc: 0.841693376068376\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.013611809438110417, Train acc: 0.8414574430199431\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.01423538571748978, Train acc: 0.8416132478632479\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.015595666300027799, Train acc: 0.8426983173076923\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01475941115974361, Train acc: 0.8415093779677113\n",
      "Val loss: 0.405739426612854, Val acc: 0.894\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.009976538837465465, Train acc: 0.8390758547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.012581212907774836, Train acc: 0.8396768162393162\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.016348089672561385, Train acc: 0.8428151709401709\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.016100294505938504, Train acc: 0.8426816239316239\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01701487970148396, Train acc: 0.8438301282051283\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01650857250405173, Train acc: 0.8439058048433048\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.016020754936704996, Train acc: 0.8433302808302808\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.015766725803797062, Train acc: 0.8435329861111112\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.016412993781586997, Train acc: 0.8442248338081672\n",
      "Val loss: 0.44871047139167786, Val acc: 0.9\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.004569923775827783, Train acc: 0.8352029914529915\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.00596976076435839, Train acc: 0.8380742521367521\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.008456234918360696, Train acc: 0.8389423076923077\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.009356226167108258, Train acc: 0.8396100427350427\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.011190795694660936, Train acc: 0.8400908119658119\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.011466603927802497, Train acc: 0.8401887464387464\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.011076487318791167, Train acc: 0.8395909645909646\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.011760032871085355, Train acc: 0.8405281784188035\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.012629249043840515, Train acc: 0.8411087369420702\n",
      "Val loss: 0.426105797290802, Val acc: 0.904\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.012074513058377128, Train acc: 0.843215811965812\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.011386588725269351, Train acc: 0.8410122863247863\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01649827996210495, Train acc: 0.8450854700854701\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.015624240168139465, Train acc: 0.8445846688034188\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.015661746212559887, Train acc: 0.8442307692307692\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.015782495891606366, Train acc: 0.8443064458689459\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.01640587284596994, Train acc: 0.8444558913308914\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01661613717293128, Train acc: 0.8445345886752137\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.016554778089776904, Train acc: 0.8437945156695157\n",
      "Val loss: 0.43405523896217346, Val acc: 0.9\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.011268175565279447, Train acc: 0.8409455128205128\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.013520210854008667, Train acc: 0.8394764957264957\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01457846088287158, Train acc: 0.8408119658119658\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.015582362301329263, Train acc: 0.8413795405982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.016647496182694396, Train acc: 0.8420673076923076\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.017075131634022096, Train acc: 0.8425258190883191\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.017873255397228387, Train acc: 0.8436164529914529\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01806515595342359, Train acc: 0.8438334668803419\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.016254428841336387, Train acc: 0.8423848528015194\n",
      "Val loss: 0.4433642327785492, Val acc: 0.892\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.006826172272364299, Train acc: 0.8360042735042735\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.012911677997336429, Train acc: 0.8402777777777778\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.014095132931684837, Train acc: 0.8395655270655271\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.016397608077933647, Train acc: 0.8420806623931624\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.017360756030449502, Train acc: 0.8427350427350427\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01586686066243044, Train acc: 0.8418803418803419\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.016396210925774115, Train acc: 0.8423954517704517\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.016745516600517128, Train acc: 0.8430321848290598\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.016477926957075187, Train acc: 0.8431416191832859\n",
      "Val loss: 0.45198535919189453, Val acc: 0.902\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.013506123907545693, Train acc: 0.8464209401709402\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.014164884248350421, Train acc: 0.8462873931623932\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.018544710005110824, Train acc: 0.8487802706552706\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.019894788726272747, Train acc: 0.8489917200854701\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0190793428156111, Train acc: 0.848798076923077\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.018124859344925297, Train acc: 0.8473780270655271\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.016805334369195978, Train acc: 0.8459249084249084\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01674299983259959, Train acc: 0.8456363514957265\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017031464594834664, Train acc: 0.8451745014245015\n",
      "Val loss: 0.4346243143081665, Val acc: 0.892\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.006311608685387505, Train acc: 0.8372061965811965\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.013519230561378675, Train acc: 0.8451522435897436\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.014562101445646368, Train acc: 0.8462428774928775\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.014815310255075112, Train acc: 0.8443843482905983\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01534334726822682, Train acc: 0.8439903846153847\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.015683660310218138, Train acc: 0.8436387108262108\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.016012284891072648, Train acc: 0.8436927655677655\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.015830982476472855, Train acc: 0.8434662126068376\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.015742627955117005, Train acc: 0.8436758072174739\n",
      "Val loss: 0.44697633385658264, Val acc: 0.902\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.01940879174786755, Train acc: 0.8452190170940171\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.017355808462852087, Train acc: 0.8414797008547008\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01633555673466109, Train acc: 0.8417913105413105\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.016044109932377808, Train acc: 0.8422809829059829\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.016171775720058342, Train acc: 0.8420138888888888\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.017066233093582326, Train acc: 0.8418135683760684\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.017487114165728783, Train acc: 0.8427960927960928\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.017218505915922996, Train acc: 0.842948717948718\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017509784482256752, Train acc: 0.8429635565052231\n",
      "Val loss: 0.4557088613510132, Val acc: 0.908\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.011092702802429851, Train acc: 0.8394764957264957\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.010906751848693587, Train acc: 0.8402777777777778\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.012547453244527182, Train acc: 0.8403222934472935\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.013558914518763876, Train acc: 0.8409455128205128\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.013711047274434668, Train acc: 0.8414797008547008\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.014806089068410064, Train acc: 0.8421474358974359\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.015560222123277638, Train acc: 0.8422809829059829\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01532735532292953, Train acc: 0.841796875\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01526280264003098, Train acc: 0.8417467948717948\n",
      "Val loss: 0.41590604186058044, Val acc: 0.902\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.012790651912363166, Train acc: 0.8448183760683761\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01547223788041335, Train acc: 0.8436832264957265\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01760694749674566, Train acc: 0.8453080484330484\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.014169411526785957, Train acc: 0.8419471153846154\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.014881009169113942, Train acc: 0.8426816239316239\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01521308113027502, Train acc: 0.8425035612535613\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.014686594224820352, Train acc: 0.8419566544566545\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.015010029778011844, Train acc: 0.8414797008547008\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.015959489917257007, Train acc: 0.841820987654321\n",
      "Val loss: 0.4316396713256836, Val acc: 0.89\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.010625782175960705, Train acc: 0.8397435897435898\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.020150494244363572, Train acc: 0.8448851495726496\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01959191555650825, Train acc: 0.8458867521367521\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.019023809359114394, Train acc: 0.8452190170940171\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01995789897747529, Train acc: 0.8454594017094017\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.019899500868259333, Train acc: 0.8457977207977208\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.020123465266419855, Train acc: 0.8463064713064713\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.019382222117776543, Train acc: 0.8455695779914529\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01918406635947377, Train acc: 0.8457828822412156\n",
      "Val loss: 0.42583924531936646, Val acc: 0.892\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.013154488852900317, Train acc: 0.8405448717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.018163920977176763, Train acc: 0.8436832264957265\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.02027238909675185, Train acc: 0.8465990028490028\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0183953897565858, Train acc: 0.8469885149572649\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.017528611625361647, Train acc: 0.8453525641025641\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.016825790036777484, Train acc: 0.8444177350427351\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.017067088567002498, Train acc: 0.8445703601953602\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.017370464996649668, Train acc: 0.8450520833333334\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017609914002368707, Train acc: 0.8450409544159544\n",
      "Val loss: 0.4342344105243683, Val acc: 0.912\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.024649324325414803, Train acc: 0.8510950854700855\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01967262190121871, Train acc: 0.8483573717948718\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0193215439632068, Train acc: 0.8479344729344729\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.02105737776837797, Train acc: 0.8485910790598291\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.020407210405056293, Train acc: 0.8477831196581197\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.02037208860586172, Train acc: 0.8473112535612536\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.019684582985073366, Train acc: 0.8467643467643468\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01877102644270302, Train acc: 0.8464042467948718\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01821324922646779, Train acc: 0.845411918328585\n",
      "Val loss: 0.4051016867160797, Val acc: 0.896\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.01761608806430784, Train acc: 0.8446848290598291\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.02332891625726325, Train acc: 0.8500934829059829\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.021991819143295288, Train acc: 0.8479789886039886\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.02128557001168911, Train acc: 0.8469551282051282\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0215030498484261, Train acc: 0.8476495726495726\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01933552214392909, Train acc: 0.8459980413105413\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.01888305512831179, Train acc: 0.8458295177045178\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01704652760273371, Train acc: 0.8442341079059829\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017233355630610177, Train acc: 0.8438390313390314\n",
      "Val loss: 0.43826889991760254, Val acc: 0.892\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.010889513625038994, Train acc: 0.8388087606837606\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01384080997389606, Train acc: 0.8416132478632479\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.015103552585993057, Train acc: 0.8433938746438746\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.01254039391493186, Train acc: 0.8411124465811965\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.014986055478071554, Train acc: 0.8420673076923076\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.0144839059678238, Train acc: 0.8419025997150997\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.015677664633635638, Train acc: 0.8430441086691086\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.015737400732488714, Train acc: 0.8430154914529915\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.015864475443605457, Train acc: 0.8428893637226971\n",
      "Val loss: 0.412227988243103, Val acc: 0.9\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.016740041410821117, Train acc: 0.844284188034188\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01671068765159346, Train acc: 0.8421474358974359\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.018239824520556676, Train acc: 0.8430822649572649\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.018373766898090005, Train acc: 0.8431156517094017\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.02032387781346965, Train acc: 0.8452991452991453\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.021614126797415253, Train acc: 0.8459312678062678\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.020927546552686027, Train acc: 0.8452953296703297\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.020270571080792662, Train acc: 0.8448517628205128\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.020288646702299996, Train acc: 0.8448035375118709\n",
      "Val loss: 0.42363518476486206, Val acc: 0.902\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.013826605855909169, Train acc: 0.8457532051282052\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.017329941830064498, Train acc: 0.8456864316239316\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01740432218608693, Train acc: 0.8444622507122507\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.017660414752287742, Train acc: 0.8452190170940171\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0171238724492554, Train acc: 0.8447382478632479\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.016356871041477237, Train acc: 0.8440616096866097\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.01566251326393295, Train acc: 0.8431013431013431\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.017455581862192888, Train acc: 0.8444511217948718\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017271346006637964, Train acc: 0.8438983855650523\n",
      "Val loss: 0.41241946816444397, Val acc: 0.904\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.017810019672426403, Train acc: 0.8466880341880342\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.02180640195679461, Train acc: 0.8484909188034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.015587442451053195, Train acc: 0.8443287037037037\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.01869606850748388, Train acc: 0.8472556089743589\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.017596703665888207, Train acc: 0.8456196581196581\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.018437453696870398, Train acc: 0.8456641737891738\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.018987816550356126, Train acc: 0.8456768925518926\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01803137428867511, Train acc: 0.8449686164529915\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01732311020662755, Train acc: 0.8442990265906932\n",
      "Val loss: 0.42316991090774536, Val acc: 0.89\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0235791983258011, Train acc: 0.8486912393162394\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.019695244538478363, Train acc: 0.8468883547008547\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01838029229063594, Train acc: 0.8451299857549858\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.01647158960501353, Train acc: 0.8424813034188035\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.018222947507841973, Train acc: 0.8441773504273504\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.019118716828843467, Train acc: 0.8453525641025641\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.01786401740506164, Train acc: 0.8440743284493285\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.0179707027780704, Train acc: 0.8442341079059829\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01888123103696057, Train acc: 0.8446699905033238\n",
      "Val loss: 0.42338693141937256, Val acc: 0.884\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.015714945446731698, Train acc: 0.844017094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.015524516503016153, Train acc: 0.8438835470085471\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01645702208548869, Train acc: 0.8447738603988604\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.017006735134328533, Train acc: 0.8449185363247863\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.017559219119895216, Train acc: 0.8446848290598291\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.018461901917416826, Train acc: 0.8449519230769231\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.018289798199766574, Train acc: 0.8450854700854701\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.018333595882878344, Train acc: 0.8450353899572649\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017239573844138035, Train acc: 0.8443583808167141\n",
      "Val loss: 0.4414321780204773, Val acc: 0.896\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.016804778423064794, Train acc: 0.8426816239316239\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01981342577526712, Train acc: 0.8456196581196581\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.018914672384234915, Train acc: 0.8449074074074074\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.017857873669037454, Train acc: 0.8441172542735043\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.018139000428028594, Train acc: 0.8434027777777777\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.019232232601214677, Train acc: 0.8441951566951567\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.019728997906485756, Train acc: 0.8446848290598291\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.019215630542518746, Train acc: 0.8444010416666666\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.019367383019989824, Train acc: 0.8445364434947769\n",
      "Val loss: 0.4416283071041107, Val acc: 0.89\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.015505041322137555, Train acc: 0.8450854700854701\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.021644336545569264, Train acc: 0.8485576923076923\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.021976004129121787, Train acc: 0.8474002849002849\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.02065287766038862, Train acc: 0.8456530448717948\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01852928537588853, Train acc: 0.8443910256410256\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.019263260851898084, Train acc: 0.844551282051282\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.020112406166772994, Train acc: 0.8452380952380952\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.018978829471728742, Train acc: 0.8447516025641025\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.018523055037655157, Train acc: 0.8449519230769231\n",
      "Val loss: 0.40083032846450806, Val acc: 0.904\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.010517623689439561, Train acc: 0.8381410256410257\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.011397654684180887, Train acc: 0.8381410256410257\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.014470852423257638, Train acc: 0.8420138888888888\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.012647550011801923, Train acc: 0.8416466346153846\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01316776907342112, Train acc: 0.8415064102564103\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.013256802772864318, Train acc: 0.8418358262108262\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.014811007749466669, Train acc: 0.8429296398046398\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.015469388956697578, Train acc: 0.8426816239316239\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.015453260876627503, Train acc: 0.8427706552706553\n",
      "Val loss: 0.445385605096817, Val acc: 0.912\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.00011858114829430214, Train acc: 0.8327991452991453\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.008564032805271639, Train acc: 0.8396100427350427\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.013715211652282976, Train acc: 0.8420138888888888\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.016034035155406363, Train acc: 0.8439169337606838\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0156517580533639, Train acc: 0.8435630341880341\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01718339582963547, Train acc: 0.8447961182336182\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.017972958903027397, Train acc: 0.845467032967033\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.017299066082789347, Train acc: 0.8446347489316239\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017091442122418657, Train acc: 0.8442248338081672\n",
      "Val loss: 0.45230334997177124, Val acc: 0.9\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.013746083801628178, Train acc: 0.8428151709401709\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01991550560690399, Train acc: 0.8482905982905983\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.021125179665041106, Train acc: 0.84806801994302\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.01936637183539888, Train acc: 0.8475894764957265\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.019686921030028254, Train acc: 0.8474091880341881\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01994768782728418, Train acc: 0.8467548076923077\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.020150581085929358, Train acc: 0.8468597374847375\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.020989835708059817, Train acc: 0.8474726228632479\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.020539864798669916, Train acc: 0.8473557692307693\n",
      "Val loss: 0.42946118116378784, Val acc: 0.898\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.018087584748227373, Train acc: 0.842948717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.021122680387945257, Train acc: 0.8454861111111112\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.02357007253203976, Train acc: 0.8477118945868946\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.024404763602293454, Train acc: 0.8479901175213675\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.023093918500802457, Train acc: 0.8467948717948718\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.022566622351309514, Train acc: 0.8473335113960114\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.023400030366084807, Train acc: 0.8482715201465202\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.022396454762699258, Train acc: 0.8476395566239316\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.021108071094224936, Train acc: 0.846213200379867\n",
      "Val loss: 0.4626863896846771, Val acc: 0.904\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.021780094529828455, Train acc: 0.8496260683760684\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.020427790971902702, Train acc: 0.8494925213675214\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.022406234700455625, Train acc: 0.8490473646723646\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.022903370296853222, Train acc: 0.8488581730769231\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.02180093827410641, Train acc: 0.8478899572649573\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.02177612233026075, Train acc: 0.8477786680911681\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.0211776933827243, Train acc: 0.8469360500610501\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.020528003700778015, Train acc: 0.846237313034188\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01996504371775295, Train acc: 0.8459015906932573\n",
      "Val loss: 0.4346246123313904, Val acc: 0.896\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.013022680058438554, Train acc: 0.8444177350427351\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.014082736948616484, Train acc: 0.8450186965811965\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01250882400067104, Train acc: 0.8411680911680912\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.013214404256934794, Train acc: 0.8413127670940171\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.015124358580662654, Train acc: 0.8427617521367521\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.016702110144147846, Train acc: 0.8441951566951567\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.017866218883330422, Train acc: 0.8450282356532357\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01776984837065395, Train acc: 0.844551282051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017998689882888868, Train acc: 0.8446403133903134\n",
      "Val loss: 0.40776488184928894, Val acc: 0.902\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.016434074976505376, Train acc: 0.8462873931623932\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.016062538847964034, Train acc: 0.8426148504273504\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01810580721268287, Train acc: 0.8431267806267806\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.02006902825883311, Train acc: 0.8436164529914529\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.02003404380928757, Train acc: 0.8443643162393163\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.02175986057842559, Train acc: 0.8466435185185185\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.020859341881941818, Train acc: 0.8459439865689866\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.02038478249540696, Train acc: 0.8454193376068376\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.019782359189117735, Train acc: 0.8449074074074074\n",
      "Val loss: 0.4364883005619049, Val acc: 0.894\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.022912098556502253, Train acc: 0.8478899572649573\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.02283662086368626, Train acc: 0.8490918803418803\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.02124826767166116, Train acc: 0.8479789886039886\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.018610306466237094, Train acc: 0.8456530448717948\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.019590586576706325, Train acc: 0.8455128205128205\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01916664431237767, Train acc: 0.8455528846153846\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.019096834100646415, Train acc: 0.8451236263736264\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.018751939328817222, Train acc: 0.8446848290598291\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.018784675899281008, Train acc: 0.8447590218423552\n",
      "Val loss: 0.44985702633857727, Val acc: 0.902\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.01734592313440437, Train acc: 0.8481570512820513\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.017319272471289348, Train acc: 0.8470219017094017\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.01843354063495951, Train acc: 0.8469106125356125\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.017543679119175315, Train acc: 0.8470219017094017\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.017601444374801767, Train acc: 0.8460202991452992\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.017812944969899974, Train acc: 0.8455528846153846\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.017987914574451935, Train acc: 0.845467032967033\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01816122077851214, Train acc: 0.8448183760683761\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017906335882085456, Train acc: 0.8447886989553656\n",
      "Val loss: 0.4358126223087311, Val acc: 0.896\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.015048045887906328, Train acc: 0.8444177350427351\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.014777099220161764, Train acc: 0.8416800213675214\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0175495139214388, Train acc: 0.8438390313390314\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.01854061997599072, Train acc: 0.8444511217948718\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01859746768943265, Train acc: 0.8442040598290599\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01925552914455066, Train acc: 0.844551282051282\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.019059451435657356, Train acc: 0.8441315628815629\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.019587039692789063, Train acc: 0.8441840277777778\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.019081181016063417, Train acc: 0.8438983855650523\n",
      "Val loss: 0.43216681480407715, Val acc: 0.898\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.013919191992180979, Train acc: 0.8426816239316239\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.013913120214755718, Train acc: 0.8412126068376068\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.013207722593236852, Train acc: 0.8412571225071225\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.013298807617945548, Train acc: 0.8404447115384616\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.012568585893027803, Train acc: 0.8404380341880342\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.014141822430143329, Train acc: 0.8416577635327636\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.01476131861754244, Train acc: 0.8423954517704517\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.014163101203421243, Train acc: 0.8416633279914529\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.014062347461921424, Train acc: 0.8418803418803419\n",
      "Val loss: 0.42921069264411926, Val acc: 0.894\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.019259000945295025, Train acc: 0.8497596153846154\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.015011264982386531, Train acc: 0.8439503205128205\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.014633789870813702, Train acc: 0.8435274216524217\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.014989139559941415, Train acc: 0.844284188034188\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01653552437439943, Train acc: 0.8450320512820513\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.017812876110402946, Train acc: 0.8460870726495726\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.0185317333073552, Train acc: 0.8465735653235653\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.017807857666769598, Train acc: 0.8455194978632479\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01793472541023863, Train acc: 0.846139007597341\n",
      "Val loss: 0.44392159581184387, Val acc: 0.896\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.0027789081263745953, Train acc: 0.828392094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.010832803244264716, Train acc: 0.8366052350427351\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.014527996466030762, Train acc: 0.8407229344729344\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.014806730624956962, Train acc: 0.8417801816239316\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.016484142929060846, Train acc: 0.8435630341880341\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.015714369820393728, Train acc: 0.8437277421652422\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.016075480479169388, Train acc: 0.844284188034188\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01784833176777913, Train acc: 0.8450020032051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.017801102919456285, Train acc: 0.8451151471984806\n",
      "Val loss: 0.425910621881485, Val acc: 0.9\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.016109765340120364, Train acc: 0.8406784188034188\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.017577015054531585, Train acc: 0.8420806623931624\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.017772400990510598, Train acc: 0.8427261396011396\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.016849575771225825, Train acc: 0.8418135683760684\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.017484906137499034, Train acc: 0.8433226495726496\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.01926325079043027, Train acc: 0.8450186965811965\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.018231207707280494, Train acc: 0.8440743284493285\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.0175604859733174, Train acc: 0.8437333066239316\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.018300093590137737, Train acc: 0.8443435422602089\n",
      "Val loss: 0.4242916405200958, Val acc: 0.892\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.015805350409613714, Train acc: 0.8417467948717948\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.01963937588227101, Train acc: 0.8424813034188035\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.017439122553224915, Train acc: 0.8410345441595442\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.018300601687186804, Train acc: 0.8426148504273504\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.01871632938711052, Train acc: 0.8431356837606837\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.018923447087959008, Train acc: 0.8431490384615384\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.018665109143589004, Train acc: 0.8433302808302808\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.01964969149766824, Train acc: 0.8438668536324786\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.01905419864998697, Train acc: 0.8430525878442545\n",
      "Val loss: 0.40578311681747437, Val acc: 0.892\n",
      "Early stopping at epoch 45 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 1090.6905 [s]\n",
      "Combinación 8/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 1.081335470207736, Train acc: 0.28118322649572647\n",
      "Iteration 234 - Batch 234/569 - Train loss: 1.037456619179147, Train acc: 0.3251201923076923\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.9973361794425551, Train acc: 0.36378205128205127\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.9600917070339887, Train acc: 0.39783653846153844\n",
      "Val loss: 0.9749643802642822, Val acc: 0.656\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.7335693815834502, Train acc: 0.5892761752136753\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.7135247532119099, Train acc: 0.5995259081196581\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.693024587427449, Train acc: 0.6128917378917379\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.6746296762910664, Train acc: 0.6246828258547008\n",
      "Val loss: 0.7451483607292175, Val acc: 0.712\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5729849448061397, Train acc: 0.6838942307692307\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5639148871613364, Train acc: 0.6894364316239316\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5560923423352744, Train acc: 0.6935096153846154\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5478294738846966, Train acc: 0.6986344818376068\n",
      "Val loss: 0.6482950448989868, Val acc: 0.762\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4967325192231398, Train acc: 0.7224225427350427\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.49123553638784295, Train acc: 0.7276642628205128\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.48341412022922114, Train acc: 0.7336404914529915\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.4756926532484527, Train acc: 0.7369624732905983\n",
      "Val loss: 0.5839851498603821, Val acc: 0.766\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.440331272335134, Train acc: 0.7546741452991453\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.43452411788141626, Train acc: 0.7586805555555556\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4335394229134943, Train acc: 0.7605279558404558\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.43051333827340704, Train acc: 0.7633213141025641\n",
      "Val loss: 0.529738187789917, Val acc: 0.786\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.41484511892000836, Train acc: 0.7728365384615384\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4047346823235862, Train acc: 0.7791132478632479\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.40074689741487857, Train acc: 0.7813612891737892\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3976310548874048, Train acc: 0.7824686164529915\n",
      "Val loss: 0.5237541198730469, Val acc: 0.808\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.37435973480216456, Train acc: 0.7952724358974359\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3741902835094012, Train acc: 0.7959401709401709\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3711007895462873, Train acc: 0.7975204772079773\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.37027294284258133, Train acc: 0.7975093482905983\n",
      "Val loss: 0.47921350598335266, Val acc: 0.802\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3577155868212382, Train acc: 0.8028846153846154\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.35398323974038803, Train acc: 0.8046875\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3493738699163127, Train acc: 0.8073584401709402\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.34937341078224343, Train acc: 0.8075253739316239\n",
      "Val loss: 0.4529193937778473, Val acc: 0.814\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3401442266936995, Train acc: 0.8136351495726496\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3372909676315438, Train acc: 0.8158386752136753\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.33690966703952885, Train acc: 0.8162393162393162\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.33348202507974756, Train acc: 0.8175247061965812\n",
      "Val loss: 0.45483291149139404, Val acc: 0.818\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.32512252096436983, Train acc: 0.8220486111111112\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.324753972582328, Train acc: 0.8217481303418803\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3193890048228098, Train acc: 0.8254763176638177\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3163647019965017, Train acc: 0.8275407318376068\n",
      "Val loss: 0.40121322870254517, Val acc: 0.838\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.31458280152744716, Train acc: 0.8310630341880342\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.310405917911448, Train acc: 0.8319310897435898\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3076459294200962, Train acc: 0.832977207977208\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.30506604018374384, Train acc: 0.8336838942307693\n",
      "Val loss: 0.40209779143333435, Val acc: 0.842\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.30644236237574846, Train acc: 0.8310630341880342\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3006949909349792, Train acc: 0.8353365384615384\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.2981899729226729, Train acc: 0.8372507122507122\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2951063107476275, Train acc: 0.8391593215811965\n",
      "Val loss: 0.38590872287750244, Val acc: 0.86\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.28677636104771215, Train acc: 0.8466880341880342\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.2806886121885389, Train acc: 0.8488915598290598\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.279547379482166, Train acc: 0.8487802706552706\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2805095146227087, Train acc: 0.8478565705128205\n",
      "Val loss: 0.3719301223754883, Val acc: 0.838\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2762817960773778, Train acc: 0.8527644230769231\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.27268413027637023, Train acc: 0.8551682692307693\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.27089378237724304, Train acc: 0.8548789173789174\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2720271101555763, Train acc: 0.8536157852564102\n",
      "Val loss: 0.361341267824173, Val acc: 0.85\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2681015410229691, Train acc: 0.8558360042735043\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.27022258236876917, Train acc: 0.8544003739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.2654278301275693, Train acc: 0.8565927706552706\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.26597283815598893, Train acc: 0.8554353632478633\n",
      "Val loss: 0.3597220778465271, Val acc: 0.868\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2579697550107271, Train acc: 0.8617120726495726\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.25952403005371744, Train acc: 0.8589743589743589\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.25796340291316694, Train acc: 0.859909188034188\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2594002345497282, Train acc: 0.8588074252136753\n",
      "Val loss: 0.34285056591033936, Val acc: 0.872\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.24849021358367723, Train acc: 0.8662526709401709\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.24999379532204735, Train acc: 0.8643496260683761\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.24903245703277424, Train acc: 0.8640491452991453\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.24858510255431518, Train acc: 0.864433092948718\n",
      "Val loss: 0.3371617794036865, Val acc: 0.866\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2566465872984666, Train acc: 0.8653846153846154\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.25309412047649044, Train acc: 0.8636151175213675\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.25027087725635266, Train acc: 0.8639155982905983\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.24887194962073594, Train acc: 0.8648170405982906\n",
      "Val loss: 0.33679839968681335, Val acc: 0.862\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2432120572298001, Train acc: 0.8658520299145299\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.24047660038002536, Train acc: 0.8670539529914529\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.24040559630448322, Train acc: 0.8672097578347578\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.23903538869359556, Train acc: 0.8685396634615384\n",
      "Val loss: 0.34337154030799866, Val acc: 0.852\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2394556004522193, Train acc: 0.8679220085470085\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.23944200390679204, Train acc: 0.8683560363247863\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.23727807389874744, Train acc: 0.8696803774928775\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.23662594244138807, Train acc: 0.8697916666666666\n",
      "Val loss: 0.3274509012699127, Val acc: 0.862\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2330294242526731, Train acc: 0.8729300213675214\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.23369742267661625, Train acc: 0.8719951923076923\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.22990026327110083, Train acc: 0.8745993589743589\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.22919492187917742, Train acc: 0.875\n",
      "Val loss: 0.33757898211479187, Val acc: 0.874\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.22472610827694592, Train acc: 0.8787393162393162\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.22623716129197013, Train acc: 0.8781383547008547\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.2281190556848151, Train acc: 0.8766915954415955\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2284413299117333, Train acc: 0.8758680555555556\n",
      "Val loss: 0.3126474618911743, Val acc: 0.882\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2194379808811041, Train acc: 0.8795405982905983\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.22207615199761513, Train acc: 0.8776041666666666\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.22306590087902853, Train acc: 0.8778712606837606\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2245545316583071, Train acc: 0.8773203792735043\n",
      "Val loss: 0.30306532979011536, Val acc: 0.876\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2229130948201204, Train acc: 0.8784722222222222\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.22179843230634674, Train acc: 0.8778378739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.22161637785767557, Train acc: 0.8782941595441596\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.22136732990033606, Train acc: 0.8783052884615384\n",
      "Val loss: 0.30105990171432495, Val acc: 0.882\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.22289786532393888, Train acc: 0.8806757478632479\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.2221406306593846, Train acc: 0.8792735042735043\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.21937632110723404, Train acc: 0.8789173789173789\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.21779853054600903, Train acc: 0.8801081730769231\n",
      "Val loss: 0.28781917691230774, Val acc: 0.892\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.222004180535292, Train acc: 0.8782719017094017\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.22020698122234425, Train acc: 0.8792735042735043\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.21751402365176425, Train acc: 0.8811209045584045\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.21478948786727384, Train acc: 0.8822616185897436\n",
      "Val loss: 0.2985800802707672, Val acc: 0.886\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2163801570223947, Train acc: 0.8837473290598291\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.21392108149762845, Train acc: 0.8839476495726496\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.21290909832189564, Train acc: 0.8838586182336182\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.21138730466875255, Train acc: 0.8849158653846154\n",
      "Val loss: 0.28216835856437683, Val acc: 0.89\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.20733613565436795, Train acc: 0.8846821581196581\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.20690661968074292, Train acc: 0.8856837606837606\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.20654341094514245, Train acc: 0.8863514957264957\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.20902903699594685, Train acc: 0.8852163461538461\n",
      "Val loss: 0.2980315089225769, Val acc: 0.88\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2068314956039445, Train acc: 0.8879540598290598\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.20608286546845722, Train acc: 0.8872863247863247\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.20580907535349202, Train acc: 0.8865295584045584\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2055894479664982, Train acc: 0.8868022168803419\n",
      "Val loss: 0.28368207812309265, Val acc: 0.896\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.20116990549951538, Train acc: 0.8890892094017094\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.20101932079618812, Train acc: 0.8894230769230769\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.20344039492118052, Train acc: 0.8889111467236467\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2038057676046832, Train acc: 0.8883547008547008\n",
      "Val loss: 0.31579530239105225, Val acc: 0.878\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2073806232621527, Train acc: 0.8848157051282052\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.20191691064427042, Train acc: 0.8893896901709402\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.2008855018520627, Train acc: 0.8892227564102564\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.20191258809760085, Train acc: 0.8877537393162394\n",
      "Val loss: 0.2783939838409424, Val acc: 0.892\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.19987119339470172, Train acc: 0.8880876068376068\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.19956182370073774, Train acc: 0.8896901709401709\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.19904346216438162, Train acc: 0.8902688746438746\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.19848685877190697, Train acc: 0.8902577457264957\n",
      "Val loss: 0.2858898937702179, Val acc: 0.878\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.20213027833363947, Train acc: 0.8875534188034188\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1998531992236773, Train acc: 0.8898571047008547\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.19644141167487175, Train acc: 0.8920717592592593\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.19669967724217308, Train acc: 0.8915765224358975\n",
      "Val loss: 0.27741673588752747, Val acc: 0.886\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1957547393364784, Train acc: 0.8932959401709402\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.19333088951997268, Train acc: 0.8923611111111112\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1920749101883326, Train acc: 0.893585292022792\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1926802237255451, Train acc: 0.8930455395299145\n",
      "Val loss: 0.2719758152961731, Val acc: 0.892\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18581062351536548, Train acc: 0.8967013888888888\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.18884196196101669, Train acc: 0.8957999465811965\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1893559108857076, Train acc: 0.8952546296296297\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1898524118666975, Train acc: 0.8949151976495726\n",
      "Val loss: 0.2690448462963104, Val acc: 0.892\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.19962808056774303, Train acc: 0.8895566239316239\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.19046909699582645, Train acc: 0.8936298076923077\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.19124535723798974, Train acc: 0.8936075498575499\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.19095448990408173, Train acc: 0.8939803685897436\n",
      "Val loss: 0.27557697892189026, Val acc: 0.896\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18923199380564892, Train acc: 0.8956330128205128\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.18584136072641763, Train acc: 0.8971688034188035\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1864142608099174, Train acc: 0.8970352564102564\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.18665704175702527, Train acc: 0.8970686431623932\n",
      "Val loss: 0.26911661028862, Val acc: 0.888\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18467417117367443, Train acc: 0.8946314102564102\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.18410740652654925, Train acc: 0.8970686431623932\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.182402470937142, Train acc: 0.8989494301994302\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.18537168672833687, Train acc: 0.8973023504273504\n",
      "Val loss: 0.27101364731788635, Val acc: 0.894\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18454201965250522, Train acc: 0.8999065170940171\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1868253548940023, Train acc: 0.8979366987179487\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.18385051640859695, Train acc: 0.8995281339031339\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.18381851265191013, Train acc: 0.8989049145299145\n",
      "Val loss: 0.2684274911880493, Val acc: 0.892\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18253753773677042, Train acc: 0.8991052350427351\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.18052851033006978, Train acc: 0.9002069978632479\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.17985709932794597, Train acc: 0.9002181267806267\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1799579647043322, Train acc: 0.9001068376068376\n",
      "Val loss: 0.27271148562431335, Val acc: 0.886\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18590907166656265, Train acc: 0.8950320512820513\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.18356827168892592, Train acc: 0.8971354166666666\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1810227839270888, Train acc: 0.8993055555555556\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.18100031172363168, Train acc: 0.8987212873931624\n",
      "Val loss: 0.27206534147262573, Val acc: 0.888\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1834221016138028, Train acc: 0.8982371794871795\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1823760241142705, Train acc: 0.8982037927350427\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.178794793890752, Train acc: 0.9002181267806267\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.17901459699257827, Train acc: 0.9002904647435898\n",
      "Val loss: 0.2607129216194153, Val acc: 0.89\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.17757371679330483, Train acc: 0.9010416666666666\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1748791864921904, Train acc: 0.9030114850427351\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.17653222649525374, Train acc: 0.9010639245014245\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.17668588147458866, Train acc: 0.9017761752136753\n",
      "Val loss: 0.26070061326026917, Val acc: 0.894\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.17193414143517485, Train acc: 0.9052483974358975\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1725082496802012, Train acc: 0.9045472756410257\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.17355879292189225, Train acc: 0.9034455128205128\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.17384642967556277, Train acc: 0.9039129273504274\n",
      "Val loss: 0.2582778036594391, Val acc: 0.9\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.17065500270607126, Train acc: 0.9042467948717948\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1725681169420226, Train acc: 0.9036458333333334\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.17287409152740088, Train acc: 0.9027777777777778\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.17214507361253104, Train acc: 0.9037126068376068\n",
      "Val loss: 0.26155799627304077, Val acc: 0.892\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1697137000469061, Train acc: 0.9071180555555556\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.17324321876224288, Train acc: 0.9052483974358975\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.17245366044363744, Train acc: 0.9048032407407407\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.17206969822191784, Train acc: 0.9049813034188035\n",
      "Val loss: 0.25649264454841614, Val acc: 0.898\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1633525475477561, Train acc: 0.9091212606837606\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16550961162290004, Train acc: 0.9082198183760684\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1670620694883868, Train acc: 0.9076745014245015\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.16770812904096058, Train acc: 0.9073517628205128\n",
      "Val loss: 0.2600387930870056, Val acc: 0.892\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.170924936222215, Train acc: 0.9047142094017094\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16631304014187592, Train acc: 0.9061164529914529\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1669446145027791, Train acc: 0.9056935541310541\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1692058963334968, Train acc: 0.9047642895299145\n",
      "Val loss: 0.281303346157074, Val acc: 0.878\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16779267533212647, Train acc: 0.9053819444444444\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16570030942431882, Train acc: 0.9058159722222222\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1678832071883726, Train acc: 0.9053596866096866\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1673005066302597, Train acc: 0.9052150106837606\n",
      "Val loss: 0.25876274704933167, Val acc: 0.89\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1632054600960169, Train acc: 0.9097222222222222\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1651717473426436, Train acc: 0.9081196581196581\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.16551125423181431, Train acc: 0.9069177350427351\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.16588652471446583, Train acc: 0.9071180555555556\n",
      "Val loss: 0.25484442710876465, Val acc: 0.9\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16408866096256125, Train acc: 0.9073183760683761\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16634755403312862, Train acc: 0.9070846688034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.16646291532068172, Train acc: 0.9063835470085471\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.16625402718145624, Train acc: 0.9065671741452992\n",
      "Val loss: 0.2476302832365036, Val acc: 0.9\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16318868762916988, Train acc: 0.9130608974358975\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16567676087729952, Train acc: 0.9104567307692307\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.16509383432885522, Train acc: 0.91056801994302\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.16322805226231232, Train acc: 0.9110910790598291\n",
      "Val loss: 0.25690606236457825, Val acc: 0.886\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16087720906123137, Train acc: 0.9122596153846154\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.15888267959284985, Train acc: 0.9113581730769231\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1602196011617992, Train acc: 0.9107460826210826\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1602797792890133, Train acc: 0.9094885149572649\n",
      "Val loss: 0.2788223922252655, Val acc: 0.89\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16174454057318532, Train acc: 0.9091880341880342\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16292342524497938, Train acc: 0.9084535256410257\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.16219145339778346, Train acc: 0.9088541666666666\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.16083594039082527, Train acc: 0.9097222222222222\n",
      "Val loss: 0.2562977373600006, Val acc: 0.894\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16392748720116085, Train acc: 0.9102564102564102\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1585607923503615, Train acc: 0.9116252670940171\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.15912719134591582, Train acc: 0.9117254273504274\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15834068252235398, Train acc: 0.9118088942307693\n",
      "Val loss: 0.2512306571006775, Val acc: 0.898\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1538791987631056, Train acc: 0.9176682692307693\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.15832019342571244, Train acc: 0.9129607371794872\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.15875492612181227, Train acc: 0.9123709045584045\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15811464231875208, Train acc: 0.9123263888888888\n",
      "Val loss: 0.2539874017238617, Val acc: 0.888\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.15752393745968485, Train acc: 0.9109909188034188\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1560303300109684, Train acc: 0.9111578525641025\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.15562225831539883, Train acc: 0.9122596153846154\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15580218377658445, Train acc: 0.912109375\n",
      "Val loss: 0.25704318284988403, Val acc: 0.886\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.15313698032982329, Train acc: 0.9126602564102564\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1525709631606045, Train acc: 0.913528311965812\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.15186372645560153, Train acc: 0.9140847578347578\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15376372479348102, Train acc: 0.9130942841880342\n",
      "Val loss: 0.24667102098464966, Val acc: 0.896\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.15606099418085864, Train acc: 0.9111244658119658\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.15478361238781205, Train acc: 0.9116586538461539\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1533220786484558, Train acc: 0.9129941239316239\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15330530553419366, Train acc: 0.9130108173076923\n",
      "Val loss: 0.282405823469162, Val acc: 0.878\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.15133012410921928, Train acc: 0.9144631410256411\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.15029943740775442, Train acc: 0.9147970085470085\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1521848801024619, Train acc: 0.9136396011396012\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15298333891436586, Train acc: 0.9136952457264957\n",
      "Val loss: 0.2715211510658264, Val acc: 0.876\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.15947123610565805, Train acc: 0.9094551282051282\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.15657468324797785, Train acc: 0.9115584935897436\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.15326017192286304, Train acc: 0.9134392806267806\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15128468572456613, Train acc: 0.9146467681623932\n",
      "Val loss: 0.25632405281066895, Val acc: 0.892\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1470523354334709, Train acc: 0.9152644230769231\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.14699974172135705, Train acc: 0.9169003739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1488233805551828, Train acc: 0.9163327991452992\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.14852793734425154, Train acc: 0.9159655448717948\n",
      "Val loss: 0.25304508209228516, Val acc: 0.892\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1495233606069516, Train acc: 0.9146634615384616\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.14848802499791497, Train acc: 0.9158653846153846\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1479477429406935, Train acc: 0.9165108618233618\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.14681953468765968, Train acc: 0.9171340811965812\n",
      "Val loss: 0.24121621251106262, Val acc: 0.898\n",
      "Epoch 64/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1420711303113872, Train acc: 0.9185363247863247\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.14442495772471795, Train acc: 0.9192040598290598\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1447969084748855, Train acc: 0.9188701923076923\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1451301624536769, Train acc: 0.9185363247863247\n",
      "Val loss: 0.2584472596645355, Val acc: 0.88\n",
      "Epoch 65/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1428714450607952, Train acc: 0.9192708333333334\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1448765123883883, Train acc: 0.9175013354700855\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.14386033561834244, Train acc: 0.9179353632478633\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1455843281160053, Train acc: 0.9176348824786325\n",
      "Val loss: 0.250847727060318, Val acc: 0.89\n",
      "Epoch 66/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.14548519877796498, Train acc: 0.9159321581196581\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1452832323873145, Train acc: 0.9169003739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.14489970388066056, Train acc: 0.9176905270655271\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1449445660234007, Train acc: 0.9175347222222222\n",
      "Val loss: 0.24496899545192719, Val acc: 0.896\n",
      "Epoch 67/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1384412569240627, Train acc: 0.9209401709401709\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13989396963236678, Train acc: 0.9205729166666666\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.14110610634088516, Train acc: 0.9197827635327636\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.14137945153838039, Train acc: 0.9200888087606838\n",
      "Val loss: 0.265186071395874, Val acc: 0.88\n",
      "Epoch 68/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.14137399464081496, Train acc: 0.9179353632478633\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.141140474149814, Train acc: 0.9188034188034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.14058662517967388, Train acc: 0.9191372863247863\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1391403147807488, Train acc: 0.9199719551282052\n",
      "Val loss: 0.26021379232406616, Val acc: 0.876\n",
      "Epoch 69/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.14264735165569517, Train acc: 0.9208066239316239\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1425984278639667, Train acc: 0.9184695512820513\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.14091918464654532, Train acc: 0.9193598646723646\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.14022307432232758, Train acc: 0.9196380876068376\n",
      "Val loss: 0.27121153473854065, Val acc: 0.874\n",
      "Epoch 70/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13760275336412284, Train acc: 0.9218082264957265\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13681652123092586, Train acc: 0.921607905982906\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13880636727708018, Train acc: 0.9210514601139601\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.13828482943722326, Train acc: 0.9214075854700855\n",
      "Val loss: 0.24621909856796265, Val acc: 0.88\n",
      "Epoch 71/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13905337758553335, Train acc: 0.9192040598290598\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13838064804291114, Train acc: 0.9205729166666666\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1382794940149003, Train acc: 0.9210514601139601\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.13905816791085607, Train acc: 0.9205061431623932\n",
      "Val loss: 0.2507700026035309, Val acc: 0.884\n",
      "Epoch 72/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13902531226730755, Train acc: 0.9188701923076923\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13698106507460275, Train acc: 0.9208400106837606\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13675329594360797, Train acc: 0.9212295227920227\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.136492975629293, Train acc: 0.9217581463675214\n",
      "Val loss: 0.24230362474918365, Val acc: 0.892\n",
      "Epoch 73/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1400117937825684, Train acc: 0.9206730769230769\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1397961862066872, Train acc: 0.9207064636752137\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13604791759595572, Train acc: 0.9228543447293447\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.135561507816116, Train acc: 0.9226595886752137\n",
      "Val loss: 0.27739661931991577, Val acc: 0.878\n",
      "Epoch 74/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1302149779139421, Train acc: 0.9240117521367521\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1330197935239372, Train acc: 0.9227096688034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13381199073842448, Train acc: 0.9230546652421653\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.13337157956427997, Train acc: 0.9233607104700855\n",
      "Val loss: 0.246019646525383, Val acc: 0.9\n",
      "Epoch 75/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13775928267556378, Train acc: 0.9216746794871795\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1355289646830314, Train acc: 0.9223758012820513\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1353697912008674, Train acc: 0.9222756410256411\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.13361136675772503, Train acc: 0.9231770833333334\n",
      "Val loss: 0.26045188307762146, Val acc: 0.886\n",
      "Epoch 76/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1335604661422917, Train acc: 0.9239449786324786\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13493463575330555, Train acc: 0.9239115918803419\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13295916433942284, Train acc: 0.92403400997151\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.13172975599638417, Train acc: 0.9244958600427351\n",
      "Val loss: 0.23608651757240295, Val acc: 0.896\n",
      "Epoch 77/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.12813860362666285, Train acc: 0.9270165598290598\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12976359591906905, Train acc: 0.9246127136752137\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1293496999178517, Train acc: 0.9254585113960114\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1304495391778202, Train acc: 0.9251302083333334\n",
      "Val loss: 0.2409328669309616, Val acc: 0.894\n",
      "Epoch 78/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1240354633738852, Train acc: 0.9278846153846154\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12625209455434075, Train acc: 0.9270165598290598\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12649721495191596, Train acc: 0.9270610754985755\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12614999959866205, Train acc: 0.9271668002136753\n",
      "Val loss: 0.24070754647254944, Val acc: 0.894\n",
      "Epoch 79/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1332818560111217, Train acc: 0.9236778846153846\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13077131792520866, Train acc: 0.9254473824786325\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13004128825970185, Train acc: 0.9249910968660968\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12850868799047083, Train acc: 0.9264155982905983\n",
      "Val loss: 0.23927663266658783, Val acc: 0.896\n",
      "Epoch 80/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1270848441327739, Train acc: 0.9273504273504274\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12612567549077874, Train acc: 0.9274839743589743\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12513923320250633, Train acc: 0.9280849358974359\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1254851757429349, Train acc: 0.9278345352564102\n",
      "Val loss: 0.24642083048820496, Val acc: 0.894\n",
      "Epoch 81/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.12631191580723494, Train acc: 0.9284855769230769\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12731274452983823, Train acc: 0.9256810897435898\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12524281074943025, Train acc: 0.9273504273504274\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12493555086991216, Train acc: 0.9277176816239316\n",
      "Val loss: 0.24467530846595764, Val acc: 0.89\n",
      "Epoch 82/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.12861569926270053, Train acc: 0.9240785256410257\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1254879139109045, Train acc: 0.9263488247863247\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12444350261379171, Train acc: 0.9276620370370371\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12632637644489098, Train acc: 0.9266826923076923\n",
      "Val loss: 0.24103783071041107, Val acc: 0.894\n",
      "Epoch 83/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.11763967930251716, Train acc: 0.9324252136752137\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12325046574458098, Train acc: 0.9293536324786325\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12398899809332654, Train acc: 0.9287971866096866\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12404051090343896, Train acc: 0.9290030715811965\n",
      "Val loss: 0.25592362880706787, Val acc: 0.874\n",
      "Epoch 84/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13895693988117397, Train acc: 0.9201388888888888\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12826534857352576, Train acc: 0.9256810897435898\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12650898563810903, Train acc: 0.9272168803418803\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12648402294542035, Train acc: 0.9272335737179487\n",
      "Val loss: 0.2501644790172577, Val acc: 0.894\n",
      "Epoch 85/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.12669441059359118, Train acc: 0.9276175213675214\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12369407385460332, Train acc: 0.930221688034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12145658272538769, Train acc: 0.9316907051282052\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12218173045633185, Train acc: 0.9305889423076923\n",
      "Val loss: 0.25559720396995544, Val acc: 0.888\n",
      "Epoch 86/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1223418249661087, Train acc: 0.9293536324786325\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1215207980802426, Train acc: 0.9298544337606838\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.11974785130927365, Train acc: 0.9308226495726496\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12042508892014496, Train acc: 0.9307725694444444\n",
      "Val loss: 0.26044538617134094, Val acc: 0.876\n",
      "Epoch 87/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.12112446237578352, Train acc: 0.9292200854700855\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12092098472719519, Train acc: 0.9298544337606838\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12100275162278418, Train acc: 0.9306891025641025\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12090220015782577, Train acc: 0.9311231303418803\n",
      "Val loss: 0.2475084513425827, Val acc: 0.894\n",
      "Epoch 88/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.12456212963303949, Train acc: 0.9292200854700855\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.11935964978148794, Train acc: 0.9314569978632479\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1206593085133452, Train acc: 0.9300881410256411\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.11897983019932723, Train acc: 0.9311732104700855\n",
      "Val loss: 0.24044424295425415, Val acc: 0.892\n",
      "Epoch 89/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.11955109518817347, Train acc: 0.9288194444444444\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1203429025844631, Train acc: 0.9291199252136753\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.118237279122032, Train acc: 0.9310674857549858\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.11861863216528526, Train acc: 0.9311231303418803\n",
      "Val loss: 0.2747194468975067, Val acc: 0.88\n",
      "Epoch 90/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1264131211190142, Train acc: 0.9279513888888888\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12329820317462978, Train acc: 0.9286191239316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.11962482441439588, Train acc: 0.9315126424501424\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.11888550487784748, Train acc: 0.9317240918803419\n",
      "Val loss: 0.26039543747901917, Val acc: 0.888\n",
      "Epoch 91/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.11319076983083008, Train acc: 0.9344951923076923\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.11356868865525621, Train acc: 0.9334935897435898\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.11467960784662823, Train acc: 0.9330706908831908\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.11607944541889378, Train acc: 0.9317407852564102\n",
      "Val loss: 0.24396216869354248, Val acc: 0.884\n",
      "Early stopping at epoch 91 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 2101.1733 [s]\n",
      "Combinación 9/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5080677444099361, Train acc: 0.5350560897435898\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.48180961430582225, Train acc: 0.5566239316239316\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.468595736556583, Train acc: 0.5645032051282052\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.46058827753250414, Train acc: 0.5711972489316239\n",
      "Val loss: 0.7038306593894958, Val acc: 0.844\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.43829185229081374, Train acc: 0.5907451923076923\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.43112784929764575, Train acc: 0.5937166132478633\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4280688102089102, Train acc: 0.5954193376068376\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.42572793046123963, Train acc: 0.5971721420940171\n",
      "Val loss: 0.6857238411903381, Val acc: 0.81\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4378688437306983, Train acc: 0.5848691239316239\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4239190615649916, Train acc: 0.5970552884615384\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.42319194628642154, Train acc: 0.5982905982905983\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.42127893444819325, Train acc: 0.5991252670940171\n",
      "Val loss: 0.6899833679199219, Val acc: 0.838\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4191402991612752, Train acc: 0.5999599358974359\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.41458140555610007, Train acc: 0.6047342414529915\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4124208452694776, Train acc: 0.6061030982905983\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.4108215457099116, Train acc: 0.6067875267094017\n",
      "Val loss: 0.6739377379417419, Val acc: 0.802\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4175548089875115, Train acc: 0.6020299145299145\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4107155838073828, Train acc: 0.6073384081196581\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4069187595294072, Train acc: 0.6082175925925926\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.407822607164709, Train acc: 0.6064035790598291\n",
      "Val loss: 0.7283105254173279, Val acc: 0.776\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4085008940126142, Train acc: 0.6097756410256411\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4037231471803453, Train acc: 0.6109107905982906\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4020564187625874, Train acc: 0.6122240028490028\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.40263949054428655, Train acc: 0.6125300480769231\n",
      "Val loss: 0.7002959251403809, Val acc: 0.8\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4064328354648036, Train acc: 0.6091746794871795\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4040889184699099, Train acc: 0.6107772435897436\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4006898523735525, Train acc: 0.6116452991452992\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.40062977832097274, Train acc: 0.6126802884615384\n",
      "Val loss: 0.6795803904533386, Val acc: 0.784\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4086303405272655, Train acc: 0.6059695512820513\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.40711327750458676, Train acc: 0.6070379273504274\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4031161674407133, Train acc: 0.6109553062678063\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.40138842292830473, Train acc: 0.6128138354700855\n",
      "Val loss: 0.6888357996940613, Val acc: 0.778\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4001172735140874, Train acc: 0.6138488247863247\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.40177095955253667, Train acc: 0.6120459401709402\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39930317952082706, Train acc: 0.6134036680911681\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3988509409957462, Train acc: 0.6135650373931624\n",
      "Val loss: 0.6750493049621582, Val acc: 0.792\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4020961889853844, Train acc: 0.6116452991452992\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.39710909459326005, Train acc: 0.6154513888888888\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.396730495144499, Train acc: 0.614761396011396\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3965511988116126, Train acc: 0.6148170405982906\n",
      "Val loss: 0.7060016393661499, Val acc: 0.78\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4090523006569626, Train acc: 0.6117120726495726\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4022525151570638, Train acc: 0.6154180021367521\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39820474131494504, Train acc: 0.6165420227920227\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.39823078879943263, Train acc: 0.616653311965812\n",
      "Val loss: 0.6781085729598999, Val acc: 0.788\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.39338932485661954, Train acc: 0.6195245726495726\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3955795474541493, Train acc: 0.6192240918803419\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3938919283725597, Train acc: 0.6197248931623932\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3938935415612327, Train acc: 0.6197415865384616\n",
      "Val loss: 0.692781388759613, Val acc: 0.786\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.40486417825405413, Train acc: 0.6164529914529915\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.40107622870013243, Train acc: 0.6172208867521367\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39968141955867453, Train acc: 0.6162304131054132\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.397647862505709, Train acc: 0.617454594017094\n",
      "Val loss: 0.6929758787155151, Val acc: 0.768\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.410789371555687, Train acc: 0.6024973290598291\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.40618849513877153, Train acc: 0.6073717948717948\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.4020984493769132, Train acc: 0.6110888532763533\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3999785276559683, Train acc: 0.6122629540598291\n",
      "Val loss: 0.682485044002533, Val acc: 0.788\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4051015453460889, Train acc: 0.6077724358974359\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3971270197986538, Train acc: 0.6154180021367521\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3949408993082508, Train acc: 0.6191684472934473\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3944212523026344, Train acc: 0.6194744925213675\n",
      "Val loss: 0.6766933798789978, Val acc: 0.808\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3982105479281173, Train acc: 0.6126469017094017\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.39943085266993594, Train acc: 0.6127136752136753\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3971861701065998, Train acc: 0.6137152777777778\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3973737528436204, Train acc: 0.6141493055555556\n",
      "Val loss: 0.7006315588951111, Val acc: 0.776\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.39885267946455216, Train acc: 0.6135817307692307\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3968740021571135, Train acc: 0.6149839743589743\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3962440609592318, Train acc: 0.6176549145299145\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.39465634919639325, Train acc: 0.6181891025641025\n",
      "Val loss: 0.7040929794311523, Val acc: 0.786\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3918475172458551, Train acc: 0.6181223290598291\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3894567020937928, Train acc: 0.6199919871794872\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3903527674172339, Train acc: 0.6193242521367521\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.39046956000165045, Train acc: 0.6199919871794872\n",
      "Val loss: 0.6874759197235107, Val acc: 0.778\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.39579661941935873, Train acc: 0.6133814102564102\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3903237031056331, Train acc: 0.6165197649572649\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3880974830385627, Train acc: 0.6181223290598291\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.38893043536406297, Train acc: 0.6184061164529915\n",
      "Val loss: 0.6634635925292969, Val acc: 0.788\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4000817272398207, Train acc: 0.6153178418803419\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3915341017592667, Train acc: 0.6215277777777778\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39340053307704437, Train acc: 0.6198361823361823\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.39553792239763796, Train acc: 0.6181223290598291\n",
      "Val loss: 0.6834666132926941, Val acc: 0.79\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3885092577363691, Train acc: 0.6207264957264957\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3895093966753055, Train acc: 0.6198584401709402\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39166228740643233, Train acc: 0.6181000712250713\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.39269809488557345, Train acc: 0.6173711271367521\n",
      "Val loss: 0.6982595920562744, Val acc: 0.778\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4205250836845137, Train acc: 0.5976228632478633\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4092193841934204, Train acc: 0.6074719551282052\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.40324573876850967, Train acc: 0.6114894943019943\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3993026176069537, Train acc: 0.6150173611111112\n",
      "Val loss: 0.6910418272018433, Val acc: 0.782\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.396330895077469, Train acc: 0.6156517094017094\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3896938789094615, Train acc: 0.6223958333333334\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3922230913768127, Train acc: 0.6181668447293447\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3910921301342483, Train acc: 0.6199252136752137\n",
      "Val loss: 0.6965755224227905, Val acc: 0.788\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3958362575270172, Train acc: 0.6151842948717948\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.39155525172877514, Train acc: 0.6194577991452992\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39126332499023175, Train acc: 0.6179442663817664\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3900730720697305, Train acc: 0.6199252136752137\n",
      "Val loss: 0.6819722652435303, Val acc: 0.76\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3879396895058135, Train acc: 0.6200587606837606\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.39259492714180905, Train acc: 0.6182892628205128\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39064372044343215, Train acc: 0.6206152065527065\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.39055502261870945, Train acc: 0.6210102831196581\n",
      "Val loss: 0.6749323606491089, Val acc: 0.792\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3940591562507499, Train acc: 0.6154513888888888\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3896160202148633, Train acc: 0.6197916666666666\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39250378730969554, Train acc: 0.6188568376068376\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3908446133136749, Train acc: 0.6196915064102564\n",
      "Val loss: 0.6801697015762329, Val acc: 0.788\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.39192032100807905, Train acc: 0.6217280982905983\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3926227874735482, Train acc: 0.6206931089743589\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39224656643690886, Train acc: 0.6195690883190883\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3921751191473415, Train acc: 0.6182224893162394\n",
      "Val loss: 0.6934277415275574, Val acc: 0.79\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.4018809153483464, Train acc: 0.610176282051282\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.39596645776023215, Train acc: 0.6170205662393162\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3944613544689624, Train acc: 0.6189681267806267\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3931709977551403, Train acc: 0.6195579594017094\n",
      "Val loss: 0.6839509010314941, Val acc: 0.78\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.401674297120836, Train acc: 0.6099759615384616\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3974503836570642, Train acc: 0.6137820512820513\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3960758174246872, Train acc: 0.6134259259259259\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.39382871157593197, Train acc: 0.6173210470085471\n",
      "Val loss: 0.7124317288398743, Val acc: 0.792\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.40567605923383665, Train acc: 0.6124465811965812\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.39946989320282245, Train acc: 0.6149505876068376\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.39652558388533415, Train acc: 0.6164752492877493\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.395032352871365, Train acc: 0.6175213675213675\n",
      "Val loss: 0.7137184143066406, Val acc: 0.76\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3913598345895099, Train acc: 0.6233306623931624\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.38885203411436486, Train acc: 0.6234308226495726\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3885544081698795, Train acc: 0.6243545227920227\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3883546508020825, Train acc: 0.6234809027777778\n",
      "Val loss: 0.7340424060821533, Val acc: 0.74\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3925217456287808, Train acc: 0.6143162393162394\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3898101729205531, Train acc: 0.6185229700854701\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3886595354800211, Train acc: 0.6202590811965812\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3891983709783636, Train acc: 0.6201088408119658\n",
      "Val loss: 0.7251827716827393, Val acc: 0.754\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.39736512889209974, Train acc: 0.6100427350427351\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3903171351322761, Train acc: 0.6160523504273504\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3893895434518146, Train acc: 0.6191239316239316\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.38851796332587546, Train acc: 0.6215611645299145\n",
      "Val loss: 0.7115490436553955, Val acc: 0.768\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3944106764263577, Train acc: 0.6160523504273504\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.3883164738997435, Train acc: 0.624198717948718\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3890200763346463, Train acc: 0.6232416310541311\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3881259164494327, Train acc: 0.6236478365384616\n",
      "Val loss: 0.6689113974571228, Val acc: 0.796\n",
      "Early stopping at epoch 34 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 764.7497 [s]\n",
      "Combinación 10/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.024457873442234, Train acc: 0.4895833333333333\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.9658922392588395, Train acc: 0.5074786324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.9420443747797583, Train acc: 0.5151353276353277\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.927794988720845, Train acc: 0.5214342948717948\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.9147841435212355, Train acc: 0.5268162393162393\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.9048403920101644, Train acc: 0.5317396723646723\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8969965714674729, Train acc: 0.5358287545787546\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8899958826537825, Train acc: 0.5400974893162394\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.886916232641153, Train acc: 0.5410731244064577\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8840553556752001, Train acc: 0.5429487179487179\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.880165875699044, Train acc: 0.5448232323232324\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.876059912208818, Train acc: 0.5475204772079773\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8718643069737526, Train acc: 0.5494123931623932\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8690223729508555, Train acc: 0.5506906288156288\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8662215343567721, Train acc: 0.5520299145299146\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8647425614424751, Train acc: 0.5531350160256411\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8627292886039251, Train acc: 0.5541258169934641\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8614606002287307, Train acc: 0.5545168566001899\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.859588609560084, Train acc: 0.5551057130004499\n",
      "Val loss: 0.6077992916107178, Val acc: 0.832\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8412641913462908, Train acc: 0.5659722222222222\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8329138906083555, Train acc: 0.5707799145299145\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8280784261871946, Train acc: 0.5738960113960114\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.826982919859071, Train acc: 0.5740518162393162\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8272019510595208, Train acc: 0.5766559829059829\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8278575054770522, Train acc: 0.5773682336182336\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8259034884572757, Train acc: 0.5780677655677655\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8247701968902197, Train acc: 0.5785256410256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8233710574967908, Train acc: 0.5787927350427351\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8245736165433868, Train acc: 0.5784455128205128\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8246259562896959, Train acc: 0.5773115773115773\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8249748110176831, Train acc: 0.5762775997150997\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8249127393800283, Train acc: 0.5755670611439843\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.824290986026163, Train acc: 0.5756639194139194\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8241994724314436, Train acc: 0.5759793447293448\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8244281696458148, Train acc: 0.5755876068376068\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8244960972146811, Train acc: 0.5755404725992961\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.823997439652087, Train acc: 0.5753650284900285\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8238856299096167, Train acc: 0.5759531039136302\n",
      "Val loss: 0.6046698689460754, Val acc: 0.834\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8080631458861196, Train acc: 0.5718482905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8168152896766989, Train acc: 0.5702457264957265\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8213733658831344, Train acc: 0.5711360398860399\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8235196148992604, Train acc: 0.5696447649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8235866671953446, Train acc: 0.569818376068376\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8228060313609251, Train acc: 0.5722489316239316\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8212730065079108, Train acc: 0.574557387057387\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8232076735450671, Train acc: 0.5729166666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8233329322036967, Train acc: 0.5724418328584995\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8232421393068428, Train acc: 0.5720886752136752\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8228280144137936, Train acc: 0.5723096348096348\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8223709274730792, Train acc: 0.5719373219373219\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8220941737872217, Train acc: 0.5728550295857988\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8196586322435092, Train acc: 0.5741758241758241\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8190684531488989, Train acc: 0.5748397435897435\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8195461158951124, Train acc: 0.5741853632478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8196663534659246, Train acc: 0.5746292106586224\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8192395835067698, Train acc: 0.5751276115859449\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8186360145012002, Train acc: 0.5755735492577598\n",
      "Val loss: 0.5576456785202026, Val acc: 0.828\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8101969972634927, Train acc: 0.5803952991452992\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8093365290735521, Train acc: 0.5822649572649573\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8079982559565465, Train acc: 0.5820868945868946\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8107806743464918, Train acc: 0.5810630341880342\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8121095799992227, Train acc: 0.5815705128205129\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8083515246199746, Train acc: 0.5819088319088319\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8079939140359416, Train acc: 0.58367673992674\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8100963601699243, Train acc: 0.5822315705128205\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8100516004213694, Train acc: 0.5827694681861348\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8096508839191534, Train acc: 0.5822382478632478\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8093032673429683, Train acc: 0.5823135198135199\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8119128807487651, Train acc: 0.5809517450142451\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8116711467053842, Train acc: 0.5803131163708086\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8112855836003109, Train acc: 0.5804716117216118\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8111940149568085, Train acc: 0.580787037037037\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8115738242482528, Train acc: 0.5801282051282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8125020101059616, Train acc: 0.5797668426344897\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8114135629883972, Train acc: 0.580321106362773\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8126193556869239, Train acc: 0.5787365047233468\n",
      "Val loss: 0.6102101802825928, Val acc: 0.82\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8320009504627978, Train acc: 0.561698717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8156131131526752, Train acc: 0.5721153846153846\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8156216663342936, Train acc: 0.5740740740740741\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8160456448920772, Train acc: 0.5767227564102564\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8175466144696261, Train acc: 0.57633547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8172142486541699, Train acc: 0.5769675925925926\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8160927479447608, Train acc: 0.5779914529914529\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8143101059791878, Train acc: 0.5791933760683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8131232813269551, Train acc: 0.5802172364672364\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8128252007767686, Train acc: 0.5794070512820513\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8157923915586272, Train acc: 0.5774087024087025\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8165403007824197, Train acc: 0.5767004985754985\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8155391742178361, Train acc: 0.5772723537146615\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8150328648104918, Train acc: 0.5777052808302808\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8148200867182849, Train acc: 0.5775462962962963\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8152257918865762, Train acc: 0.5774071848290598\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8161015969955364, Train acc: 0.5766245600804424\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8155264759335423, Train acc: 0.5762998575498576\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8149161312931892, Train acc: 0.5766559829059829\n",
      "Val loss: 0.6333458423614502, Val acc: 0.804\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8078854206280831, Train acc: 0.5889423076923077\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8097578495995611, Train acc: 0.5850694444444444\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8160059436773642, Train acc: 0.5805733618233618\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8213696128282791, Train acc: 0.5767895299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8213142932989659, Train acc: 0.575\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8154497608499989, Train acc: 0.5771011396011396\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8148706259162726, Train acc: 0.5768467643467643\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.813340812539443, Train acc: 0.5791599893162394\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8129050579958605, Train acc: 0.5805436847103513\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8121787787757369, Train acc: 0.5819711538461538\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8123988937340509, Train acc: 0.5807595182595182\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8112037585374297, Train acc: 0.5815304487179487\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8119549759119299, Train acc: 0.5811349441157133\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8115975660282176, Train acc: 0.5809867216117216\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8124672778990873, Train acc: 0.5799501424501424\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8115739969170501, Train acc: 0.5802116720085471\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8126145344454179, Train acc: 0.5794997486173957\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8128559026170207, Train acc: 0.5793120845204178\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8132663985066622, Train acc: 0.5787505623031939\n",
      "Val loss: 0.6184272766113281, Val acc: 0.83\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.7974924597984705, Train acc: 0.5806623931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8022802701363196, Train acc: 0.577590811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8036216490628713, Train acc: 0.5819088319088319\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8046881238110045, Train acc: 0.5807959401709402\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8038547807269626, Train acc: 0.58125\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8027211180609516, Train acc: 0.58244301994302\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8064317063534217, Train acc: 0.57997557997558\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8086867403143492, Train acc: 0.5793936965811965\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.809354647102519, Train acc: 0.5800688509021842\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8087555562329088, Train acc: 0.5814903846153846\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.809569871138489, Train acc: 0.5803467365967366\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8092763748264041, Train acc: 0.5802394943019943\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8089143988100186, Train acc: 0.579922748191979\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8103764598258977, Train acc: 0.579441391941392\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8095995001303844, Train acc: 0.5796296296296296\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8089370374114085, Train acc: 0.5798944978632479\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8092680893812184, Train acc: 0.579719708396179\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8088881782090676, Train acc: 0.5797424026590693\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8078550453315808, Train acc: 0.5802266081871345\n",
      "Val loss: 0.6010120511054993, Val acc: 0.81\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8164274132149851, Train acc: 0.5710470085470085\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8082708558465681, Train acc: 0.5762553418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8096553895548199, Train acc: 0.5759437321937322\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8088092845983994, Train acc: 0.5774572649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.813055783459264, Train acc: 0.576068376068376\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8127943838593626, Train acc: 0.5762553418803419\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8113572970851437, Train acc: 0.5782203907203908\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8112085252745539, Train acc: 0.5790932158119658\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8107554083309735, Train acc: 0.5798314339981007\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8110817090058938, Train acc: 0.5802350427350428\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.809673998415146, Train acc: 0.5819493006993007\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.808664791179858, Train acc: 0.5821536680911681\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8080867164860738, Train acc: 0.5825115055884287\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8080226134904575, Train acc: 0.582360347985348\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8083575910143024, Train acc: 0.5817663817663817\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8087694812884443, Train acc: 0.5814636752136753\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.808841489543862, Train acc: 0.5820135746606335\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8087792460084074, Train acc: 0.5820572174738842\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8094097247168763, Train acc: 0.5810841205578048\n",
      "Val loss: 0.6022470593452454, Val acc: 0.804\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8371266396636636, Train acc: 0.5638354700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8176239764588511, Train acc: 0.5738514957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8205392331818909, Train acc: 0.5718482905982906\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.821324140088171, Train acc: 0.5714476495726496\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8189615768245143, Train acc: 0.5723290598290598\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8147011254927372, Train acc: 0.5754540598290598\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.816354101539677, Train acc: 0.574557387057387\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8174432579778198, Train acc: 0.5752537393162394\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8148082362280952, Train acc: 0.5776353276353277\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8130736218048976, Train acc: 0.5787660256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8139995299945437, Train acc: 0.5792055167055167\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8139388194026431, Train acc: 0.5791266025641025\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8145415276622082, Train acc: 0.5788954635108481\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8143680227035713, Train acc: 0.5783730158730159\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8136668573417555, Train acc: 0.5784366096866097\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8138041994892634, Train acc: 0.5783587072649573\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8137559283912451, Train acc: 0.5788084464555053\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8141413282068819, Train acc: 0.5787333808167141\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8126659809711867, Train acc: 0.5794253261358524\n",
      "Val loss: 0.5807716846466064, Val acc: 0.824\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8103100942750262, Train acc: 0.5817307692307693\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8094374351521842, Train acc: 0.5842681623931624\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8111406934906614, Train acc: 0.5799501424501424\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8087523231903712, Train acc: 0.5834668803418803\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8114408573533735, Train acc: 0.5807692307692308\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8110959548556227, Train acc: 0.5813301282051282\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8109318144592174, Train acc: 0.5815781440781441\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8115890725937664, Train acc: 0.5818309294871795\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8107360156513007, Train acc: 0.5817901234567902\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8135854247288826, Train acc: 0.5798878205128205\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8138269257341695, Train acc: 0.5799096736596736\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8123558808510799, Train acc: 0.5811298076923077\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8114481250131546, Train acc: 0.5813404010519395\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.813385702096499, Train acc: 0.5798611111111112\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8122520461041703, Train acc: 0.5803418803418804\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8128232836532288, Train acc: 0.580078125\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.812441023706251, Train acc: 0.5803638763197587\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8129004290980152, Train acc: 0.5799798195631529\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8119267726797164, Train acc: 0.5804937022042285\n",
      "Val loss: 0.6043728590011597, Val acc: 0.808\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8049509346994579, Train acc: 0.5777243589743589\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8087276511976862, Train acc: 0.5810630341880342\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8073867852361793, Train acc: 0.5817307692307693\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.805066662784825, Train acc: 0.5831330128205128\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8071027056274251, Train acc: 0.5814102564102565\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8090215575406354, Train acc: 0.5802617521367521\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.807982628344034, Train acc: 0.5805860805860806\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8103886440587349, Train acc: 0.5788928952991453\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8096041128336534, Train acc: 0.5797127255460589\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8100702739424176, Train acc: 0.580181623931624\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8097122668432652, Train acc: 0.5800310800310801\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8113342152319403, Train acc: 0.5793936965811965\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8096897681632534, Train acc: 0.5809089414858646\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.819521101293983, Train acc: 0.5806814713064713\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8283059581052883, Train acc: 0.5789351851851852\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8274742895339289, Train acc: 0.5783754006410257\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8274772931367923, Train acc: 0.578384238310709\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8273206427339588, Train acc: 0.5778282288698955\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8266522331222778, Train acc: 0.5772323436797121\n",
      "Val loss: 0.6051848530769348, Val acc: 0.828\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.807435062705961, Train acc: 0.5897435897435898\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8152433598143423, Train acc: 0.5846688034188035\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8165177983096522, Train acc: 0.5810185185185185\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8155477789477406, Train acc: 0.5795272435897436\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.817033545481853, Train acc: 0.5792200854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8127258086306417, Train acc: 0.5802617521367521\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8122428869589781, Train acc: 0.5804334554334555\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8116125281677287, Train acc: 0.5805622329059829\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8107174975127123, Train acc: 0.5802469135802469\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8107231215534048, Train acc: 0.5806089743589744\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8123013096070605, Train acc: 0.5798125485625486\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8125217738654199, Train acc: 0.5794382122507122\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.813644855685488, Train acc: 0.57836127547666\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8135540214286473, Train acc: 0.5780296092796092\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8130207172825805, Train acc: 0.5784722222222223\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8137572395304838, Train acc: 0.5780415331196581\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.813413881818034, Train acc: 0.5779443187531422\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8130355317142048, Train acc: 0.5784069325735992\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8129497266974044, Train acc: 0.5781179712100765\n",
      "Val loss: 0.6138820052146912, Val acc: 0.842\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8202318044809195, Train acc: 0.5774572649572649\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8071129403562627, Train acc: 0.5854700854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8096226567556376, Train acc: 0.5836894586894587\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.807387670645347, Train acc: 0.5824652777777778\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8042315202391046, Train acc: 0.5850961538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8019114473351726, Train acc: 0.5867610398860399\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8027358292368888, Train acc: 0.5847069597069597\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8030268576664802, Train acc: 0.5844017094017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8043277948557028, Train acc: 0.5838675213675214\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8052372099497379, Train acc: 0.583173076923077\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8050305830848801, Train acc: 0.5834790209790209\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8043239427427961, Train acc: 0.5844239672364673\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8042890936978784, Train acc: 0.5849769888231426\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8053028979435363, Train acc: 0.5845543345543346\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8065439905196513, Train acc: 0.5838319088319088\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8080988554681978, Train acc: 0.5829827724358975\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8068330026020532, Train acc: 0.5835375816993464\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8062595009294331, Train acc: 0.5833481718898386\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8067648536816836, Train acc: 0.582981893837157\n",
      "Val loss: 0.6031261682510376, Val acc: 0.816\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8124705812869928, Train acc: 0.5811965811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8169629776324981, Train acc: 0.5817307692307693\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.809507610366555, Train acc: 0.5869836182336182\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8078741012857511, Train acc: 0.586471688034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8098287870231856, Train acc: 0.5849358974358975\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8108843280124528, Train acc: 0.5834668803418803\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8096404271029727, Train acc: 0.583409645909646\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8099560484481163, Train acc: 0.5826989850427351\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8124092900232259, Train acc: 0.5812559354226021\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.81199625035127, Train acc: 0.5814903846153846\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8106609195957095, Train acc: 0.5817550505050505\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8104397495969747, Train acc: 0.5826655982905983\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8109250439773023, Train acc: 0.582223865877712\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8125060204693977, Train acc: 0.5810057997557998\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8131256847979337, Train acc: 0.5807514245014245\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.812810533767582, Train acc: 0.5807959401709402\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8127061941924558, Train acc: 0.5807723730517849\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8123087981204588, Train acc: 0.5810778727445394\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8127374388160226, Train acc: 0.5811684660368871\n",
      "Val loss: 0.6286032795906067, Val acc: 0.818\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.817035014812763, Train acc: 0.5868055555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8110681680532602, Train acc: 0.5848023504273504\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8077095035474184, Train acc: 0.582977207977208\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8097639130985635, Train acc: 0.5787259615384616\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8065483578249939, Train acc: 0.5787927350427351\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8076714586328577, Train acc: 0.5805733618233618\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8068421834554428, Train acc: 0.5810439560439561\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8093922985797255, Train acc: 0.5803285256410257\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8095442343075164, Train acc: 0.5806920702754036\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.809942578454303, Train acc: 0.5796207264957265\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8097736188026019, Train acc: 0.5797882672882673\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.809974807152721, Train acc: 0.5790153133903134\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.809786941466717, Train acc: 0.5792652859960552\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8110364584368227, Train acc: 0.5785065628815629\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8105525460847762, Train acc: 0.5788995726495727\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8112772568328004, Train acc: 0.5787593482905983\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8098744152028576, Train acc: 0.5796097285067874\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8117736179632792, Train acc: 0.5782140313390314\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8131529747689034, Train acc: 0.5775275528565003\n",
      "Val loss: 0.6042333245277405, Val acc: 0.78\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8410557584884839, Train acc: 0.5681089743589743\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8252903910783621, Train acc: 0.5755876068376068\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8194523582770954, Train acc: 0.5776353276353277\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8189106938930658, Train acc: 0.577590811965812\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.814431793975015, Train acc: 0.5793803418803419\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8108017788483546, Train acc: 0.5814191595441596\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8096449153909462, Train acc: 0.582226800976801\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8102866260923891, Train acc: 0.582565438034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8099424815811889, Train acc: 0.5814339981006648\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8108740125965869, Train acc: 0.5797275641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8103580969937939, Train acc: 0.5801282051282052\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8130591195448171, Train acc: 0.5793269230769231\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8134042601456852, Train acc: 0.5786489151873767\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8133997110074548, Train acc: 0.5786782661782662\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8127287669059557, Train acc: 0.5790954415954416\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8113500159393009, Train acc: 0.5798444177350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8107428633729197, Train acc: 0.5801753393665159\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8109059656730517, Train acc: 0.5803656220322887\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8104695132654957, Train acc: 0.5806623931623932\n",
      "Val loss: 0.5989747047424316, Val acc: 0.814\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.82710650181159, Train acc: 0.5611645299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8305190618221576, Train acc: 0.5679754273504274\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8250175991289297, Train acc: 0.5710470085470085\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8185554230824496, Train acc: 0.5755208333333334\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8150990837659591, Train acc: 0.5765491452991452\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8142835082661393, Train acc: 0.5775462962962963\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8155379929239788, Train acc: 0.5767322954822954\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8166065292480664, Train acc: 0.5758213141025641\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8171348819705496, Train acc: 0.5762405033238367\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8171852085834894, Train acc: 0.575667735042735\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8165460633925903, Train acc: 0.5753690753690753\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.8154113061959587, Train acc: 0.5766114672364673\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8174062703762456, Train acc: 0.576060157790927\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.8161209586812171, Train acc: 0.5760454822954822\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8163578973366664, Train acc: 0.5763710826210826\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8164075376927598, Train acc: 0.5764556623931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.8162083832416899, Train acc: 0.5770801910507792\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8159183109672661, Train acc: 0.5772643637226971\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8167743021257815, Train acc: 0.5765575798470536\n",
      "Val loss: 0.5863754749298096, Val acc: 0.824\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8122237895289038, Train acc: 0.5777243589743589\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8010240132227923, Train acc: 0.5814636752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8006180647941057, Train acc: 0.5803952991452992\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.802727448570932, Train acc: 0.5817307692307693\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8067784191706242, Train acc: 0.5805555555555556\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8048850331890617, Train acc: 0.5814636752136753\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.8037078715724386, Train acc: 0.5813492063492064\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8047853250725147, Train acc: 0.5805622329059829\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8059972275865723, Train acc: 0.5796236942070275\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.8050429558397358, Train acc: 0.5800480769230769\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.8058112449625618, Train acc: 0.5810266122766122\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.806785865026152, Train acc: 0.5811298076923077\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.8079655632660783, Train acc: 0.580991124260355\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.807553915222777, Train acc: 0.5815209096459096\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.8066939111278947, Train acc: 0.5816773504273505\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.8057823370123266, Train acc: 0.5816306089743589\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.806785488455304, Train acc: 0.5814008295625943\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.8065617388894415, Train acc: 0.5816417378917379\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.8064868748724273, Train acc: 0.581941632928475\n",
      "Val loss: 0.5934489369392395, Val acc: 0.824\n",
      "Early stopping at epoch 18 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 504.7421 [s]\n",
      "Combinación 11/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 1.3239034709767399, Train acc: 0.18776709401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 1.315206522615547, Train acc: 0.20873397435897437\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 1.2973314822569193, Train acc: 0.2285434472934473\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 1.284015736518762, Train acc: 0.24873130341880342\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 1.2717767405713725, Train acc: 0.2652777777777778\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 1.256854426487219, Train acc: 0.2831196581196581\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 1.2419880646052377, Train acc: 0.29914529914529914\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 1.2260219803095882, Train acc: 0.3146701388888889\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 1.210291211421673, Train acc: 0.32956433998100665\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 1.1941231646598913, Train acc: 0.3438301282051282\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 1.1773647750730003, Train acc: 0.3578331390831391\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 1.1618719078027284, Train acc: 0.3696581196581197\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 1.1461736991717422, Train acc: 0.3816568047337278\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 1.1325304731635675, Train acc: 0.39203678266178266\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 1.1195053235757724, Train acc: 0.4011930199430199\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 1.1057080921326947, Train acc: 0.4109742254273504\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 1.0925373752372478, Train acc: 0.42007604323780795\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 1.081147447072769, Train acc: 0.42752849002849\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 1.0701310617721032, Train acc: 0.4351102114260009\n",
      "Val loss: 0.8279204368591309, Val acc: 0.672\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.8422605787586962, Train acc: 0.5953525641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8375400824424548, Train acc: 0.6002938034188035\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.827281907922522, Train acc: 0.6040776353276354\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.8251236612216021, Train acc: 0.6043002136752137\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.8206537221232031, Train acc: 0.6064636752136752\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.8164184135079724, Train acc: 0.6091524216524217\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.813251293113089, Train acc: 0.6089743589743589\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.8080030554889613, Train acc: 0.6123464209401709\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.8023624194313658, Train acc: 0.6163046058879392\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.7995601837451641, Train acc: 0.6180021367521368\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.7947919463222122, Train acc: 0.6208964646464646\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.7892719451603387, Train acc: 0.6241319444444444\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.7849459098719359, Train acc: 0.6260889217619987\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.7793880957969088, Train acc: 0.6291781135531136\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.7752169665787635, Train acc: 0.6313212250712251\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.7707678038531389, Train acc: 0.6332799145299145\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.7672301411568789, Train acc: 0.6348039215686274\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.7630772555683866, Train acc: 0.6372418091168092\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.7594991971579855, Train acc: 0.6390435222672065\n",
      "Val loss: 0.6710917949676514, Val acc: 0.73\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6772658822373447, Train acc: 0.6749465811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6667349343625908, Train acc: 0.6861645299145299\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.665141256701233, Train acc: 0.6879451566951567\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.6634528893563483, Train acc: 0.6897702991452992\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6659417897717566, Train acc: 0.690758547008547\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.6649728325697092, Train acc: 0.6911502849002849\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.6628714034991096, Train acc: 0.6926510989010989\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6613787824654171, Train acc: 0.6928084935897436\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6595689721709755, Train acc: 0.693880579297246\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6572294455562901, Train acc: 0.6951923076923077\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6552724507351485, Train acc: 0.6964840714840714\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6550205224472233, Train acc: 0.6964253917378918\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.652776865466185, Train acc: 0.6973413872452334\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.650735584953038, Train acc: 0.6984126984126984\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6485430754487671, Train acc: 0.6991809116809117\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.6468853559185807, Train acc: 0.7001368856837606\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.6454939993558186, Train acc: 0.7007918552036199\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.645025336900894, Train acc: 0.700661799620133\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.6436230007775239, Train acc: 0.7012201979307242\n",
      "Val loss: 0.5618382096290588, Val acc: 0.782\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.6010508651916797, Train acc: 0.7283653846153846\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.6058829391104543, Train acc: 0.7191506410256411\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.601528178337972, Train acc: 0.7240918803418803\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5995556938215199, Train acc: 0.7252270299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.6011275843677357, Train acc: 0.7256410256410256\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5996583251871614, Train acc: 0.7252938034188035\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5974751520215082, Train acc: 0.7261523199023199\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5945942154170102, Train acc: 0.7279313568376068\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5948963534911247, Train acc: 0.7281576448243114\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5933088686221685, Train acc: 0.7291666666666666\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5925839432654866, Train acc: 0.729458041958042\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5918126681718732, Train acc: 0.7296118233618234\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5907069212468653, Train acc: 0.7294132149901381\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5896734933167587, Train acc: 0.7298534798534798\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5884438857563541, Train acc: 0.7303062678062678\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5878846465299526, Train acc: 0.7305188301282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5858104626487523, Train acc: 0.7315076671694318\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5847260085096386, Train acc: 0.7315111585944919\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5820519952853414, Train acc: 0.7331168466036887\n",
      "Val loss: 0.5038378238677979, Val acc: 0.782\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5420304287193168, Train acc: 0.7521367521367521\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5445635148093232, Train acc: 0.7510683760683761\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5485783922366607, Train acc: 0.7458155270655271\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5431217700879798, Train acc: 0.7506677350427351\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5455659463874295, Train acc: 0.7485042735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5473256922005928, Train acc: 0.7475071225071225\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5455722828487773, Train acc: 0.7490460927960928\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5451092507339951, Train acc: 0.7493990384615384\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5439631252886563, Train acc: 0.7499406457739791\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5420168308366058, Train acc: 0.751522435897436\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5427465870015457, Train acc: 0.7512626262626263\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.5419679273906935, Train acc: 0.7523815883190883\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.540563028553142, Train acc: 0.752732577251808\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5414362009228076, Train acc: 0.7521939865689866\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5403323298845536, Train acc: 0.7528133903133903\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5389543835742351, Train acc: 0.7534388354700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5387597789471925, Train acc: 0.7538021618903972\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5380502206336173, Train acc: 0.7540212488129154\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5365830862034134, Train acc: 0.75472334682861\n",
      "Val loss: 0.4849218428134918, Val acc: 0.784\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.5079723344399378, Train acc: 0.7660256410256411\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.5139961724097912, Train acc: 0.7640224358974359\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.5118184145699199, Train acc: 0.7637108262108262\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.5182046024208395, Train acc: 0.7617521367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.5192085940104264, Train acc: 0.7602564102564102\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.5185438157656254, Train acc: 0.7592147435897436\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.5172329884891254, Train acc: 0.75873778998779\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.5154428582988743, Train acc: 0.7593149038461539\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.5138042036794189, Train acc: 0.7601792497625831\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.5116000224891891, Train acc: 0.7625\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.5113343886608473, Train acc: 0.7634518259518259\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.51028840709743, Train acc: 0.7649572649572649\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.5097672866901075, Train acc: 0.7652654503616042\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.5098362080434449, Train acc: 0.7653960622710623\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.5097830927185184, Train acc: 0.7656695156695157\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5094089296081254, Train acc: 0.7657084668803419\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5089903544951109, Train acc: 0.7656171442936148\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5085641938099834, Train acc: 0.7661740265906932\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5075170131776443, Train acc: 0.7670799595141701\n",
      "Val loss: 0.4390689730644226, Val acc: 0.804\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4862744533098661, Train acc: 0.7905982905982906\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4889676866368351, Train acc: 0.7788461538461539\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.490001579380443, Train acc: 0.7799145299145299\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4891194840335948, Train acc: 0.7803151709401709\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4897118768376163, Train acc: 0.780715811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.49086654107709893, Train acc: 0.7791577635327636\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4892171705198521, Train acc: 0.7802579365079365\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4869341197081356, Train acc: 0.7815504807692307\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.48503465288215214, Train acc: 0.7824964387464387\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4845939849431698, Train acc: 0.7815705128205128\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.48514469816597655, Train acc: 0.7812014374514374\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.4834409366836745, Train acc: 0.7818732193732194\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.48399383087325926, Train acc: 0.7819690992767916\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.485125617457324, Train acc: 0.7817078754578755\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4848578727907605, Train acc: 0.7818910256410256\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.4838887909825286, Train acc: 0.7823517628205128\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4822317416296945, Train acc: 0.7828682755153343\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4817901173180211, Train acc: 0.7830751424501424\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.48187713906189045, Train acc: 0.7827963337831759\n",
      "Val loss: 0.432991623878479, Val acc: 0.822\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4633010804143726, Train acc: 0.7825854700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.46592607877702796, Train acc: 0.7873931623931624\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4605261284401614, Train acc: 0.7889957264957265\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4603277453754702, Train acc: 0.7895299145299145\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.46093321620908556, Train acc: 0.7892628205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.46059954593069535, Train acc: 0.7896634615384616\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.46164514468266415, Train acc: 0.7888431013431013\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.46330014708587247, Train acc: 0.788528311965812\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.46283128437323445, Train acc: 0.7885802469135802\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.46336803233776336, Train acc: 0.7883547008547008\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.46535617283313147, Train acc: 0.78753885003885\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.46652148883694255, Train acc: 0.7870815527065527\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.46646208370460795, Train acc: 0.7871055226824457\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.4661540638778236, Train acc: 0.7877174908424909\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4661983665068265, Train acc: 0.7879451566951567\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.465399413361636, Train acc: 0.7880442040598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4646133661884468, Train acc: 0.7885400955253896\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.46432330156521695, Train acc: 0.7889215337132004\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.4640151874752555, Train acc: 0.7891081871345029\n",
      "Val loss: 0.4003196656703949, Val acc: 0.824\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.45446916166533774, Train acc: 0.7908653846153846\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4471592100130187, Train acc: 0.7944711538461539\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.44723410874690084, Train acc: 0.7942485754985755\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.44694171177271086, Train acc: 0.7932024572649573\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4458915224696836, Train acc: 0.7950320512820512\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4466159637921896, Train acc: 0.7953614672364673\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4473633235627478, Train acc: 0.7957493894993894\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4488626109261034, Train acc: 0.7949385683760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.44893415416577603, Train acc: 0.7951388888888888\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4479832665787803, Train acc: 0.7958600427350427\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4487178045112955, Train acc: 0.7956487956487956\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.45017472809536163, Train acc: 0.7950053418803419\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.45022261605146763, Train acc: 0.7950977975016437\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.44960582800327786, Train acc: 0.7947000915750916\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.44952696493887834, Train acc: 0.7950320512820512\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.44995963450871473, Train acc: 0.7945713141025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4493087312649578, Train acc: 0.7951860231271995\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.448691101544886, Train acc: 0.7954950142450142\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.44821545431408, Train acc: 0.7953778677462888\n",
      "Val loss: 0.39256349205970764, Val acc: 0.83\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4481989090513979, Train acc: 0.7932692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4404075391526915, Train acc: 0.8000801282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4442246435289709, Train acc: 0.7997685185185185\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4447583101817176, Train acc: 0.7992120726495726\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.44355858561829625, Train acc: 0.7990384615384616\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.4424572531399224, Train acc: 0.7995459401709402\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4409262369840573, Train acc: 0.7994123931623932\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4401858639067564, Train acc: 0.8003138354700855\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4400858144927681, Train acc: 0.8003027065527065\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.43996953638190894, Train acc: 0.8009081196581197\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.43684931225617357, Train acc: 0.8025932400932401\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.43562083669070506, Train acc: 0.8032407407407407\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.43639866336641303, Train acc: 0.8024531558185405\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.43463581294658277, Train acc: 0.8032852564102564\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.43456200660973193, Train acc: 0.8034544159544159\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.4342311166354224, Train acc: 0.8037359775641025\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.4341896949490451, Train acc: 0.8038115887380594\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4336079434055662, Train acc: 0.8043536324786325\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.43292979042083896, Train acc: 0.8046980431848852\n",
      "Val loss: 0.3907594084739685, Val acc: 0.838\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4204508718262371, Train acc: 0.8183760683760684\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.4344793033396077, Train acc: 0.8078258547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4306253128581577, Train acc: 0.8087606837606838\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4250609770926655, Train acc: 0.8102964743589743\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.42556334726321393, Train acc: 0.8092948717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.42435981505192244, Train acc: 0.8102297008547008\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.4223780801954141, Train acc: 0.8115079365079365\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.42180451285890025, Train acc: 0.8107972756410257\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4235965485303377, Train acc: 0.8101258309591642\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4237558677028387, Train acc: 0.8100961538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4231759730405185, Train acc: 0.809489121989122\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.4239855879477286, Train acc: 0.8096509971509972\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.42219324512829676, Train acc: 0.8104454306377383\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.42224518165119695, Train acc: 0.8103441697191697\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.42119505621938624, Train acc: 0.8106837606837607\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.4212502199583329, Train acc: 0.8105134882478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.42162754254433427, Train acc: 0.8104889391654098\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4212808591461363, Train acc: 0.8106748575498576\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.42017787938256335, Train acc: 0.8114035087719298\n",
      "Val loss: 0.3604353368282318, Val acc: 0.86\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4116721000426855, Train acc: 0.8159722222222222\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.414126010850454, Train acc: 0.8126335470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.40870853044368605, Train acc: 0.8126780626780626\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.4085864334916457, Train acc: 0.8139022435897436\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.40983730274387914, Train acc: 0.8140491452991453\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.413437568330527, Train acc: 0.8129896723646723\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.41424209456231276, Train acc: 0.8131868131868132\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4139362507237074, Train acc: 0.8130008012820513\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.4143811132040685, Train acc: 0.813301282051282\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.41335261976107573, Train acc: 0.8134615384615385\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.41235343805096736, Train acc: 0.8139325951825952\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.41229617236940946, Train acc: 0.8138577279202279\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4123579677412339, Train acc: 0.8140614727153189\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.41349534552103145, Train acc: 0.8137210012210012\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4129392612184215, Train acc: 0.813764245014245\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.4121110044881447, Train acc: 0.8144698183760684\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.41091638014597054, Train acc: 0.8148881347410759\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.41003964252677966, Train acc: 0.8152896486229819\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.4091430260115682, Train acc: 0.8154520917678812\n",
      "Val loss: 0.3521278202533722, Val acc: 0.846\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.38637722315441847, Train acc: 0.8242521367521367\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3964102666856896, Train acc: 0.8227831196581197\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.4031813544170809, Train acc: 0.8198896011396012\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.40390117078000665, Train acc: 0.8205128205128205\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.4046678606516276, Train acc: 0.8200320512820513\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.40202961675292065, Train acc: 0.8200231481481481\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.40275231454980825, Train acc: 0.8191010378510378\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.40277938993695456, Train acc: 0.8191773504273504\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.402019099377499, Train acc: 0.8190586419753086\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.40312296776180595, Train acc: 0.8192040598290599\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.40275346490655095, Train acc: 0.8192987567987567\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.40155598784443997, Train acc: 0.8202457264957265\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4012219876854299, Train acc: 0.821026462853386\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.4006213557170715, Train acc: 0.8212187118437119\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.4012682553709742, Train acc: 0.8204594017094017\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.40054329502213204, Train acc: 0.8210803952991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.40086923377066774, Train acc: 0.8209527400703871\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.4004076944216591, Train acc: 0.821210232668566\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.39930406381726746, Train acc: 0.8217920602789024\n",
      "Val loss: 0.33613333106040955, Val acc: 0.87\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3841811030084251, Train acc: 0.8250534188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.38562922424867624, Train acc: 0.8259882478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.386399476765058, Train acc: 0.8268340455840456\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3949847745621561, Train acc: 0.8235176282051282\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3991604349934138, Train acc: 0.8207264957264957\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3970699601002738, Train acc: 0.8214476495726496\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.39477430918095896, Train acc: 0.8229929792429792\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.39436210348851913, Train acc: 0.8240852029914529\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3935029586142058, Train acc: 0.8247269705603039\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.39279222536163455, Train acc: 0.8251869658119658\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3906980590444715, Train acc: 0.8265588578088578\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.39258248824360875, Train acc: 0.8262108262108262\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.39179598305920565, Train acc: 0.8264505259697568\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.39177226061877024, Train acc: 0.8269230769230769\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.39184159962115467, Train acc: 0.8271367521367521\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.39112175112335473, Train acc: 0.8272569444444444\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.39105581111602894, Train acc: 0.8270959024635495\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.39093753311638024, Train acc: 0.8272050094966762\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.38999791268730183, Train acc: 0.8279071075123706\n",
      "Val loss: 0.33451953530311584, Val acc: 0.872\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.38781152500046623, Train acc: 0.8338675213675214\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.37896292217266864, Train acc: 0.8341346153846154\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3790544831430131, Train acc: 0.8347578347578347\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3810819841793969, Train acc: 0.8325320512820513\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3827390904100532, Train acc: 0.83125\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.38330078982559707, Train acc: 0.8307514245014245\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.38583554413001325, Train acc: 0.8298992673992674\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3827490645150344, Train acc: 0.8316639957264957\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.381668995333533, Train acc: 0.8326507597340931\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.38132273170173675, Train acc: 0.8329594017094017\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3831911551322507, Train acc: 0.8322892385392385\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.38280181491752807, Train acc: 0.8323762464387464\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3813075048752242, Train acc: 0.8327375082182774\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.38217233646756565, Train acc: 0.8317689255189256\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.38148928637857793, Train acc: 0.8319266381766381\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3819569811686619, Train acc: 0.8317140758547008\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.38195881010780025, Train acc: 0.8319507290095526\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.381605912930379, Train acc: 0.8320868945868946\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.37968746188976077, Train acc: 0.8326023391812866\n",
      "Val loss: 0.32695138454437256, Val acc: 0.874\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3661546586160986, Train acc: 0.8384081196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36792857855813116, Train acc: 0.8360042735042735\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.37494904044856375, Train acc: 0.8336894586894587\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3747090970476468, Train acc: 0.8349358974358975\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.37322378703671644, Train acc: 0.8358974358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.36947517381434425, Train acc: 0.8377403846153846\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.36914259026120433, Train acc: 0.8379120879120879\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3708321998675919, Train acc: 0.8366720085470085\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3718102707369262, Train acc: 0.8360636277302944\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.37316430858057786, Train acc: 0.8348290598290599\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3729978529121307, Train acc: 0.8351544289044289\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.37291057550075046, Train acc: 0.835403311965812\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.37231888045495, Train acc: 0.8356549967126891\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3722453973596058, Train acc: 0.8354319291819292\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.372328870588558, Train acc: 0.8352207977207977\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.37143658909102917, Train acc: 0.8357204861111112\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.37085090393304465, Train acc: 0.836035696329814\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.37033884588837734, Train acc: 0.8359597578347578\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.37027145522674676, Train acc: 0.8359761583445794\n",
      "Val loss: 0.3201717436313629, Val acc: 0.872\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.37252559608373886, Train acc: 0.8416132478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.36273383502012646, Train acc: 0.8446848290598291\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3646550318199685, Train acc: 0.8422364672364673\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.36687654263188696, Train acc: 0.8418135683760684\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3653936787548228, Train acc: 0.8405448717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3651143118441954, Train acc: 0.8396545584045584\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3664389545955355, Train acc: 0.83871336996337\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3661280592314453, Train acc: 0.8385416666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3675490073215814, Train acc: 0.8380519943019943\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.366395269608141, Train acc: 0.8387553418803418\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.36557915946866715, Train acc: 0.8390151515151515\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3658342882619369, Train acc: 0.8386974715099715\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3644406019436302, Train acc: 0.839620315581854\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36332788198767857, Train acc: 0.8400106837606838\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3636744202754097, Train acc: 0.8393518518518519\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.36478097398136544, Train acc: 0.8391593215811965\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3648177547564813, Train acc: 0.8390837104072398\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.36447746004176956, Train acc: 0.8391500474833808\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3635518244738735, Train acc: 0.8398701079622132\n",
      "Val loss: 0.3221721351146698, Val acc: 0.862\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.35649540931241125, Train acc: 0.844017094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35811973949018705, Train acc: 0.8420138888888888\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.35627312970976543, Train acc: 0.844551282051282\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3532256862291923, Train acc: 0.8456196581196581\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.35290652693082125, Train acc: 0.8441773504273504\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3520351842342958, Train acc: 0.8441061253561254\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.35221338948924025, Train acc: 0.8435973748473748\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.353646687637919, Train acc: 0.8429821047008547\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.354760334249462, Train acc: 0.8429783950617284\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.35435838899296573, Train acc: 0.8429220085470085\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.353552023037145, Train acc: 0.8430944055944056\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.35453753404722593, Train acc: 0.8419916310541311\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.35612623424885853, Train acc: 0.8413461538461539\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3563772763360988, Train acc: 0.8415750915750916\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3557675312132577, Train acc: 0.8418625356125357\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.35521227386421883, Train acc: 0.8424312232905983\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3551171670994188, Train acc: 0.8423831070889894\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3550059891572082, Train acc: 0.8423848528015194\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.35548478575759357, Train acc: 0.8422598965362124\n",
      "Val loss: 0.3249094486236572, Val acc: 0.874\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.36135123440852535, Train acc: 0.8405448717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.361115012731817, Train acc: 0.8417467948717948\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3579285503749834, Train acc: 0.8421474358974359\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.35658862398794067, Train acc: 0.8440838675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3524498978868509, Train acc: 0.8459401709401709\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3519136606267205, Train acc: 0.8443287037037037\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3528313442009418, Train acc: 0.8441697191697192\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3535717186462293, Train acc: 0.8434161324786325\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3513320996536602, Train acc: 0.8444622507122507\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3517989920499997, Train acc: 0.844284188034188\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3518707117278537, Train acc: 0.8437985625485626\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.35140226340913705, Train acc: 0.8441506410256411\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.35203631869826485, Train acc: 0.844284188034188\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.35196891458880186, Train acc: 0.8447611416361417\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.35289709490758403, Train acc: 0.8441417378917379\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.352846946209096, Train acc: 0.8439503205128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.35231240271263353, Train acc: 0.844284188034188\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3516324142176315, Train acc: 0.8441654795821463\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.35171919712546273, Train acc: 0.8444950517318939\n",
      "Val loss: 0.31224554777145386, Val acc: 0.868\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.34763553681281895, Train acc: 0.8512286324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.33726889649644876, Train acc: 0.8543002136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3465886744975704, Train acc: 0.8496260683760684\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.34948616441434777, Train acc: 0.8477564102564102\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3499584127313051, Train acc: 0.8464209401709402\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3475651044154439, Train acc: 0.8475338319088319\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3479265210028824, Train acc: 0.8463064713064713\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3461393067199323, Train acc: 0.8464543269230769\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3466047317723603, Train acc: 0.8460944919278253\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3468771543767717, Train acc: 0.8465277777777778\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3451644114877887, Train acc: 0.847829254079254\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.34490735490840896, Train acc: 0.8479567307692307\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3444802535797012, Train acc: 0.8483933267587114\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.34522633390530705, Train acc: 0.8482142857142857\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.34450050838227964, Train acc: 0.8488425925925925\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3444370064510303, Train acc: 0.8486745459401709\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.34335941042940243, Train acc: 0.8489033433886375\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.343613181203066, Train acc: 0.8486467236467237\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.34319913875960145, Train acc: 0.84847334682861\n",
      "Val loss: 0.3171643316745758, Val acc: 0.858\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3380161472874829, Train acc: 0.8544337606837606\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3336713809488166, Train acc: 0.8553685897435898\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3342006076604892, Train acc: 0.8556801994301995\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3361759479038226, Train acc: 0.8544337606837606\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.33733859091487706, Train acc: 0.8538995726495726\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.33694696818024683, Train acc: 0.8551905270655271\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.33824403701356914, Train acc: 0.8537469474969475\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3367790578124233, Train acc: 0.8541332799145299\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.33655532269764266, Train acc: 0.8541666666666666\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3343627406618534, Train acc: 0.8548344017094017\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3359063477852406, Train acc: 0.8537296037296037\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.33709147380415533, Train acc: 0.8528311965811965\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3372656953111438, Train acc: 0.8531393819855358\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.337329884097253, Train acc: 0.8529075091575091\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.33747593239162044, Train acc: 0.8525462962962963\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.33923889368645144, Train acc: 0.8513287927350427\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3387390842025396, Train acc: 0.8512443438914027\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3379997466208815, Train acc: 0.8516886277302944\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.33715916946161667, Train acc: 0.8520017993702205\n",
      "Val loss: 0.3022213578224182, Val acc: 0.866\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.31829857526936084, Train acc: 0.8533653846153846\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.335636711305278, Train acc: 0.8489583333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3326158562616745, Train acc: 0.8522079772079773\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3330067586726867, Train acc: 0.8526308760683761\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3332029651882302, Train acc: 0.8514957264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.33505578557735155, Train acc: 0.8499376780626781\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3355337509996871, Train acc: 0.8495879120879121\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.334205292738401, Train acc: 0.8501268696581197\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3345419499467694, Train acc: 0.8506350902184235\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3347202645153062, Train acc: 0.8506143162393163\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.33426786718923357, Train acc: 0.8509858197358198\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.33635112710636733, Train acc: 0.8502715455840456\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.33540520794018563, Train acc: 0.8507560815253123\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3334967355324599, Train acc: 0.8520489926739927\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.33383736112858153, Train acc: 0.8522079772079773\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.33469216847139543, Train acc: 0.8518963675213675\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.334230808931688, Train acc: 0.8519513574660633\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3343602873575993, Train acc: 0.8519853988603988\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.33403639039556776, Train acc: 0.8521283175888439\n",
      "Val loss: 0.29234957695007324, Val acc: 0.88\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3250279410654663, Train acc: 0.8653846153846154\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3248728711444598, Train acc: 0.8613782051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.32774219468787863, Train acc: 0.8579950142450142\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3272330088652352, Train acc: 0.8569711538461539\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3266735066078667, Train acc: 0.8583867521367521\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3255859041995133, Train acc: 0.8577279202279202\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.32621616570868045, Train acc: 0.8565705128205128\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3244069396621651, Train acc: 0.8582064636752137\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.32685256128863616, Train acc: 0.856451804368471\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3290449297326243, Train acc: 0.8554754273504274\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3296904624747322, Train acc: 0.8552836052836053\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3305191028640311, Train acc: 0.8548121438746439\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.331551974175166, Train acc: 0.8538173898750822\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3313304194236704, Train acc: 0.8540140415140415\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.33074053290732564, Train acc: 0.8545762108262108\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.33025089553835935, Train acc: 0.8548844818376068\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.330365475332725, Train acc: 0.8542766465560583\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.32998094686854484, Train acc: 0.8544931149097815\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.32972618912690616, Train acc: 0.8546446243814665\n",
      "Val loss: 0.29479101300239563, Val acc: 0.882\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.32100497810249656, Train acc: 0.8595085470085471\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.32566516566225606, Train acc: 0.8625801282051282\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.32351170609734337, Train acc: 0.8634259259259259\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3207599780976008, Train acc: 0.8645165598290598\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3227173337696964, Train acc: 0.862232905982906\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.32690847220371594, Train acc: 0.8595975783475783\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3277841842891387, Train acc: 0.8586309523809523\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.33059656130484283, Train acc: 0.8570379273504274\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.330693110894387, Train acc: 0.8563924501424501\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.32960551619275, Train acc: 0.8561965811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3276668734495364, Train acc: 0.8566190753690753\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.32563492867490673, Train acc: 0.8577946937321937\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3258196031435942, Train acc: 0.8573307034845496\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3255000169842671, Train acc: 0.8573336385836385\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.32556170997371703, Train acc: 0.857175925925926\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.32572878713313586, Train acc: 0.8572549412393162\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.32494078336827653, Train acc: 0.8576388888888888\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3254494237141279, Train acc: 0.8572530864197531\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3245960565981863, Train acc: 0.8576388888888888\n",
      "Val loss: 0.33901071548461914, Val acc: 0.858\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3293836700101184, Train acc: 0.8557692307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.32733952983194947, Train acc: 0.8567040598290598\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3229332255926567, Train acc: 0.8575498575498576\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.31837925716088367, Train acc: 0.8591746794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3168960235821895, Train acc: 0.8607905982905983\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.31753114072812927, Train acc: 0.8600427350427351\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.31972434971213193, Train acc: 0.8607677045177046\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3189421614202169, Train acc: 0.8617454594017094\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.31834437460386517, Train acc: 0.8616156220322887\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3202330078770462, Train acc: 0.8612179487179488\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.31982431268983785, Train acc: 0.8610625485625486\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3203214882605011, Train acc: 0.8608440170940171\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3207064758567415, Train acc: 0.8607618343195266\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.32131148731934134, Train acc: 0.8608440170940171\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3212798001716959, Train acc: 0.860772792022792\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.32151616702620417, Train acc: 0.8608607104700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.321547709274316, Train acc: 0.86103255404726\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3218148066772808, Train acc: 0.8604433760683761\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3200094634503649, Train acc: 0.8611813990103464\n",
      "Val loss: 0.27815863490104675, Val acc: 0.888\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3194235034095935, Train acc: 0.8600427350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3116526935154047, Train acc: 0.8608440170940171\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.309536261882028, Train acc: 0.8622685185185185\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.31197553662917554, Train acc: 0.8614449786324786\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3116506075502461, Train acc: 0.861965811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.31159908700193095, Train acc: 0.8618678774928775\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3143137982968009, Train acc: 0.8610347985347986\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3149387295413603, Train acc: 0.8612446581196581\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3136192787534151, Train acc: 0.8614672364672364\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3131519771666608, Train acc: 0.8607104700854701\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.31252725333809017, Train acc: 0.8610625485625486\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.31202692964626344, Train acc: 0.8612001424501424\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3125598005494893, Train acc: 0.8612549309664694\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.31158467904824916, Train acc: 0.8620268620268621\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3127791814484827, Train acc: 0.8612891737891738\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3137998865941205, Train acc: 0.860793936965812\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.31391917518960466, Train acc: 0.8610011312217195\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3145398554681373, Train acc: 0.8612446581196581\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.31451503832598715, Train acc: 0.8613079172289698\n",
      "Val loss: 0.2921922206878662, Val acc: 0.878\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.31359924025769925, Train acc: 0.8669871794871795\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3178825547488836, Train acc: 0.8640491452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3180325012176465, Train acc: 0.8610220797720798\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.31515204288765913, Train acc: 0.8625133547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.31329228046358143, Train acc: 0.8631410256410257\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3109987188671899, Train acc: 0.8643607549857549\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.310839740047743, Train acc: 0.8647359584859585\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.31017054307760084, Train acc: 0.8644497863247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3094185580767458, Train acc: 0.864227207977208\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3080315074668481, Train acc: 0.8646367521367522\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.30925115755966864, Train acc: 0.8630293317793318\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.31045783201280314, Train acc: 0.8622017450142451\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.310318203396816, Train acc: 0.8623438527284681\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3105286521079776, Train acc: 0.8624656593406593\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.31007387081527304, Train acc: 0.8625\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3106094568561858, Train acc: 0.8623130341880342\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3106148842218355, Train acc: 0.8626194067370538\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.31171530645414974, Train acc: 0.8624910968660968\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3109066126129941, Train acc: 0.8629526540710751\n",
      "Val loss: 0.2888638973236084, Val acc: 0.876\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2982323837076497, Train acc: 0.8763354700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.29848143668510974, Train acc: 0.8713942307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2999539302497508, Train acc: 0.8691239316239316\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3003342179979524, Train acc: 0.8691907051282052\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.30382129618754755, Train acc: 0.8674679487179487\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3052941319397372, Train acc: 0.8667646011396012\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.30756108366380536, Train acc: 0.8661477411477412\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.305882580148486, Train acc: 0.8673210470085471\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.3067774132304495, Train acc: 0.8668684710351378\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.30692114976481494, Train acc: 0.8663995726495727\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3064893866312439, Train acc: 0.866501554001554\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.30623720254632314, Train acc: 0.8664752492877493\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.30745260670222707, Train acc: 0.8662269888231426\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.30679133458015245, Train acc: 0.8662049755799756\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.30736932071865114, Train acc: 0.8661502849002849\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3083500859534575, Train acc: 0.8653512286324786\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3076927029064638, Train acc: 0.8655103066867773\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3079094174800039, Train acc: 0.8651175213675214\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.30801013368464675, Train acc: 0.8649769455690508\n",
      "Val loss: 0.27390462160110474, Val acc: 0.892\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.29040467694529104, Train acc: 0.875801282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.3009398001381475, Train acc: 0.8677884615384616\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.30369634034796655, Train acc: 0.8676103988603988\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3043099619034264, Train acc: 0.8679220085470085\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.30363257298102747, Train acc: 0.868215811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.30364123553547084, Train acc: 0.8679665242165242\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3033218219858095, Train acc: 0.8683608058608059\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3033725376933431, Train acc: 0.8687566773504274\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.30246337831501496, Train acc: 0.8684710351377019\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.302644107879227, Train acc: 0.8681089743589744\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3037475849666055, Train acc: 0.8672299922299922\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3037721800625834, Train acc: 0.8672320156695157\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3049346225080249, Train acc: 0.8668844510190664\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.3058337027955259, Train acc: 0.8665674603174603\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3051920457910269, Train acc: 0.8665420227920227\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3048761794821192, Train acc: 0.8667033920940171\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3046823938837478, Train acc: 0.8663430115635998\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3056570946222131, Train acc: 0.8657407407407407\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.30511403568105766, Train acc: 0.8658906882591093\n",
      "Val loss: 0.2793880105018616, Val acc: 0.888\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2931690687297756, Train acc: 0.8720619658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.294444218914733, Train acc: 0.875267094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.29253185877942633, Train acc: 0.875\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2944970332308967, Train acc: 0.8747996794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2969850575694671, Train acc: 0.8728098290598291\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.29990688960818823, Train acc: 0.870414886039886\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3005078646271366, Train acc: 0.8697344322344323\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3008293392232213, Train acc: 0.8682558760683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.30097690373942154, Train acc: 0.8689755460588794\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3010883108863973, Train acc: 0.868616452991453\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.30085223030353947, Train acc: 0.8680798368298368\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3014701753164883, Train acc: 0.8676326566951567\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3000282925712208, Train acc: 0.8686719263642341\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.30018423259331045, Train acc: 0.8685325091575091\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3011492384762166, Train acc: 0.867948717948718\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3005911175750642, Train acc: 0.8680221688034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.29929843903816544, Train acc: 0.8688097033685269\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2993445460669562, Train acc: 0.8691090930674265\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.29913954218925787, Train acc: 0.8693769680611786\n",
      "Val loss: 0.2869882881641388, Val acc: 0.886\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.30420258259161925, Train acc: 0.8685897435897436\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.30203502650699043, Train acc: 0.8680555555555556\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.30308249030272844, Train acc: 0.8674323361823362\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.304402703148687, Train acc: 0.8669871794871795\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3028711174097326, Train acc: 0.8671474358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3006950889016234, Train acc: 0.8675658831908832\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2976243442404328, Train acc: 0.8692765567765568\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2960387344758671, Train acc: 0.8694911858974359\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.29743157490974703, Train acc: 0.8688271604938271\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.29738648710533594, Train acc: 0.8690705128205128\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.29619594524153553, Train acc: 0.8698766511266511\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2966047112997144, Train acc: 0.8703258547008547\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2974954977616873, Train acc: 0.8702333990795529\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2971832345880031, Train acc: 0.8704594017094017\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2969848762613925, Train acc: 0.8702279202279203\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.29775067826963836, Train acc: 0.8699085202991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2985754876096474, Train acc: 0.8692496229260935\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2972096961826539, Train acc: 0.8700587606837606\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.298139990711676, Train acc: 0.8694472559604138\n",
      "Val loss: 0.2704109847545624, Val acc: 0.878\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2932749460650306, Train acc: 0.8707264957264957\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2934580474264092, Train acc: 0.8699252136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.29324692521679435, Train acc: 0.8695690883190883\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.29257471440757954, Train acc: 0.8677884615384616\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2928622644808557, Train acc: 0.867681623931624\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2941649665265341, Train acc: 0.8680555555555556\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.29625850735784887, Train acc: 0.8677503052503053\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.29426718249312067, Train acc: 0.8693910256410257\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.29578916412925904, Train acc: 0.8685007122507122\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2962467051533043, Train acc: 0.8684294871794872\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2937882705226182, Train acc: 0.8694881507381508\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.29579706068158657, Train acc: 0.8689458689458689\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2953089151058755, Train acc: 0.8689801117685733\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2951459688053754, Train acc: 0.8690857753357754\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.29424346929092354, Train acc: 0.8692485754985755\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.29405629800425637, Train acc: 0.8694577991452992\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.29353161996381255, Train acc: 0.8700194821518351\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.29337671413975225, Train acc: 0.8701923076923077\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.29319981907905357, Train acc: 0.8702063652721548\n",
      "Val loss: 0.2663033604621887, Val acc: 0.898\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.31071018930683786, Train acc: 0.8624465811965812\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2978942217544103, Train acc: 0.8683226495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.29821005612187235, Train acc: 0.8679665242165242\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2914105195902352, Train acc: 0.8707264957264957\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.29611622326903875, Train acc: 0.8705662393162393\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.29644045397172286, Train acc: 0.8713051994301995\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2932767651711337, Train acc: 0.872214590964591\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2908462063114867, Train acc: 0.8718282585470085\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2911576678921581, Train acc: 0.8717058404558404\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2909590869632542, Train acc: 0.8719284188034188\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2914183291359844, Train acc: 0.8716006216006216\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2906123798637733, Train acc: 0.8719284188034188\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.29091591546207884, Train acc: 0.8716510519395135\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2900458145181629, Train acc: 0.8717948717948718\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.29003184561928114, Train acc: 0.8716524216524216\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.28944919805806607, Train acc: 0.8720786591880342\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.29002210887241686, Train acc: 0.8720148315736551\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2902566867474562, Train acc: 0.8715722934472935\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.29005012641276695, Train acc: 0.8716261808367072\n",
      "Val loss: 0.25882381200790405, Val acc: 0.894\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.27303460959950066, Train acc: 0.8739316239316239\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.27291590234853774, Train acc: 0.8767361111111112\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2777774404957254, Train acc: 0.874198717948718\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2816861944241274, Train acc: 0.874198717948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2814905432363351, Train acc: 0.8742521367521368\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2815448125914546, Train acc: 0.8746438746438746\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2817778743585169, Train acc: 0.874732905982906\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2839716379849129, Train acc: 0.8738648504273504\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2851539366442537, Train acc: 0.8735754985754985\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2852817211682216, Train acc: 0.8736111111111111\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.28516262747989946, Train acc: 0.8738587801087802\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2863492256168414, Train acc: 0.8729522792022792\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2870766118996673, Train acc: 0.8728427021696252\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.287379857359661, Train acc: 0.8729204822954822\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.28694763534046985, Train acc: 0.8732727920227921\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2867130646939811, Train acc: 0.8732305021367521\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2878656307597775, Train acc: 0.8733188788335847\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2882590971635929, Train acc: 0.8731155033238367\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.28845386282011726, Train acc: 0.873144399460189\n",
      "Val loss: 0.2710016369819641, Val acc: 0.892\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.28342464314694077, Train acc: 0.875534188034188\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.27687787756514853, Train acc: 0.8794070512820513\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2774948545139909, Train acc: 0.8794515669515669\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2800752690826089, Train acc: 0.8768696581196581\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2790396584213799, Train acc: 0.8776175213675214\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2800970160988746, Train acc: 0.8764690170940171\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2808546258480048, Train acc: 0.8766025641025641\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2850867570696287, Train acc: 0.8749666132478633\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.28533782874714275, Train acc: 0.8750890313390314\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.28485873060412387, Train acc: 0.8758279914529915\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2842832474818087, Train acc: 0.8760440947940948\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2835097362305916, Train acc: 0.8754896723646723\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2819546140867538, Train acc: 0.8762121959237343\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2815408680260509, Train acc: 0.8765071733821734\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2811153802076126, Train acc: 0.8764423076923077\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2804320519173152, Train acc: 0.876953125\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2817018526812754, Train acc: 0.8763354700854701\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.28256764271582235, Train acc: 0.8757864434947769\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.28240295925717657, Train acc: 0.8759980881691408\n",
      "Val loss: 0.25513535737991333, Val acc: 0.892\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.29294439061329913, Train acc: 0.8736645299145299\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2896962741819712, Train acc: 0.8754006410256411\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.279111055567054, Train acc: 0.8789173789173789\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.28373406453328764, Train acc: 0.875534188034188\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.28240720522709384, Train acc: 0.8769230769230769\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2831242989292342, Train acc: 0.8765580484330484\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2808423912623426, Train acc: 0.8768696581196581\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2789024350305016, Train acc: 0.8775040064102564\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.27774999132919403, Train acc: 0.8785018993352327\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.27880534869101314, Train acc: 0.8778846153846154\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2796785539217299, Train acc: 0.877573815073815\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2793028532100199, Train acc: 0.8775373931623932\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.27829265221708466, Train acc: 0.8779791255752795\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.27859396930506525, Train acc: 0.8781097374847375\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.27827977932076847, Train acc: 0.8781695156695156\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2777739316543453, Train acc: 0.8784722222222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2772350406687184, Train acc: 0.8785822021116139\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.27806323263634986, Train acc: 0.8782496438746439\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.27742778747147717, Train acc: 0.8785987404408457\n",
      "Val loss: 0.2765427529811859, Val acc: 0.888\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2748555679861297, Train acc: 0.8808760683760684\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2784788723812144, Train acc: 0.8802083333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.27728115064635916, Train acc: 0.8810541310541311\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.27660328463420397, Train acc: 0.882011217948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.274036738053601, Train acc: 0.8810897435897436\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2764999180915434, Train acc: 0.880920584045584\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2741994311002803, Train acc: 0.8811431623931624\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.27524713550208724, Train acc: 0.8815771901709402\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.27790626243167227, Train acc: 0.8793625356125356\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.27787486901904784, Train acc: 0.8796474358974359\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.27817855357961685, Train acc: 0.8796134421134422\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.27853594730976977, Train acc: 0.8795628561253561\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2792557490099268, Train acc: 0.8787598619329389\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2788130297974862, Train acc: 0.878968253968254\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2776999602948668, Train acc: 0.879255698005698\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2771675486384262, Train acc: 0.8789229433760684\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2771248677403649, Train acc: 0.8786764705882353\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.27712045362836557, Train acc: 0.8785464150047484\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.277052054613212, Train acc: 0.8783878767431399\n",
      "Val loss: 0.26409095525741577, Val acc: 0.9\n",
      "Epoch 38/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.25484790589310163, Train acc: 0.8952991452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2650372180928532, Train acc: 0.8864850427350427\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.26387224323049907, Train acc: 0.8854166666666666\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2653052042851336, Train acc: 0.8842815170940171\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2674170961746803, Train acc: 0.8833867521367521\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.26731295830588736, Train acc: 0.8834134615384616\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.26839548547698755, Train acc: 0.8835088522588522\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2699251120798608, Train acc: 0.882278311965812\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.27143471729778046, Train acc: 0.8816179962013295\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.27005951808940654, Train acc: 0.8821581196581196\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2717704391910202, Train acc: 0.8811674436674437\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2711049752347065, Train acc: 0.8812989672364673\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2719844370825678, Train acc: 0.8804651545036161\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.27216449526385367, Train acc: 0.8805898962148963\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2727524698714585, Train acc: 0.8803952991452991\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2733074123214962, Train acc: 0.8803585737179487\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2736944352272409, Train acc: 0.880121920563097\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.273670953761252, Train acc: 0.8800599477682811\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.273345171191968, Train acc: 0.8802153621232569\n",
      "Val loss: 0.2498164027929306, Val acc: 0.904\n",
      "Epoch 39/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2595866198341052, Train acc: 0.8880876068376068\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2654606423571578, Train acc: 0.8867521367521367\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.26835850832129476, Train acc: 0.8848824786324786\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2724032311612724, Train acc: 0.8814770299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2698116107374175, Train acc: 0.8819444444444444\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2690076721020234, Train acc: 0.8812767094017094\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2676589751254508, Train acc: 0.8819826007326007\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.26716413978550935, Train acc: 0.8824118589743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2672335932638004, Train acc: 0.8829831433998101\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2668626884587555, Train acc: 0.8833333333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.26845950535162066, Train acc: 0.8829885392385393\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.26795107871559354, Train acc: 0.8830128205128205\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2689379236338211, Train acc: 0.8824580867850098\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.26973472218573896, Train acc: 0.8820398351648352\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2706238268978066, Train acc: 0.881445868945869\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2707587218322019, Train acc: 0.8814603365384616\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.27078558617145526, Train acc: 0.8816930618401206\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.27152431831435997, Train acc: 0.8814696106362773\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.27105308446934295, Train acc: 0.8815930049482681\n",
      "Val loss: 0.26630541682243347, Val acc: 0.896\n",
      "Epoch 40/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.26875131220644355, Train acc: 0.8838141025641025\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.27151353591973454, Train acc: 0.8823450854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.27662561191750046, Train acc: 0.8795405982905983\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.27431322736904407, Train acc: 0.8796073717948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2724891048274998, Train acc: 0.8791132478632478\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2715480614007793, Train acc: 0.8802083333333334\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.27252082842810393, Train acc: 0.8799603174603174\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.27147518722818065, Train acc: 0.8805422008547008\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2699396892292434, Train acc: 0.881113485280152\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2681902071413321, Train acc: 0.8818910256410256\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.26841702254339994, Train acc: 0.8819201631701632\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2683871071042753, Train acc: 0.8823896011396012\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2674978647006334, Train acc: 0.8822937212360289\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2667720963330132, Train acc: 0.8827457264957265\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2666750432674022, Train acc: 0.8830484330484331\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2662751354181614, Train acc: 0.8830962873931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2662776043482196, Train acc: 0.8830285319255907\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2678398180413365, Train acc: 0.8826715337132004\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.26842672512055893, Train acc: 0.8822115384615384\n",
      "Val loss: 0.26812833547592163, Val acc: 0.9\n",
      "Epoch 41/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.25238317601446414, Train acc: 0.8859508547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.25283387949706143, Train acc: 0.8870192307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.25142767864075144, Train acc: 0.8885327635327636\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2549778442575127, Train acc: 0.8836805555555556\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.25661658582269636, Train acc: 0.8838141025641025\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2543693654812299, Train acc: 0.8858173076923077\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2567724180345279, Train acc: 0.8856456043956044\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.25794755826648486, Train acc: 0.8849492521367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.258276940488804, Train acc: 0.8850902184235517\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.25939792272372125, Train acc: 0.8848290598290598\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.26103343372902577, Train acc: 0.8841054778554779\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2615208444676423, Train acc: 0.8843037749287749\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2619422076467148, Train acc: 0.884327744904668\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.26156192606767603, Train acc: 0.8842910561660562\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.26159652367106867, Train acc: 0.8844551282051282\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.26153560950317317, Train acc: 0.8843315972222222\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.261892689640136, Train acc: 0.884175465057818\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2614144247417946, Train acc: 0.8843334520417854\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2617904859596258, Train acc: 0.8840530814215025\n",
      "Val loss: 0.26116830110549927, Val acc: 0.892\n",
      "Epoch 42/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.255060829476923, Train acc: 0.8870192307692307\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2654960123328572, Train acc: 0.8770032051282052\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.26452268706767307, Train acc: 0.8798967236467237\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2598185698286845, Train acc: 0.8830128205128205\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.25935316682498677, Train acc: 0.8836538461538461\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2613793252423363, Train acc: 0.8833244301994302\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2597975372695006, Train acc: 0.8841193528693528\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.260928938161327, Train acc: 0.8839476495726496\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.26193054370292007, Train acc: 0.8829831433998101\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2602916375948833, Train acc: 0.8836004273504273\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2606019902838323, Train acc: 0.8835470085470085\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2617492676364985, Train acc: 0.8829683048433048\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2634626451309067, Train acc: 0.8825608152531229\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.26454130178988344, Train acc: 0.8819444444444444\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.26357631377684765, Train acc: 0.8823539886039886\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.26254537181419313, Train acc: 0.8824619391025641\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.26255128772010633, Train acc: 0.8829028406234288\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2612216262905686, Train acc: 0.8835173314339981\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2622104270054422, Train acc: 0.8830409356725146\n",
      "Val loss: 0.27220419049263, Val acc: 0.892\n",
      "Epoch 43/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2599764173993698, Train acc: 0.8814102564102564\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.24750074026230565, Train acc: 0.8884882478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.24793892681726024, Train acc: 0.8887108262108262\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2550582935651525, Train acc: 0.8850160256410257\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.25539786587031477, Train acc: 0.885042735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2555475012476203, Train acc: 0.8861289173789174\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.25651957347115756, Train acc: 0.8861416361416361\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.25767295351888764, Train acc: 0.8864182692307693\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.259890316752938, Train acc: 0.8856540835707503\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2609838851974306, Train acc: 0.8853365384615385\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.261465070712747, Train acc: 0.8855866355866356\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2631179987986246, Train acc: 0.8846153846153846\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2614414624742463, Train acc: 0.8854783037475346\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2616524182519306, Train acc: 0.8850923382173382\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.26039424293443686, Train acc: 0.8855235042735042\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2603830309429516, Train acc: 0.8857505341880342\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.26023850122503206, Train acc: 0.8853223981900452\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.26028576638312534, Train acc: 0.8854018281101614\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2605865441540904, Train acc: 0.8853463787674314\n",
      "Val loss: 0.2688065469264984, Val acc: 0.892\n",
      "Epoch 44/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.27203574754361415, Train acc: 0.8790064102564102\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2633045142373213, Train acc: 0.8836805555555556\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2651308294268925, Train acc: 0.8855947293447294\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2636165857139943, Train acc: 0.8858840811965812\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2638173890992617, Train acc: 0.8839743589743589\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2596509427235507, Train acc: 0.8854166666666666\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.26180704628482404, Train acc: 0.8848443223443223\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.26008787238572395, Train acc: 0.8856837606837606\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.25944242858507466, Train acc: 0.8862179487179487\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2590893734851454, Train acc: 0.8864583333333333\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2592076297441285, Train acc: 0.8860479797979798\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.25885990306607676, Train acc: 0.8855724715099715\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.25918089690998464, Train acc: 0.8854372123602893\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2589720810521326, Train acc: 0.885321275946276\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2590491671486628, Train acc: 0.8856837606837606\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2571274121112835, Train acc: 0.8860844017094017\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.25613908191919027, Train acc: 0.8864536199095022\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.25630148266887, Train acc: 0.8861882716049383\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2566947774778091, Train acc: 0.8862320062977957\n",
      "Val loss: 0.2554143965244293, Val acc: 0.904\n",
      "Epoch 45/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.24694685383230194, Train acc: 0.8840811965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2504379701537964, Train acc: 0.8863514957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2525093081882197, Train acc: 0.8847934472934473\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2518435103070532, Train acc: 0.8849492521367521\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.25367782863541544, Train acc: 0.8849893162393162\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2547148673017735, Train acc: 0.8853276353276354\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2547212895347085, Train acc: 0.8851114163614163\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.25517082216743475, Train acc: 0.8853498931623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.25671760096141305, Train acc: 0.88497150997151\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2580221412209874, Train acc: 0.8851762820512821\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2581261609041978, Train acc: 0.8851495726495726\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2575386551625369, Train acc: 0.8855279558404558\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.25746121596028193, Train acc: 0.8858481262327417\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.25641413485365255, Train acc: 0.8861034798534798\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2551671825816319, Train acc: 0.8867343304843305\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2558314272771693, Train acc: 0.8868022168803419\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.25439712255930946, Train acc: 0.8873963046757164\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2542528169740837, Train acc: 0.8875979344729344\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2550624302876221, Train acc: 0.8872722672064778\n",
      "Val loss: 0.2552105486392975, Val acc: 0.9\n",
      "Epoch 46/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.26732337608551365, Train acc: 0.8824786324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.26525886175342095, Train acc: 0.8848824786324786\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.25696102126456394, Train acc: 0.8865740740740741\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.25546029054074204, Train acc: 0.8874198717948718\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2553456599895771, Train acc: 0.8873397435897435\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.25407506097797994, Train acc: 0.8883101851851852\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.256074936380462, Train acc: 0.8877060439560439\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2546875040229951, Train acc: 0.8883547008547008\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.255060878553112, Train acc: 0.8891856600189934\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2534729066790424, Train acc: 0.8899572649572649\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2535399601772346, Train acc: 0.8894473581973582\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.25391395320250865, Train acc: 0.8892450142450142\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.25277767462422, Train acc: 0.8893203484549639\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2525457519655808, Train acc: 0.8894230769230769\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2523171453585482, Train acc: 0.8896189458689459\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2518260811523208, Train acc: 0.8902243589743589\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2511444366277163, Train acc: 0.8903971845148315\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.25127357514601656, Train acc: 0.890357905982906\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2518230124158442, Train acc: 0.890238416554206\n",
      "Val loss: 0.25961804389953613, Val acc: 0.898\n",
      "Epoch 47/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.24057113114967305, Train acc: 0.8912927350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2399102757151565, Train acc: 0.890090811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.24144067495339616, Train acc: 0.8908475783475783\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2469495518738006, Train acc: 0.8899572649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.24816607042637645, Train acc: 0.8894764957264957\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2486992920045754, Train acc: 0.8893785612535613\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2488835559104898, Train acc: 0.8896138583638583\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2511128349524214, Train acc: 0.8881209935897436\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.24829537857674125, Train acc: 0.8891559829059829\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.25004077648950945, Train acc: 0.889102564102564\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2497872643276945, Train acc: 0.8890345765345765\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2494589503605099, Train acc: 0.8896011396011396\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.250376758409969, Train acc: 0.8893408941485864\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.25014924269592587, Train acc: 0.889575702075702\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.248921117188669, Train acc: 0.8903311965811965\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.24880402332418558, Train acc: 0.8904246794871795\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2488705340010578, Train acc: 0.8902400703871292\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.24912027564476416, Train acc: 0.8900166191832859\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.24866836331868514, Train acc: 0.8900837831758884\n",
      "Val loss: 0.26083990931510925, Val acc: 0.898\n",
      "Epoch 48/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.24792584152812633, Train acc: 0.8910256410256411\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.25046044228295994, Train acc: 0.8904914529914529\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2520073954536025, Train acc: 0.8878205128205128\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.251630382755628, Train acc: 0.8894230769230769\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2502498303977852, Train acc: 0.8908119658119659\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.24858442963528157, Train acc: 0.890625\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.24763364445813846, Train acc: 0.8915598290598291\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.24741782021955547, Train acc: 0.8911258012820513\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2471334015541946, Train acc: 0.8899572649572649\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.24732185588815273, Train acc: 0.8896901709401709\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2461161436201902, Train acc: 0.8904671717171717\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.24672203937126191, Train acc: 0.890357905982906\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.24553166253934325, Train acc: 0.8908818211702827\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.24541848060994503, Train acc: 0.8909302503052503\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.24536566013878908, Train acc: 0.8910612535612535\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2457976431912209, Train acc: 0.8908253205128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.24551257094796305, Train acc: 0.891088486676722\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.24548155672543975, Train acc: 0.8908179012345679\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.24603004326262132, Train acc: 0.8908850652271705\n",
      "Val loss: 0.2730805575847626, Val acc: 0.896\n",
      "Epoch 49/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.263892589089198, Train acc: 0.8803418803418803\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2566978308953281, Train acc: 0.8863514957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.24684026622959013, Train acc: 0.8925391737891738\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.24447574525371066, Train acc: 0.8934962606837606\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.24429216745317492, Train acc: 0.892948717948718\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.245444617799332, Train acc: 0.8939191595441596\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.24453908474308522, Train acc: 0.8946886446886447\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2432165251304515, Train acc: 0.895065438034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.24119037498365328, Train acc: 0.8960410731244065\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2423076909704086, Train acc: 0.8958333333333334\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.24263608925154528, Train acc: 0.8955662393162394\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.24193817441831966, Train acc: 0.8953659188034188\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.24219075804925294, Train acc: 0.8950525969756739\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.242858435640223, Train acc: 0.894459706959707\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.24400363633265862, Train acc: 0.8941061253561253\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.24283757507721457, Train acc: 0.8944811698717948\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2426859959435559, Train acc: 0.8943250377073907\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.24272091565025725, Train acc: 0.8944385090218424\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2429223052999805, Train acc: 0.8941042510121457\n",
      "Val loss: 0.2494223564863205, Val acc: 0.904\n",
      "Epoch 50/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2462734883157616, Train acc: 0.8961004273504274\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2398946221726827, Train acc: 0.8974358974358975\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2384349882730052, Train acc: 0.8974358974358975\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.23585072988405442, Train acc: 0.8977697649572649\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.23639111466005316, Train acc: 0.8975427350427351\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.23753033180925412, Train acc: 0.8961449430199431\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.237689540331435, Train acc: 0.8959859584859585\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.23962713986173526, Train acc: 0.8955996260683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.24043620942056124, Train acc: 0.8954475308641975\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.241581049360908, Train acc: 0.8951121794871795\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.24052607997233033, Train acc: 0.8954933954933955\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.24011331261467306, Train acc: 0.895210113960114\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2410872354344229, Train acc: 0.8947649572649573\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.24067310688469074, Train acc: 0.8948031135531136\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.24087862155461243, Train acc: 0.8949786324786325\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2398857268003317, Train acc: 0.8952991452991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.24073540279243746, Train acc: 0.894812091503268\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.24069065389427705, Train acc: 0.8945127018043685\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.24114394089222974, Train acc: 0.8942026540710751\n",
      "Val loss: 0.2512361407279968, Val acc: 0.896\n",
      "Epoch 51/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.22757955736074692, Train acc: 0.8990384615384616\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.22887998669702783, Train acc: 0.9019764957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2347884366654942, Train acc: 0.8989494301994302\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2339917432484973, Train acc: 0.8993723290598291\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.23471873913310531, Train acc: 0.8979700854700855\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.23499708920398837, Train acc: 0.8976139601139601\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.23732718074369052, Train acc: 0.8968635531135531\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.23858633083808753, Train acc: 0.8959334935897436\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2363312986607735, Train acc: 0.8968423551756886\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2362983035353514, Train acc: 0.8966346153846154\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.23514055696308103, Train acc: 0.8970716783216783\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.23621779158074632, Train acc: 0.8961894586894587\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.23687074338572736, Train acc: 0.8957511505588429\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.23659201920878536, Train acc: 0.8959668803418803\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.23665392528104987, Train acc: 0.896011396011396\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.23677014772238958, Train acc: 0.8960169604700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.23687247651577176, Train acc: 0.8958647561588738\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.23540342577396126, Train acc: 0.8965307454890789\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2359429037217696, Train acc: 0.8964378092667566\n",
      "Val loss: 0.25615033507347107, Val acc: 0.902\n",
      "Epoch 52/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.23630915365667424, Train acc: 0.8950320512820513\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.23474570472016293, Train acc: 0.8936965811965812\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.22902490581754606, Train acc: 0.8959223646723646\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2309088693995379, Train acc: 0.8946981837606838\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2323328887231839, Train acc: 0.8933760683760684\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.23310426697453374, Train acc: 0.8942752849002849\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2348007991482414, Train acc: 0.893658424908425\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.23435594086957157, Train acc: 0.8938635149572649\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.23449906053380637, Train acc: 0.8944978632478633\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.23418154701526858, Train acc: 0.8943376068376069\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2347596838416151, Train acc: 0.8941336441336442\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.23488115860239478, Train acc: 0.8943643162393162\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.23503887945055021, Train acc: 0.894723865877712\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.23423057891455762, Train acc: 0.8951655982905983\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.23350537882698566, Train acc: 0.895477207977208\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.23381939520422593, Train acc: 0.8954660790598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.23379512669873515, Train acc: 0.8955348164906989\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2339644216050563, Train acc: 0.8956107549857549\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.23364797176385163, Train acc: 0.8957911605937922\n",
      "Val loss: 0.25603219866752625, Val acc: 0.9\n",
      "Epoch 53/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.22938286197873262, Train acc: 0.9001068376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.22794699183322936, Train acc: 0.8978365384615384\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2303289580570157, Train acc: 0.8989494301994302\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.23249034004874974, Train acc: 0.8967681623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.23454142726766758, Train acc: 0.8963141025641026\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2329420646190898, Train acc: 0.895744301994302\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.23202651388424655, Train acc: 0.8965964590964591\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.23297310643630403, Train acc: 0.8959668803418803\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.23517586429517606, Train acc: 0.8958036562203229\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.23409084671009808, Train acc: 0.8958600427350427\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.23429518090249868, Train acc: 0.89614898989899\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2336933421384957, Train acc: 0.896545584045584\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.23530420591169326, Train acc: 0.8956689677843525\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.23470736045267557, Train acc: 0.8956425518925519\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2352274234266023, Train acc: 0.8956908831908832\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.23402632619294092, Train acc: 0.8961838942307693\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.23450894985308654, Train acc: 0.8962418300653595\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.23416417111188936, Train acc: 0.896204297245964\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2339343459965863, Train acc: 0.8964940395861448\n",
      "Val loss: 0.2531518340110779, Val acc: 0.904\n",
      "Epoch 54/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.21203938298500502, Train acc: 0.905448717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.22232009696527424, Train acc: 0.9015758547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2277425732198264, Train acc: 0.8982371794871795\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.22432092355930397, Train acc: 0.9001068376068376\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.22683041479725105, Train acc: 0.8986111111111111\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.22812050660047944, Train acc: 0.8993945868945868\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.22779767906181833, Train acc: 0.8999542124542125\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.227439775199303, Train acc: 0.9001402243589743\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.22684420601722183, Train acc: 0.900403608736942\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.22629596604050226, Train acc: 0.9007745726495726\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.22755805996956524, Train acc: 0.8995969308469308\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.22800202571927566, Train acc: 0.8995503917378918\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.22768116705878874, Train acc: 0.8997781065088757\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.22805753994564798, Train acc: 0.8992483211233211\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2294156951081549, Train acc: 0.8985220797720798\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.22963430459857878, Train acc: 0.8984875801282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.22945266760522365, Train acc: 0.898535696329814\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2303704376591605, Train acc: 0.8980442782526116\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.23001280839391994, Train acc: 0.8978857399910032\n",
      "Val loss: 0.25745710730552673, Val acc: 0.904\n",
      "Epoch 55/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.24195659192454103, Train acc: 0.8915598290598291\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.23642542026937008, Train acc: 0.8963675213675214\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2361404456707657, Train acc: 0.89627849002849\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2350460813484258, Train acc: 0.8959668803418803\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.23405045421205015, Train acc: 0.8965811965811966\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.23152919761581808, Train acc: 0.8978810541310541\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2296966811612485, Train acc: 0.8993437118437119\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.23057554228804433, Train acc: 0.8984708867521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.230714610824671, Train acc: 0.8983558879392213\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.22965722019728432, Train acc: 0.8986111111111111\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2298964848329415, Train acc: 0.8988684926184927\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2294892408006806, Train acc: 0.8988603988603988\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.23087546212952595, Train acc: 0.8982782708744247\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2304266531939217, Train acc: 0.8985424297924298\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.23014417311277485, Train acc: 0.8985933048433048\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.22990953491037536, Train acc: 0.8986712072649573\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.22986717972587917, Train acc: 0.8988970588235294\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.22922642775063898, Train acc: 0.8993500712250713\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.22930687750902093, Train acc: 0.8990946918578497\n",
      "Val loss: 0.2660149931907654, Val acc: 0.898\n",
      "Epoch 56/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.21669089797342944, Train acc: 0.9086538461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.21334269907102626, Train acc: 0.9047809829059829\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.22352743528762095, Train acc: 0.9019764957264957\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.22242516592049447, Train acc: 0.9025106837606838\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.22201963530009627, Train acc: 0.9022970085470086\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2212039323051346, Train acc: 0.90193198005698\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.22337778408867245, Train acc: 0.9004884004884005\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.22427974047903448, Train acc: 0.9004073183760684\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.22547142744304902, Train acc: 0.8998990978157645\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.22612337738784968, Train acc: 0.8999198717948718\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.22661543074812923, Train acc: 0.8996940559440559\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.22577288932302314, Train acc: 0.900329415954416\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.22570634850060212, Train acc: 0.900620479947403\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.22512761698592276, Train acc: 0.9004120879120879\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.22397362048554625, Train acc: 0.9011039886039887\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.22410903225899634, Train acc: 0.901191907051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.224390174955684, Train acc: 0.9009238310708899\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.22467227366815717, Train acc: 0.9009971509971509\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.22459265102560705, Train acc: 0.9008659469185785\n",
      "Val loss: 0.26849737763404846, Val acc: 0.898\n",
      "Epoch 57/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2454436900269272, Train acc: 0.8952991452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.23613268079665992, Train acc: 0.9002403846153846\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.22650305104264168, Train acc: 0.9020655270655271\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.223861071249295, Train acc: 0.9029113247863247\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2253583987108153, Train acc: 0.9014957264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.22539518154796712, Train acc: 0.9012197293447294\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.22576401622849943, Train acc: 0.9018238705738706\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.225334853053284, Train acc: 0.9022102029914529\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2245530228662072, Train acc: 0.9019171415004749\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2238887980930571, Train acc: 0.9018963675213675\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.22383281698836868, Train acc: 0.9020493395493395\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.22430934293986302, Train acc: 0.9010194088319088\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.22471565705951227, Train acc: 0.9009697567389875\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.22531084244404448, Train acc: 0.9007364163614163\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.22519865775974388, Train acc: 0.9004807692307693\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.22541870128634012, Train acc: 0.9003739316239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.22415293018952154, Train acc: 0.9010023881347411\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.22310711216652043, Train acc: 0.9013087606837606\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.222976211919288, Train acc: 0.9010205802968961\n",
      "Val loss: 0.2731531262397766, Val acc: 0.888\n",
      "Epoch 58/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.22712820800196412, Train acc: 0.8987713675213675\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.22133677806227636, Train acc: 0.9015758547008547\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.22051905227182936, Train acc: 0.9009081196581197\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.22099597758462286, Train acc: 0.9028445512820513\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.22055038009953296, Train acc: 0.9033653846153846\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2204899683255076, Train acc: 0.9039797008547008\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.22186485602905898, Train acc: 0.9029304029304029\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.22275475336588982, Train acc: 0.9022435897435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.22167606360623074, Train acc: 0.9024810066476733\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.22313480583521036, Train acc: 0.9013354700854701\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.22242929381785137, Train acc: 0.9017822455322455\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2230538122456765, Train acc: 0.9018206908831908\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.22390109588398815, Train acc: 0.9011135765943459\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.22343163580038972, Train acc: 0.9011370573870574\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.22254972532603814, Train acc: 0.9016915954415955\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.22198927528264686, Train acc: 0.9017928685897436\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.22235098028299854, Train acc: 0.9017722473604827\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.22162302493773126, Train acc: 0.9023029439696106\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.22133616693535743, Train acc: 0.9024685110211426\n",
      "Val loss: 0.25369513034820557, Val acc: 0.9\n",
      "Epoch 59/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.21162084152555874, Train acc: 0.9027777777777778\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.21877663042873907, Train acc: 0.9053151709401709\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.22216977321269504, Train acc: 0.9029558404558404\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.22606665071131837, Train acc: 0.9013087606837606\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2254150467678013, Train acc: 0.9018696581196581\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.22510659345491998, Train acc: 0.9007745726495726\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.22509199175278638, Train acc: 0.9013659951159951\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2232360805456455, Train acc: 0.9022435897435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.22301448922310685, Train acc: 0.9019764957264957\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.22205210312182067, Train acc: 0.9021100427350427\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.22103165784197995, Train acc: 0.9022921522921523\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.21960177823922403, Train acc: 0.9030448717948718\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.22023756175077253, Train acc: 0.9027572320841551\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.22012514085651028, Train acc: 0.9025488400488401\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.21975212345732922, Train acc: 0.9027777777777778\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2208951044266518, Train acc: 0.9023270566239316\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.22047682349790634, Train acc: 0.9024007038712921\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2199057201905298, Train acc: 0.9026293922127255\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.21937261677497527, Train acc: 0.902566914080072\n",
      "Val loss: 0.26577770709991455, Val acc: 0.892\n",
      "Epoch 60/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.20190697580448583, Train acc: 0.9086538461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2118026565314613, Train acc: 0.9049145299145299\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2135573809600284, Train acc: 0.9049145299145299\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.21495765355280322, Train acc: 0.9037126068376068\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.21472657482720847, Train acc: 0.9041132478632479\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.21454720173105385, Train acc: 0.9034900284900285\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2153576559936389, Train acc: 0.9027014652014652\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.21531521232448456, Train acc: 0.9032118055555556\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.21461041066494876, Train acc: 0.9032822886989553\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.21531227057178814, Train acc: 0.903258547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.21507640404984205, Train acc: 0.9036033411033411\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2158731435375166, Train acc: 0.9032006766381766\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2165017845617322, Train acc: 0.9032708744247205\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2158763497858621, Train acc: 0.9036744505494505\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2156299715495517, Train acc: 0.9039351851851852\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2163847044078458, Train acc: 0.9035122863247863\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.21555851659114306, Train acc: 0.9040818250377074\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.21546346190626975, Train acc: 0.9040687321937322\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.21501029819229997, Train acc: 0.9041273054430949\n",
      "Val loss: 0.26156744360923767, Val acc: 0.896\n",
      "Epoch 61/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.20656589622426236, Train acc: 0.9030448717948718\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2049047247403198, Train acc: 0.9085202991452992\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.20585625563506726, Train acc: 0.9077635327635327\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.20950192913540408, Train acc: 0.9069177350427351\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2106141054795848, Train acc: 0.9064102564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2101301140340305, Train acc: 0.907051282051282\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.20931012526391043, Train acc: 0.9071657509157509\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.21075786970571703, Train acc: 0.9074185363247863\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.21053914557735579, Train acc: 0.9079712725546059\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.21034525780277885, Train acc: 0.9075053418803419\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2120237480574045, Train acc: 0.9065899378399378\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.21217209770957118, Train acc: 0.9064948361823362\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2128247008913054, Train acc: 0.905715811965812\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2138330192829372, Train acc: 0.9050289987789988\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.21403518024502996, Train acc: 0.9048433048433049\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.21531147779658055, Train acc: 0.9042134081196581\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2152178149788358, Train acc: 0.9043174962292609\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2144378187366587, Train acc: 0.9046474358974359\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.21391053871158217, Train acc: 0.905195681511471\n",
      "Val loss: 0.27804034948349, Val acc: 0.888\n",
      "Epoch 62/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.20388170710613585, Train acc: 0.9075854700854701\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2110892641557078, Train acc: 0.9063835470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.21292512278016815, Train acc: 0.9060719373219374\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.21316387192306355, Train acc: 0.9065838675213675\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2119178580868448, Train acc: 0.9064102564102564\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.21288075432329945, Train acc: 0.9061609686609686\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2137039221586689, Train acc: 0.9061355311355311\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.21462963597498771, Train acc: 0.9058493589743589\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.21420645844704292, Train acc: 0.905982905982906\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.21444251477463633, Train acc: 0.9061164529914529\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2139385161456806, Train acc: 0.905982905982906\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.21424735795993072, Train acc: 0.9056712962962963\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2137799447672999, Train acc: 0.9055719921104537\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.21306070477971228, Train acc: 0.9059065934065934\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.21326850677954506, Train acc: 0.9058760683760684\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.21215188124965334, Train acc: 0.9058493589743589\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.21186472998829928, Train acc: 0.9059043489190548\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2115765944059588, Train acc: 0.905715811965812\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.21190314014658862, Train acc: 0.9056595816464238\n",
      "Val loss: 0.2630153000354767, Val acc: 0.898\n",
      "Epoch 63/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.20556036121824867, Train acc: 0.906517094017094\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.20220213370700169, Train acc: 0.9114583333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.20442748795717192, Train acc: 0.9097222222222222\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2074462684810671, Train acc: 0.9079193376068376\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2075284187992414, Train acc: 0.9084935897435897\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2074590734500661, Train acc: 0.9082086894586895\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2083714350293844, Train acc: 0.9073565323565324\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.20846233919310647, Train acc: 0.9072516025641025\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2072855615767262, Train acc: 0.9080603038936372\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.20703404135683662, Train acc: 0.9081730769230769\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2070389925599932, Train acc: 0.9081196581196581\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.20656814788364702, Train acc: 0.9086093304843305\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.2065316985565177, Train acc: 0.9083867521367521\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2086869197054988, Train acc: 0.9071466727716728\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.20906060740691304, Train acc: 0.9068019943019943\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.20972644385650882, Train acc: 0.9067674946581197\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2094698439311478, Train acc: 0.9073183760683761\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.20966172214431186, Train acc: 0.9076299857549858\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.20860863889404582, Train acc: 0.9079650247413406\n",
      "Val loss: 0.25840866565704346, Val acc: 0.904\n",
      "Epoch 64/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.19283758466824508, Train acc: 0.9091880341880342\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.21037939082608265, Train acc: 0.9045138888888888\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2046903865756812, Train acc: 0.9072293447293447\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.20595963323950514, Train acc: 0.9068509615384616\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.20710475020683727, Train acc: 0.9075854700854701\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2051650702083043, Train acc: 0.9082086894586895\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.20605986243766597, Train acc: 0.9078144078144078\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.20597246813420683, Train acc: 0.9083533653846154\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.20415318510004257, Train acc: 0.9094254510921178\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.20511784631536048, Train acc: 0.909107905982906\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.20573303577574817, Train acc: 0.9088723776223776\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.20597651513808132, Train acc: 0.9083199786324786\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.20642527023833648, Train acc: 0.9082223865877712\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2066024055247342, Train acc: 0.9081387362637363\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.20722683405562004, Train acc: 0.9077457264957265\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.20661816624009138, Train acc: 0.9082198183760684\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.20673344707769206, Train acc: 0.9079939668174962\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2071739748254744, Train acc: 0.9078377255460589\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.20755340550404866, Train acc: 0.9076417004048583\n",
      "Val loss: 0.26403969526290894, Val acc: 0.894\n",
      "Early stopping at epoch 64 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 1808.9483 [s]\n",
      "Combinación 12/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.6433690786361694, Train acc: 0.5432024572649573\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.6138341570766563, Train acc: 0.5578926282051282\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.6011938464607608, Train acc: 0.5655048076923077\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5929827283844988, Train acc: 0.5705128205128205\n",
      "Val loss: 0.6197395920753479, Val acc: 0.836\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5554930707837782, Train acc: 0.5965544871794872\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5538550662434, Train acc: 0.5965544871794872\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5515843983898815, Train acc: 0.5980902777777778\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5486747189464732, Train acc: 0.5995259081196581\n",
      "Val loss: 0.6300168633460999, Val acc: 0.836\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5547786984178755, Train acc: 0.5940838675213675\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5446591932549436, Train acc: 0.6017962072649573\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5445477546450079, Train acc: 0.6009392806267806\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.54335983371378, Train acc: 0.6019798344017094\n",
      "Val loss: 0.6377946138381958, Val acc: 0.848\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5449900295999315, Train acc: 0.6008947649572649\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5392318430364641, Train acc: 0.6028645833333334\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5408849244953221, Train acc: 0.6023860398860399\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5376936315089209, Train acc: 0.6049011752136753\n",
      "Val loss: 0.6292759776115417, Val acc: 0.832\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.538995247366082, Train acc: 0.5998931623931624\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5334119307689178, Train acc: 0.6067708333333334\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5344199798046014, Train acc: 0.6060140669515669\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5348542623030834, Train acc: 0.6048844818376068\n",
      "Val loss: 0.6366768479347229, Val acc: 0.826\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5379713913823805, Train acc: 0.6071047008547008\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.534044578544095, Train acc: 0.6098424145299145\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5326109819772237, Train acc: 0.6103988603988604\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5307768408177246, Train acc: 0.6106770833333334\n",
      "Val loss: 0.6660622954368591, Val acc: 0.778\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5280248991444579, Train acc: 0.6106436965811965\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5277169902100523, Train acc: 0.6110109508547008\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5246509659154462, Train acc: 0.6134259259259259\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5262337952342808, Train acc: 0.6131810897435898\n",
      "Val loss: 0.6614291667938232, Val acc: 0.806\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5280568243091942, Train acc: 0.609375\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.524880628555249, Train acc: 0.6128138354700855\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5250949930091868, Train acc: 0.6128917378917379\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5236790482050333, Train acc: 0.6139489850427351\n",
      "Val loss: 0.6482805609703064, Val acc: 0.804\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5210397663788918, Train acc: 0.6179220085470085\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5232993780802457, Train acc: 0.6141159188034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5231519062974175, Train acc: 0.6135149572649573\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5209437964054254, Train acc: 0.6152510683760684\n",
      "Val loss: 0.6556958556175232, Val acc: 0.794\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5227094944725689, Train acc: 0.6151175213675214\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5185768008232117, Train acc: 0.6182224893162394\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5197978209905815, Train acc: 0.6162081552706553\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5214595701067876, Train acc: 0.6165030715811965\n",
      "Val loss: 0.6832199096679688, Val acc: 0.792\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5163797249651363, Train acc: 0.6205261752136753\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5175443520912757, Train acc: 0.6182224893162394\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.515114140816224, Train acc: 0.6207710113960114\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5160979559151535, Train acc: 0.6197081997863247\n",
      "Val loss: 0.6668933033943176, Val acc: 0.77\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5329806093986218, Train acc: 0.6097088675213675\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5250453258681501, Train acc: 0.6139489850427351\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5251283628648502, Train acc: 0.6142717236467237\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5238894391645733, Train acc: 0.6151676014957265\n",
      "Val loss: 0.6359339952468872, Val acc: 0.806\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5344756872226031, Train acc: 0.6069711538461539\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.528652748108929, Train acc: 0.6101428952991453\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5278881379002519, Train acc: 0.6124243233618234\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5237552683450218, Train acc: 0.6148337339743589\n",
      "Val loss: 0.6288070678710938, Val acc: 0.806\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5192463242600107, Train acc: 0.6124465811965812\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5172804040022385, Train acc: 0.6176215277777778\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5165123745077356, Train acc: 0.6194577991452992\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5158941596746445, Train acc: 0.6198083600427351\n",
      "Val loss: 0.6575089693069458, Val acc: 0.786\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5213449969250932, Train acc: 0.617988782051282\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.5203122132354312, Train acc: 0.6172542735042735\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5201036497059032, Train acc: 0.6179665242165242\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5169432236470728, Train acc: 0.6196581196581197\n",
      "Val loss: 0.6590688824653625, Val acc: 0.81\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5284278522699307, Train acc: 0.6093082264957265\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.521860321617534, Train acc: 0.6161858974358975\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.5217696334901358, Train acc: 0.6162081552706553\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5184064880650268, Train acc: 0.6189903846153846\n",
      "Val loss: 0.6400653719902039, Val acc: 0.794\n",
      "Early stopping at epoch 16 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 363.4131 [s]\n",
      "Combinación 13/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.9524583230670701, Train acc: 0.4391025641025641\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.8589602166261429, Train acc: 0.4934561965811966\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.8089501866248259, Train acc: 0.5220797720797721\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.7729823821757593, Train acc: 0.5430021367521367\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.7454847587479485, Train acc: 0.558974358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.7213621129762073, Train acc: 0.5732282763532763\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.7007794253555409, Train acc: 0.5849358974358975\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.6860565007942864, Train acc: 0.5944845085470085\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.6720198644475494, Train acc: 0.6028015194681862\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.6582703228434946, Train acc: 0.6119391025641026\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.6454955285014158, Train acc: 0.6202894327894328\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.6357042188955169, Train acc: 0.6263577279202279\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.6266198407694511, Train acc: 0.6325402695595004\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.6161969412250106, Train acc: 0.6391941391941391\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.6089015808964726, Train acc: 0.6432692307692308\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.5999932469895635, Train acc: 0.6491052350427351\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.5933308852279288, Train acc: 0.653390522875817\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.5856099893805082, Train acc: 0.6582828822412156\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.5796777467857971, Train acc: 0.6616059379217274\n",
      "Val loss: 0.4545724093914032, Val acc: 0.838\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.4478579797805884, Train acc: 0.7403846153846154\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.44116577595217615, Train acc: 0.7417200854700855\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.439645402579226, Train acc: 0.7431445868945868\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.43985251222665495, Train acc: 0.7436565170940171\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.43657257503423935, Train acc: 0.7446581196581197\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.43299911451390666, Train acc: 0.7482193732193733\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.43266108135382336, Train acc: 0.7482448107448108\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.4299739480941978, Train acc: 0.7504006410256411\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.42956211826174123, Train acc: 0.750534188034188\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.4255032658704326, Train acc: 0.7533920940170941\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.4218561895591118, Train acc: 0.7554147241647242\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.4205118266287183, Train acc: 0.7569444444444444\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.4175082343354341, Train acc: 0.7591222879684418\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.4144309335030042, Train acc: 0.7610271672771672\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.41238171318147937, Train acc: 0.7623397435897435\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.41131456692217505, Train acc: 0.7629707532051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.409360563098755, Train acc: 0.7642816742081447\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.40731456735704696, Train acc: 0.7655062915479582\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.4052746862883617, Train acc: 0.7665035987404408\n",
      "Val loss: 0.4869631826877594, Val acc: 0.854\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.39124279653924143, Train acc: 0.7702991452991453\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.37482957732983124, Train acc: 0.7776442307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.3689459138191663, Train acc: 0.7839209401709402\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3683879983285044, Train acc: 0.7849225427350427\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3681520555773352, Train acc: 0.7850961538461538\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3678988989474427, Train acc: 0.7857460826210826\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.36853841740424237, Train acc: 0.7861340048840049\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.3685641800586739, Train acc: 0.7862246260683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.36633632454494014, Train acc: 0.7868589743589743\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.36374508538562006, Train acc: 0.788301282051282\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3632074092466329, Train acc: 0.7881458818958819\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.36206300642627937, Train acc: 0.7897079772079773\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.36156575200512253, Train acc: 0.7902284681130834\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.36205050164563024, Train acc: 0.7897779304029304\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.3609271697763704, Train acc: 0.790491452991453\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.35956878173682427, Train acc: 0.7917835202991453\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.358134358514488, Train acc: 0.79322209653092\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.35783476086027377, Train acc: 0.7935363247863247\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3584355762747028, Train acc: 0.7934519793072424\n",
      "Val loss: 0.4843665063381195, Val acc: 0.856\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.3519158130272841, Train acc: 0.7980769230769231\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.35036570470557254, Train acc: 0.8026175213675214\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.34549600866615265, Train acc: 0.8062678062678063\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3430332063546038, Train acc: 0.8058226495726496\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.34202019401595124, Train acc: 0.8056089743589744\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3391392554270576, Train acc: 0.8070245726495726\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3373431887964335, Train acc: 0.8080738705738706\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.33583304265307057, Train acc: 0.8086271367521367\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.33579704154137757, Train acc: 0.8082858499525166\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.33512700661125344, Train acc: 0.8088141025641026\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3339832253065235, Train acc: 0.809586247086247\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3344489265541066, Train acc: 0.8089832621082621\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.334935705278517, Train acc: 0.8085963182117029\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.335215736376133, Train acc: 0.8086271367521367\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.33345753622700347, Train acc: 0.809579772079772\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3344817917125347, Train acc: 0.8087272970085471\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3333038557124174, Train acc: 0.8094048516842635\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.3321769120842077, Train acc: 0.8097993827160493\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.33159829557505055, Train acc: 0.8102789023841656\n",
      "Val loss: 0.45830851793289185, Val acc: 0.856\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.32799808031473404, Train acc: 0.8095619658119658\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.31387009439814806, Train acc: 0.8187767094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.31229527697943554, Train acc: 0.8201566951566952\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.3150641510629246, Train acc: 0.8180422008547008\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.3129555870579858, Train acc: 0.819818376068376\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.3189002439549506, Train acc: 0.8167735042735043\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.3174723022150033, Train acc: 0.8166208791208791\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.31735525843806756, Train acc: 0.8174746260683761\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.31629272432182365, Train acc: 0.8184947768281101\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.3162283167243004, Train acc: 0.8185363247863248\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.3167780961352165, Train acc: 0.8184246309246309\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.3151365108151212, Train acc: 0.8191773504273504\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.3143273328261341, Train acc: 0.8197937212360289\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.31495700054261855, Train acc: 0.8193490537240538\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.31363362962873576, Train acc: 0.819818376068376\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.3121958681039958, Train acc: 0.8203959668803419\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.3113614484150845, Train acc: 0.8206228004022121\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.31065482387260834, Train acc: 0.8211657169990503\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.3103563361778165, Train acc: 0.8213000449842555\n",
      "Val loss: 0.43265634775161743, Val acc: 0.848\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.28915740804285067, Train acc: 0.8306623931623932\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.297713025385498, Train acc: 0.8262553418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2936920819968579, Train acc: 0.8298611111111112\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.299308374333076, Train acc: 0.8263221153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2959758531588774, Train acc: 0.8273504273504273\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2938165148438891, Train acc: 0.8292824074074074\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.29152232153802854, Train acc: 0.8302808302808303\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.29271225422684455, Train acc: 0.8290598290598291\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2922257942634883, Train acc: 0.8298611111111112\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.29201829961986625, Train acc: 0.8300747863247864\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.29291216300825046, Train acc: 0.8290112665112666\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.29265250704991513, Train acc: 0.8295049857549858\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.29153788951883813, Train acc: 0.8306418474687706\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.29123570462540976, Train acc: 0.8314064407814408\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2897716636416579, Train acc: 0.8320868945868946\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2897707330078905, Train acc: 0.8320813301282052\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.28945912036421073, Train acc: 0.8322649572649573\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2885055139745742, Train acc: 0.8325320512820513\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.28893759559121046, Train acc: 0.8324898785425101\n",
      "Val loss: 0.41944801807403564, Val acc: 0.872\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2642173297130145, Train acc: 0.8421474358974359\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2673775593185017, Train acc: 0.8416132478632479\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.2751016733992813, Train acc: 0.8400997150997151\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.28288900680266893, Train acc: 0.8371394230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.28719766840466066, Train acc: 0.833707264957265\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.28894660791695287, Train acc: 0.8319088319088319\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.28660934523057296, Train acc: 0.8330280830280831\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2845834953407956, Train acc: 0.8339009081196581\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2837198112595115, Train acc: 0.8341346153846154\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2834817073284051, Train acc: 0.8344818376068376\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.28250037728813887, Train acc: 0.8351544289044289\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2819226517042204, Train acc: 0.8356036324786325\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.28282715700552075, Train acc: 0.8355317225509533\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.28266766781990343, Train acc: 0.8362141330891331\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.2823964830901888, Train acc: 0.8363960113960114\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2816983909688444, Train acc: 0.837089342948718\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.28057985452025785, Train acc: 0.8377796631473102\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.27989084930190106, Train acc: 0.8380519943019943\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.27859088305251534, Train acc: 0.8383097165991903\n",
      "Val loss: 0.41534268856048584, Val acc: 0.856\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.27256971024549925, Train acc: 0.8464209401709402\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2618794824577804, Train acc: 0.8489583333333334\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.26132186143486585, Train acc: 0.8486467236467237\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.26744124328351426, Train acc: 0.8456196581196581\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.26872029898003635, Train acc: 0.8457799145299145\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.26950480296486123, Train acc: 0.8450854700854701\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.27014919367683676, Train acc: 0.8447039072039072\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.26976316169095343, Train acc: 0.8448517628205128\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2685753416981113, Train acc: 0.8456493352326686\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.26851639406293887, Train acc: 0.8453792735042736\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.26687412599583604, Train acc: 0.8459838772338772\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2668928454638037, Train acc: 0.8458422364672364\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.26720002550266825, Train acc: 0.8453731097961867\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2663588293888339, Train acc: 0.8462873931623932\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.267220902035379, Train acc: 0.8460648148148148\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.26706524479051685, Train acc: 0.8459869123931624\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.26702055108792944, Train acc: 0.8459495977878331\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.26735671504214054, Train acc: 0.846139007597341\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.26688121351716326, Train acc: 0.8465896311291048\n",
      "Val loss: 0.4196420907974243, Val acc: 0.868\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.246782663031521, Train acc: 0.8544337606837606\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2499563851290279, Train acc: 0.8575053418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.25507617383091535, Train acc: 0.8524750712250713\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2541841497469662, Train acc: 0.8532986111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.25843265841149876, Train acc: 0.8513888888888889\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.25743817045562967, Train acc: 0.8526531339031339\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2565008269779848, Train acc: 0.8528311965811965\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.25411252376551813, Train acc: 0.8540998931623932\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.25273732811297445, Train acc: 0.8547602089268755\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2525490716736541, Train acc: 0.8545138888888889\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.2530566519003278, Train acc: 0.8538995726495726\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.25265673276281087, Train acc: 0.8543892450142451\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.25228614560441703, Train acc: 0.8547008547008547\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2522697996016096, Train acc: 0.8542620573870574\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.25394051043566135, Train acc: 0.8531695156695157\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2547176028164024, Train acc: 0.8529146634615384\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2554766515381316, Train acc: 0.8522498743086978\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.25548603280260807, Train acc: 0.8524157169990503\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2543741977273801, Train acc: 0.8531685784975259\n",
      "Val loss: 0.36624616384506226, Val acc: 0.89\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2643597258461846, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.24876263062668663, Train acc: 0.8551014957264957\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.24566361346305945, Train acc: 0.8559472934472935\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.24328981031082633, Train acc: 0.8581063034188035\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.24165735914666428, Train acc: 0.8603632478632479\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.24502326467437963, Train acc: 0.858840811965812\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.245293661533549, Train acc: 0.8587454212454212\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.24451353840338877, Train acc: 0.8590411324786325\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.24533965060061086, Train acc: 0.8587962962962963\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.24640127174619936, Train acc: 0.8578258547008547\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.24718233289974276, Train acc: 0.8575660450660451\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.24732834131502018, Train acc: 0.8577056623931624\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.24888083954965654, Train acc: 0.8565705128205128\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.24874094372412806, Train acc: 0.8566849816849816\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.24853843041295, Train acc: 0.8564992877492877\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2482483251991435, Train acc: 0.8566539797008547\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.248534729435493, Train acc: 0.8564291101055806\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.24924257379962736, Train acc: 0.8559918091168092\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.24943873504100486, Train acc: 0.8556145973909132\n",
      "Val loss: 0.3868046998977661, Val acc: 0.882\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.24776463121430486, Train acc: 0.8589743589743589\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.24704785645008087, Train acc: 0.8609775641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.24431095473956518, Train acc: 0.8624465811965812\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.24484648007867682, Train acc: 0.8607772435897436\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.24076606450427293, Train acc: 0.861965811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.23870308383068128, Train acc: 0.8636039886039886\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2396338461592375, Train acc: 0.8635149572649573\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.23965636233233997, Train acc: 0.8632478632478633\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.23975797325853834, Train acc: 0.8630104463437797\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.2399389701648655, Train acc: 0.8619123931623932\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.24021521759477807, Train acc: 0.8622766122766122\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2391126450904754, Train acc: 0.8625133547008547\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.23926130192872336, Train acc: 0.8621589414858646\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.24020876612637068, Train acc: 0.8613209706959707\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.240444375171281, Train acc: 0.8610576923076924\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.23988831667309132, Train acc: 0.8616286057692307\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.24037677304357544, Train acc: 0.8615196078431373\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2410470309746005, Train acc: 0.8611853038936372\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.2411760311377676, Train acc: 0.8610267656320288\n",
      "Val loss: 0.38194310665130615, Val acc: 0.864\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.22852218240244776, Train acc: 0.8632478632478633\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.22455969987771449, Train acc: 0.8656517094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.22895654798233273, Train acc: 0.8663639601139601\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.22822519878928477, Train acc: 0.8671875\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.23123243970748705, Train acc: 0.8658653846153846\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2365058278638413, Train acc: 0.8633368945868946\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.24065476515863696, Train acc: 0.8609203296703297\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2406847667362955, Train acc: 0.8612112713675214\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2389195898206372, Train acc: 0.8625059354226021\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.23590852335477486, Train acc: 0.8644764957264958\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.234747570387226, Train acc: 0.8647047397047397\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.23405881010611512, Train acc: 0.8655181623931624\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.23322973728062368, Train acc: 0.8659804404996713\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2325015329688751, Train acc: 0.8663576007326007\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.23337181905905405, Train acc: 0.8662037037037037\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2343087134580327, Train acc: 0.8654013087606838\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.23461420330616503, Train acc: 0.8650389643036702\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2339687384771146, Train acc: 0.8655626780626781\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.23420378025512248, Train acc: 0.865286212325686\n",
      "Val loss: 0.3963087797164917, Val acc: 0.876\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2321464871494179, Train acc: 0.8608440170940171\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.237089384570081, Train acc: 0.8639155982905983\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.23271906299468798, Train acc: 0.8661858974358975\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.23333581387359872, Train acc: 0.8625133547008547\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2320067106403856, Train acc: 0.8644764957264958\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.23040123190465475, Train acc: 0.8652065527065527\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.23009255266451573, Train acc: 0.8651175213675214\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.23049688377441505, Train acc: 0.8653846153846154\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.23002739004429928, Train acc: 0.8658000949667616\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.22887224352512603, Train acc: 0.8669070512820513\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.23097845133923706, Train acc: 0.8654574592074592\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.23142665304094978, Train acc: 0.8655181623931624\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.23091672829191282, Train acc: 0.8654873438527285\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.2310405304252883, Train acc: 0.8655563186813187\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.23075594814584466, Train acc: 0.8657941595441595\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2306133142680439, Train acc: 0.8656850961538461\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.23009754143479244, Train acc: 0.8660287833081951\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.23124247613201115, Train acc: 0.8658297720797721\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.23056046788723578, Train acc: 0.8662421277552856\n",
      "Val loss: 0.3857381343841553, Val acc: 0.886\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.21686701272797382, Train acc: 0.8709935897435898\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.22235924712358376, Train acc: 0.8701923076923077\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.21797183355544708, Train acc: 0.8738425925925926\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.2204499274619624, Train acc: 0.8723958333333334\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2167384998156474, Train acc: 0.8748397435897436\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.21621606529735432, Train acc: 0.8751780626780626\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.21663988841940923, Train acc: 0.8753815628815629\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.21801897339745718, Train acc: 0.8748998397435898\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2172649207773485, Train acc: 0.8751483855650523\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.217766020179559, Train acc: 0.8749465811965812\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.21966461111916621, Train acc: 0.8735431235431236\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2196388333409685, Train acc: 0.8737090455840456\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.21914927496340306, Train acc: 0.8734590729783037\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.21868725790117977, Train acc: 0.8736072954822954\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.21898062874873478, Train acc: 0.8733440170940171\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.21938567670285064, Train acc: 0.8732471955128205\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.21926521001942736, Train acc: 0.872941804927099\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.21995684810527596, Train acc: 0.8725813152896487\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.22070281877543976, Train acc: 0.8723150022492128\n",
      "Val loss: 0.3861119747161865, Val acc: 0.886\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.23203744376317048, Train acc: 0.8688568376068376\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.22334349028065673, Train acc: 0.8696581196581197\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.21892772619201248, Train acc: 0.8729522792022792\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.21846009504336578, Train acc: 0.8731971153846154\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2147484205217443, Train acc: 0.8746794871794872\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.21414372000174645, Train acc: 0.8736645299145299\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.21655035511987406, Train acc: 0.8726343101343101\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.21809503076295567, Train acc: 0.8720619658119658\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.21697050468874113, Train acc: 0.8728335707502374\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.21562398483610562, Train acc: 0.8737980769230769\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.21595974056963413, Train acc: 0.8737859362859363\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.2149051836031115, Train acc: 0.8746216168091168\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.21443682362342023, Train acc: 0.8748561801446417\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.21372160240461974, Train acc: 0.8754387973137974\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.21303423978069908, Train acc: 0.8756410256410256\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2121576175181211, Train acc: 0.8762353098290598\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.21149819852225202, Train acc: 0.8766811211664153\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2122270886495299, Train acc: 0.8761870845204178\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.21248255774975358, Train acc: 0.8761667791273055\n",
      "Val loss: 0.36937621235847473, Val acc: 0.874\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2082762839193018, Train acc: 0.8774038461538461\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.20035981603412548, Train acc: 0.8822115384615384\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.20089804620994123, Train acc: 0.8801638176638177\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.20166466518854484, Train acc: 0.8802751068376068\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.20247140258041202, Train acc: 0.8793269230769231\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.20104198540929716, Train acc: 0.8802528490028491\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.2000593522038215, Train acc: 0.8804563492063492\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.20193627857977253, Train acc: 0.8795405982905983\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.2033759925624131, Train acc: 0.8791844729344729\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.20381770801977214, Train acc: 0.8792200854700855\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.20377306457791294, Train acc: 0.8792735042735043\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.20409815782346788, Train acc: 0.87931801994302\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.20506300948643513, Train acc: 0.8787804076265615\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.20454528721752185, Train acc: 0.878968253968254\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.20433498522919466, Train acc: 0.8794159544159544\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.2040877032857866, Train acc: 0.8795405982905983\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.2042755174533454, Train acc: 0.8797762694821518\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.2043543703118224, Train acc: 0.8797483380816714\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.20508362623991538, Train acc: 0.8794843679712101\n",
      "Val loss: 0.38791394233703613, Val acc: 0.872\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.20429049928983053, Train acc: 0.8808760683760684\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.2012535774618642, Train acc: 0.8824786324786325\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.1999300709816805, Train acc: 0.8839031339031339\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.20200606448273373, Train acc: 0.8817441239316239\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.2000531934774839, Train acc: 0.8830128205128205\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.2007325900416089, Train acc: 0.8835915242165242\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.20079607861309842, Train acc: 0.8833562271062271\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.2029293350652497, Train acc: 0.8823450854700855\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.20487412140976216, Train acc: 0.8814696106362773\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.20432799056045012, Train acc: 0.8819711538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.20364558077312386, Train acc: 0.8821386946386947\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.20251368136381323, Train acc: 0.8827012108262108\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.20265500250907417, Train acc: 0.8827457264957265\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.20300467120840582, Train acc: 0.8829746642246642\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.202956486584624, Train acc: 0.882710113960114\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.20326897505527505, Train acc: 0.8823450854700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.20427121894406935, Train acc: 0.8816616390145802\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.20488886197308984, Train acc: 0.8810986467236467\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.20429884212688987, Train acc: 0.8814946018893387\n",
      "Val loss: 0.364616334438324, Val acc: 0.888\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.2034900142596318, Train acc: 0.8827457264957265\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.1876140729420715, Train acc: 0.890892094017094\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.1844470912319982, Train acc: 0.8897792022792023\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.18559445039584085, Train acc: 0.8892895299145299\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.1875833607891686, Train acc: 0.8886752136752136\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.18928264353180205, Train acc: 0.8880876068376068\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.18972032700957542, Train acc: 0.8884310134310134\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.19048514297534513, Train acc: 0.8880876068376068\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.19100699280826455, Train acc: 0.8876424501424501\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1917863497685673, Train acc: 0.8869925213675214\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.1920931089695517, Train acc: 0.8864364801864801\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.190570566585643, Train acc: 0.8871082621082621\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.19002453772685465, Train acc: 0.8874917817225509\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1905495449946113, Train acc: 0.8869810744810744\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.1910896793814466, Train acc: 0.8871082621082621\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.19201133170953164, Train acc: 0.8866853632478633\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.19218546396347072, Train acc: 0.8868935394670688\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1928433537242646, Train acc: 0.8866185897435898\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.19309485452672784, Train acc: 0.8863866396761133\n",
      "Val loss: 0.38258087635040283, Val acc: 0.888\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.18885968956682417, Train acc: 0.8904914529914529\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.19452122140389222, Train acc: 0.8880876068376068\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.19039770924722368, Train acc: 0.8892450142450142\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.1931248511482253, Train acc: 0.8877537393162394\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.19184398252485144, Train acc: 0.8886217948717948\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.1925005133866075, Train acc: 0.8883547008547008\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.19178736540096583, Train acc: 0.8888125763125763\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.1890137313475084, Train acc: 0.889823717948718\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.1898326217236682, Train acc: 0.8892450142450142\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.19059217383082097, Train acc: 0.8887553418803419\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.1905608918711763, Train acc: 0.888548951048951\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.1905448743091774, Train acc: 0.8886440527065527\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.19047952562394227, Train acc: 0.8884985207100592\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1900551353873496, Train acc: 0.888640873015873\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.19056237430823833, Train acc: 0.8883547008547008\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.19092311090829536, Train acc: 0.8881877670940171\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.19014864405940202, Train acc: 0.8885275263951734\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.19206463140484162, Train acc: 0.8874495489078822\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.19257679945289471, Train acc: 0.8869770580296896\n",
      "Val loss: 0.3779055178165436, Val acc: 0.886\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.18390012831769437, Train acc: 0.8955662393162394\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.19925742781060374, Train acc: 0.8839476495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.19677997450543266, Train acc: 0.8831018518518519\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.19330056075356963, Train acc: 0.8860844017094017\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.19437833183850997, Train acc: 0.8856837606837606\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.19157054614901883, Train acc: 0.8871082621082621\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.1891304690286208, Train acc: 0.8887362637362637\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.18975769074108356, Train acc: 0.8879874465811965\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.18906800879089466, Train acc: 0.888829534662868\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.18941195134678457, Train acc: 0.8886752136752136\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.18760897603577759, Train acc: 0.8896658896658897\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.18747172167158535, Train acc: 0.8897124287749287\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.18565314195016588, Train acc: 0.89038872452334\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.18558662328346018, Train acc: 0.8904532967032966\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.18667108115137812, Train acc: 0.8899928774928775\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.18590953931785548, Train acc: 0.8904413728632479\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.18567519372375002, Train acc: 0.8903971845148315\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.18561281180336492, Train acc: 0.8905062915479582\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.18536046194268732, Train acc: 0.8908850652271705\n",
      "Val loss: 0.3703380227088928, Val acc: 0.872\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.17811109507695222, Train acc: 0.8912927350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.1743463962378665, Train acc: 0.8939636752136753\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.17719012961598204, Train acc: 0.8928952991452992\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.18008179580553985, Train acc: 0.8922275641025641\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.18323799425210707, Train acc: 0.8905982905982905\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.18442561840399718, Train acc: 0.8905359686609686\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.18485795145215278, Train acc: 0.8903006715506715\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.18364704594525516, Train acc: 0.891426282051282\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.18362933550125513, Train acc: 0.8908772554605888\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.18215768635909782, Train acc: 0.8919337606837607\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.18216077438624403, Train acc: 0.8920697358197358\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.18208857752552568, Train acc: 0.8922275641025641\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.18270680550816495, Train acc: 0.8918885601577909\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.18188518963035502, Train acc: 0.892342032967033\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.181467460792119, Train acc: 0.892485754985755\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.18029700571066168, Train acc: 0.893112313034188\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.17992955229356097, Train acc: 0.8933823529411765\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1798311217749051, Train acc: 0.893355294396961\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.17987062421753447, Train acc: 0.8930780476833109\n",
      "Val loss: 0.39977702498435974, Val acc: 0.884\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.1895756464228671, Train acc: 0.8864850427350427\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.18291469733429772, Train acc: 0.8904914529914529\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.18071822057931852, Train acc: 0.8895121082621082\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.18158004210036024, Train acc: 0.8896233974358975\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.1809732531125729, Train acc: 0.8897435897435897\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.18174795912541555, Train acc: 0.8899127492877493\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.18239934385841727, Train acc: 0.8898046398046398\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.18234332752788168, Train acc: 0.8902911324786325\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.18092240163205583, Train acc: 0.8913817663817664\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1788462004154666, Train acc: 0.8925747863247864\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.17824395851354408, Train acc: 0.8931381118881119\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.1781959193692333, Train acc: 0.8932959401709402\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.17866404625457968, Train acc: 0.893224030243261\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.17812015480101473, Train acc: 0.8933340964590964\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.17813288677282144, Train acc: 0.89375\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.1778130085947804, Train acc: 0.8939803685897436\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.17751079014853174, Train acc: 0.8941365007541479\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1771903339232135, Train acc: 0.8943049620132953\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.17730127895704217, Train acc: 0.8943854026090868\n",
      "Val loss: 0.35820272564888, Val acc: 0.884\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.17292657150672033, Train acc: 0.8912927350427351\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.17252865249020422, Train acc: 0.8907585470085471\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.1684072895607038, Train acc: 0.8956552706552706\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.16773639724422723, Train acc: 0.8986378205128205\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.1676270535104295, Train acc: 0.9000534188034188\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.16707183397308714, Train acc: 0.9004184472934473\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.16834153728025156, Train acc: 0.8999923687423688\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.16818606368719768, Train acc: 0.9000734508547008\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.1683397163820063, Train acc: 0.9005816714150048\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1684880485965146, Train acc: 0.900667735042735\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.16857138487402562, Train acc: 0.9008109945609946\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.1679331190470192, Train acc: 0.9007300569800569\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.16888059967642627, Train acc: 0.9002095660749507\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1694726538645406, Train acc: 0.8997061965811965\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.17002249610984427, Train acc: 0.8994301994301994\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.17018139068212393, Train acc: 0.899238782051282\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.17063756371380517, Train acc: 0.898865635997989\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.17089406814486327, Train acc: 0.8985636277302944\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.17103004791060386, Train acc: 0.8986026765632029\n",
      "Val loss: 0.371560275554657, Val acc: 0.886\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.16553587918607598, Train acc: 0.8990384615384616\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.16789410903285712, Train acc: 0.9011752136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.17022590003801547, Train acc: 0.9000178062678063\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.1707263555791643, Train acc: 0.8997729700854701\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.17184282194854866, Train acc: 0.8990384615384616\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.16922543030179124, Train acc: 0.90059650997151\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.17112054811365293, Train acc: 0.8998397435897436\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.1695225566275354, Train acc: 0.9007745726495726\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.16906368031970456, Train acc: 0.9007300569800569\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.16793134877314933, Train acc: 0.9016025641025641\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.16956640049386784, Train acc: 0.9006410256410257\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.17014235181686205, Train acc: 0.9004852207977208\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.16960938474718165, Train acc: 0.9006410256410257\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.16967752786928628, Train acc: 0.9004502442002442\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.16957789712142401, Train acc: 0.9001602564102564\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.16968734449364692, Train acc: 0.8999232104700855\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.16976698936380413, Train acc: 0.8999183006535948\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.16929328346807165, Train acc: 0.9000919990503324\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.16929139615821237, Train acc: 0.8999240890688259\n",
      "Val loss: 0.42918819189071655, Val acc: 0.852\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.1627082897302432, Train acc: 0.9009081196581197\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.16021142390548673, Train acc: 0.9043803418803419\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.1569801053812361, Train acc: 0.9063390313390314\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.1629600584284108, Train acc: 0.9026442307692307\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.16230850603081223, Train acc: 0.9029380341880342\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.1625715245272529, Train acc: 0.9023771367521367\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.1642226394770783, Train acc: 0.9015186202686203\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.16400862395222116, Train acc: 0.9010416666666666\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.1658926494596464, Train acc: 0.900403608736942\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.16604517530809101, Train acc: 0.9003472222222222\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.1658258055045147, Train acc: 0.9006167443667443\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.16602723864557226, Train acc: 0.9003739316239316\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.16557629084622366, Train acc: 0.9002917488494412\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.16559521945350336, Train acc: 0.9002022283272283\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.16609858731683502, Train acc: 0.8998753561253561\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.1651753459770519, Train acc: 0.9004240117521367\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.1646212418626876, Train acc: 0.9007510055304173\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1651334344371375, Train acc: 0.9002552231718899\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.16527915564987825, Train acc: 0.9001771255060729\n",
      "Val loss: 0.388828843832016, Val acc: 0.878\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.17309785704327446, Train acc: 0.8982371794871795\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.16509165875932091, Train acc: 0.9023771367521367\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.16286698145064873, Train acc: 0.9025997150997151\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.16239581916194695, Train acc: 0.9034455128205128\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.1637273090517419, Train acc: 0.902724358974359\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.16302696831970134, Train acc: 0.9024216524216524\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.1616495552953783, Train acc: 0.9032738095238095\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.15989426614191288, Train acc: 0.9046474358974359\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.16005066773037852, Train acc: 0.904320987654321\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1593072847996512, Train acc: 0.9047542735042735\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.15929147054057022, Train acc: 0.9045017482517482\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.15998929914649565, Train acc: 0.9040687321937322\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.1595009480168912, Train acc: 0.9039694280078896\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1600675714840164, Train acc: 0.9034264346764347\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.16069842800710277, Train acc: 0.9029024216524216\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.16055065443994015, Train acc: 0.9028946314102564\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.16021039070212223, Train acc: 0.9030920060331825\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.16016185447814232, Train acc: 0.9030745489078822\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.16035630507373724, Train acc: 0.9029324111560953\n",
      "Val loss: 0.4108428657054901, Val acc: 0.86\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.17189426159756815, Train acc: 0.8947649572649573\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.16094316478468415, Train acc: 0.9045138888888888\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.16065809589165908, Train acc: 0.9028668091168092\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.15761813558001295, Train acc: 0.9045806623931624\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.15818967275385165, Train acc: 0.9047542735042735\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.15750332139538564, Train acc: 0.9045138888888888\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.1568407445909485, Train acc: 0.904265873015873\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.1580034925435216, Train acc: 0.9032451923076923\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.15686728535016603, Train acc: 0.9042616334283001\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1574018869198795, Train acc: 0.9038461538461539\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.15893030658121154, Train acc: 0.903239121989122\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.15845722957193173, Train acc: 0.9037571225071225\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.15777832848178486, Train acc: 0.9039694280078896\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.15813872565188977, Train acc: 0.9038652319902319\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.15831752936130236, Train acc: 0.9038995726495727\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.15852670882167852, Train acc: 0.9035623664529915\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.15999890049912932, Train acc: 0.9030134489693313\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.15966937533541736, Train acc: 0.9033713200379867\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.15976897552039673, Train acc: 0.9032135627530364\n",
      "Val loss: 0.40050166845321655, Val acc: 0.872\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.153016264239947, Train acc: 0.9041132478632479\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.15088325968155494, Train acc: 0.9077190170940171\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.15485287419496438, Train acc: 0.905982905982906\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.15932223753223562, Train acc: 0.9051148504273504\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.1592764729108566, Train acc: 0.9038461538461539\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.1608462800348756, Train acc: 0.9033119658119658\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.15956577910644812, Train acc: 0.9038079975579976\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.15891412883583042, Train acc: 0.9036792200854701\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.15735622502488403, Train acc: 0.9047364672364673\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.15639527892837157, Train acc: 0.9053418803418803\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.15447763299747502, Train acc: 0.9060557498057498\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.15483248083616083, Train acc: 0.905715811965812\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.1553145730225057, Train acc: 0.9054076265614727\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.15616023064260082, Train acc: 0.904800061050061\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.15580761394864134, Train acc: 0.9051816239316239\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.1559398351674979, Train acc: 0.9050146901709402\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.1561861274011954, Train acc: 0.9048831070889894\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.15634506954639046, Train acc: 0.9046474358974359\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.15723042008753998, Train acc: 0.904169478182636\n",
      "Val loss: 0.38877663016319275, Val acc: 0.884\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.1757025858785352, Train acc: 0.8993055555555556\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.16401779400105151, Train acc: 0.9011752136752137\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.15985523777724672, Train acc: 0.9024216524216524\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.16257998266090185, Train acc: 0.9001736111111112\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.15926845278750118, Train acc: 0.9019764957264957\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.16026592560303518, Train acc: 0.9016203703703703\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.15879927468241645, Train acc: 0.9029685592185592\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.15658417541501868, Train acc: 0.9040130876068376\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.15564943895324115, Train acc: 0.9043506647673314\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.156297874234171, Train acc: 0.9043002136752136\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.1554388769879856, Train acc: 0.9049630924630925\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.15565560796852634, Train acc: 0.9051148504273504\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.15452052104206637, Train acc: 0.9060445430637738\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.15360309273659528, Train acc: 0.9067460317460317\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.15375223276961564, Train acc: 0.906321225071225\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.15357892113363641, Train acc: 0.9064336271367521\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.15294675833169633, Train acc: 0.906579939668175\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1520598075562223, Train acc: 0.9070364434947769\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.15108050954634147, Train acc: 0.9073605488079173\n",
      "Val loss: 0.4029798209667206, Val acc: 0.872\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.16344418988013878, Train acc: 0.9051816239316239\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.16218172141119966, Train acc: 0.9049145299145299\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.15657239585605442, Train acc: 0.9074964387464387\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.15696165081845898, Train acc: 0.9071848290598291\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.15698897335519138, Train acc: 0.9065705128205128\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.156555794221893, Train acc: 0.907051282051282\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.15450908083546003, Train acc: 0.9073946886446886\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.15296352624447426, Train acc: 0.9082532051282052\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.15221821360382032, Train acc: 0.9085648148148148\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.15190356316474768, Train acc: 0.9084935897435897\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.1512447818708494, Train acc: 0.9086295648795649\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.15057944674097914, Train acc: 0.9089209401709402\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.15098761322857282, Train acc: 0.9084278435239974\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.15137205149191493, Train acc: 0.9082722832722833\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.15132835489562435, Train acc: 0.9081552706552707\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.15118038842183912, Train acc: 0.9082365117521367\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.14987209241494515, Train acc: 0.9088895173453997\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.14947905765045064, Train acc: 0.9092473884140551\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.14997806414137324, Train acc: 0.9089490553306343\n",
      "Val loss: 0.4069242477416992, Val acc: 0.872\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.1361710786437377, Train acc: 0.9107905982905983\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.13945408060382575, Train acc: 0.9109241452991453\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.14357041958135758, Train acc: 0.9099002849002849\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.13902829431443134, Train acc: 0.9131944444444444\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.1413616702087924, Train acc: 0.9120192307692307\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.14457512226624367, Train acc: 0.9109241452991453\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.14567656481185967, Train acc: 0.9109813797313797\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.1457110286459454, Train acc: 0.9101228632478633\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.1449181780233229, Train acc: 0.9106125356125356\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1453978650781334, Train acc: 0.9102297008547009\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.14472235327625607, Train acc: 0.9105477855477856\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.14542472984461363, Train acc: 0.9101896367521367\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.14642186865533832, Train acc: 0.9100304076265615\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1466456929788048, Train acc: 0.9097031440781441\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.147370447783049, Train acc: 0.9093482905982906\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.1472049757408408, Train acc: 0.9094050480769231\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.14676575361933164, Train acc: 0.9101621417797888\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.146934780066679, Train acc: 0.9099299620132953\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.14729706846509386, Train acc: 0.9095535312640576\n",
      "Val loss: 0.4058219790458679, Val acc: 0.878\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.14330918590227762, Train acc: 0.9123931623931624\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.14038187727077395, Train acc: 0.9151976495726496\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.14018582934752488, Train acc: 0.9158653846153846\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.14149510287321532, Train acc: 0.9142628205128205\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.14391014710960226, Train acc: 0.9124465811965812\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.14374423509946577, Train acc: 0.9124821937321937\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.1439822844624228, Train acc: 0.9118589743589743\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.14339983169562542, Train acc: 0.9121928418803419\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.14351899937460338, Train acc: 0.9123041310541311\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1434273667442493, Train acc: 0.9121260683760684\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.14366201938310147, Train acc: 0.912004662004662\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.14339951835466586, Train acc: 0.9120592948717948\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.14403944295499765, Train acc: 0.9120849769888232\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1452485711810717, Train acc: 0.91128663003663\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.14442809610455123, Train acc: 0.9116274928774929\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.14568651170652902, Train acc: 0.9110576923076923\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.1467270058504357, Train acc: 0.9106020613373554\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.14627159054674993, Train acc: 0.91090930674264\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.1452426642440913, Train acc: 0.9114794197031039\n",
      "Val loss: 0.3910295069217682, Val acc: 0.878\n",
      "Epoch 33/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.13826503407241952, Train acc: 0.9137286324786325\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.14489914941736776, Train acc: 0.9083867521367521\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.15045484378297105, Train acc: 0.907051282051282\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.1504143027859366, Train acc: 0.9074519230769231\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.14849867158465915, Train acc: 0.9084401709401709\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.14837498234802501, Train acc: 0.9086093304843305\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.14809891768937, Train acc: 0.9089209401709402\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.14892119575196353, Train acc: 0.9083867521367521\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.14967413913858355, Train acc: 0.9082977207977208\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.1501703390517296, Train acc: 0.9078792735042736\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.15049587213886553, Train acc: 0.9084110334110335\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.14981349695081214, Train acc: 0.9086093304843305\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.1495690952440299, Train acc: 0.9084072978303748\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.14912188071325438, Train acc: 0.9088064713064713\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.1474715403809167, Train acc: 0.9099358974358974\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.14560291436142647, Train acc: 0.9107739049145299\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.14544747094073565, Train acc: 0.9111362493715435\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.14504653343830012, Train acc: 0.911176400759734\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.1453696760980331, Train acc: 0.9109030589293747\n",
      "Val loss: 0.39118754863739014, Val acc: 0.856\n",
      "Epoch 34/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.13522849435734952, Train acc: 0.9153311965811965\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.139664208691599, Train acc: 0.9115918803418803\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.13941384295792322, Train acc: 0.9125712250712251\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.13977522301113504, Train acc: 0.9121928418803419\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.13815653345778456, Train acc: 0.912767094017094\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.13958163800360132, Train acc: 0.9121260683760684\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.14098813663144688, Train acc: 0.9117826617826618\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.13882595419278768, Train acc: 0.9129607371794872\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.13890195073310466, Train acc: 0.9136989553656221\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.13975316629323184, Train acc: 0.9133547008547008\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.13948490332288158, Train acc: 0.9137529137529138\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.13991658681435803, Train acc: 0.9133502492877493\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.13984264445140282, Train acc: 0.9128862590401052\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1400475761419906, Train acc: 0.9126221001221001\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.1404104332935776, Train acc: 0.9127849002849003\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.1396018757015212, Train acc: 0.9130442040598291\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.1404217699998704, Train acc: 0.9125345651080945\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1401822843039647, Train acc: 0.9124228395061729\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.14079696139437198, Train acc: 0.9121260683760684\n",
      "Val loss: 0.3965860903263092, Val acc: 0.874\n",
      "Epoch 35/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.12994955983172116, Train acc: 0.9172008547008547\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.1262916962050984, Train acc: 0.9182692307692307\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.1328169690323012, Train acc: 0.9160434472934473\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.1332566825211303, Train acc: 0.9157318376068376\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.13561929727976138, Train acc: 0.9143696581196581\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.13565855782128808, Train acc: 0.9148860398860399\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.13602520134277252, Train acc: 0.9143009768009768\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.1360206718150622, Train acc: 0.9141292735042735\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.13685513618919584, Train acc: 0.9141144349477682\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.13561392277479173, Train acc: 0.9146901709401709\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.13448636298644606, Train acc: 0.915282634032634\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.13478241852375855, Train acc: 0.9153534544159544\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.1351623943444148, Train acc: 0.9152284681130834\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.13450983671587466, Train acc: 0.915789072039072\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.1360239557730846, Train acc: 0.9150284900284901\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.1372007744975834, Train acc: 0.9145633012820513\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.1373777024414205, Train acc: 0.9146870286576169\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1381221281051466, Train acc: 0.914099596391263\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.13820193831281408, Train acc: 0.9140097840755735\n",
      "Val loss: 0.40418070554733276, Val acc: 0.876\n",
      "Epoch 36/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.1535677690154467, Train acc: 0.9043803418803419\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.1505390236265639, Train acc: 0.9047809829059829\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.14117491928770332, Train acc: 0.9107015669515669\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.142356720362973, Train acc: 0.9107238247863247\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.14186248811136962, Train acc: 0.9114316239316239\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.1423650554187617, Train acc: 0.9112802706552706\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.1418420547057712, Train acc: 0.9116681929181929\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.14153993505443263, Train acc: 0.9113915598290598\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.14150501819502595, Train acc: 0.9119183285849952\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.14064107155825337, Train acc: 0.912366452991453\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.1397840589829073, Train acc: 0.9128545066045066\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.13881120552173537, Train acc: 0.9132167022792023\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.1387601713065173, Train acc: 0.9135026298487837\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.1383568772657232, Train acc: 0.9137667887667887\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.13782588581413965, Train acc: 0.9142806267806268\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.13827448580652857, Train acc: 0.9140791933760684\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.13817013416564783, Train acc: 0.913964303670186\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1378759676970874, Train acc: 0.9142331433998101\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.13764342791021703, Train acc: 0.9145158569500674\n",
      "Val loss: 0.4155677556991577, Val acc: 0.866\n",
      "Epoch 37/100\n",
      "Iteration 117 - Batch 117/2273 - Train loss: 0.1366642256323089, Train acc: 0.9206730769230769\n",
      "Iteration 234 - Batch 234/2273 - Train loss: 0.12789075299460664, Train acc: 0.9234775641025641\n",
      "Iteration 351 - Batch 351/2273 - Train loss: 0.13943417484943682, Train acc: 0.9157763532763533\n",
      "Iteration 468 - Batch 468/2273 - Train loss: 0.13669961595382446, Train acc: 0.9160657051282052\n",
      "Iteration 585 - Batch 585/2273 - Train loss: 0.13467265800533132, Train acc: 0.9169871794871794\n",
      "Iteration 702 - Batch 702/2273 - Train loss: 0.13633746938233363, Train acc: 0.9152421652421653\n",
      "Iteration 819 - Batch 819/2273 - Train loss: 0.13554266024203526, Train acc: 0.9153693528693528\n",
      "Iteration 936 - Batch 936/2273 - Train loss: 0.13636749451104394, Train acc: 0.914596688034188\n",
      "Iteration 1053 - Batch 1053/2273 - Train loss: 0.13629833193529478, Train acc: 0.9150047483380817\n",
      "Iteration 1170 - Batch 1170/2273 - Train loss: 0.13429500251753718, Train acc: 0.9162126068376069\n",
      "Iteration 1287 - Batch 1287/2273 - Train loss: 0.13495110892332518, Train acc: 0.9155740093240093\n",
      "Iteration 1404 - Batch 1404/2273 - Train loss: 0.13592766394621117, Train acc: 0.9151531339031339\n",
      "Iteration 1521 - Batch 1521/2273 - Train loss: 0.13502858691333863, Train acc: 0.9157010190664037\n",
      "Iteration 1638 - Batch 1638/2273 - Train loss: 0.13499119135486337, Train acc: 0.9158653846153846\n",
      "Iteration 1755 - Batch 1755/2273 - Train loss: 0.1351947931355221, Train acc: 0.9156695156695157\n",
      "Iteration 1872 - Batch 1872/2273 - Train loss: 0.1354257210921974, Train acc: 0.9156984508547008\n",
      "Iteration 1989 - Batch 1989/2273 - Train loss: 0.13510207172957062, Train acc: 0.9155668677727501\n",
      "Iteration 2106 - Batch 2106/2273 - Train loss: 0.1349594530712279, Train acc: 0.9154795821462488\n",
      "Iteration 2223 - Batch 2223/2273 - Train loss: 0.1348051928080957, Train acc: 0.9153593117408907\n",
      "Val loss: 0.40423986315727234, Val acc: 0.864\n",
      "Early stopping at epoch 37 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 1038.4237 [s]\n",
      "Combinación 14/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.8541186579272279, Train acc: 0.3731303418803419\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.7340980266403948, Train acc: 0.47622863247863245\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.6542733974776037, Train acc: 0.5380163817663818\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.5995200309488509, Train acc: 0.5791599893162394\n",
      "Val loss: 0.5942783355712891, Val acc: 0.814\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.36947359182895756, Train acc: 0.7403178418803419\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.35459963837240494, Train acc: 0.7528378739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.3462360432514778, Train acc: 0.7584134615384616\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.33897046318166274, Train acc: 0.7633213141025641\n",
      "Val loss: 0.45825785398483276, Val acc: 0.85\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2924458562818348, Train acc: 0.7904647435897436\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.2870543378795314, Train acc: 0.7955395299145299\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.2813700897741182, Train acc: 0.7994791666666666\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.27644732397081506, Train acc: 0.8030515491452992\n",
      "Val loss: 0.4290657043457031, Val acc: 0.852\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.24835417362359855, Train acc: 0.8216479700854701\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.2383145786758162, Train acc: 0.828125\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.23605854388995048, Train acc: 0.83028400997151\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.23346304855285546, Train acc: 0.8318142361111112\n",
      "Val loss: 0.3714746832847595, Val acc: 0.868\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2223299474288256, Train acc: 0.8378739316239316\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.21782763862711751, Train acc: 0.8409121260683761\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.21285480175942098, Train acc: 0.8451077279202279\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.211485678759905, Train acc: 0.8458700587606838\n",
      "Val loss: 0.3438187837600708, Val acc: 0.874\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.19564770607866794, Train acc: 0.8524305555555556\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.19497540593147278, Train acc: 0.8566038995726496\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.19616329321834097, Train acc: 0.8556579415954416\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.19567940766230607, Train acc: 0.8565705128205128\n",
      "Val loss: 0.3361169695854187, Val acc: 0.88\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18971603905033863, Train acc: 0.8595085470085471\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.18688447480527764, Train acc: 0.8622128739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.18282703736908415, Train acc: 0.8650952635327636\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.18099528366429174, Train acc: 0.865501469017094\n",
      "Val loss: 0.3166036009788513, Val acc: 0.886\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.17624251404379168, Train acc: 0.8675881410256411\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1729499723157312, Train acc: 0.8704927884615384\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1707687094340637, Train acc: 0.8703035968660968\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1693142919968336, Train acc: 0.8718449519230769\n",
      "Val loss: 0.31740668416023254, Val acc: 0.874\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.15952849821147755, Train acc: 0.8776041666666666\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16400345535869273, Train acc: 0.8756343482905983\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1619229269163561, Train acc: 0.8763799857549858\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1618935283050578, Train acc: 0.876418936965812\n",
      "Val loss: 0.3305330276489258, Val acc: 0.876\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.14591739498651946, Train acc: 0.8868856837606838\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.150077021211131, Train acc: 0.8843149038461539\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1501199649355011, Train acc: 0.8838363603988604\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1500514983239337, Train acc: 0.8828291933760684\n",
      "Val loss: 0.343412846326828, Val acc: 0.868\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1452568676481899, Train acc: 0.8843482905982906\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.14412929518864706, Train acc: 0.8850494123931624\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.14487623951883397, Train acc: 0.8843928062678063\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.14434224048740843, Train acc: 0.8855502136752137\n",
      "Val loss: 0.3411300778388977, Val acc: 0.878\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13661488814231676, Train acc: 0.8932291666666666\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13609289155047163, Train acc: 0.8921941773504274\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13586942609558758, Train acc: 0.8920049857549858\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.13455783464332932, Train acc: 0.8918603098290598\n",
      "Val loss: 0.3417702317237854, Val acc: 0.892\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.130549093724316, Train acc: 0.8941639957264957\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1295738235498086, Train acc: 0.895065438034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12785378896612726, Train acc: 0.8958555911680912\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12739157549336425, Train acc: 0.8957498664529915\n",
      "Val loss: 0.35353749990463257, Val acc: 0.884\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1308333784596533, Train acc: 0.8944310897435898\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.12637893728211394, Train acc: 0.8965010683760684\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12504867551333546, Train acc: 0.8969907407407407\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1221527570587957, Train acc: 0.8985376602564102\n",
      "Val loss: 0.37402409315109253, Val acc: 0.882\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.11963834034072028, Train acc: 0.9011752136752137\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.11443459471830955, Train acc: 0.9030448717948718\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.11143784721692403, Train acc: 0.9049813034188035\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1114017269295505, Train acc: 0.9048811431623932\n",
      "Val loss: 0.3720789849758148, Val acc: 0.878\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.09813631370536283, Train acc: 0.9105902777777778\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.10039425953331156, Train acc: 0.9090878739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.10420415620518546, Train acc: 0.9077190170940171\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.10476093315797994, Train acc: 0.9075353899572649\n",
      "Val loss: 0.36268800497055054, Val acc: 0.88\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.10156448592997006, Train acc: 0.9099225427350427\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1010020223692951, Train acc: 0.9109241452991453\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.10051033261664573, Train acc: 0.9116141381766382\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.09988199406836787, Train acc: 0.9118923611111112\n",
      "Val loss: 0.3785575032234192, Val acc: 0.872\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.09445228051935506, Train acc: 0.9165998931623932\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.09467188364420182, Train acc: 0.9150641025641025\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.09539164331054416, Train acc: 0.914596688034188\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.09391866796291791, Train acc: 0.9149472489316239\n",
      "Val loss: 0.3934047520160675, Val acc: 0.886\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.09010497130389906, Train acc: 0.9159989316239316\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.0888498291755334, Train acc: 0.9168336004273504\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.09014330829820062, Train acc: 0.9164440883190883\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.08831796963882242, Train acc: 0.9176515758547008\n",
      "Val loss: 0.40099048614501953, Val acc: 0.88\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.08167995321444976, Train acc: 0.9231436965811965\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.0847735375038579, Train acc: 0.9192374465811965\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.08413290586906282, Train acc: 0.9191372863247863\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.0833272949880005, Train acc: 0.9195546207264957\n",
      "Val loss: 0.3973267078399658, Val acc: 0.882\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.07864634081339225, Train acc: 0.9214743589743589\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.07852943585469173, Train acc: 0.9223758012820513\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.07967914063536544, Train acc: 0.9211627492877493\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.07894738311441536, Train acc: 0.9212239583333334\n",
      "Val loss: 0.4095776677131653, Val acc: 0.876\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.07819782617764595, Train acc: 0.9216746794871795\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.07753729081561422, Train acc: 0.921875\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.07741625216441957, Train acc: 0.9226095085470085\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.07620954698222315, Train acc: 0.9232271634615384\n",
      "Val loss: 0.40279680490493774, Val acc: 0.89\n",
      "Early stopping at epoch 22 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 503.0726 [s]\n",
      "Combinación 15/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.9987373357145195, Train acc: 0.5020032051282052\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.8195447712881953, Train acc: 0.6000934829059829\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.7181432142726376, Train acc: 0.6529558404558404\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.6503054318137658, Train acc: 0.6889690170940171\n",
      "Val loss: 0.44270530343055725, Val acc: 0.806\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.38218210191808194, Train acc: 0.8292601495726496\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.36963546199676317, Train acc: 0.832565438034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.36060771842797595, Train acc: 0.8361823361823362\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.3495231066695136, Train acc: 0.8424312232905983\n",
      "Val loss: 0.3287566602230072, Val acc: 0.86\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.3072666614993006, Train acc: 0.8623798076923077\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.2977690840633506, Train acc: 0.8644497863247863\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.28897872957748566, Train acc: 0.8684784544159544\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2834872415241523, Train acc: 0.8712606837606838\n",
      "Val loss: 0.28722816705703735, Val acc: 0.89\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2599349429464748, Train acc: 0.8838141025641025\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.25897488825851017, Train acc: 0.8820446047008547\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.25453751341060354, Train acc: 0.8847266737891738\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2510846432011861, Train acc: 0.8863181089743589\n",
      "Val loss: 0.2794746160507202, Val acc: 0.88\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.24416328750104985, Train acc: 0.8894230769230769\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.23637944475835204, Train acc: 0.8939636752136753\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.23315470593522417, Train acc: 0.8952546296296297\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.23216269316517898, Train acc: 0.8963341346153846\n",
      "Val loss: 0.2653496265411377, Val acc: 0.892\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.2167776263295076, Train acc: 0.9039129273504274\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.21655660272281393, Train acc: 0.9027777777777778\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.21940262146570064, Train acc: 0.9013087606837606\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.21697924437367508, Train acc: 0.9022268963675214\n",
      "Val loss: 0.24603576958179474, Val acc: 0.892\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.20759613079647732, Train acc: 0.9060496794871795\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.20281226031927982, Train acc: 0.9090878739316239\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.20661735171690965, Train acc: 0.9061164529914529\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.2066365742466898, Train acc: 0.9070679754273504\n",
      "Val loss: 0.23961547017097473, Val acc: 0.912\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.20265174236817238, Train acc: 0.9076522435897436\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1951550647106945, Train acc: 0.9123931623931624\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.19525787474168332, Train acc: 0.9120592948717948\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.19503504099945226, Train acc: 0.9123597756410257\n",
      "Val loss: 0.24441687762737274, Val acc: 0.89\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.18470739363095698, Train acc: 0.9147970085470085\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.18677310454539764, Train acc: 0.9152978098290598\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.18659542305686874, Train acc: 0.9153311965811965\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.18542474280628893, Train acc: 0.9157318376068376\n",
      "Val loss: 0.22950977087020874, Val acc: 0.898\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.17132105983984777, Train acc: 0.9235443376068376\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.17303004418300766, Train acc: 0.9207732371794872\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1753551902891564, Train acc: 0.9206063034188035\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.17544903590256333, Train acc: 0.9209234775641025\n",
      "Val loss: 0.24399559199810028, Val acc: 0.906\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16506437982758906, Train acc: 0.9250801282051282\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16632747430449876, Train acc: 0.9237780448717948\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.16695919930085837, Train acc: 0.9240117521367521\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.16492020714486766, Train acc: 0.9244791666666666\n",
      "Val loss: 0.26603853702545166, Val acc: 0.902\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.16510233232098767, Train acc: 0.9248130341880342\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.16054332456909692, Train acc: 0.9257478632478633\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.15704585289173995, Train acc: 0.9276620370370371\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.15659274049421662, Train acc: 0.9283019497863247\n",
      "Val loss: 0.24769103527069092, Val acc: 0.898\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.158496945268578, Train acc: 0.9258814102564102\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.15784456373916733, Train acc: 0.9280181623931624\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.15216363244756334, Train acc: 0.9297542735042735\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1495334373978086, Train acc: 0.9312733707264957\n",
      "Val loss: 0.2844295799732208, Val acc: 0.876\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1528337355862316, Train acc: 0.9313568376068376\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1462143668347699, Train acc: 0.9335269764957265\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.14339714977051798, Train acc: 0.9346732549857549\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1425499300368958, Train acc: 0.9351128472222222\n",
      "Val loss: 0.3346107304096222, Val acc: 0.878\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1324179369606014, Train acc: 0.9393028846153846\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13514638262299392, Train acc: 0.9384348290598291\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.13422835816727405, Train acc: 0.9386796652421653\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1346276097365806, Train acc: 0.9383847489316239\n",
      "Val loss: 0.2777183949947357, Val acc: 0.894\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13371445671615437, Train acc: 0.9401709401709402\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.13288107119564319, Train acc: 0.9395365918803419\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12991771322709542, Train acc: 0.9405270655270656\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12935439213855654, Train acc: 0.9406550480769231\n",
      "Val loss: 0.3016494810581207, Val acc: 0.89\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.13099639439302632, Train acc: 0.9394364316239316\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1254504151069201, Train acc: 0.9423410790598291\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.12656295115918856, Train acc: 0.9417512464387464\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.12707040815526605, Train acc: 0.94140625\n",
      "Val loss: 0.29572540521621704, Val acc: 0.896\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.11708870011135045, Train acc: 0.9462473290598291\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.11921598827545969, Train acc: 0.9452457264957265\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.11860328616622166, Train acc: 0.9454237891737892\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.11758200557599975, Train acc: 0.9459468482905983\n",
      "Val loss: 0.30954164266586304, Val acc: 0.894\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.1150205321291573, Train acc: 0.944778311965812\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.11569009522278594, Train acc: 0.9460803952991453\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.1155111391383868, Train acc: 0.9468037749287749\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1143029896924511, Train acc: 0.9471487713675214\n",
      "Val loss: 0.344455748796463, Val acc: 0.89\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.11206166054575871, Train acc: 0.9463141025641025\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.11037105460388538, Train acc: 0.9482171474358975\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.10863507756607825, Train acc: 0.94880698005698\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.1097242182566442, Train acc: 0.9488181089743589\n",
      "Val loss: 0.3648868203163147, Val acc: 0.896\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.10403175607451007, Train acc: 0.9518563034188035\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.1058085206259265, Train acc: 0.9506209935897436\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.10636984698071099, Train acc: 0.9504540598290598\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.10612827780632636, Train acc: 0.9504039797008547\n",
      "Val loss: 0.3343948721885681, Val acc: 0.888\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.10488558430065456, Train acc: 0.9513888888888888\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.10417395901794617, Train acc: 0.9523904914529915\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.10285651874847901, Train acc: 0.9529469373219374\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.10276541623294863, Train acc: 0.9526075053418803\n",
      "Val loss: 0.3841206729412079, Val acc: 0.88\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.10468661485828905, Train acc: 0.9509214743589743\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.0995164407401258, Train acc: 0.9535590277777778\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.10018570702045392, Train acc: 0.952857905982906\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.10023602407871403, Train acc: 0.9529747596153846\n",
      "Val loss: 0.4118627607822418, Val acc: 0.876\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.10144505681645156, Train acc: 0.9533253205128205\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.10163063480335678, Train acc: 0.9525240384615384\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.09984674883682673, Train acc: 0.9535924145299145\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.09930651491253167, Train acc: 0.953659188034188\n",
      "Val loss: 0.37526029348373413, Val acc: 0.896\n",
      "Early stopping at epoch 24 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 556.1900 [s]\n",
      "Combinación 16/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.6144582486560202, Train acc: 0.46607905982905984\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4953683181705638, Train acc: 0.5662393162393162\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.42351017888115344, Train acc: 0.6185897435897436\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.37708498371971977, Train acc: 0.6523103632478633\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3423255327420357, Train acc: 0.6763354700854701\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3154117086334446, Train acc: 0.6958912037037037\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.29497428385766, Train acc: 0.7106608669108669\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.27737797163108474, Train acc: 0.7227397168803419\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.26205376673627784, Train acc: 0.7335588793922128\n",
      "Val loss: 0.4439365267753601, Val acc: 0.826\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1239262141733088, Train acc: 0.8314636752136753\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.11974880010144323, Train acc: 0.836471688034188\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11922787210540554, Train acc: 0.8372061965811965\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.11590205852547263, Train acc: 0.8395098824786325\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.11252546784205314, Train acc: 0.8421207264957264\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.11049107642594905, Train acc: 0.8433048433048433\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.10745900541871459, Train acc: 0.845066391941392\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.10477646373403378, Train acc: 0.8470552884615384\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.10341022035561623, Train acc: 0.8480086657169991\n",
      "Val loss: 0.37094977498054504, Val acc: 0.88\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.07500009964673947, Train acc: 0.8632478632478633\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.07595395685261132, Train acc: 0.8639823717948718\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.07450884869295647, Train acc: 0.8664975071225072\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.0739539014095934, Train acc: 0.8670873397435898\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.072279799493969, Train acc: 0.86875\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.07212087694566134, Train acc: 0.8694355413105413\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.06988698560676296, Train acc: 0.8708028083028083\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.06841750984263216, Train acc: 0.8717114049145299\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.06762588508108743, Train acc: 0.8727890550807218\n",
      "Val loss: 0.36506009101867676, Val acc: 0.876\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.05904544851718805, Train acc: 0.8768696581196581\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.05766811075373592, Train acc: 0.8804086538461539\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.05237615201887582, Train acc: 0.8835470085470085\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.051439527199309096, Train acc: 0.8838474893162394\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.05012645089728201, Train acc: 0.8846955128205128\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.04933824939945145, Train acc: 0.8849937678062678\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.04887384518307789, Train acc: 0.885321275946276\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.04878375946711271, Train acc: 0.8853665865384616\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.04794461752048358, Train acc: 0.8858618233618234\n",
      "Val loss: 0.3698156476020813, Val acc: 0.884\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.037689736001511924, Train acc: 0.8922275641025641\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.04071312747959398, Train acc: 0.8888221153846154\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.03750011944702888, Train acc: 0.8918714387464387\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.03881165862847597, Train acc: 0.8910924145299145\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.03716672740430913, Train acc: 0.8916399572649573\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.036335726239402746, Train acc: 0.8924278846153846\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.03575054455152798, Train acc: 0.8929143772893773\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.034663449654467084, Train acc: 0.8936131143162394\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.034030089237977526, Train acc: 0.893815289648623\n",
      "Val loss: 0.3510502576828003, Val acc: 0.902\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.02400347692334754, Train acc: 0.8994391025641025\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.023512084259946123, Train acc: 0.8993723290598291\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.023132785942479757, Train acc: 0.8993055555555556\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.021151862019657068, Train acc: 0.9004740918803419\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.021293322754721355, Train acc: 0.9010683760683761\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.020239547574282372, Train acc: 0.9022213319088319\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.01949734336290604, Train acc: 0.9021100427350427\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.01944585335560334, Train acc: 0.9020933493589743\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.019017719135664808, Train acc: 0.9025848765432098\n",
      "Val loss: 0.3616105914115906, Val acc: 0.894\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.02009201992271293, Train acc: 0.8999732905982906\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.013815018993157607, Train acc: 0.9064503205128205\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.012563242447002661, Train acc: 0.9072738603988604\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.011764092705188653, Train acc: 0.9083199786324786\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.011857861229497143, Train acc: 0.9080929487179488\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.01098110707227321, Train acc: 0.9086538461538461\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.010911022095744282, Train acc: 0.9081196581196581\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.009225118172983838, Train acc: 0.9093048878205128\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.008642779654360678, Train acc: 0.9094996438746439\n",
      "Val loss: 0.3819073438644409, Val acc: 0.882\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.00012325909402635362, Train acc: 0.9110576923076923\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.00020168352330851759, Train acc: 0.9116586538461539\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.002120379369143407, Train acc: 0.9126602564102564\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0037946052785612578, Train acc: 0.9140625\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.003218164708879259, Train acc: 0.9139690170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.0039590955035299314, Train acc: 0.9149305555555556\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.003979160599365048, Train acc: 0.9150259462759462\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.003918697882411826, Train acc: 0.9145799946581197\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.004550991263258265, Train acc: 0.9151828110161444\n",
      "Val loss: 0.40002521872520447, Val acc: 0.896\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.003130383725859161, Train acc: 0.9157318376068376\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.00844986583942022, Train acc: 0.9193376068376068\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.010585813780455847, Train acc: 0.9202279202279202\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.010641075734399322, Train acc: 0.9207732371794872\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.012574535761124049, Train acc: 0.9214209401709401\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.014649864742558905, Train acc: 0.9228543447293447\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.015646926612935513, Train acc: 0.9238209706959707\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.015667002520754807, Train acc: 0.9238948985042735\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.015709992855475953, Train acc: 0.9240117521367521\n",
      "Val loss: 0.38518089056015015, Val acc: 0.896\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.01235345260709779, Train acc: 0.9189369658119658\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.019738678366709977, Train acc: 0.9259481837606838\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.02111191559381295, Train acc: 0.9264155982905983\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.02090489902557471, Train acc: 0.9257478632478633\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.02023886788604606, Train acc: 0.9250801282051282\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.02119064543321941, Train acc: 0.92536948005698\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.021822333663374514, Train acc: 0.9255189255189256\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.021939045598364283, Train acc: 0.9256477029914529\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.02374825628734382, Train acc: 0.9267420465337132\n",
      "Val loss: 0.3775777816772461, Val acc: 0.896\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.022215075217760526, Train acc: 0.922409188034188\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.023664173916873768, Train acc: 0.9238782051282052\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.026311542233850203, Train acc: 0.9260594729344729\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.02914689277481829, Train acc: 0.9277510683760684\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.03107283242747315, Train acc: 0.9292467948717948\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.030498688122485778, Train acc: 0.9286636396011396\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.030758087560807394, Train acc: 0.9286095848595849\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.03123411061799424, Train acc: 0.9290865384615384\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.031003233147935422, Train acc: 0.928789767331434\n",
      "Val loss: 0.4210614562034607, Val acc: 0.886\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.029953585730658636, Train acc: 0.9288194444444444\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.03343003950057886, Train acc: 0.9314903846153846\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.03253419981722818, Train acc: 0.9310007122507122\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.033440959504526906, Train acc: 0.9313568376068376\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.03517973937539973, Train acc: 0.9326121794871794\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.03584148473719247, Train acc: 0.9330484330484331\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.035639103370126206, Train acc: 0.9334935897435898\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.03731290728617937, Train acc: 0.9343783386752137\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.0369389496411127, Train acc: 0.9338645536562203\n",
      "Val loss: 0.4227975010871887, Val acc: 0.884\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.03975240313089811, Train acc: 0.9338942307692307\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.04156862848844284, Train acc: 0.9344951923076923\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.04454057670047141, Train acc: 0.936698717948718\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.04467708961321758, Train acc: 0.937767094017094\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.045109236953604935, Train acc: 0.937767094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.045426228319817456, Train acc: 0.9375667735042735\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.04451531437436012, Train acc: 0.9370993589743589\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.0445283600089387, Train acc: 0.9371327457264957\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.04431184246103082, Train acc: 0.9371883903133903\n",
      "Val loss: 0.4260513186454773, Val acc: 0.884\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.052919662915743314, Train acc: 0.9425747863247863\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.05143334582830087, Train acc: 0.9403044871794872\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.051088200929837346, Train acc: 0.9415064102564102\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.05243865527913102, Train acc: 0.9417067307692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.05173921299795819, Train acc: 0.9412126068376069\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.050954079271381736, Train acc: 0.9408386752136753\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.05074657174111577, Train acc: 0.9404571123321124\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.0508253342893898, Train acc: 0.9404046474358975\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.050479389720040055, Train acc: 0.9402451329534662\n",
      "Val loss: 0.4340999126434326, Val acc: 0.876\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.05597612338188367, Train acc: 0.9453792735042735\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.05338180345347804, Train acc: 0.9427751068376068\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.05487739878502327, Train acc: 0.9436431623931624\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.0557866006070732, Train acc: 0.9443776709401709\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.056122584984852716, Train acc: 0.9437767094017094\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.055561180552865706, Train acc: 0.943420584045584\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.055438705016114045, Train acc: 0.9433379120879121\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.055995586750089615, Train acc: 0.9435596955128205\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.05614722958091091, Train acc: 0.9437618708452041\n",
      "Val loss: 0.4139385521411896, Val acc: 0.89\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.06275498612314208, Train acc: 0.9477831196581197\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.06434706286487417, Train acc: 0.9493189102564102\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.0653036873564761, Train acc: 0.9497418091168092\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.06598385703614634, Train acc: 0.9502537393162394\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0654537211626004, Train acc: 0.95\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.06427987212808724, Train acc: 0.9491185897435898\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.06307478429196955, Train acc: 0.9485080891330891\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.06193520475783919, Train acc: 0.9477163461538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.061242696283436454, Train acc: 0.9468482905982906\n",
      "Val loss: 0.4323807954788208, Val acc: 0.896\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.06218818237638881, Train acc: 0.9464476495726496\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.060659932784545116, Train acc: 0.9453125\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.061168792920234874, Train acc: 0.9456908831908832\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.06154981160011047, Train acc: 0.9459802350427351\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.061399052897070205, Train acc: 0.9455395299145299\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.061907124103304324, Train acc: 0.9461582977207977\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.06174418956077725, Train acc: 0.9460851648351648\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.062336583136238605, Train acc: 0.9466312767094017\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.06250705542387786, Train acc: 0.9468334520417854\n",
      "Val loss: 0.45504269003868103, Val acc: 0.872\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.05946017865441803, Train acc: 0.9460470085470085\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.06416212990243211, Train acc: 0.9467147435897436\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.06364085905232661, Train acc: 0.9469818376068376\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.06435513687439454, Train acc: 0.9479500534188035\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.0661994310016306, Train acc: 0.9486645299145299\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.06642421099365267, Train acc: 0.9486956908831908\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.06598397867437975, Train acc: 0.948622557997558\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.06629671248742658, Train acc: 0.9485009348290598\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.0670205062327788, Train acc: 0.9487773029439696\n",
      "Val loss: 0.46807101368904114, Val acc: 0.874\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.07456701662805346, Train acc: 0.9515224358974359\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.07101271078627333, Train acc: 0.9493189102564102\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.06859617499884038, Train acc: 0.9480947293447294\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.06953648853505778, Train acc: 0.9490518162393162\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.07040781735355019, Train acc: 0.9496527777777778\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.07083122054396192, Train acc: 0.950431801994302\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.0708492590758099, Train acc: 0.9504731379731379\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.07111788219493678, Train acc: 0.9507044604700855\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.07058566486054337, Train acc: 0.9503205128205128\n",
      "Val loss: 0.4499445855617523, Val acc: 0.88\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: -0.07129512128666934, Train acc: 0.9527243589743589\n",
      "Iteration 234 - Batch 234/1137 - Train loss: -0.07423150195525242, Train acc: 0.9542601495726496\n",
      "Iteration 351 - Batch 351/1137 - Train loss: -0.07472849809206449, Train acc: 0.9538372507122507\n",
      "Iteration 468 - Batch 468/1137 - Train loss: -0.07406101802475432, Train acc: 0.9534922542735043\n",
      "Iteration 585 - Batch 585/1137 - Train loss: -0.07267694203262655, Train acc: 0.9524572649572649\n",
      "Iteration 702 - Batch 702/1137 - Train loss: -0.07317087610392829, Train acc: 0.9524350071225072\n",
      "Iteration 819 - Batch 819/1137 - Train loss: -0.07303472778444907, Train acc: 0.9520566239316239\n",
      "Iteration 936 - Batch 936/1137 - Train loss: -0.07343276338572176, Train acc: 0.9521067040598291\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: -0.073864350716273, Train acc: 0.9523830721747388\n",
      "Val loss: 0.4554400146007538, Val acc: 0.874\n",
      "Early stopping at epoch 20 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 494.2527 [s]\n",
      "Combinación 17/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.7221912300994253, Train acc: 0.6578525641025641\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.6538841230237585, Train acc: 0.6866319444444444\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.6209421314914682, Train acc: 0.7012998575498576\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.6038135357646861, Train acc: 0.7098357371794872\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.5943506987685832, Train acc: 0.7145299145299145\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.5863747730469092, Train acc: 0.7177706552706553\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.5768796658559596, Train acc: 0.7226419413919414\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.5720075110339711, Train acc: 0.7248096955128205\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.5672262910141451, Train acc: 0.7269705603038936\n",
      "Val loss: 0.3465319275856018, Val acc: 0.878\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.5268539103687319, Train acc: 0.7467948717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.5309731729774394, Train acc: 0.7467280982905983\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.5220017815247561, Train acc: 0.7495103276353277\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.5157522742564862, Train acc: 0.7517694978632479\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.513225107620924, Train acc: 0.752991452991453\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.5135924354662583, Train acc: 0.7528267450142451\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.5123062911414984, Train acc: 0.7530715811965812\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.5115770199614712, Train acc: 0.7539396367521367\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.5114367006794114, Train acc: 0.7539173789173789\n",
      "Val loss: 0.34189721941947937, Val acc: 0.884\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.5135894501820589, Train acc: 0.7489316239316239\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.5108703433448433, Train acc: 0.7496661324786325\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.5040845410776275, Train acc: 0.7541844729344729\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.5015874836179945, Train acc: 0.7557091346153846\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.5022484998927157, Train acc: 0.7558226495726496\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.5011274724604398, Train acc: 0.7567886396011396\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.5010323633277227, Train acc: 0.7566582722832723\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.5003030503280143, Train acc: 0.7578125\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.5003387782252186, Train acc: 0.757315408357075\n",
      "Val loss: 0.3411741852760315, Val acc: 0.884\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.5009344762716538, Train acc: 0.7585470085470085\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.49208272624219584, Train acc: 0.7627537393162394\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4971996840418574, Train acc: 0.7577902421652422\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.49669480400207716, Train acc: 0.7593482905982906\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.49608974176594334, Train acc: 0.7604433760683761\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4950977768908199, Train acc: 0.7610844017094017\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4952626312390352, Train acc: 0.7609317765567766\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.49317152890512067, Train acc: 0.7618522970085471\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.49316701686393616, Train acc: 0.7616631054131054\n",
      "Val loss: 0.3494298756122589, Val acc: 0.892\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.5029284477743328, Train acc: 0.7538728632478633\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.5053332777868988, Train acc: 0.7557425213675214\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4953112800916036, Train acc: 0.7629540598290598\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.49277406739882934, Train acc: 0.7635884081196581\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48910998304684955, Train acc: 0.7649572649572649\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.48922459258992446, Train acc: 0.7642450142450142\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4881781156974252, Train acc: 0.7646329365079365\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4883072760561083, Train acc: 0.7640558226495726\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4884470348004942, Train acc: 0.7641856600189934\n",
      "Val loss: 0.34827202558517456, Val acc: 0.872\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.5034702229194152, Train acc: 0.7529380341880342\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.49599568291097623, Train acc: 0.7562099358974359\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4896113251009558, Train acc: 0.7606392450142451\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4886698868666959, Train acc: 0.7600827991452992\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.49182547906525115, Train acc: 0.7588942307692308\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.49130753196372606, Train acc: 0.7606615028490028\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.49056932941461223, Train acc: 0.7611607142857143\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.49013667200238276, Train acc: 0.7609842414529915\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.493259918655765, Train acc: 0.7600605413105413\n",
      "Val loss: 0.3599936068058014, Val acc: 0.878\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4880779242923117, Train acc: 0.765892094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47988533177691645, Train acc: 0.7686965811965812\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.48110842734490367, Train acc: 0.7682514245014245\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.48222932152641124, Train acc: 0.7675614316239316\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4823925814312747, Train acc: 0.7676549145299145\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.48165031649872786, Train acc: 0.7674724002849003\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.48313560938151034, Train acc: 0.7668269230769231\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48348300396186167, Train acc: 0.7664930555555556\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.48159583453495847, Train acc: 0.7671236942070275\n",
      "Val loss: 0.34592166543006897, Val acc: 0.892\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.49555929068826204, Train acc: 0.7613514957264957\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4930813748102922, Train acc: 0.7634214743589743\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4894500347623798, Train acc: 0.7659366096866097\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.48810365764249086, Train acc: 0.7656583867521367\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4888491593874418, Train acc: 0.763568376068376\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.48851462829316783, Train acc: 0.7643340455840456\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4882471272141942, Train acc: 0.764842796092796\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48801745407474345, Train acc: 0.7648737980769231\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4860791689932969, Train acc: 0.7657437084520418\n",
      "Val loss: 0.3392771780490875, Val acc: 0.896\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4581670429971483, Train acc: 0.7727029914529915\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4694717052655342, Train acc: 0.7690972222222222\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.47053085135938094, Train acc: 0.7691862535612536\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4772472960444597, Train acc: 0.7669270833333334\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4779992747765321, Train acc: 0.7663194444444444\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.48140881049242457, Train acc: 0.7656695156695157\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.48005197556537005, Train acc: 0.766159188034188\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48094160598503727, Train acc: 0.7664429754273504\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.48159102900245593, Train acc: 0.7668714387464387\n",
      "Val loss: 0.3429860770702362, Val acc: 0.876\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.473343292872111, Train acc: 0.7747061965811965\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47467533709147036, Train acc: 0.7697649572649573\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.48040577624937747, Train acc: 0.7672275641025641\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4779228477651237, Train acc: 0.7691306089743589\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.47875511136829346, Train acc: 0.7672008547008548\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4799445247293537, Train acc: 0.7665153133903134\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.48104351034822335, Train acc: 0.7662545787545788\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4824294187128544, Train acc: 0.7659922542735043\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.48212987481698694, Train acc: 0.7665895061728395\n",
      "Val loss: 0.3636143207550049, Val acc: 0.868\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4928660976071643, Train acc: 0.7652243589743589\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.48556585355192167, Train acc: 0.7659588675213675\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4869467590609167, Train acc: 0.7646901709401709\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.48310875758910793, Train acc: 0.7659588675213675\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.47908048018431054, Train acc: 0.7664797008547009\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4820829619709243, Train acc: 0.7646011396011396\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4823990592298636, Train acc: 0.7649954212454212\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4838826772557874, Train acc: 0.7645065438034188\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4838421809197831, Train acc: 0.764497269705603\n",
      "Val loss: 0.3553614318370819, Val acc: 0.892\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4986372747991839, Train acc: 0.7642895299145299\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4968599851569559, Train acc: 0.7612847222222222\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.48666661875879663, Train acc: 0.7662037037037037\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4835073481130804, Train acc: 0.7685296474358975\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4850366103852916, Train acc: 0.7668803418803419\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4850268248903785, Train acc: 0.7658030626780626\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.48411684606101485, Train acc: 0.7652243589743589\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48493365055093396, Train acc: 0.7651408920940171\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4836916363420423, Train acc: 0.765625\n",
      "Val loss: 0.36302101612091064, Val acc: 0.878\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4680890565754002, Train acc: 0.7775106837606838\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47438327649719697, Train acc: 0.7743055555555556\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4778520914054664, Train acc: 0.7715010683760684\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4787137173434608, Train acc: 0.7683961004273504\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.47810672394230835, Train acc: 0.7695245726495726\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4793359129517167, Train acc: 0.76911948005698\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.479992864878623, Train acc: 0.7688110500610501\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.479490815096686, Train acc: 0.7687633547008547\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4798702597901126, Train acc: 0.7688004510921178\n",
      "Val loss: 0.36026376485824585, Val acc: 0.866\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4712423824856424, Train acc: 0.7737713675213675\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4654004968639113, Train acc: 0.7759748931623932\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4708570015226674, Train acc: 0.7729700854700855\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4715407171692604, Train acc: 0.7718015491452992\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4753565732230488, Train acc: 0.7693376068376069\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.47728088209432074, Train acc: 0.768051103988604\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.47702862225900494, Train acc: 0.7678189865689866\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47650369511455554, Train acc: 0.767845219017094\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47685500552058785, Train acc: 0.7684146486229819\n",
      "Val loss: 0.36648231744766235, Val acc: 0.882\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.48293909455975914, Train acc: 0.7638888888888888\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47914477304006237, Train acc: 0.7682291666666666\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4770153993554944, Train acc: 0.7694088319088319\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4765144934893673, Train acc: 0.7698317307692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4771837988470355, Train acc: 0.7697382478632478\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.48033045312957545, Train acc: 0.7674278846153846\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4815675268024752, Train acc: 0.7665216727716728\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4794260681503349, Train acc: 0.7681623931623932\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47888769233441875, Train acc: 0.7686372269705603\n",
      "Val loss: 0.3690732717514038, Val acc: 0.868\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.47710437168422926, Train acc: 0.7690972222222222\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4841559144676241, Train acc: 0.765625\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4868770692593012, Train acc: 0.7629095441595442\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.48323295360956436, Train acc: 0.7651241987179487\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48169516335185775, Train acc: 0.7663728632478632\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4821404905570538, Train acc: 0.7668936965811965\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.48190223555716255, Train acc: 0.7678189865689866\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48145791260987264, Train acc: 0.7679620726495726\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4808570534868231, Train acc: 0.7686817426400759\n",
      "Val loss: 0.3769002854824066, Val acc: 0.868\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4763652441593317, Train acc: 0.7637553418803419\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4771708753755969, Train acc: 0.7669604700854701\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.47768202445276103, Train acc: 0.7682514245014245\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4768356466586264, Train acc: 0.7703993055555556\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4778288523101399, Train acc: 0.7700854700854701\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.47789196137264583, Train acc: 0.7696981837606838\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.47855554502889497, Train acc: 0.7688492063492064\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4784675935108183, Train acc: 0.7690304487179487\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47796059641120325, Train acc: 0.7685036799620133\n",
      "Val loss: 0.3307042717933655, Val acc: 0.888\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.47320023026221836, Train acc: 0.7684294871794872\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.477942981908464, Train acc: 0.7662259615384616\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4765215624771227, Train acc: 0.7681178774928775\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.47823068926222306, Train acc: 0.7679954594017094\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4806304147100856, Train acc: 0.7660256410256411\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4811394030594418, Train acc: 0.7660256410256411\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4798473008204438, Train acc: 0.765892094017094\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4807963296293448, Train acc: 0.7658754006410257\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47999658310345433, Train acc: 0.7666340218423552\n",
      "Val loss: 0.3468739688396454, Val acc: 0.88\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4599385220780332, Train acc: 0.7740384615384616\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4686927913855284, Train acc: 0.7711004273504274\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4737394503463707, Train acc: 0.7686075498575499\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.47538193840629017, Train acc: 0.7696314102564102\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4756521398949827, Train acc: 0.770405982905983\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4763255457465465, Train acc: 0.7702768874643875\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4759181945497154, Train acc: 0.7700702075702076\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47553642054335177, Train acc: 0.7705495459401709\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4757281015543064, Train acc: 0.7707294634377968\n",
      "Val loss: 0.3819672465324402, Val acc: 0.866\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4843186048360971, Train acc: 0.7657585470085471\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.48882629677780676, Train acc: 0.7634882478632479\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.48984610660802946, Train acc: 0.7633992165242165\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.48702848384268266, Train acc: 0.7659588675213675\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4852108057238098, Train acc: 0.7660790598290599\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4836632959608339, Train acc: 0.7665375712250713\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4842066234284705, Train acc: 0.7671130952380952\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48326784795802885, Train acc: 0.7671107104700855\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.48231657606470846, Train acc: 0.7677914292497626\n",
      "Val loss: 0.4129771590232849, Val acc: 0.86\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4706993678696135, Train acc: 0.7716346153846154\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47675992344689166, Train acc: 0.7709001068376068\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.47813028920749656, Train acc: 0.7701210826210826\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4814817608676405, Train acc: 0.7673944978632479\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48108879052675685, Train acc: 0.7675213675213676\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.47986747366920157, Train acc: 0.7681178774928775\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4785670076243316, Train acc: 0.7691163003663004\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47804284420533055, Train acc: 0.7696647970085471\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4768233855961621, Train acc: 0.7702397910731245\n",
      "Val loss: 0.3734152615070343, Val acc: 0.876\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.48348171588702077, Train acc: 0.7632211538461539\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4762535169083848, Train acc: 0.765892094017094\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.47678787509600323, Train acc: 0.7662037037037037\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.47536671575572753, Train acc: 0.7682959401709402\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4774164067374335, Train acc: 0.7677617521367521\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.47554079101466046, Train acc: 0.7688746438746439\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4769149355326526, Train acc: 0.767609126984127\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4774325104732799, Train acc: 0.7676949786324786\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4775783353098664, Train acc: 0.7680733618233618\n",
      "Val loss: 0.34672293066978455, Val acc: 0.888\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4684113376160972, Train acc: 0.7755074786324786\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47287325981335765, Train acc: 0.7718349358974359\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4742786801099098, Train acc: 0.7724804131054132\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4779305239645844, Train acc: 0.7712673611111112\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.47837288540143236, Train acc: 0.7709134615384615\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4769111233431729, Train acc: 0.7714788105413105\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.47819674624555425, Train acc: 0.771195818070818\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47758829152673227, Train acc: 0.7714509882478633\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4788911572577041, Train acc: 0.7703584995251662\n",
      "Val loss: 0.3799692392349243, Val acc: 0.868\n",
      "Epoch 24/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4792457065011701, Train acc: 0.7689636752136753\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.48099322782622445, Train acc: 0.7619524572649573\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4839145853648498, Train acc: 0.7616631054131054\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4813991012608903, Train acc: 0.7647903311965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4774006260256482, Train acc: 0.7665064102564103\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4781701201811815, Train acc: 0.7676059472934473\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.47957122180342526, Train acc: 0.7670940170940171\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48150405867232215, Train acc: 0.7659588675213675\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4804950304831761, Train acc: 0.7666785375118709\n",
      "Val loss: 0.38447657227516174, Val acc: 0.868\n",
      "Epoch 25/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.49854596455891925, Train acc: 0.7542735042735043\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4860220103819146, Train acc: 0.7591479700854701\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.480286373414545, Train acc: 0.765892094017094\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4772840766634187, Train acc: 0.7686298076923077\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4778021052097663, Train acc: 0.7681623931623932\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4772914857139275, Train acc: 0.7685185185185185\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4789121570349904, Train acc: 0.768658424908425\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4775219814549399, Train acc: 0.7694477831196581\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4781901030487258, Train acc: 0.7689043209876543\n",
      "Val loss: 0.4043864905834198, Val acc: 0.85\n",
      "Epoch 26/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.493653908244565, Train acc: 0.7588141025641025\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.48245476861285347, Train acc: 0.7644230769230769\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4848727769831307, Train acc: 0.7648682336182336\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4813783890925921, Train acc: 0.7669938568376068\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48043277732327455, Train acc: 0.7676549145299145\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.48174745239253736, Train acc: 0.7667378917378918\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4804475002163641, Train acc: 0.7672466422466423\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47848172495380425, Train acc: 0.768112313034188\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4776881544055649, Train acc: 0.7684443257359924\n",
      "Val loss: 0.3666576147079468, Val acc: 0.868\n",
      "Epoch 27/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.48725107388618666, Train acc: 0.7677617521367521\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4870154509941737, Train acc: 0.7651575854700855\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.48566258242327265, Train acc: 0.7665153133903134\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.47785966091940546, Train acc: 0.7705662393162394\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.47937741223563496, Train acc: 0.7705395299145299\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.47832369116636425, Train acc: 0.770744301994302\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4785608110701499, Train acc: 0.77117673992674\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4793452754553057, Train acc: 0.7705328525641025\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47693962931406575, Train acc: 0.7713081671415005\n",
      "Val loss: 0.3791556656360626, Val acc: 0.882\n",
      "Epoch 28/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4801743950089838, Train acc: 0.7668269230769231\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47187762739311934, Train acc: 0.7727029914529915\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4759071604308919, Train acc: 0.7695423789173789\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4808403418486954, Train acc: 0.7667601495726496\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48209671604837107, Train acc: 0.7656784188034188\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.478711435236992, Train acc: 0.7672498219373219\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4768798531957598, Train acc: 0.7689827533577533\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47587231140679276, Train acc: 0.7699151976495726\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47779668250597895, Train acc: 0.7686965811965812\n",
      "Val loss: 0.37883004546165466, Val acc: 0.87\n",
      "Epoch 29/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4812380615462605, Train acc: 0.7669604700854701\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.48389869788263595, Train acc: 0.766426282051282\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4818563477436022, Train acc: 0.7676727207977208\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.48376332248887444, Train acc: 0.7672943376068376\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48482702950127105, Train acc: 0.7662927350427351\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4810180211720983, Train acc: 0.7678507834757835\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4794652535692676, Train acc: 0.7687919719169719\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.48005748621355265, Train acc: 0.7688802083333334\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.48105229307160646, Train acc: 0.767821106362773\n",
      "Val loss: 0.396859347820282, Val acc: 0.848\n",
      "Epoch 30/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.48004250419445527, Train acc: 0.7637553418803419\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47369148219243074, Train acc: 0.7686298076923077\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4768432617357314, Train acc: 0.7674946581196581\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4787009469846375, Train acc: 0.7672943376068376\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.47367799625946927, Train acc: 0.768616452991453\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4714857735312902, Train acc: 0.7695201210826211\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4711371162617454, Train acc: 0.7704899267399268\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47249250201524323, Train acc: 0.7698985042735043\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47193417313941183, Train acc: 0.7708630104463438\n",
      "Val loss: 0.377642959356308, Val acc: 0.872\n",
      "Epoch 31/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.4703907189715622, Train acc: 0.7740384615384616\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4740346326277806, Train acc: 0.7705662393162394\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4781011341304181, Train acc: 0.7688746438746439\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.4775614147512322, Train acc: 0.7681623931623932\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.47788548250483653, Train acc: 0.7672542735042736\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.4774276136587828, Train acc: 0.7684740028490028\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.476999420531649, Train acc: 0.7685057997557998\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.47811834183004165, Train acc: 0.7676115117521367\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.47773914017908253, Train acc: 0.768088200379867\n",
      "Val loss: 0.3723616302013397, Val acc: 0.858\n",
      "Epoch 32/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.46743651345754283, Train acc: 0.7725694444444444\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.47393591969441146, Train acc: 0.7704994658119658\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.4779927307723934, Train acc: 0.770477207977208\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.47895915768085384, Train acc: 0.7688635149572649\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4794654858418, Train acc: 0.7677083333333333\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.48001080220750936, Train acc: 0.7687188390313391\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.479300290637255, Train acc: 0.7691163003663004\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4796294646703789, Train acc: 0.7691306089743589\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4815159055129302, Train acc: 0.768014007597341\n",
      "Val loss: 0.3903983533382416, Val acc: 0.848\n",
      "Early stopping at epoch 32 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 773.8100 [s]\n",
      "Combinación 18/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.7493686538476211, Train acc: 0.6455662393162394\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.6091365586233954, Train acc: 0.7138087606837606\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.5470597373793947, Train acc: 0.7442129629629629\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.510708797022573, Train acc: 0.7605502136752137\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.48397508724632426, Train acc: 0.7735309829059829\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.46468995781111244, Train acc: 0.7818287037037037\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.4483222877913779, Train acc: 0.7895108363858364\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4358168154254428, Train acc: 0.7953892895299145\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.4244317396244093, Train acc: 0.8009407644824311\n",
      "Val loss: 0.28723636269569397, Val acc: 0.878\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3133457088444987, Train acc: 0.8557692307692307\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.32255332296093303, Train acc: 0.8508947649572649\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3191860070468014, Train acc: 0.8517183048433048\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.31775463475949234, Train acc: 0.8525641025641025\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3140693062773118, Train acc: 0.8541666666666666\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.31157719835299375, Train acc: 0.85588051994302\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3072811018616434, Train acc: 0.8575816544566545\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3059717782207916, Train acc: 0.8581730769230769\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3044604253720807, Train acc: 0.8584253323836657\n",
      "Val loss: 0.25942501425743103, Val acc: 0.898\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.2993545384488554, Train acc: 0.8597756410256411\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.28889094528734177, Train acc: 0.8641826923076923\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.2821196465410738, Train acc: 0.8685007122507122\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2786191320285583, Train acc: 0.8700921474358975\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.27627540125041944, Train acc: 0.8712873931623931\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.2735305616998265, Train acc: 0.8715945512820513\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.27051190759927507, Train acc: 0.8729395604395604\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.269485905726687, Train acc: 0.8733974358974359\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.26879029953100514, Train acc: 0.8740503323836657\n",
      "Val loss: 0.2417677491903305, Val acc: 0.908\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.252831344675814, Train acc: 0.8823450854700855\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.2553445952188255, Train acc: 0.8815438034188035\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.25133424999452386, Train acc: 0.8825676638176638\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2529359978392848, Train acc: 0.8815438034188035\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.25345871717247187, Train acc: 0.8816773504273504\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.25183954235283057, Train acc: 0.8824786324786325\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.24911044525285053, Train acc: 0.8840430402930403\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.24928470149349707, Train acc: 0.8834635416666666\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.2498472937137766, Train acc: 0.8835618471035138\n",
      "Val loss: 0.24919770658016205, Val acc: 0.892\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.22587247861501497, Train acc: 0.890625\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.22643667004174656, Train acc: 0.8930288461538461\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.22501373284647608, Train acc: 0.895210113960114\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.22558903068495104, Train acc: 0.894798344017094\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.22855415610421417, Train acc: 0.8935897435897436\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.22857640220908357, Train acc: 0.8935630341880342\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.22768674527951066, Train acc: 0.8941353785103785\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.22911716720614678, Train acc: 0.8935296474358975\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.22858369826619554, Train acc: 0.8940378679962013\n",
      "Val loss: 0.23648889362812042, Val acc: 0.892\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.21331125252649316, Train acc: 0.9007745726495726\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.21875350847521907, Train acc: 0.8983707264957265\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.2207064705123419, Train acc: 0.8964120370370371\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.2171557113353131, Train acc: 0.8986044337606838\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.22021161613938137, Train acc: 0.8974358974358975\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.21823051161639542, Train acc: 0.8979923433048433\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.21735537854979617, Train acc: 0.8986759768009768\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.21650153959297344, Train acc: 0.8991553151709402\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.21515944380762458, Train acc: 0.8996913580246914\n",
      "Val loss: 0.21899037063121796, Val acc: 0.916\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.2039357975252673, Train acc: 0.9105235042735043\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.20939836581038612, Train acc: 0.9063835470085471\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.21177498164170147, Train acc: 0.9046029202279202\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.20978912114141843, Train acc: 0.9050480769230769\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.20875453288611184, Train acc: 0.9049412393162393\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.2092427036796625, Train acc: 0.9041800213675214\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.2089749912757504, Train acc: 0.9037126068376068\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.2097841825726259, Train acc: 0.9034788995726496\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.21008208239449283, Train acc: 0.9032674501424501\n",
      "Val loss: 0.22804343700408936, Val acc: 0.916\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.20106864711030936, Train acc: 0.9097222222222222\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.19795234582554072, Train acc: 0.9099225427350427\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.19725147784393057, Train acc: 0.9097667378917379\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1985316100395006, Train acc: 0.9098223824786325\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1957372690877344, Train acc: 0.9107638888888889\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.19716314003508315, Train acc: 0.9103899572649573\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1961519714485813, Train acc: 0.9114201770451771\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1965619499882699, Train acc: 0.9107739049145299\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.19737306722605216, Train acc: 0.909974477682811\n",
      "Val loss: 0.2098458707332611, Val acc: 0.914\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.19272588040584174, Train acc: 0.9106570512820513\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.18899993509308904, Train acc: 0.9127938034188035\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.19126444227165645, Train acc: 0.9114583333333334\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.18710944315211642, Train acc: 0.914596688034188\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.18861201227857516, Train acc: 0.9142895299145299\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.18930410945581067, Train acc: 0.9135505698005698\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.18904841072193287, Train acc: 0.9139766483516484\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.18783184718818235, Train acc: 0.9144965277777778\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.18744178911616208, Train acc: 0.9147821699905033\n",
      "Val loss: 0.22954405844211578, Val acc: 0.92\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1876333314511511, Train acc: 0.9130608974358975\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.18423486098201355, Train acc: 0.9149305555555556\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.18184041689306582, Train acc: 0.9159989316239316\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.18408239447376412, Train acc: 0.9152310363247863\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.18388982876243753, Train acc: 0.914823717948718\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.18198478593891673, Train acc: 0.9158208689458689\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.181820324243425, Train acc: 0.915789072039072\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.18134333056389776, Train acc: 0.9163494925213675\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.17988561540629788, Train acc: 0.9173047245963912\n",
      "Val loss: 0.241786390542984, Val acc: 0.918\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.17492657887120533, Train acc: 0.9172008547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.174616757239032, Train acc: 0.9184695512820513\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.1751063272590481, Train acc: 0.9182692307692307\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.17537862420655215, Train acc: 0.9177684294871795\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.17436156732659056, Train acc: 0.9186698717948718\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.173920906538892, Train acc: 0.9192930911680912\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.17421886981698037, Train acc: 0.9191849816849816\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.17397911036705488, Train acc: 0.9194210737179487\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.17295835188750075, Train acc: 0.9203317901234568\n",
      "Val loss: 0.23205327987670898, Val acc: 0.912\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.16346283925649446, Train acc: 0.9285523504273504\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.16461583596264195, Train acc: 0.9249465811965812\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.16473740119582567, Train acc: 0.9241452991452992\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.16558489219373107, Train acc: 0.9236444978632479\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.16855442241216317, Train acc: 0.9225160256410256\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.16852915409602162, Train acc: 0.9222756410256411\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.16801855415196792, Train acc: 0.922409188034188\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.16775219744612646, Train acc: 0.9224926549145299\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.16792780529541393, Train acc: 0.922676282051282\n",
      "Val loss: 0.2368815392255783, Val acc: 0.916\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.15988094646197099, Train acc: 0.9293536324786325\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.15540957423802623, Train acc: 0.9305555555555556\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.15895679572920854, Train acc: 0.9277510683760684\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.15918473303158823, Train acc: 0.9276175213675214\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.15852274232440525, Train acc: 0.9269764957264958\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1595380814517835, Train acc: 0.9264378561253561\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1595638724764916, Train acc: 0.9264537545787546\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1596907666231641, Train acc: 0.9261818910256411\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.15958501650807638, Train acc: 0.9263710826210826\n",
      "Val loss: 0.26941317319869995, Val acc: 0.908\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.15219134757788771, Train acc: 0.9288194444444444\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.14686122371090782, Train acc: 0.9321581196581197\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.14765844829486646, Train acc: 0.9308226495726496\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.15206388206595284, Train acc: 0.929153311965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.15316021428403692, Train acc: 0.9280982905982906\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.15275047296536104, Train acc: 0.9283742877492878\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.15378026302189762, Train acc: 0.9280372405372406\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1523821070177369, Train acc: 0.9290865384615384\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.15281462618428418, Train acc: 0.9290271842355176\n",
      "Val loss: 0.2553160488605499, Val acc: 0.91\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.13814417731303436, Train acc: 0.9354967948717948\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1524811772327138, Train acc: 0.9305555555555556\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.15368327902805093, Train acc: 0.9293091168091168\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1517972935580163, Train acc: 0.9295205662393162\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.14991806267927854, Train acc: 0.9303685897435897\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1492954991503149, Train acc: 0.9303997507122507\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1494637458688683, Train acc: 0.9302503052503053\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.14885343965063366, Train acc: 0.9306557158119658\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.14915777511845174, Train acc: 0.9304813627730294\n",
      "Val loss: 0.259255588054657, Val acc: 0.912\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.1507746022608545, Train acc: 0.9289529914529915\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1517912768559833, Train acc: 0.9290865384615384\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.1515720710476749, Train acc: 0.9287304131054132\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.14834599597299966, Train acc: 0.930221688034188\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.14664554325306517, Train acc: 0.9316239316239316\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.14594410678791014, Train acc: 0.9323584401709402\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1452022096431008, Train acc: 0.9324824481074481\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.14539299933160219, Train acc: 0.9323584401709402\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1460164472573956, Train acc: 0.9324548907882241\n",
      "Val loss: 0.25216665863990784, Val acc: 0.914\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.13202705173793003, Train acc: 0.9385683760683761\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13682298734784126, Train acc: 0.9354967948717948\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.14160313355362314, Train acc: 0.9339387464387464\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13890746154655248, Train acc: 0.9355635683760684\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13907253763869276, Train acc: 0.9354700854700855\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13796217431794544, Train acc: 0.9360754985754985\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1388108702205913, Train acc: 0.9355349511599511\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1379849694701087, Train acc: 0.9361812232905983\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.13831293433267847, Train acc: 0.9363277540360874\n",
      "Val loss: 0.2543477416038513, Val acc: 0.914\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.14953219479857346, Train acc: 0.9326923076923077\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.14047299680483138, Train acc: 0.9370993589743589\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13983388576242659, Train acc: 0.9360309829059829\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.1400854661105535, Train acc: 0.9354967948717948\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.1385878455053028, Train acc: 0.9358974358974359\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.13730274050621225, Train acc: 0.9362535612535613\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1359305642636195, Train acc: 0.936698717948718\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1352902036996033, Train acc: 0.9372829861111112\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.13495195718218572, Train acc: 0.9371883903133903\n",
      "Val loss: 0.2562277913093567, Val acc: 0.912\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.14091802894687042, Train acc: 0.9329594017094017\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13593183855852511, Train acc: 0.9345619658119658\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.13328175840002518, Train acc: 0.9365206552706553\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.13194284566606468, Train acc: 0.9378004807692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.13407313633296225, Train acc: 0.9375267094017095\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.1323458163009325, Train acc: 0.9383903133903134\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.13352920699032236, Train acc: 0.9374046092796092\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.13345966471406895, Train acc: 0.9374165331196581\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.1330438917617739, Train acc: 0.9379303181386515\n",
      "Val loss: 0.33331534266471863, Val acc: 0.894\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12791355648356625, Train acc: 0.9368322649572649\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.13137355860736635, Train acc: 0.9371661324786325\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.1287419994566006, Train acc: 0.9391470797720798\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.12876783211070758, Train acc: 0.9391693376068376\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.12762592360377312, Train acc: 0.9396367521367521\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12726604487969834, Train acc: 0.9399706196581197\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1277259177731871, Train acc: 0.9397321428571429\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12736999973431867, Train acc: 0.9401375534188035\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12748965974950213, Train acc: 0.940230294396961\n",
      "Val loss: 0.3552339971065521, Val acc: 0.896\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.111328010694084, Train acc: 0.9481837606837606\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.11623780100614342, Train acc: 0.9449786324786325\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11857184910472811, Train acc: 0.9442663817663818\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.12254977898083182, Train acc: 0.9428084935897436\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.12257841172763426, Train acc: 0.942815170940171\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12332230997391236, Train acc: 0.9424189814814815\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.1246346427680372, Train acc: 0.9417735042735043\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12499874624877404, Train acc: 0.9415731837606838\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12574079222729517, Train acc: 0.941491571699905\n",
      "Val loss: 0.2912609577178955, Val acc: 0.9\n",
      "Epoch 22/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12243483046817984, Train acc: 0.9424412393162394\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.12109591906619632, Train acc: 0.9451121794871795\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.12358717543425893, Train acc: 0.9436876780626781\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.12120797821424073, Train acc: 0.9438434829059829\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.12202284887878813, Train acc: 0.9436965811965812\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12353844008013139, Train acc: 0.9429976851851852\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.12515582388746796, Train acc: 0.9422886141636142\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.1255539861153493, Train acc: 0.9420072115384616\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12450966547116821, Train acc: 0.9422334995251662\n",
      "Val loss: 0.29076871275901794, Val acc: 0.908\n",
      "Epoch 23/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.12021441592110528, Train acc: 0.9436431623931624\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.1187142585682818, Train acc: 0.9448450854700855\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.11877344379419105, Train acc: 0.9443554131054132\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.12075760715808241, Train acc: 0.9437767094017094\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.12223880207882477, Train acc: 0.9427350427350427\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.12415636532231529, Train acc: 0.9414618945868946\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.12310023149373112, Train acc: 0.9420787545787546\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.12369061291862565, Train acc: 0.9419571314102564\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.12257871079521301, Train acc: 0.9423373694207028\n",
      "Val loss: 0.2769460082054138, Val acc: 0.918\n",
      "Early stopping at epoch 23 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 562.6112 [s]\n",
      "Combinación 19/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.5468406529508085, Train acc: 0.5262419871794872\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.4177340116256323, Train acc: 0.6211605235042735\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.34381579299937626, Train acc: 0.6736111111111112\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.29623057992539853, Train acc: 0.7055121527777778\n",
      "Val loss: 0.5003387331962585, Val acc: 0.81\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.10777714415493174, Train acc: 0.8386084401709402\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.09992693275467962, Train acc: 0.8423143696581197\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.09189530969345332, Train acc: 0.8492031695156695\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.0873820358234593, Train acc: 0.8518963675213675\n",
      "Val loss: 0.41641589999198914, Val acc: 0.868\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.05710174767380087, Train acc: 0.8727964743589743\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.05131213621706025, Train acc: 0.8766025641025641\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.04785790691348562, Train acc: 0.8794070512820513\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.04570130901968377, Train acc: 0.8806256677350427\n",
      "Val loss: 0.4050430655479431, Val acc: 0.876\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.030333443839325864, Train acc: 0.8916933760683761\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.030014944127482228, Train acc: 0.8912593482905983\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.028965895162348734, Train acc: 0.8928062678062678\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.028180006095486827, Train acc: 0.8939636752136753\n",
      "Val loss: 0.36255037784576416, Val acc: 0.896\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.016895913924926367, Train acc: 0.9031116452991453\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.015070058469079498, Train acc: 0.9033787393162394\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.013911006117818023, Train acc: 0.9029558404558404\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.012925729601301698, Train acc: 0.9037793803418803\n",
      "Val loss: 0.37252485752105713, Val acc: 0.89\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: 0.00818641241799053, Train acc: 0.9075854700854701\n",
      "Iteration 234 - Batch 234/569 - Train loss: 0.006156878338919746, Train acc: 0.9079861111111112\n",
      "Iteration 351 - Batch 351/569 - Train loss: 0.005422970423331628, Train acc: 0.9075409544159544\n",
      "Iteration 468 - Batch 468/569 - Train loss: 0.0046545334606089145, Train acc: 0.908470219017094\n",
      "Val loss: 0.3343486785888672, Val acc: 0.898\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.0023272822045872356, Train acc: 0.9127938034188035\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.006027947124252971, Train acc: 0.9137620192307693\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.005092840503763269, Train acc: 0.9140179843304843\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.0067828052446373506, Train acc: 0.9145299145299145\n",
      "Val loss: 0.36525416374206543, Val acc: 0.89\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.009663988637109088, Train acc: 0.9127270299145299\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.012143466335076552, Train acc: 0.9159989316239316\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.01560819098073193, Train acc: 0.9186253561253561\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.01656483705991354, Train acc: 0.9196881677350427\n",
      "Val loss: 0.350288450717926, Val acc: 0.9\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.022603973873660095, Train acc: 0.9248130341880342\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.02444619220546168, Train acc: 0.9244457799145299\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.02672143828155648, Train acc: 0.9259926994301995\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.026520227647235252, Train acc: 0.9256310096153846\n",
      "Val loss: 0.3385387063026428, Val acc: 0.912\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.028069087837496374, Train acc: 0.9250801282051282\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.03189845816192464, Train acc: 0.9286525106837606\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.030616795320456525, Train acc: 0.9281739672364673\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.03190831801830194, Train acc: 0.9286358173076923\n",
      "Val loss: 0.36614352464675903, Val acc: 0.894\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.039275304119811096, Train acc: 0.9326923076923077\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.041499801935293734, Train acc: 0.9345953525641025\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.04027265437647828, Train acc: 0.9335381054131054\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.04137671942639555, Train acc: 0.9340945512820513\n",
      "Val loss: 0.34946057200431824, Val acc: 0.904\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.04823821553817162, Train acc: 0.9411725427350427\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.04571928173048884, Train acc: 0.9387686965811965\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.046645510570276154, Train acc: 0.9384570868945868\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.04615470830701355, Train acc: 0.9376335470085471\n",
      "Val loss: 0.3483245372772217, Val acc: 0.906\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.05273655286202064, Train acc: 0.9415064102564102\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.05416933822835612, Train acc: 0.9413728632478633\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.05306269122324778, Train acc: 0.9413283475783476\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.05311822757506982, Train acc: 0.941139155982906\n",
      "Val loss: 0.348970890045166, Val acc: 0.904\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.04757435861815754, Train acc: 0.9391025641025641\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.052934722513215154, Train acc: 0.9427751068376068\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.054474613435587654, Train acc: 0.9433760683760684\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.05664122391205568, Train acc: 0.9452123397435898\n",
      "Val loss: 0.3446488082408905, Val acc: 0.9\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.06757494144969517, Train acc: 0.9506543803418803\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.06420911823073004, Train acc: 0.9482171474358975\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.06338961324800453, Train acc: 0.9475827991452992\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.06308220008499602, Train acc: 0.9474158653846154\n",
      "Val loss: 0.36558693647384644, Val acc: 0.898\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.06542982453973885, Train acc: 0.9504540598290598\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.06612542997568081, Train acc: 0.9495526175213675\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.06721607351574803, Train acc: 0.9498530982905983\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.06683122646859568, Train acc: 0.9496193910256411\n",
      "Val loss: 0.3695668876171112, Val acc: 0.904\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.06378773389718471, Train acc: 0.9484508547008547\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.06709585612655705, Train acc: 0.949752938034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.0682068536763857, Train acc: 0.9506321225071225\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.06936107690517719, Train acc: 0.9511551816239316\n",
      "Val loss: 0.3931941092014313, Val acc: 0.892\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.05725013700305906, Train acc: 0.9436431623931624\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.06490620805157556, Train acc: 0.9483840811965812\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.06807102267219131, Train acc: 0.9506098646723646\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.06882898757855098, Train acc: 0.9507545405982906\n",
      "Val loss: 0.34499454498291016, Val acc: 0.918\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.08309340400573535, Train acc: 0.9575988247863247\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.08126546913742, Train acc: 0.957298344017094\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.07965155807655422, Train acc: 0.9564859330484331\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.07869087433458394, Train acc: 0.9554954594017094\n",
      "Val loss: 0.3641491234302521, Val acc: 0.9\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.08441463826049088, Train acc: 0.9588675213675214\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.08155083210549803, Train acc: 0.9578659188034188\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.08104515245497397, Train acc: 0.9569310897435898\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.08108987926672666, Train acc: 0.95703125\n",
      "Val loss: 0.3848360776901245, Val acc: 0.89\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/569 - Train loss: -0.08459160801691887, Train acc: 0.9589342948717948\n",
      "Iteration 234 - Batch 234/569 - Train loss: -0.08321325964907296, Train acc: 0.9589009081196581\n",
      "Iteration 351 - Batch 351/569 - Train loss: -0.08275692687075362, Train acc: 0.9584668803418803\n",
      "Iteration 468 - Batch 468/569 - Train loss: -0.0831168453153382, Train acc: 0.9586338141025641\n",
      "Val loss: 0.3732876777648926, Val acc: 0.902\n",
      "Early stopping at epoch 21 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 482.6657 [s]\n",
      "Combinación 20/20\n",
      "Epoch 1/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.5595366351115398, Train acc: 0.7426549145299145\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.4893281564243838, Train acc: 0.7723023504273504\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.46605856576536453, Train acc: 0.7827190170940171\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.44626878472602266, Train acc: 0.792434561965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.4354685792810897, Train acc: 0.7977297008547009\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.42572025918637924, Train acc: 0.8018607549857549\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.41885727356496166, Train acc: 0.8051549145299145\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.4113587564669358, Train acc: 0.8083767361111112\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.405678413405038, Train acc: 0.8106896961063628\n",
      "Val loss: 0.2814657986164093, Val acc: 0.9\n",
      "Epoch 2/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3759392268127865, Train acc: 0.8193108974358975\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.36410611594079906, Train acc: 0.8280582264957265\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.36235850930553554, Train acc: 0.8297275641025641\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3609742208296417, Train acc: 0.8299946581196581\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.36168585455315744, Train acc: 0.8297008547008548\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.35860557733183235, Train acc: 0.8309740028490028\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3578108424850727, Train acc: 0.8318261599511599\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3569966033101082, Train acc: 0.8320145566239316\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.35567675874783444, Train acc: 0.8323985042735043\n",
      "Val loss: 0.26838919520378113, Val acc: 0.9\n",
      "Epoch 3/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.36191443513091814, Train acc: 0.8298611111111112\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.34609384446317315, Train acc: 0.8396768162393162\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.34694394626338937, Train acc: 0.8374732905982906\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3512025963928964, Train acc: 0.8353699252136753\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.34971590974392036, Train acc: 0.837232905982906\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.34723421867586607, Train acc: 0.8376958689458689\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.34563867986129465, Train acc: 0.8379120879120879\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3469959092923464, Train acc: 0.8369391025641025\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3450625605053372, Train acc: 0.8377107075023742\n",
      "Val loss: 0.25906306505203247, Val acc: 0.904\n",
      "Epoch 4/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3514182835053175, Train acc: 0.8357371794871795\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.34338286315274036, Train acc: 0.8415464743589743\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.34119505836413455, Train acc: 0.8428151709401709\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.33846877499395966, Train acc: 0.8441506410256411\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3402981552303347, Train acc: 0.8423344017094017\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3411228721934846, Train acc: 0.8416577635327636\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3401732247354638, Train acc: 0.8419948107448108\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3388145712769439, Train acc: 0.842598157051282\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3385696128392831, Train acc: 0.842622269705603\n",
      "Val loss: 0.2758697271347046, Val acc: 0.902\n",
      "Epoch 5/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3339352203994735, Train acc: 0.844551282051282\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3383243784435794, Train acc: 0.8398103632478633\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3342265999979443, Train acc: 0.8411680911680912\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.33503360371304375, Train acc: 0.8416800213675214\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3337866546761276, Train acc: 0.8427884615384615\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.33412036555487545, Train acc: 0.8421474358974359\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.33356358813206066, Train acc: 0.8425289987789988\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3326962728244372, Train acc: 0.8430321848290598\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3320986819572938, Train acc: 0.843290004748338\n",
      "Val loss: 0.26588016748428345, Val acc: 0.898\n",
      "Epoch 6/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3321699345977897, Train acc: 0.8486912393162394\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.32879472632183987, Train acc: 0.8474893162393162\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3244311803596312, Train acc: 0.8503828347578347\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3289342835927621, Train acc: 0.8490584935897436\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3250103894334573, Train acc: 0.8498931623931624\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.32732993537331917, Train acc: 0.8484686609686609\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.33001838892843843, Train acc: 0.8472985347985348\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3308607391201151, Train acc: 0.8462039262820513\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.33270494893886654, Train acc: 0.8454267568850902\n",
      "Val loss: 0.25299495458602905, Val acc: 0.906\n",
      "Epoch 7/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3283112628592385, Train acc: 0.8454861111111112\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3270463563947596, Train acc: 0.8456864316239316\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3292943818701638, Train acc: 0.8437945156695157\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32958476673652476, Train acc: 0.8456864316239316\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3311927899208843, Train acc: 0.8454594017094017\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3316261372993305, Train acc: 0.8447516025641025\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.32980904031331143, Train acc: 0.8452571733821734\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.33067385947061145, Train acc: 0.8446013621794872\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32995603284519964, Train acc: 0.8450854700854701\n",
      "Val loss: 0.2685448229312897, Val acc: 0.898\n",
      "Epoch 8/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3382219062146977, Train acc: 0.8405448717948718\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.33162653146900684, Train acc: 0.8457532051282052\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.329666534592623, Train acc: 0.8464209401709402\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32682840666200363, Train acc: 0.8473557692307693\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3280628879865011, Train acc: 0.8465811965811966\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.329420886127188, Train acc: 0.8461538461538461\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3315893768791168, Train acc: 0.8441887973137974\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.33146257494759357, Train acc: 0.8441172542735043\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3315614042317199, Train acc: 0.8443583808167141\n",
      "Val loss: 0.25344640016555786, Val acc: 0.902\n",
      "Epoch 9/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.33572315456520796, Train acc: 0.8390758547008547\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.33441688718958795, Train acc: 0.8416132478632479\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.33360327453355165, Train acc: 0.8421029202279202\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.333340322614735, Train acc: 0.8425480769230769\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.33372816829345164, Train acc: 0.8430021367521368\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3328101672232151, Train acc: 0.8431712962962963\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.33261797770002677, Train acc: 0.8427960927960928\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.33205513607583237, Train acc: 0.8438000801282052\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.33072953724991333, Train acc: 0.8445067663817664\n",
      "Val loss: 0.2847749590873718, Val acc: 0.902\n",
      "Epoch 10/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.32526402748548067, Train acc: 0.8496260683760684\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.32942141611606646, Train acc: 0.8460870726495726\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3286515865740273, Train acc: 0.8480235042735043\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.326764326574456, Train acc: 0.8479567307692307\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3293738813481779, Train acc: 0.8463141025641026\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3278483645698623, Train acc: 0.8469106125356125\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.32998113072180485, Train acc: 0.845734126984127\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.32870588393515754, Train acc: 0.8464209401709402\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3279412952795667, Train acc: 0.8466286799620133\n",
      "Val loss: 0.285965234041214, Val acc: 0.89\n",
      "Epoch 11/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.32396927883482385, Train acc: 0.8481570512820513\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.328629769766942, Train acc: 0.8438167735042735\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3237012192139938, Train acc: 0.8462873931623932\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32619378798537785, Train acc: 0.8455194978632479\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.32775629091466596, Train acc: 0.8449252136752137\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.32485479407245954, Train acc: 0.8464877136752137\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3255872084908142, Train acc: 0.8466498778998779\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3263430522961749, Train acc: 0.8461371527777778\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32661295922053846, Train acc: 0.8463319088319088\n",
      "Val loss: 0.2792222797870636, Val acc: 0.894\n",
      "Epoch 12/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.34184578774321794, Train acc: 0.8402777777777778\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.33418222484935045, Train acc: 0.8436164529914529\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3266888096546515, Train acc: 0.8466880341880342\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3263827261124921, Train acc: 0.8455194978632479\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3265741640176529, Train acc: 0.8462606837606838\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3251911137688194, Train acc: 0.8473112535612536\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3268195431911465, Train acc: 0.8474511599511599\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3274485969470225, Train acc: 0.8470886752136753\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32767651262644465, Train acc: 0.8465544871794872\n",
      "Val loss: 0.27727583050727844, Val acc: 0.896\n",
      "Epoch 13/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3256918559981208, Train acc: 0.8468215811965812\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.32761293082919896, Train acc: 0.8475560897435898\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.33031343386383816, Train acc: 0.8458867521367521\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3282103604422166, Train acc: 0.8451522435897436\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.32942385739750335, Train acc: 0.844551282051282\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3279973483429505, Train acc: 0.8450186965811965\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3259387774454368, Train acc: 0.8467071123321124\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3253826626027242, Train acc: 0.8470385950854701\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3249873519156054, Train acc: 0.8475486704653371\n",
      "Val loss: 0.2751663029193878, Val acc: 0.89\n",
      "Epoch 14/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3268318104947734, Train acc: 0.8474893162393162\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3279346817005903, Train acc: 0.8489583333333334\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3236777408637552, Train acc: 0.8498041310541311\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32175228469328493, Train acc: 0.8507278311965812\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3243728645973735, Train acc: 0.8494123931623931\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3243342784374018, Train acc: 0.8485799501424501\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3263320890374673, Train acc: 0.8471459096459096\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.32543929474444216, Train acc: 0.8474893162393162\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32387714303768145, Train acc: 0.8484686609686609\n",
      "Val loss: 0.26823049783706665, Val acc: 0.9\n",
      "Epoch 15/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.33583866314500826, Train acc: 0.8384081196581197\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3297766291687631, Train acc: 0.8448851495726496\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.33007430631211, Train acc: 0.844017094017094\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32568006013703144, Train acc: 0.8470219017094017\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.32279184674605343, Train acc: 0.8485576923076923\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3230957786454434, Train acc: 0.8483796296296297\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3214814494817685, Train acc: 0.8496451465201466\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3226331950475772, Train acc: 0.8491085737179487\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32390993488253805, Train acc: 0.8484686609686609\n",
      "Val loss: 0.2720990478992462, Val acc: 0.894\n",
      "Epoch 16/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3401905333893931, Train acc: 0.8397435897435898\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3312974005428135, Train acc: 0.8452857905982906\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3267698473485447, Train acc: 0.8454415954415955\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32623842664254016, Train acc: 0.8448183760683761\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.32484263849054645, Train acc: 0.8460202991452992\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.32661364601123705, Train acc: 0.8456196581196581\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3265200433968333, Train acc: 0.8451045482295483\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.32476371871387094, Train acc: 0.8466546474358975\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32564167284246637, Train acc: 0.8465099715099715\n",
      "Val loss: 0.30646222829818726, Val acc: 0.884\n",
      "Epoch 17/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.33424933993408823, Train acc: 0.8416132478632479\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3284124167811157, Train acc: 0.8464877136752137\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3247415664953384, Train acc: 0.8486912393162394\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32391695108296525, Train acc: 0.8485576923076923\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.32432323197523755, Train acc: 0.8481570512820513\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.32275878260193386, Train acc: 0.8495147792022792\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.32147104382260233, Train acc: 0.8507135225885226\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.32239545352406734, Train acc: 0.8498931623931624\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32307119743238716, Train acc: 0.8495073599240266\n",
      "Val loss: 0.2641167640686035, Val acc: 0.9\n",
      "Epoch 18/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3334340738435077, Train acc: 0.8433493589743589\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3260975466070012, Train acc: 0.8477564102564102\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.323057964868695, Train acc: 0.8476673789173789\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3227323407355027, Train acc: 0.8478899572649573\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3232709395325082, Train acc: 0.8484775641025641\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.32410749820647416, Train acc: 0.8480457621082621\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.3219222006125328, Train acc: 0.84878663003663\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.32106893254905683, Train acc: 0.8495759882478633\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.3223923747724051, Train acc: 0.8489583333333334\n",
      "Val loss: 0.26990389823913574, Val acc: 0.904\n",
      "Epoch 19/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.3306824953382851, Train acc: 0.8428151709401709\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3188362148964507, Train acc: 0.8475560897435898\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.32118816431770975, Train acc: 0.8482460826210826\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32357464515819, Train acc: 0.8482238247863247\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.32522219251363704, Train acc: 0.8479700854700855\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.32761199666224317, Train acc: 0.8462428774928775\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.32515918624488427, Train acc: 0.8471268315018315\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3257698142885143, Train acc: 0.8467548076923077\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.326573789926336, Train acc: 0.8465841642924976\n",
      "Val loss: 0.25798141956329346, Val acc: 0.892\n",
      "Epoch 20/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.32374735303923613, Train acc: 0.844017094017094\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3216387029960115, Train acc: 0.8469551282051282\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.3189269966337076, Train acc: 0.8494925213675214\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.3176721063657449, Train acc: 0.8498931623931624\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3220935958955023, Train acc: 0.8483173076923077\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.3225837587949387, Train acc: 0.8482238247863247\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.32232971080056916, Train acc: 0.8483478327228328\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.3212729087815835, Train acc: 0.8483740651709402\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32268701039488157, Train acc: 0.8476228632478633\n",
      "Val loss: 0.25954878330230713, Val acc: 0.896\n",
      "Epoch 21/100\n",
      "Iteration 117 - Batch 117/1137 - Train loss: 0.32962551216284436, Train acc: 0.8501602564102564\n",
      "Iteration 234 - Batch 234/1137 - Train loss: 0.3261109877091188, Train acc: 0.8490251068376068\n",
      "Iteration 351 - Batch 351/1137 - Train loss: 0.32209827362472176, Train acc: 0.8506944444444444\n",
      "Iteration 468 - Batch 468/1137 - Train loss: 0.32443858890069854, Train acc: 0.8502270299145299\n",
      "Iteration 585 - Batch 585/1137 - Train loss: 0.3227090650644058, Train acc: 0.85\n",
      "Iteration 702 - Batch 702/1137 - Train loss: 0.32203997359571296, Train acc: 0.8500712250712251\n",
      "Iteration 819 - Batch 819/1137 - Train loss: 0.32125103966729834, Train acc: 0.8503510378510378\n",
      "Iteration 936 - Batch 936/1137 - Train loss: 0.32051957772774065, Train acc: 0.8500600961538461\n",
      "Iteration 1053 - Batch 1053/1137 - Train loss: 0.32119135003880106, Train acc: 0.8499821937321937\n",
      "Val loss: 0.276407390832901, Val acc: 0.902\n",
      "Early stopping at epoch 21 due to no improvement after 15 epochs.\n",
      "Tiempo total de entrenamiento: 512.7958 [s]\n",
      "Búsqueda completada\n",
      "Mejor precisión de validación: 0.934\n",
      "Mejores parámetros: {'dropout_p': 0.3, 'batch_size': 64, 'lr': 0.001, 'beta': 0.4}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 1. Define los rangos de los hiperparámetros\n",
    "param_grid = {\n",
    "    'dropout_p': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'lr': [1e-4, 1e-3, 1e-2, 1e-5],\n",
    "    'beta': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "}\n",
    "\n",
    "# 2. Número de combinaciones a probar\n",
    "num_combinations = 20\n",
    "\n",
    "# 3. Realiza la búsqueda aleatoria\n",
    "best_val_acc = 0\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for i in range(num_combinations):\n",
    "    print(f\"Combinación {i + 1}/{num_combinations}\")\n",
    "    \n",
    "    # Genera una combinación aleatoria de hiperparámetros\n",
    "    params = {k: random.choice(v) for k, v in param_grid.items()}\n",
    "    \n",
    "    # Inicializa el modelo\n",
    "    model = CNNModel(params['dropout_p'])\n",
    "    \n",
    "    # Entrena el modelo y evalúalo\n",
    "    curves = train_model(\n",
    "        model,\n",
    "        Train_images,\n",
    "        Train_labels,\n",
    "        metadata_train,\n",
    "        Val_images,\n",
    "        Val_labels,\n",
    "        metadata_val,\n",
    "        epochs=100,  # Puedes ajustar este número según tus necesidades\n",
    "        criterion=perdida_regularizada_entropia,\n",
    "        batch_size=params['batch_size'],\n",
    "        lr=params['lr'],\n",
    "        use_gpu=True,\n",
    "        beta=params['beta'],\n",
    "        patience=15\n",
    "    )\n",
    "    \n",
    "\n",
    "    val_acc = max(curves['val_acc'])\n",
    "    \n",
    "    # Guarda los resultados\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'val_acc': val_acc,\n",
    "        'curves': curves,\n",
    "    })\n",
    "    \n",
    "    # Actualiza los mejores parámetros si es necesario\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_params = params\n",
    "\n",
    "print(\"Búsqueda completada\")\n",
    "print(f\"Mejor precisión de validación: {best_val_acc}\")\n",
    "print(f\"Mejores parámetros: {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
